<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Viewer: DLB-MC.json</title>
    <style>
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif;
            line-height: 1.6;
            max-width: 1000px;
            margin: 0 auto;
            padding: 20px;
            background-color: #f5f5f5;
        }
        .header {
            background-color: #2c3e50;
            color: white;
            padding: 20px;
            border-radius: 8px;
            margin-bottom: 20px;
        }
        .header h1 {
            margin: 0 0 10px 0;
        }
        .stats {
            font-size: 14px;
            opacity: 0.9;
        }
        .question-card {
            background-color: white;
            border-radius: 8px;
            padding: 25px;
            margin-bottom: 20px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }
        .question-header {
            display: flex;
            justify-content: space-between;
            align-items: center;
            margin-bottom: 15px;
            padding-bottom: 10px;
            border-bottom: 2px solid #3498db;
        }
        .question-number {
            font-size: 18px;
            font-weight: bold;
            color: #3498db;
        }
        .question-type {
            background-color: #3498db;
            color: white;
            padding: 4px 12px;
            border-radius: 4px;
            font-size: 12px;
            text-transform: uppercase;
        }
        .question-text {
            font-size: 16px;
            margin-bottom: 20px;
            white-space: pre-wrap;
            line-height: 1.8;
        }
        .answer-section {
            background-color: #e8f5e9;
            border-left: 4px solid #4caf50;
            padding: 15px;
            margin: 20px 0;
            border-radius: 4px;
        }
        .answer-label {
            font-weight: bold;
            color: #2e7d32;
            margin-bottom: 8px;
        }
        .answer-text {
            color: #1b5e20;
            font-size: 15px;
        }
        .reference-section {
            background-color: #f9f9f9;
            border-left: 4px solid #9e9e9e;
            padding: 15px;
            margin-top: 20px;
            border-radius: 4px;
            max-height: 400px;
            overflow-y: auto;
        }
        .reference-label {
            font-weight: bold;
            color: #616161;
            margin-bottom: 8px;
        }
        .reference-text {
            font-size: 13px;
            color: #424242;
            font-family: 'Courier New', monospace;
            white-space: pre-wrap;
            line-height: 1.5;
        }
        .toggle-button {
            background-color: #9e9e9e;
            color: white;
            border: none;
            padding: 8px 16px;
            border-radius: 4px;
            cursor: pointer;
            font-size: 13px;
            margin-top: 10px;
        }
        .toggle-button:hover {
            background-color: #757575;
        }
        .hidden {
            display: none;
        }
        .navigation {
            position: fixed;
            top: 20px;
            right: 20px;
            background-color: white;
            padding: 15px;
            border-radius: 8px;
            box-shadow: 0 2px 8px rgba(0,0,0,0.2);
        }
        .nav-button {
            display: block;
            width: 100%;
            padding: 8px;
            margin: 5px 0;
            background-color: #3498db;
            color: white;
            border: none;
            border-radius: 4px;
            cursor: pointer;
        }
        .nav-button:hover {
            background-color: #2980b9;
        }
    </style>
</head>
<body>
    <div class="header">
        <h1>Question Viewer</h1>
        <div class="stats">
            <strong>Source:</strong> DLB-MC.json<br>
            <strong>Total Questions:</strong> 165
        </div>
    </div>

    <div class="question-card" id="q1">
        <div class="question-header">
            <span class="question-number">Question 1</span>
            <span class="question-type">multiple-choice</span>
        </div>

        <div class="question-text">Convolutional neural networks (CNNs) are inspired by the organization and function of the biological visual cortex, with specific architectural choices reflecting neural mechanisms for processing visual information. Understanding how biological principles inform CNN design can clarify the purpose of key components within these networks.

Which architectural feature in convolutional neural networks most directly mimics the function of complex cells in the brain's primary visual cortex (V1) by providing invariance to small positional shifts and certain lighting changes?

1) Two-dimensional feature maps   
2) Pooling layers such as max pooling   
3) Fully connected layers   
4) ReLU activation functions   
5) Batch normalization   
6) Dropout regularization   
7) Softmax output units</div>

        <div class="answer-section">
            <div class="answer-label">‚úì Correct Answer:</div>
            <div class="answer-text">The correct answer is 2) Pooling layers such as max pooling.</div>
        </div>

        <div class="reference-section">
            <div class="reference-label">üìö Reference Text:</div>
            <button class="toggle-button" onclick="toggleReference(1)">
                Show/Hide Reference
            </button>
            <div id="ref1" class="reference-text hidden">structure of theimage in the retina.For example, light arrivingatthelowerhalfoftheretinaaÔ¨Äectsonlythecorrespondinghalfof V1.Convolutionalnetworkscapturethispropertybyhavingtheirfeatures deÔ¨Ånedintermsoftwodimensionalmaps. 2.V1containsmany si m pl e c e l l s.Asimplecell‚Äôsactivitycantosomeextent becharacterizedbyalinear function oftheimagein asmall, spatially localizedreceptiveÔ¨Åeld.Thedetectorunitsofaconvolutionalnetworkare designedtoemulatethesepropertiesofsimplecells. 3.V1alsocontainsmany c o m pl e x c e l l s.Thesecellsrespondtofeaturesthat aresimilartothosedetectedbysimplecells,butcomplexcellsareinvariant tosmallshiftsinthepositionofthefeature.Thisinspiresthepoolingunits ofconvolutionalnetworks.Complexcellsarealsoinvarianttosomechanges inlightingthatcannotbecapturedsimplybypoolingoverspatiallocations. Theseinvarianceshaveinspiredsomeofthecross-channelpoolingstrategies inconvolutionalnetworks,suchasmaxoutunits( ,). Goodfellow etal.2013a ThoughweknowthemostaboutV1,itisgenerallybelievedthatthesame basicprinciplesapplytootherareasofthevisualsystem.Inourcartoonviewof thevisualsystem,thebasicstrategyofdetectionfollowedbypoolingisrepeatedly appliedaswemovedeeperintothebrain.Aswepassthroughmultipleanatomical layersofthebrain,weeventuallyÔ¨ÅndcellsthatrespondtosomespeciÔ¨Åcconcept andareinvarianttomanytransformationsoftheinput.Thesecellshavebeen 3 6 5</div>
        </div>
    </div>

    <div class="question-card" id="q2">
        <div class="question-header">
            <span class="question-number">Question 2</span>
            <span class="question-type">multiple-choice</span>
        </div>

        <div class="question-text">Deep learning leverages multi-layered artificial neural networks and advanced computational techniques to solve complex scientific and practical problems across diverse fields. Mastery of its mathematical foundations is essential for effective model design and optimization.

Which mathematical concept is fundamental for representing and manipulating the multi-dimensional data structures commonly used in deep learning algorithms?

1) Linear algebra   
2) Differential calculus   
3) Number theory   
4) Game theory   
5) Topology   
6) Set theory   
7) Graph theory</div>

        <div class="answer-section">
            <div class="answer-label">‚úì Correct Answer:</div>
            <div class="answer-text">The correct answer is 1) Linear algebra.</div>
        </div>

        <div class="reference-section">
            <div class="reference-label">üìö Reference Text:</div>
            <button class="toggle-button" onclick="toggleReference(2)">
                Show/Hide Reference
            </button>
            <div id="ref2" class="reference-text hidden">CHAPTER1.INTRODUCTION thatneuroscientistscanstudy(,).Deeplearningalsoprovidesuseful DiCarlo2013 toolsforprocessingmassiveamountsofdataandmakingusefulpredictionsin scientiÔ¨ÅcÔ¨Åelds.Ithasbeensuccessfullyusedtopredicthowmoleculeswillinteract inordertohelppharmaceutical companiesdesignnewdrugs(,), Dahl e t a l .2014 tosearchforsubatomicparticles(,),andtoautomatically parse Baldi e t a l .2014 microscopeimagesusedtoconstructa3-Dmapofthehumanbrain(Knowles- Barley2014 e t a l .,).WeexpectdeeplearningtoappearinmoreandmorescientiÔ¨Åc Ô¨Åeldsinthefuture. Insummary,deeplearningisanapproachtomachinelearningthathasdrawn heavilyonourknowledgeofthehumanbrain,statisticsandappliedmathasit developedoverthepastseveraldecades.Inrecentyears,ithasseentremendous growthinitspopularityandusefulness,dueinlargeparttomorepowerfulcom- puters,largerdatasetsandtechniquestotraindeepernetworks.Theyearsahead arefullofchallengesandopportunitiestoimprovedeeplearningevenfurtherand bringittonewfrontiers. 2 6 CHAPTER1.INTRODUCTION 1950 198520002015 2056 Year10‚àí 210‚àí 1100101102103104105106107108109101 0101 1Numberofneurons(logarithmicscale) 123 456 78 91011 121314 151617 181920 SpongeRoundwormLeechAntBeeFrogOctopusHuman Figure1.11:Sincetheintroductionofhiddenunits,artiÔ¨Åcialneuralnetworkshavedoubled insizeroughlyevery2.4years.Biologicalneuralnetworksizesfrom (). Wikipedia2015 1.Perceptron(,,) Rosenblatt19581962 2.Adaptivelinearelement( ,) WidrowandHoÔ¨Ä1960 3.Neocognitron(Fukushima1980,) 4.Earlyback-propagationnetwork( ,) Rumelhart e t al.1986b 5.Recurrentneuralnetworkforspeechrecognition(RobinsonandFallside1991,) 6.Multilayerperceptronforspeechrecognition( ,) Bengio e t al.1991 7.MeanÔ¨Åeldsigmoidbeliefnetwork(,) Saul e t al.1996 8.LeNet-5( ,) LeCun e t al.1998b 9.Echostatenetwork( ,) JaegerandHaas2004 10.Deepbeliefnetwork( ,) Hinton e t al.2006 11.GPU-acceleratedconvolutionalnetwork( ,) Chellapilla e t al.2006 12.DeepBoltzmannmachine(SalakhutdinovandHinton2009a,) 13.GPU-accelerateddeepbeliefnetwork(,) Raina e t al.2009 14.Unsupervisedconvolutionalnetwork( ,) Jarrett e t al.2009 15.GPU-acceleratedmultilayerperceptron( ,) Ciresan e t al.2010 16.OMP-1network( ,) CoatesandNg2011 17.Distributedautoencoder(,) Le e t al.2012 18.Multi-GPUconvolutionalnetwork( ,) Krizhevsky e t al.2012 19.COTSHPCunsupervisedconvolutionalnetwork( ,) Coates e t al.2013 20.GoogLeNet( ,) Szegedy e t al.2014a 2 7 CHAPTER1.INTRODUCTION 2010 2011 2012 2013 2014 2015 Year000 .005 .010 .015 .020 .025 .030 .ILSVRC classiÔ¨Åcationerrorrate Figure1.12:SincedeepnetworksreachedthescalenecessarytocompeteintheImageNet LargeScaleVisualRecognitionChallenge,theyhaveconsistentlywonthecompetition everyyear,andyieldedlowerandlowererrorrateseachtime. DatafromRussakovsky e t a l . e t a l . ()and2014b He().2015 2 8 P a rt I AppliedMathandMachine LearningBasics 29 This part of t he b o ok in t r o duces t he bas ic mathematical c oncepts needed t o unders t an d deep learning. W e b e gin with general ideas f r om applied math t hat allo w us t o deÔ¨Åne f unctions of many v ariables , Ô¨Å nd t he highes t and low e s t p oints on t hes e f unctions and q uantify degrees of b e lief. N e x t , w e des c r ib e t he f undamen t al goals of machine learning. W e des c r ibe how t o accomplis h t hes e goals b y s p e c ifying a mo del t hat r e pres e n t s c e r t ain b e liefs , des igning a c os t f unction t hat meas ures how well t hos e beliefs c orres p ond with r e alit y and us ing a t r aining algorithm t o minimize t hat c os t f unction. This e lementary f r amew ork is t he bas is f or a broad v ariety of mac hine learning algorithms , including approac hes t o machine learning t hat are not deep. In t he s ubs e q uen t parts of t he bo ok, we develop deep learning algorithms within t his f r amew ork. 3 0 C h a p t e r 2 L i n e ar A l ge b ra Linearalgebraisabranchofmathematics thatiswidelyusedthroughoutscience andengineering.However,becauselinearalgebraisaformofcontinuousrather thandiscretemathematics,manycomputerscientistshavelittleexperiencewithit. Agoodunderstandingoflinearalgebraisessentialforunderstandingandworking withmanymachinelearningalgorithms,especiallydeeplearningalgorithms.We thereforeprecedeourintroductiontodeeplearningwithafocusedpresentationof thekeylinearalgebraprerequisites. Ifyouarealreadyfamiliarwithlinearalgebra,feelfreetoskipthischapter.If youhavepreviousexperiencewiththeseconceptsbutneedadetailedreference sheettoreviewkeyformulas,werecommend TheMatrixCookbook(Petersenand Pedersen2006,).Ifyouhavenoexposureatalltolinearalgebra,thischapter willteachyouenoughtoreadthisbook,butwehighlyrecommendthatyoualso consultanotherresourcefocusedexclusivelyonteachinglinearalgebra,suchas Shilov1977().Thischapterwillcompletelyomitmanyimportantlinearalgebra topicsthatarenotessentialforunderstandingdeeplearning. 2.1Scalars,Vectors,MatricesandTensors Thestudyoflinearalgebrainvolvesseveraltypesofmathematical objects: ‚Ä¢Scalars:Ascalarisjustasinglenumber,incontrasttomostoftheother objectsstudiedinlinearalgebra,whichareusuallyarraysofmultiplenumbers. Wewritescalarsinitalics.Weusuallygivescalarslower-casevariablenames. Whenweintroducethem,wespecifywhatkindofnumbertheyare.For 31 CHAPTER2.LINEARALGEBRA example,wemightsay‚ÄúLet s‚àà Rbetheslopeoftheline,‚ÄùwhiledeÔ¨Åninga real-valuedscalar,or‚ÄúLet n‚àà Nbethenumberofunits,‚ÄùwhiledeÔ¨Åninga</div>
        </div>
    </div>

    <div class="question-card" id="q3">
        <div class="question-header">
            <span class="question-number">Question 3</span>
            <span class="question-type">multiple-choice</span>
        </div>

        <div class="question-text">Bayesian networks are probabilistic graphical models that use directed acyclic graphs to represent conditional dependencies between random variables. Their structure enables efficient reasoning in complex domains by leveraging conditional independence assumptions.

In a Bayesian network modeling a relay race with three runners‚ÄîAlice, Bob, and Carol‚Äîwhere each runner‚Äôs finishing time depends only on the previous runner, which of the following best describes how the joint probability distribution over their finishing times is factorized?

1) p(t0, t1, t2) = p(t0|t1) * p(t1|t2) * p(t2)   
2) p(t0, t1, t2) = p(t0) * p(t1) * p(t2)   
3) p(t0, t1, t2) = p(t2|t1) * p(t1|t0)   
4) p(t0, t1, t2) = p(t0) * p(t1|t0) * p(t2|t1)   
5) p(t0, t1, t2) = p(t2) * p(t1|t2) * p(t0|t1)   
6) p(t0, t1, t2) = p(t1) * p(t2|t0) * p(t0|t1)   
7) p(t0, t1, t2) = p(t2|t1, t0) * p(t1|t0) * p(t0)</div>

        <div class="answer-section">
            <div class="answer-label">‚úì Correct Answer:</div>
            <div class="answer-text">The correct answer is 4) p(t0, t1, t2) = p(t0) * p(t1|t0) * p(t2|t1).</div>
        </div>

        <div class="reference-section">
            <div class="reference-label">üìö Reference Text:</div>
            <button class="toggle-button" onclick="toggleReference(3)">
                Show/Hide Reference
            </button>
            <div id="ref3" class="reference-text hidden">2Ju d e a P e a rl s u g g e s t e d u s i n g t h e t e rm ‚Äú B a y e s i a n n e t wo rk ‚Äù wh e n o n e wis h e s t o ‚Äú e m p h a s i z e t h e j u d g m e n t a l ‚Äù n a t u re o f t h e v a l u e s c o m p u t e d b y t h e n e t wo rk , i . e . t o h i g h l i g h t t h a t t h e y u s u a l l y re p re s e n t d e g re e s o f b e l i e f ra t h e r t h a n f re q u e n c i e s o f e v e n t s . 5 6 3 CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING t 0 t 0 t 1 t 1 t 2 t 2A l i c e B ob C ar ol Figure16.2:Adirectedgraphicalmodeldepictingtherelayraceexample.Alice‚ÄôsÔ¨Ånishing timet 0inÔ¨ÇuencesBob‚ÄôsÔ¨Ånishingtimet 1,becauseBobdoesnotgettostartrunninguntil AliceÔ¨Ånishes.Likewise,CarolonlygetstostartrunningafterBobÔ¨Ånishes,soBob‚Äôs Ô¨Ånishingtimet 1directlyinÔ¨ÇuencesCarol‚ÄôsÔ¨Ånishingtimet 2. thatis,theypointfromonevertextoanother.Thisdirectionisrepresentedin thedrawingwithanarrow.Thedirectionofthearrowindicateswhichvariable‚Äôs probabilitydistributionisdeÔ¨Ånedintermsoftheother‚Äôs.Drawinganarrowfrom atobmeansthatwedeÔ¨Ånetheprobabilitydistributionoverbviaaconditional distribution,withaasoneofthevariablesontherightsideoftheconditioning bar.Inotherwords,thedistributionoverbdependsonthevalueofa. Continuingwiththerelayraceexamplefromsection,supposewename 16.1 Alice‚ÄôsÔ¨Ånishingtimet 0,Bob‚ÄôsÔ¨Ånishingtimet 1,andCarol‚ÄôsÔ¨Ånishingtimet 2. Aswesawearlier,ourestimateoft 1dependsont 0.Ourestimateoft 2depends directlyont 1butonlyindirectlyont 0.Wecandrawthisrelationshipinadirected graphicalmodel,illustratedinÔ¨Ågure.16.2 Formally,adirectedgraphicalmodeldeÔ¨Ånedonvariables xisdeÔ¨Ånedbya directedacyclicgraph Gwhoseverticesaretherandomvariablesinthemodel, andasetoflocalconditionalprobabilitydistributions p(x i| P aG(x i)) where P aG(x i)givestheparentsofx iinG.Theprobabilitydistributionoverxisgiven by p() = Œ†x i p(x i| P aG(x i)) . (16.1) Inourrelayraceexample,thismeansthat,usingthegraphdrawninÔ¨Ågure,16.2 p(t 0 ,t 1 ,t 2) = ( pt 0)( pt 1|t 0)( pt 2|t 1) . (16.2) ThisisourÔ¨Årsttimeseeingastructuredprobabilisticmodelinaction.We canexaminethecostofusingit,inordertoobservehowstructuredmodelinghas manyadvantagesrelativetounstructuredmodeling. Supposewerepresentedtimebydiscretizingtimerangingfromminute0to minute10into6secondchunks.Thiswouldmaket 0,t 1andt 2eachbeadiscrete variablewith100possiblevalues.Ifweattemptedtorepresent p(t 0 ,t 1 ,t 2)witha table,itwouldneedtostore999,999values(100valuesoft 0√ó100valuesoft 1√ó 100valuesoft 2,minus1,sincetheprobabilityofoneoftheconÔ¨Ågurations ismade 5 6 4 CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING redundantbytheconstraintthatthesumoftheprobabilitiesbe1).Ifinstead,we onlymakeatableforeachoftheconditionalprobabilitydistributions,thenthe distributionovert 0requires99values,thetabledeÔ¨Åningt 1givent 0requires9900 values,andsodoesthetabledeÔ¨Åningt 2givent 1.Thiscomestoatotalof19,899 values.Thismeansthatusingthedirectedgraphicalmodelreducedournumberof parametersbyafactorofmorethan50! Ingeneral,tomodel ndiscretevariableseachhaving kvalues,thecostofthe singletableapproachscaleslike O( kn),aswehaveobservedbefore.Nowsuppose webuildadirectedgraphicalmodeloverthesevariables. If misthemaximum numberofvariablesappearing(oneithersideoftheconditioningbar)inasingle conditionalprobabilitydistribution,thenthecostofthetablesforthedirected modelscaleslike O( km).Aslongaswecandesignamodelsuchthat m < < n,we getverydramaticsavings. Inotherwords,solongaseachvariablehasfewparentsinthegraph,the distributioncanberepresentedwithveryfewparameters. Somerestrictionson thegraphstructure,suchasrequiringittobeatree,canalsoguaranteethat operationslikecomputingmarginalorconditionaldistributionsoversubsetsof variablesareeÔ¨Écient. Itisimportanttorealizewhatkindsofinformationcanandcannotbeencodedin thegraph.Thegraphencodesonlysimplifyingassumptionsaboutwhichvariables areconditionallyindependentfromeachother.Itisalsopossibletomakeother kindsofsimplifyingassumptions. Forexample,supposeweassumeBobalways runsthesameregardlessofhowAliceperformed.(Inreality,Alice‚Äôsperformance probablyinÔ¨ÇuencesBob‚Äôsperformance‚ÄîdependingonBob‚Äôspersonality,ifAlice runsespeciallyfastinagivenrace,thismightencourageBobtopushhardand matchherexceptionalperformance,oritmightmakehimoverconÔ¨Ådentandlazy). ThentheonlyeÔ¨ÄectAlicehasonBob‚ÄôsÔ¨ÅnishingtimeisthatwemustaddAlice‚Äôs Ô¨ÅnishingtimetothetotalamountoftimewethinkBobneedstorun.This observationallowsustodeÔ¨Åneamodelwith O( k)parametersinsteadof O( k2). However,notethatt 0andt 1arestilldirectlydependentwiththisassumption, becauset 1representstheabsolutetimeatwhichBobÔ¨Ånishes,notthetotaltime hehimselfspendsrunning.Thismeansourgraphmuststillcontainanarrowfrom t 0tot 1.TheassumptionthatBob‚Äôspersonalrunningtimeisindependentfrom allotherfactorscannotbeencodedinagraphovert 0,t 1,andt 2.Instead,we encodethisinformationinthedeÔ¨Ånitionoftheconditionaldistributionitself.The conditionaldistributionisnolongera k k√ó‚àí1elementtableindexedbyt 0andt 1 butisnowaslightlymorecomplicatedformulausingonly k‚àí1parameters.The directedgraphicalmodelsyntaxdoesnotplaceanyconstraintonhowwedeÔ¨Åne 5 6 5</div>
        </div>
    </div>

    <div class="question-card" id="q4">
        <div class="question-header">
            <span class="question-number">Question 4</span>
            <span class="question-type">multiple-choice</span>
        </div>

        <div class="question-text">Neural network architecture and training methods are foundational in modern artificial intelligence, influencing how models learn and generalize from data. Understanding the interplay between network depth, connectivity, and algorithms is key to designing effective learning systems.

Which of the following statements best explains why increasing the depth of a neural network often improves its ability to generalize, as opposed to merely increasing the number of parameters in a shallow model?

1) Shallow models with more parameters always avoid overfitting due to increased complexity.   
2) Deeper networks have fewer layers, so they are less prone to memorizing training data.   
3) Adding parameters to shallow models enables hierarchical feature representation.   
4) Deep neural networks learn compositional functions and hierarchical representations, which enhance generalization across complex tasks.   
5) Increasing depth reduces the need for regularization in neural networks.   
6) Shallow networks perform better on spatial data because of their simplicity.   
7) Test accuracy is independent of both depth and parameter count in neural networks.</div>

        <div class="answer-section">
            <div class="answer-label">‚úì Correct Answer:</div>
            <div class="answer-text">The correct answer is 4) Deep neural networks learn compositional functions and hierarchical representations, which enhance generalization across complex tasks..</div>
        </div>

        <div class="reference-section">
            <div class="reference-label">üìö Reference Text:</div>
            <button class="toggle-button" onclick="toggleReference(4)">
                Show/Hide Reference
            </button>
            <div id="ref4" class="reference-text hidden">unit,thenumberoflinearregionsis OÓÄê k( 1 ) + l‚àí dÓÄë . (6.43) 2 0 0 CHAPTER6.DEEPFEEDFORWARDNETWORKS Ofcourse,thereisnoguaranteethatthekindsoffunctionswewanttolearnin applicationsofmachinelearning(andinparticularforAI)sharesuchaproperty. Wemayalsowanttochooseadeepmodelforstatisticalreasons. Anytime wechooseaspeciÔ¨Åcmachinelearningalgorithm,weareimplicitlystatingsome setofpriorbeliefswehaveaboutwhatkindoffunctionthealgorithmshould learn.Choosingadeepmodelencodesaverygeneralbeliefthatthefunctionwe wanttolearnshouldinvolvecompositionofseveralsimplerfunctions.Thiscanbe interpretedfromarepresentationlearningpointofviewassayingthatwebelieve thelearningproblemconsistsofdiscoveringasetofunderlyingfactorsofvariation thatcaninturnbedescribedintermsofother,simplerunderlyingfactorsof variation.Alternately,wecaninterprettheuseofadeeparchitectureasexpressing abeliefthatthefunctionwewanttolearnisacomputerprogramconsistingof multiplesteps,whereeachstepmakesuseofthepreviousstep‚Äôsoutput. These intermediateoutputsarenotnecessarilyfactorsofvariation,butcaninsteadbe analogoustocountersorpointersthatthenetworkusestoorganizeitsinternal processing.Empirically,greaterdepthdoesseemtoresultinbettergeneralization forawidevarietyoftasks( ,; ,;,; Bengio e t a l .2007Erhan e t a l .2009Bengio2009 Mesnil2011Ciresan2012Krizhevsky2012Sermanet e t a l .,; e t a l .,; e t a l .,; e t a l ., 2013Farabet2013Couprie 2013Kahou 2013Goodfellow ; e t a l .,; e t a l .,; e t a l .,; e t a l . e t a l . ,;2014dSzegedy ,).SeeÔ¨ÅgureandÔ¨Ågureforexamplesof 2014a 6.6 6.7 someoftheseempiricalresults.Thissuggeststhatusingdeeparchitecturesdoes indeedexpressausefulprioroverthespaceoffunctionsthemodellearns. 6.4.2OtherArchitecturalConsiderations Sofarwehavedescribedneuralnetworksasbeingsimplechainsoflayers,withthe mainconsiderationsbeingthedepthofthenetworkandthewidthofeachlayer. Inpractice,neuralnetworksshowconsiderablymorediversity. Manyneuralnetworkarchitectures havebeendevelopedforspeciÔ¨Åctasks. Specializedarchitecturesforcomputervisioncalledconvolutionalnetworksare describedinchapter.Feedforwardnetworksmayalsobegeneralizedtothe 9 recurrentneuralnetworksforsequenceprocessing,describedinchapter,which10 havetheirownarchitecturalconsiderations. Ingeneral,thelayersneednotbeconnectedinachain,eventhoughthisisthe mostcommonpractice.Manyarchitecturesbuildamainchainbutthenaddextra architecturalfeaturestoit,suchasskipconnectionsgoingfromlayer itolayer i+2orhigher.TheseskipconnectionsmakeiteasierforthegradienttoÔ¨Çowfrom outputlayerstolayersnearertheinput. 2 0 1 CHAPTER6.DEEPFEEDFORWARDNETWORKS 3 4 5 6 7 8 9 1 0 1 1 N u m b e r o f h i d d e n l a y e r s9 2 0 .9 2 5 .9 3 0 .9 3 5 .9 4 0 .9 4 5 .9 5 0 .9 5 5 .9 6 0 .9 6 5 .T e s t a c c u r a c y ( p e r c e n t ) Figure6.6:Empiricalresultsshowingthatdeepernetworksgeneralizebetterwhenused totranscribemulti-digitnumbersfromphotographsofaddresses.DatafromGoodfellow e t a l .(). Thetestsetaccuracyconsistentlyincreaseswithincreasingdepth. See 2014d Ô¨Ågureforacontrolexperimentdemonstratingthatotherincreasestothemodelsize 6.7 donotyieldthesameeÔ¨Äect. Anotherkeyconsiderationofarchitecturedesignisexactlyhowtoconnecta pairoflayerstoeachother.Inthedefaultneuralnetworklayerdescribedbyalinear transformationviaamatrixW,everyinputunitisconnectedtoeveryoutput unit.Manyspecializednetworksinthechaptersaheadhavefewerconnections,so thateachunitintheinputlayerisconnectedtoonlyasmallsubsetofunitsin theoutputlayer.Thesestrategiesforreducingthenumberofconnectionsreduce thenumberofparametersandtheamountofcomputationrequiredtoevaluate thenetwork,butareoftenhighlyproblem-dependent. Forexample,convolutional networks,describedinchapter,usespecializedpatternsofsparseconnections 9 thatareveryeÔ¨Äectiveforcomputervisionproblems.Inthischapter,itisdiÔ¨Écult togivemuchmorespeciÔ¨Åcadviceconcerningthearchitectureofagenericneural network.Subsequentchaptersdeveloptheparticulararchitecturalstrategiesthat havebeenfoundtoworkwellfordiÔ¨Äerentapplicationdomains. 2 0 2 CHAPTER6.DEEPFEEDFORWARDNETWORKS 0 0 0 2 0 4 0 6 0 8 1 0 . . . . . . N u m b e r o f p a r a m e t e r s √ó 1 089 19 29 39 49 59 69 7T e s t a c c u r a c y ( p e r c e n t ) 3,convolutional 3,fullyconnected 11,convolutional Figure6.7:Deepermodelstendtoperformbetter.Thisisnotmerelybecausethemodelis larger.ThisexperimentfromGoodfellow2014d e t a l .()showsthatincreasingthenumber ofparametersinlayersofconvolutionalnetworkswithoutincreasingtheirdepthisnot nearlyaseÔ¨Äectiveatincreasingtestsetperformance.Thelegendindicatesthedepthof networkusedtomakeeachcurveandwhetherthecurverepresentsvariationinthesizeof theconvolutionalorthefullyconnectedlayers.Weobservethatshallowmodelsinthis contextoverÔ¨Åtataround20millionparameterswhiledeeponescanbeneÔ¨Åtfromhaving over60million.Thissuggeststhatusingadeepmodelexpressesausefulpreferenceover thespaceoffunctionsthemodelcanlearn.SpeciÔ¨Åcally,itexpressesabeliefthatthe functionshouldconsistofmanysimplerfunctionscomposedtogether.Thiscouldresult eitherinlearningarepresentationthatiscomposedinturnofsimplerrepresentations(e.g., cornersdeÔ¨Ånedintermsofedges)orinlearningaprogramwithsequentiallydependent steps(e.g.,Ô¨Årstlocateasetofobjects,thensegmentthemfromeachother,thenrecognize them). 2 0 3 CHAPTER6.DEEPFEEDFORWARDNETWORKS 6. 5 Bac k - Prop a g a t i o n an d O t h er D i Ô¨Ä eren t i at i on A l go- ri t h m s Whenweuseafeedforwardneuralnetworktoacceptaninputxandproducean output ÀÜy,informationÔ¨Çowsforwardthroughthenetwork.Theinputsxprovide theinitialinformationthatthenpropagatesuptothehiddenunitsateachlayer andÔ¨Ånallyproduces ÀÜy.Thisiscalledforwardpropagation.Duringtraining, forwardpropagationcancontinueonwarduntilitproducesascalarcost J(Œ∏). Theback-propagationalgorithm( ,),oftensimplycalled Rumelhart e t a l .1986a backprop,allowstheinformationfromthecosttothenÔ¨Çowbackwardsthrough thenetwork,inordertocomputethegradient. Computingananalyticalexpressionforthegradientisstraightforward,but numericallyevaluatingsuchanexpressioncanbecomputationally expensive.The back-propagationalgorithmdoessousingasimpleandinexpensiveprocedure. Thetermback-propagation isoften misunders toodasmeaningthewhole learningalgorithmformulti-layerneuralnetworks.Actually,back-propagation refersonlytothemethodforcomputingthegradient,whileanotheralgorithm, suchasstochasticgradientdescent,isusedtoperformlearningusingthisgradient. Furthermore,back-propagation isoftenmisunderstoodasbeingspeciÔ¨Åctomulti- layerneuralnetworks,butinprincipleitcancomputederivativesofanyfunction (forsomefunctions,thecorrectresponseistoreportthatthederivativeofthe functionisundeÔ¨Åned).SpeciÔ¨Åcally,wewilldescribehowtocomputethegradient ‚àá x f(xy ,)foranarbitraryfunction f,wherexisasetofvariableswhosederivatives aredesired,andyisanadditionalsetofvariablesthatareinputstothefunction butwhosederivativesarenotrequired.Inlearningalgorithms,thegradientwemost oftenrequireisthegradientofthecostfunctionwithrespecttotheparameters, ‚àá Œ∏ J(Œ∏).Manymachinelearningtasksinvolvecomputingotherderivatives,either aspartof thelearning process, or to analyzethelearned model. The back- propagationalgorithmcanbeappliedtothesetasksaswell,andisnotrestricted tocomputingthegradientofthecostfunctionwithrespecttotheparameters.The ideaofcomputingderivativesbypropagatinginformationthroughanetworkis verygeneral,andcanbeusedtocomputevaluessuchastheJacobianofafunction fwithmultipleoutputs.Werestrictourdescriptionheretothemostcommonly usedcasewherehasasingleoutput. f 2 0 4 CHAPTER6.DEEPFEEDFORWARDNETWORKS 6.5.1ComputationalGraphs Sofarwehavediscussedneuralnetworkswitharelativelyinformalgraphlanguage. Todescribetheback-propagationalgorithmmoreprecisely,itishelpfultohavea moreprecise language. computationalgraph Manywaysofformalizingcomputationasgraphsarepossible. Here,weuseeachnodeinthegraphtoindicateavariable.Thevariablemay beascalar,vector,matrix,tensor,orevenavariableofanothertype. Toformalizeourgraphs,wealsoneedtointroducetheideaofanoperation. Anoperationisasimplefunctionofoneormorevariables.Ourgraphlanguage isaccompanied byasetofallowableoperations.Functionsmorecomplicated thantheoperationsinthissetmaybedescribedbycomposingmanyoperations together. Withoutlossofgenerality, wedeÔ¨Åneanoperationtoreturnonlyasingle outputvariable.Thisdoesnotlosegeneralitybecausetheoutputvariablecanhave multipleentries,suchasavector.Softwareimplementationsofback-propagation usuallysupportoperationswithmultipleoutputs,butweavoidthiscaseinour descriptionbecauseitintroducesmanyextradetailsthatarenotimportantto conceptualunderstanding. Ifavariable yiscomputedbyapplyinganoperationtoavariable x,then wedrawadirectededgefrom xto y. Wesometimesannotatetheoutputnode withthenameoftheoperationapplied,andothertimesomitthislabelwhenthe operationisclearfromcontext. Examplesofcomputational graphsareshowninÔ¨Ågure.6.8 6.5.2ChainRuleofCalculus Thechainruleofcalculus(nottobeconfusedwiththechainruleofprobability)is usedtocomputethederivativesoffunctionsformedbycomposingotherfunctions whosederivativesareknown.Back-propagati onisanalgorithmthatcomputesthe chainrule,withaspeciÔ¨ÅcorderofoperationsthatishighlyeÔ¨Écient. Let xbearealnumber,andlet fand gbothbefunctionsmappingfromareal numbertoarealnumber.Supposethat y= g( x)and z= f( g( x)) = f( y).Then thechainrulestatesthatd z d x=d z d yd y d x. (6.44) Wecangeneralizethisbeyondthescalarcase.Supposethatx‚àà Rm,y‚àà Rn, 2 0</div>
        </div>
    </div>

    <div class="question-card" id="q5">
        <div class="question-header">
            <span class="question-number">Question 5</span>
            <span class="question-type">multiple-choice</span>
        </div>

        <div class="question-text">Deep learning has transformed artificial intelligence by enabling neural networks to achieve remarkable performance in complex tasks, leveraging advances in algorithms, data availability, and computational hardware. Understanding the evolution and architecture of neural networks is crucial to appreciating their broad impact.

Which of the following innovations was specifically introduced to address the challenge of modeling long-term dependencies in sequential data, such as language or time series?

1) Convolutional neural networks   
2) Greedy layer-wise pre-training   
3) Support vector machines   
4) Word embeddings (Word2Vec, GloVe)   
5) Residual neural networks   
6) Principal component analysis   
7) Long Short-Term Memory (LSTM) networks </div>

        <div class="answer-section">
            <div class="answer-label">‚úì Correct Answer:</div>
            <div class="answer-text">The correct answer is 7) Long Short-Term Memory (LSTM) networks.</div>
        </div>

        <div class="reference-section">
            <div class="reference-label">üìö Reference Text:</div>
            <button class="toggle-button" onclick="toggleReference(5)">
                Show/Hide Reference
            </button>
            <div id="ref5" class="reference-text hidden">di st r i but e d r e pr e se n t at i o n(Hinton e t a l ., 1986).Thisistheideathateachinputtoasystemshouldberepresentedby manyfeatures,andeachfeatureshouldbeinvolvedintherepresentationofmany possibleinputs.Forexample,supposewehaveavisionsystemthatcanrecognize cars,trucks,andbirdsandtheseobjectscaneachbered,green,orblue.Oneway ofrepresentingtheseinputswouldbetohaveaseparateneuronorhiddenunit thatactivatesforeachoftheninepossiblecombinations:redtruck,redcar,red bird,greentruck,andsoon.ThisrequiresninediÔ¨Äerentneurons,andeachneuron mustindependentlylearntheconceptofcolorandobjectidentity.Onewayto improveonthissituationistouseadistributedrepresentation,withthreeneurons describingthecolorandthreeneuronsdescribingtheobjectidentity.Thisrequires onlysixneuronstotalinsteadofnine,andtheneurondescribingrednessisableto 1 7 CHAPTER1.INTRODUCTION learnaboutrednessfromimagesofcars,trucksandbirds,notonlyfromimages ofonespeciÔ¨Åccategoryofobjects. Theconceptofdistributedrepresentationis centraltothisbook,andwillbedescribedingreaterdetailinchapter.15 Anothermajoraccomplishmentoftheconnectionistmovementwasthesuc- cessfuluseofback-propagation totraindeepneuralnetworkswithinternalrepre- sentationsandthepopularization oftheback-propagation algorithm(Rumelhart e t a l .,;,).Thisalgorithmhaswaxedandwanedinpopularity 1986aLeCun1987 butasofthiswritingiscurrentlythedominantapproachtotrainingdeepmodels. Duringthe1990s,researchersmadeimportantadvancesinmodelingsequences withneuralnetworks.()and ()identiÔ¨Åedsomeof Hochreiter1991Bengio e t a l .1994 thefundamentalmathematical diÔ¨Écultiesinmodelinglongsequences,describedin section.10.7HochreiterandSchmidhuber1997()introducedthelongshort-term memoryorLSTMnetworktoresolvesomeofthesediÔ¨Éculties.Today,theLSTM iswidelyusedformanysequencemodelingtasks,includingmanynaturallanguage processingtasksatGoogle. Thesecondwaveofneuralnetworksresearchlasteduntilthemid-1990s.Ven- turesbasedonneuralnetworksandotherAItechnologiesbegantomakeunrealisti- callyambitiousclaimswhileseekinginvestments.WhenAIresearchdidnotfulÔ¨Åll theseunreasonableexpectations,investorsweredisappointed.Simultaneously, otherÔ¨Åeldsofmachinelearningmadeadvances.Kernelmachines(,Boser e t a l . 1992CortesandVapnik1995Sch√∂lkopf1999 Jor- ; ,; e t a l .,)andgraphicalmodels( dan1998,)bothachievedgoodresultsonmanyimportanttasks.Thesetwofactors ledtoadeclineinthepopularityofneuralnetworksthatlasteduntil2007. Duringthistime,neuralnetworkscontinuedtoobtainimpressiveperformance onsometasks( ,; ,).TheCanadianInstitute LeCun e t a l .1998bBengio e t a l .2001 forAdvancedResearch(CIFAR)helpedtokeepneuralnetworksresearchalive viaitsNeuralComputation andAdaptivePerception(NCAP)researchinitiative. ThisprogramunitedmachinelearningresearchgroupsledbyGeoÔ¨ÄreyHinton atUniversityofToronto,YoshuaBengioatUniversityofMontreal,andYann LeCunatNewYorkUniversity.TheCIFARNCAPresearchinitiativehada multi-disciplinarynaturethatalsoincludedneuroscientistsandexpertsinhuman andcomputervision. Atthispointintime,deepnetworksweregenerallybelievedtobeverydiÔ¨Écult totrain. Wenowknowthatalgorithmsthathaveexistedsincethe1980swork quitewell,butthiswasnotapparentcirca2006.Theissueisperhapssimplythat thesealgorithmsweretoocomputationally costlytoallowmuchexperimentation withthehardwareavailableatthetime. Thethirdwaveofneuralnetworksresearchbeganwithabreakthrough in 1 8 CHAPTER1.INTRODUCTION 2006.GeoÔ¨ÄreyHintonshowedthatakindofneuralnetworkcalledadeepbelief networkcouldbeeÔ¨Écientlytrainedusingastrategycalledgreedylayer-wisepre- training( ,),whichwillbedescribedinmoredetailinsection. Hinton e t a l .2006 15.1 TheotherCIFAR-aÔ¨Éliatedresearchgroupsquicklyshowedthatthesamestrategy couldbeusedtotrainmanyotherkindsofdeepnetworks( ,; Bengio e t a l .2007 Ranzato 2007a e t a l .,)andsystematicallyhelpedtoimprovegeneralization on testexamples.Thiswaveofneuralnetworksresearchpopularizedtheuseofthe term‚Äúdeeplearning‚Äùtoemphasizethatresearcherswerenowabletotraindeeper neuralnetworksthanhadbeenpossiblebefore,andtofocusattentiononthe theoreticalimportanceofdepth( ,; , BengioandLeCun2007DelalleauandBengio 2011Pascanu2014aMontufar2014 ; e t a l .,; e t a l .,).Atthistime,deepneural networksoutperformedcompetingAIsystemsbasedonothermachinelearning technologiesaswellashand-designedfunctionality.Thisthirdwaveofpopularity ofneuralnetworkscontinuestothetimeofthiswriting,thoughthefocusofdeep learningresearchhaschangeddramatically withinthetimeofthiswave.The thirdwavebeganwithafocusonnewunsupervisedlearningtechniquesandthe abilityofdeepmodelstogeneralizewellfromsmalldatasets,buttodaythereis moreinterestinmucholdersupervisedlearningalgorithmsandtheabilityofdeep modelstoleveragelargelabeleddatasets. 1 . 2 . 2 In creasin g D a t a s et S i zes Onemaywonderwhydeeplearninghasonlyrecentlybecomerecognizedasa crucialtechnologythoughtheÔ¨ÅrstexperimentswithartiÔ¨Åcialneuralnetworkswere conductedinthe1950s.Deeplearninghasbeensuccessfullyusedincommercial applicationssincethe1990s,butwasoftenregardedasbeingmoreofanartthan atechnologyandsomethingthatonlyanexpertcoulduse,untilrecently.Itistrue thatsomeskillisrequiredtogetgoodperformancefromadeeplearningalgorithm. Fortunately,theamountofskillrequiredreducesastheamountoftrainingdata increases.Thelearningalgorithmsreachinghumanperformanceoncomplextasks todayarenearlyidenticaltothelearningalgorithmsthatstruggledtosolvetoy problemsinthe1980s,thoughthemodelswetrainwiththesealgorithmshave undergonechangesthatsimplifythetrainingofverydeeparchitectures.Themost importantnewdevelopmentisthattodaywecanprovidethesealgorithmswith theresourcestheyneedtosucceed.Figureshowshowthesizeofbenchmark 1.8 datasetshasincreasedremarkablyovertime.Thistrendisdrivenbytheincreasing digitizationofsociety.Asmoreandmoreofouractivitiestakeplaceoncomputers, moreandmoreofwhatwedoisrecorded.Asourcomputersareincreasingly networkedtogether,itbecomeseasiertocentralizetheserecordsandcuratethem 1 9 CHAPTER1.INTRODUCTION intoadatasetappropriateformachinelearningapplications.Theageof‚ÄúBig Data‚Äùhasmademachinelearningmucheasierbecausethekeyburdenofstatistical estimation‚Äîgeneralizingwelltonewdataafterobservingonlyasmallamount ofdata‚Äîhasbeenconsiderablylightened.Asof2016,aroughruleofthumb isthatasuperviseddeeplearningalgorithmwillgenerallyachieveacceptable performancewitharound5,000labeledexamplespercategory,andwillmatchor exceedhumanperformancewhentrainedwithadatasetcontainingatleast10 millionlabeledexamples.Workingsuccessfullywithdatasetssmallerthanthisis animportantresearcharea,focusinginparticularonhowwecantakeadvantage oflargequantitiesofunlabeledexamples,withunsupervisedorsemi-supervised learning. 1 . 2 . 3 In creasin g Mo d el S i zes Anotherkeyreasonthatneuralnetworksarewildlysuccessfultodayafterenjoying comparativelylittlesuccesssincethe1980sisthatwehavethecomputational resourcestorunmuchlargermodelstoday.Oneofthemaininsightsofconnection- ismisthatanimalsbecomeintelligentwhenmanyoftheirneuronsworktogether. Anindividualneuronorsmallcollectionofneuronsisnotparticularlyuseful. Biologicalneuronsarenotespeciallydenselyconnected.AsseeninÔ¨Ågure,1.10 ourmachinelearningmodelshavehadanumberofconnectionsperneuronthat waswithinanorderofmagnitudeofevenmammalianbrainsfordecades. Intermsofthetotalnumberofneurons,neuralnetworkshavebeenastonishingly smalluntilquiterecently,asshowninÔ¨Ågure.Sincetheintroductionofhidden 1.11 units,artiÔ¨Åcialneuralnetworkshavedoubledinsizeroughlyevery2.4years.This growthisdrivenbyfastercomputerswithlargermemoryandbytheavailability oflargerdatasets.Largernetworksareabletoachievehigheraccuracyonmore complextasks.Thistrendlookssettocontinuefordecades.Unlessnewtechnologies allowfasterscaling,artiÔ¨Åcialneuralnetworkswillnothavethesamenumberof neuronsasthehumanbrainuntilatleastthe2050s.Biologicalneuronsmay representmorecomplicatedfunctionsthancurrentartiÔ¨Åcialneurons,sobiological neuralnetworksmaybeevenlargerthanthisplotportrays. Inretrospect,itisnotparticularlysurprisingthatneuralnetworkswithfewer neuronsthanaleechwereunabletosolvesophisticatedartiÔ¨Åcialintelligenceprob- lems.Eventoday‚Äôsnetworks,whichweconsiderquitelargefromacomputational systemspointofview,aresmallerthanthenervoussystemofevenrelatively primitivevertebrateanimalslikefrogs. Theincreaseinmodelsizeovertime,duetotheavailabilityoffasterCPUs, 2 0 CHAPTER1.INTRODUCTION 1900 1950 198520002015 Year100101102103104105106107108109Datasetsize(numberexamples) IrisMNISTPublicSVHN ImageNet CIFAR-10ImageNet10k ILSVRC 2014Sports-1M RotatedTvs.C Tvs.Gvs.FCriminalsCanadianHansard WMT Figure1.8:Datasetsizeshaveincreasedgreatlyovertime.Intheearly1900s,statisticians studieddatasetsusinghundredsorthousandsofmanuallycompiledmeasurements(,Garson 1900Gosset1908Anderson1935Fisher1936 ;,;,;,).Inthe1950sthrough1980s,thepioneers ofbiologicallyinspiredmachinelearningoftenworkedwithsmall,syntheticdatasets,such aslow-resolutionbitmapsofletters,thatweredesignedtoincurlowcomputationalcostand demonstratethatneuralnetworkswereabletolearnspeciÔ¨Åckindsoffunctions(Widrow andHoÔ¨Ä1960Rumelhart1986b ,; e t a l .,).Inthe1980sand1990s,machinelearning becamemorestatisticalinnatureandbegantoleveragelargerdatasetscontainingtens ofthousandsofexamplessuchastheMNISTdataset(showninÔ¨Ågure)ofscans 1.9 ofhandwrittennumbers( ,).IntheÔ¨Årstdecadeofthe2000s,more LeCun e t a l .1998b sophisticateddatasetsofthissamesize,suchastheCIFAR-10dataset(Krizhevskyand Hinton2009,)continuedtobeproduced.Towardtheendofthatdecadeandthroughout theÔ¨Årsthalfofthe2010s,signiÔ¨Åcantlylargerdatasets,containinghundredsofthousands totensofmillionsofexamples,completelychangedwhatwaspossiblewithdeeplearning. ThesedatasetsincludedthepublicStreetViewHouseNumbersdataset( , Netzer e t a l . 2011),variousversionsoftheImageNetdataset( ,,; Deng e t a l .20092010aRussakovsky e t a l . e t a l . ,),andtheSports-1Mdataset( 2014a Karpathy,).Atthetopofthe 2014 graph,weseethatdatasetsoftranslatedsentences,suchasIBM‚Äôsdatasetconstructed fromtheCanadianHansard( ,)andtheWMT2014EnglishtoFrench Brown e t a l .1990 dataset(Schwenk2014,)aretypicallyfaraheadofotherdatasetsizes. 2 1 CHAPTER1.INTRODUCTION Figure1.9:ExampleinputsfromtheMNISTdataset.The‚ÄúNIST‚ÄùstandsforNational InstituteofStandardsandTechnology,theagencythatoriginallycollectedthisdata. The‚ÄúM‚Äùstandsfor‚ÄúmodiÔ¨Åed,‚Äùsincethedatahasbeenpreprocessedforeasierusewith machinelearningalgorithms.TheMNISTdatasetconsistsofscansofhandwrittendigits andassociatedlabelsdescribingwhichdigit0‚Äì9iscontainedineachimage.Thissimple classiÔ¨Åcationproblemisoneofthesimplestandmostwidelyusedtestsindeeplearning research.Itremainspopulardespitebeingquiteeasyformoderntechniquestosolve. GeoÔ¨ÄreyHintonhasdescribeditas‚Äúthe d r o s o p h i l aofmachinelearning,‚Äùmeaningthat itallowsmachinelearningresearcherstostudytheiralgorithmsincontrolledlaboratory conditions,muchasbiologistsoftenstudyfruitÔ¨Çies. 2 2 CHAPTER1.INTRODUCTION theadventofgeneralpurposeGPUs(describedinsection),fasternetwork 12.1.2 connectivityandbettersoftwareinfrastructurefordistributedcomputing,isoneof themostimportanttrendsinthehistoryofdeeplearning.Thistrendisgenerally expectedtocontinuewellintothefuture. 1 . 2 . 4 In creasin g A ccu ra cy , Co m p l e xi t y a n d Rea l - W o rl d Im p a ct Sincethe1980s,deeplearninghasconsistentlyimprovedinitsabilitytoprovide accuraterecognitionorprediction.Moreover,deeplearninghasconsistentlybeen appliedwithsuccesstobroaderandbroadersetsofapplications. Theearliestdeepmodelswereusedtorecognizeindividualobjectsintightly cropped,extremelysmallimages( ,).Sincethentherehas Rumelhart e t a l .1986a beenagradualincreaseinthesizeofimagesneuralnetworkscouldprocess.Modern objectrecognitionnetworksprocessrichhigh-resolutionphotographs anddonot havearequirementthatthephotobecroppedneartheobjecttoberecognized ( ,).Similarly,theearliestnetworkscouldonlyrecognize Krizhevsky e t a l .2012 twokindsofobjects(orinsomecases,theabsenceorpresenceofasinglekindof object),whilethesemodernnetworkstypicallyrecognizeatleast1,000diÔ¨Äerent categoriesofobjects. ThelargestcontestinobjectrecognitionistheImageNet LargeScaleVisualRecognitionChallenge(ILSVRC)heldeachyear.Adramatic momentinthemeteoricriseofdeeplearningcamewhenaconvolutionalnetwork wonthischallengefortheÔ¨Årsttimeandbyawidemargin,bringingdownthe state-of-the-art top-5errorratefrom26.1%to15.3%( ,), Krizhevsky e t a l .2012 meaningthattheconvolutionalnetworkproducesarankedlistofpossiblecategories foreachimageandthecorrectcategoryappearedintheÔ¨ÅrstÔ¨Åveentriesofthis listforallbut15.3%ofthetestexamples.Sincethen,thesecompetitionsare consistentlywonbydeepconvolutionalnets,andasofthiswriting,advancesin deeplearninghavebroughtthelatesttop-5errorrateinthiscontestdownto3.6%, asshowninÔ¨Ågure.1.12 Deeplearninghasalsohadadramaticimpactonspeechrecognition.After improvingthroughoutthe1990s,theerrorratesforspeechrecognitionstagnated startinginabout2000.Theintroductionofdeeplearning(,; Dahl e t a l .2010Deng e t a l . e t a l . e t a l . ,;2010bSeide,;2011Hinton,)tospeechrecognitionresulted 2012a inasuddendropoferrorrates,withsomeerrorratescutinhalf.Wewillexplore thishistoryinmoredetailinsection.12.3 Deepnetworkshavealsohadspectacularsuccessesforpedestriandetectionand imagesegmentation( ,; Sermanet e t a</div>
        </div>
    </div>

    <div class="question-card" id="q6">
        <div class="question-header">
            <span class="question-number">Question 6</span>
            <span class="question-type">multiple-choice</span>
        </div>

        <div class="question-text">Convolutional neural networks (CNNs) are powerful models for image classification and rely on architectural choices including convolution, pooling, and padding. One critical decision involves how padding is applied during convolution operations, as it affects feature extraction and the overall performance of the network.

Which padding strategy in convolutional neural networks most commonly provides the best balance between preserving spatial dimensions and achieving optimal test accuracy?

1) Full padding   
2) A compromise between valid and same padding   
3) No padding (valid padding)   
4) Minimal padding only at the borders   
5) Maximal zero-padding on all sides   
6) Randomized padding per layer   
7) Padding applied only to feature channels</div>

        <div class="answer-section">
            <div class="answer-label">‚úì Correct Answer:</div>
            <div class="answer-text">The correct answer is 2) A compromise between valid and same padding.</div>
        </div>

        <div class="reference-section">
            <div class="reference-label">üìö Reference Text:</div>
            <button class="toggle-button" onclick="toggleReference(6)">
                Show/Hide Reference
            </button>
            <div id="ref6" class="reference-text hidden">CHAPTER9.CONVOLUTIONALNETWORKS Input image: 256x256x3Output of convolution + ReLU: 256x256x64Output of pooling with stride 4: 64x64x64Output of convolution + ReLU: 64x64x64Output of pooling with stride 4: 16x16x64Output of reshape to vector: 16,384 unitsOutput of matrix multiply: 1,000 unitsOutput of softmax: 1,000 class probabilities Input image: 256x256x3Output of convolution + ReLU: 256x256x64Output of pooling with stride 4: 64x64x64Output of convolution + ReLU: 64x64x64Output of pooling to 3x3 grid: 3x3x64Output of reshape to vector: 576 unitsOutput of matrix multiply: 1,000 unitsOutput of softmax: 1,000 class probabilities Input image: 256x256x3Output of convolution + ReLU: 256x256x64Output of pooling with stride 4: 64x64x64Output of convolution + ReLU: 64x64x64Output of convolution: 16x16x1,000Output of average pooling: 1x1x1,000Output of softmax: 1,000 class probabilities Output of pooling with stride 4: 16x16x64 Figure9.11:ExamplesofarchitecturesforclassiÔ¨Åcationwithconvolutionalnetworks.The speciÔ¨ÅcstridesanddepthsusedinthisÔ¨Ågurearenotadvisableforrealuse;theyare designedtobeveryshallowinordertoÔ¨Åtontothepage. Realconvolutionalnetworks alsoofteninvolvesigniÔ¨Åcantamountsofbranching,unlikethechainstructuresused hereforsimplicity. ( L e f t )AconvolutionalnetworkthatprocessesaÔ¨Åxedimagesize. Afteralternatingbetweenconvolutionandpoolingforafewlayers,thetensorforthe convolutionalfeaturemapisreshapedtoÔ¨Çattenoutthespatialdimensions.Therest ofthenetworkisanordinaryfeedforwardnetworkclassiÔ¨Åer,asdescribedinchapter.6 ( C e n t e r )Aconvolutionalnetworkthatprocessesavariable-sizedimage,butstillmaintains afullyconnectedsection.Thisnetworkusesapoolingoperationwithvariably-sizedpools butaÔ¨Åxednumberofpools,inordertoprovideaÔ¨Åxed-sizevectorof576unitstothe fullyconnectedportionofthenetwork. Aconvolutionalnetworkthatdoesnot ( R i g h t ) haveanyfullyconnectedweightlayer.Instead,thelastconvolutionallayeroutputsone featuremapperclass.Themodelpresumablylearnsamapofhowlikelyeachclassisto occurateachspatiallocation.Averagingafeaturemapdowntoasinglevalueprovides theargumenttothesoftmaxclassiÔ¨Åeratthetop. 3 4 6 CHAPTER9.CONVOLUTIONALNETWORKS saysthatthefunctionthelayershouldlearncontainsonlylocalinteractionsandis equivarianttotranslation.Likewise,theuseofpoolingisaninÔ¨Ånitelystrongprior thateachunitshouldbeinvarianttosmalltranslations. Ofcourse,implementing aconvolutionalnetasafullyconnectednetwithan inÔ¨Ånitelystrongpriorwouldbeextremelycomputationally wasteful.Butthinking ofaconvolutionalnetasafullyconnectednetwithaninÔ¨Ånitelystrongpriorcan giveussomeinsightsintohowconvolutionalnetswork. OnekeyinsightisthatconvolutionandpoolingcancauseunderÔ¨Åtting. Like anyprior,convolutionandpoolingareonlyusefulwhentheassumptionsmade bythepriorarereasonablyaccurate.Ifataskreliesonpreservingprecisespatial information, thenusingpoolingonallfeaturescanincreasethetrainingerror. Someconvolutionalnetworkarchitectures ( ,)aredesignedto Szegedy etal.2014a usepoolingonsomechannelsbutnotonotherchannels,inordertogetboth highlyinvariantfeaturesandfeaturesthatwillnotunderÔ¨Åtwhenthetranslation invariancepriorisincorrect.Whenataskinvolvesincorporatinginformationfrom verydistantlocationsintheinput,thenthepriorimposedbyconvolutionmaybe inappropriate. Anotherkeyinsightfromthisviewisthatweshouldonlycompareconvolu- tionalmodelstootherconvolutionalmodelsinbenchmarksofstatisticallearning performance.Modelsthatdonotuseconvolutionwouldbeabletolearneven ifwepermutedallofthepixelsintheimage.Formanyimagedatasets,there areseparatebenchmarksformodelsthatare p e r m ut at i o n i nv ar i antandmust discovertheconceptoftopologyvialearning,andmodelsthathavetheknowledge ofspatialrelationshipshard-codedintothembytheirdesigner. 9.5VariantsoftheBasicConvolutionFunction Whendiscussingconvolutioninthecontextofneuralnetworks,weusuallydo notreferexactlytothestandarddiscreteconvolutionoperationasitisusually understoodinthemathematical literature.ThefunctionsusedinpracticediÔ¨Äer slightly.HerewedescribethesediÔ¨Äerencesindetail,andhighlightsomeuseful propertiesofthefunctionsusedinneuralnetworks. First,whenwerefertoconvolutioninthecontextofneuralnetworks,weusually actuallymeananoperationthatconsistsofmanyapplicationsofconvolutionin parallel.Thisisbecauseconvolutionwithasinglekernelcanonlyextractonekind offeature,albeitatmanyspatiallocations.Usuallywewanteachlayerofour networktoextractmanykindsoffeatures,atmanylocations. 3 4 7 CHAPTER9.CONVOLUTIONALNETWORKS Additionally,theinputisusuallynotjustagridofrealvalues.Rather,itisa gridofvector-valuedobservations. Forexample,acolorimagehasared,green andblueintensityateachpixel.Inamultilayerconvolutionalnetwork,theinput tothesecondlayeristheoutputoftheÔ¨Årstlayer,whichusuallyhastheoutput ofmanydiÔ¨Äerentconvolutionsateachposition.Whenworkingwithimages,we usuallythinkoftheinputandoutputoftheconvolutionasbeing3-Dtensors,with oneindexintothediÔ¨Äerentchannelsandtwoindicesintothespatialcoordinates ofeachchannel.Softwareimplementationsusuallyworkinbatchmode,sothey willactuallyuse4-Dtensors,withthefourthaxisindexingdiÔ¨Äerentexamplesin thebatch,butwewillomitthebatchaxisinourdescriptionhereforsimplicity. Becauseconvolutionalnetworksusuallyusemulti-channelconvolution,the linearoperationstheyarebasedonarenotguaranteedtobecommutative,evenif kernel-Ô¨Çippingisused.Thesemulti-channeloperationsareonlycommutativeif eachoperationhasthesamenumberofoutputchannelsasinputchannels. Assumewehavea4-Dkerneltensor Kwithelement K i , j , k, lgivingtheconnection strengthbetweenaunitinchannel ioftheoutputandaunitinchannel jofthe input,withanoÔ¨Äsetof krowsand lcolumnsbetweentheoutputunitandthe inputunit.Assumeourinputconsistsofobserveddata Vwithelement V i , j , kgiving thevalueoftheinputunitwithinchannel iatrow jandcolumn k.Assumeour outputconsistsof Zwiththesameformatas V.If Zisproducedbyconvolving K acrosswithoutÔ¨Çipping,then V K Z i , j , k=ÓÅò l , m , nV l , j m , k n + ‚àí 1 + ‚àí 1 K i , l , m , n (9.7) wherethesummationover l, mand nisoverallvaluesforwhichthetensorindexing operationsinsidethesummationisvalid.Inlinearalgebranotation,weindexinto arraysusingafortheÔ¨Årstentry.Thisnecessitatesthe 1 ‚àí1intheaboveformula. ProgramminglanguagessuchasCandPythonindexstartingfrom,rendering0 theaboveexpressionevensimpler. Wemaywanttoskipoversomepositionsofthekernelinordertoreducethe computational cost(attheexpenseofnotextractingourfeaturesasÔ¨Ånely).We canthinkofthisasdownsamplingtheoutputofthefullconvolutionfunction.If wewanttosampleonlyevery spixelsineachdirectionintheoutput,thenwecan deÔ¨Åneadownsampledconvolutionfunctionsuchthat c Z i , j , k= ( ) c K V , , s i , j , k=ÓÅò l , m , nÓÄÇ Vl , j s m , k s n ( ‚àí √ó 1 ) + ( ‚àí √ó 1 ) + K i , l , m , nÓÄÉ .(9.8) Wereferto sasthe st r i deofthisdownsampledconvolution.Itisalsopossible 3 4 8 CHAPTER9.CONVOLUTIONALNETWORKS todeÔ¨Åneaseparatestrideforeachdirectionofmotion.SeeÔ¨Ågureforan9.12 illustration. Oneessentialfeatureofanyconvolutionalnetworkimplementationistheability toimplicitlyzero-padtheinput Vinordertomakeitwider.Withoutthisfeature, thewidthoftherepresentationshrinksbyonepixellessthanthekernelwidth ateachlayer. Zeropaddingtheinputallowsustocontrolthekernelwidthand thesizeoftheoutputindependently.Withoutzeropadding,weareforcedto choosebetweenshrinkingthespatialextentofthenetworkrapidlyandusingsmall kernels‚ÄîbothscenariosthatsigniÔ¨Åcantlylimittheexpressivepowerofthenetwork. SeeÔ¨Ågureforanexample. 9.13 Threespecialcasesofthezero-paddingsettingareworthmentioning.Oneis theextremecaseinwhichnozero-paddingisusedwhatsoever,andtheconvolution kernelisonlyallowedtovisitpositionswheretheentirekerneliscontainedentirely withintheimage.InMATLABterminology,thisiscalled v al i dconvolution.In thiscase,allpixelsintheoutputareafunctionofthesamenumberofpixelsin theinput,sothebehaviorofanoutputpixelissomewhatmoreregular.However, thesizeoftheoutputshrinksateachlayer.Iftheinputimagehaswidth mand thekernelhaswidth k,theoutputwillbeofwidth m k ‚àí+1. Therateofthis shrinkagecanbedramaticifthekernelsusedarelarge.Sincetheshrinkageis greaterthan0,itlimitsthenumberofconvolutionallayersthatcanbeincluded inthenetwork.Aslayersareadded,thespatialdimensionofthenetworkwill eventuallydropto1 √ó1,atwhichpointadditionallayerscannotmeaningfully beconsideredconvolutional.Anotherspecialcaseofthezero-paddingsettingis whenjustenoughzero-paddingisaddedtokeepthesizeoftheoutputequalto thesizeoftheinput.MATLABcallsthis sameconvolution.Inthiscase,the networkcancontainasmanyconvolutionallayersastheavailablehardwarecan support,sincetheoperationofconvolutiondoesnotmodifythearchitectural possibilitiesavailabletothenextlayer.However,theinputpixelsneartheborder inÔ¨Çuencefeweroutputpixelsthantheinputpixelsnearthecenter.Thiscanmake theborderpixelssomewhatunderrepresen tedinthemodel.Thismotivatesthe otherextremecase,whichMATLABreferstoas f ul lconvolution,inwhichenough zeroesareaddedforeverypixeltobevisited ktimesineachdirection,resulting inanoutputimageofwidth m+ k ‚àí1.Inthiscase,theoutputpixelsnearthe borderareafunctionoffewerpixelsthantheoutputpixelsnearthecenter.This canmakeitdiÔ¨Éculttolearnasinglekernelthatperformswellatallpositionsin theconvolutionalfeaturemap.Usuallytheoptimalamountofzeropadding(in termsoftestsetclassiÔ¨Åcationaccuracy)liessomewherebetween‚Äúvalid‚Äùand‚Äúsame‚Äù convolution. 3 4 9 CHAPTER9.CONVOLUTIONALNETWORKS x 1 x 1 x 2 x 2 x 3 x 3s 1 s 1 s 2 s 2 x 4 x 4 x 5 x 5s 3 s 3 x 1 x 1 x 2 x 2 x 3 x 3z 2 z 2</div>
        </div>
    </div>

    <div class="question-card" id="q7">
        <div class="question-header">
            <span class="question-number">Question 7</span>
            <span class="question-type">multiple-choice</span>
        </div>

        <div class="question-text">Effective training of deep neural networks requires careful consideration of both parameter initialization and optimization algorithms. Adaptive learning rate methods have significantly impacted the speed and stability of deep model convergence.

Which optimization algorithm combines the adaptive per-parameter learning rate approach of RMSProp with momentum, introduces bias correction for moment estimates, and is noted for its robustness to hyperparameter selection?

1) Stochastic Gradient Descent (SGD)   
2) AdaDelta   
3) Delta-Bar-Delta   
4) Quasi-Newton Method   
5) AdaGrad   
6) RMSProp   
7) Adam </div>

        <div class="answer-section">
            <div class="answer-label">‚úì Correct Answer:</div>
            <div class="answer-text">The correct answer is 7) Adam.</div>
        </div>

        <div class="reference-section">
            <div class="reference-label">üìö Reference Text:</div>
            <button class="toggle-button" onclick="toggleReference(7)">
                Show/Hide Reference
            </button>
            <div id="ref7" class="reference-text hidden">CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS canview hasagatethatdetermineswhether u h u‚âàor u h‚âà0. Inthese situations,wewanttosetthebiasfor hsothat h‚âà1mostofthetimeat initialization. Otherwise udoesnothaveachancetolearn.Forexample, Jozefowicz2015etal.()advocatesettingthebiastofortheforgetgateof 1 theLSTMmodel,describedinsection.10.10 Anothercommontypeofparameterisavarianceorprecisionparameter.For example,wecanperformlinearregressionwithaconditionalvarianceestimate usingthemodel p y y (| Nx) = (|wTx+1) b , /Œ≤ (8.24) where Œ≤isaprecisionparameter.Wecanusuallyinitializevarianceorprecision parametersto1safely.Anotherapproachistoassumetheinitialweightsareclose enoughtozerothatthebiasesmaybesetwhileignoringtheeÔ¨Äectoftheweights, thensetthebiasestoproducethecorrectmarginalmeanoftheoutput,andset thevarianceparameterstothemarginalvarianceoftheoutputinthetrainingset. Besidesthesesimpleconstantorrandommethodsofinitializingmodelparame- ters,itispossibletoinitializemodelparametersusingmachinelearning.Acommon strategydiscussedinpartofthisbookistoinitializeasupervisedmodelwith III theparameterslearnedbyanunsupervisedmodeltrainedonthesameinputs. Onecanalsoperformsupervisedtrainingonarelatedtask.Evenperforming supervisedtrainingonanunrelatedtaskcansometimesyieldaninitialization that oÔ¨Äersfasterconvergencethanarandominitialization. Someoftheseinitialization strategiesmayyieldfasterconvergenceandbettergeneralization becausethey encodeinformationaboutthedistributionintheinitialparametersofthemodel. Othersapparentlyperformwellprimarilybecausetheysettheparameterstohave therightscaleorsetdiÔ¨ÄerentunitstocomputediÔ¨Äerentfunctionsfromeachother. 8.5AlgorithmswithAdaptiveLearningRates Neuralnetworkresearchershavelongrealizedthatthelearningratewasreliablyone ofthehyperparameters thatisthemostdiÔ¨ÉculttosetbecauseithasasigniÔ¨Åcant impactonmodelperformance.Aswehavediscussedinsectionsand,the 4.38.2 costisoftenhighlysensitivetosomedirectionsinparameterspaceandinsensitive toothers.Themomentumalgorithmcanmitigatetheseissuessomewhat,but doessoattheexpenseofintroducinganotherhyperparameter. Inthefaceofthis, itisnaturaltoaskifthereisanotherway.Ifwebelievethatthedirectionsof sensitivityaresomewhataxis-aligned,itcanmakesensetouseaseparatelearning 3 0 6 CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS rateforeachparameter,andautomatically adapttheselearningratesthroughout thecourseoflearning. The algorithm(,)isanearlyheuristicapproach delta-bar-delta Jacobs1988 toadaptingindividuallearningratesformodelparametersduringtraining.The approachisbasedonasimpleidea:ifthepartialderivativeoftheloss,withrespect toagivenmodelparameter,remainsthesamesign,thenthelearningrateshould increase.Ifthepartialderivativewithrespecttothatparameterchangessign, thenthelearningrateshoulddecrease. Ofcourse,thiskindofrulecanonlybe appliedtofullbatchoptimization. Morerecently,anumberofincremental(ormini-batch-bas ed)methodshave beenintroducedthatadaptthelearningratesofmodelparameters.Thissection willbrieÔ¨Çyreviewafewofthesealgorithms. 8.5.1AdaGrad TheAdaGradalgorithm,showninalgorithm ,individuallyadaptsthelearning 8.4 ratesofallmodelparametersbyscalingtheminverselyproportionaltothesquare rootofthesumofalloftheirhistoricalsquaredvalues(,).The Duchietal.2011 parameterswiththelargestpartialderivativeofthelosshaveacorrespondingly rapiddecreaseintheirlearningrate,whileparameterswithsmallpartialderivatives havearelativelysmalldecreaseintheirlearningrate.TheneteÔ¨Äectisgreater progressinthemoregentlyslopeddirectionsofparameterspace. Inthecontextofconvexoptimization, theAdaGradalgorithmenjoyssome desirabletheoreticalproperties.However,empiricallyithasbeenfoundthat‚Äîfor trainingdeepneuralnetworkmodels‚Äîtheaccumulation ofsquaredgradientsfrom thebeginningoftrainingcanresultinaprematureandexcessivedecreaseinthe eÔ¨Äectivelearningrate.AdaGradperformswellforsomebutnotalldeeplearning models. 8.5.2RMSProp TheRMSPropalgorithm(,)modiÔ¨ÅesAdaGradtoperformbetterin Hinton2012 thenon-convexsettingbychangingthegradientaccumulation intoanexponentially weightedmovingaverage.AdaGradisdesignedtoconvergerapidlywhenapplied toaconvexfunction. When appliedtoanon-convexfunctiontotrainaneural network,thelearningtrajectorymaypassthroughmanydiÔ¨Äerentstructuresand eventuallyarriveataregionthatisalocallyconvexbowl.AdaGradshrinksthe learningrateaccordingtotheentirehistoryofthesquaredgradientandmay 3 0 7 CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS Algorithm8.4TheAdaGradalgorithm Require:Globallearningrate ÓÄè Require:InitialparameterŒ∏ Require:Smallconstant,perhaps Œ¥ 10‚àí 7,fornumericalstability Initializegradientaccumulationvariabler= 0 while do stoppingcriterionnotmet Sampleaminibatchof mexamplesfromthetrainingset{x( 1 ), . . . ,x( ) m}with correspondingtargetsy( ) i. Computegradient:g‚Üê1 m‚àá Œ∏ÓÅê i L f((x( ) i;)Œ∏ ,y( ) i) Accumulatesquaredgradient:rrgg ‚Üê+ÓÄå Computeupdate: ‚àÜŒ∏‚Üê‚àíÓÄè Œ¥ +‚àörÓÄåg.(Divisionandsquarerootapplied element-wise) Applyupdate:Œ∏Œ∏Œ∏ ‚Üê +‚àÜ endwhile havemadethelearningratetoosmallbeforearrivingatsuchaconvexstructure. RMSPropusesanexponentiallydecayingaveragetodiscardhistoryfromthe extremepastsothatitcanconvergerapidlyafterÔ¨Åndingaconvexbowl,asifit wereaninstanceoftheAdaGradalgorithminitializedwithinthatbowl. RMSPropisshowninitsstandardforminalgorithm andcombinedwith 8.5 Nesterovmomentuminalgorithm .ComparedtoAdaGrad,theuseofthe 8.6 movingaverageintroducesanewhyperparameter, œÅ,thatcontrolsthelengthscale ofthemovingaverage. Empirically,RMSProphasbeenshowntobeaneÔ¨Äectiveandpracticalop- timizationalgorithmfordeepneuralnetworks.Itiscurrentlyoneofthego-to optimization methodsbeingemployedroutinelybydeeplearningpractitioners. 8.5.3Adam Adam( ,)isyetanotheradaptivelearningrateoptimization KingmaandBa2014 algorithmandispresentedinalgorithm .Thename‚ÄúAdam‚Äù derivesfrom 8.7 thephrase‚Äúadaptivemoments.‚ÄùInthecontextoftheearlieralgorithms,itis perhapsbestseenasavariantonthecombinationofRMSPropandmomentum withafewimportantdistinctions.First,inAdam,momentumisincorporated directlyasanestimateoftheÔ¨Årstordermoment(withexponentialweighting)of thegradient.ThemoststraightforwardwaytoaddmomentumtoRMSPropisto applymomentumtotherescaledgradients.Theuseofmomentumincombination withrescalingdoesnothaveacleartheoreticalmotivation.Second,Adamincludes 3 0 8 CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS Algorithm8.5TheRMSPropalgorithm Require:Globallearningrate,decayrate. ÓÄè œÅ Require:InitialparameterŒ∏ Require:Smallconstant Œ¥, usually 10‚àí 6, usedtostabilizedivision bysmall numbers. Initializeaccumulation variablesr= 0 while do stoppingcriterionnotmet Sampleaminibatchof mexamplesfromthetrainingset{x( 1 ), . . . ,x( ) m}with correspondingtargetsy( ) i. Computegradient:g‚Üê1 m‚àá Œ∏ÓÅê i L f((x( ) i;)Œ∏ ,y( ) i) Accumulatesquaredgradient:rrgg ‚Üê œÅ+(1 )‚àí œÅÓÄå Computeparameterupdate: ‚àÜŒ∏=‚àíÓÄè‚àö Œ¥ + rÓÄåg.(1‚àö Œ¥ + rappliedelement-wise) Applyupdate:Œ∏Œ∏Œ∏ ‚Üê +‚àÜ endwhile biascorrectionstotheestimatesofboththeÔ¨Årst-ordermoments(themomentum term)andthe(uncentered)second-ordermomentstoaccountfortheirinitialization attheorigin(seealgorithm ).RMSPropalsoincorporatesanestimateofthe 8.7 (uncentered)second-ordermoment,howeveritlacksthecorrectionfactor.Thus, unlikeinAdam,theRMSPropsecond-ordermomentestimatemayhavehighbias earlyintraining.Adamisgenerallyregardedasbeingfairlyrobusttothechoice ofhyperparameters ,thoughthelearningratesometimesneedstobechangedfrom thesuggesteddefault. 8.5.4ChoosingtheRightOptimizationAlgorithm Inthissection,wediscussedaseriesofrelatedalgorithmsthateachseektoaddress thechallengeofoptimizingdeepmodelsbyadaptingthelearningrateforeach modelparameter.Atthispoint,anaturalquestionis:whichalgorithmshouldone choose? Unfortunately,thereiscurrentlynoconsensusonthispoint. () Schauletal.2014 presentedavaluablecomparisonofalargenumberofoptimization algorithms acrossawiderangeoflearningtasks.Whiletheresultssuggestthatthefamilyof algorithmswithadaptivelearningrates(representedbyRMSPropandAdaDelta) performedfairlyrobustly,nosinglebestalgorithmhasemerged. Currently,themostpopularoptimization algorithmsactivelyinuseinclude SGD,SGDwithmomentum,RMSProp,RMSPropwithmomentum,AdaDelta andAdam.Thechoiceofwhichalgorithmtouse,atthispoint,seemstodepend 3 0 9 CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS Algorithm8.6RMSPropalgorithmwithNesterovmomentum Require:Globallearningrate,decayrate,momentumcoeÔ¨Écient. ÓÄè œÅ Œ± Require:Initialparameter,initialvelocity. Œ∏ v Initializeaccumulation variabler= 0 while do stoppingcriterionnotmet Sampleaminibatchof mexamplesfromthetrainingset{x( 1 ), . . . ,x( ) m}with correspondingtargetsy( ) i. Computeinterimupdate: ÀúŒ∏Œ∏v ‚Üê + Œ± Computegradient:g‚Üê1 m‚àá Àú Œ∏ÓÅê i L f((x( ) i;ÀúŒ∏y) ,( ) i) Accumulategradient:rrgg ‚Üê œÅ+(1 )‚àí œÅÓÄå Computevelocityupdate:vv‚Üê Œ±‚àíÓÄè‚àörÓÄåg.(1‚àörappliedelement-wise) Applyupdate:Œ∏Œ∏v ‚Üê + endwhile largelyontheuser‚Äôsfamiliaritywiththealgorithm(foreaseofhyperparameter tuning). 8.6ApproximateSecond-OrderMethods Inthissectionwediscusstheapplicationofsecond-ordermethodstothetraining ofdeepnetworks.See ()foranearliertreatmentofthissubject. LeCunetal.1998a Forsimplicityofexposition,theonlyobjectivefunctionweexamineistheempirical risk: J() = Œ∏ E x ,y ‚àº ÀÜ pdata ( ) x , y[((;))] = L fxŒ∏ , y1 mm ÓÅò i = 1L f((x( ) i;)Œ∏ , y( ) i) .(8.25) Howeverthemethodswediscusshereextendreadilytomoregeneralobjective functionsthat,forinstance,includeparameterregularizationtermssuchasthose discussedinchapter.7 8.6.1Newton‚ÄôsMethod Insection,weintroducedsecond-ordergradientmethods.IncontrasttoÔ¨Årst- 4.3 ordermethods,second-ordermethodsmakeuseofsecondderivativestoimprove optimization. Themostwidelyusedsecond-ordermethodisNewton‚Äôsmethod.We nowdescribeNewton‚Äôsmethodinmoredetail,withemphasisonitsapplicationto neuralnetworktraining. 3 1 0 CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS Algorithm8.7TheAdamalgorithm Require:Stepsize(Suggesteddefault: ) ÓÄè 0001 . Require:Exponentialdecayratesformomentestimates, œÅ 1and œÅ 2in[0 ,1). (Suggesteddefaults:andrespectively) 09 . 0999 . Require:Smallconstant Œ¥usedfornumericalstabilization.(Suggesteddefault: 10‚àí 8) Require:InitialparametersŒ∏ Initialize1stand2ndmomentvariables ,s= 0r= 0 Initializetimestep t= 0 while do stoppingcriterionnotmet Sampleaminibatchof mexamplesfromthetrainingset{x( 1 ), . . . ,x( ) m}with correspondingtargetsy( ) i. Computegradient:g‚Üê1 m‚àá Œ∏ÓÅê i L f((x( ) i;)Œ∏ ,y( ) i) t t‚Üê+1 UpdatebiasedÔ¨Årstmomentestimate:s‚Üê œÅ 1s+(1‚àí œÅ 1)g Updatebiasedsecondmomentestimate:r‚Üê œÅ 2r+(1‚àí œÅ 2)ggÓÄå CorrectbiasinÔ¨Årstmoment:ÀÜs‚Üês 1 ‚àí œÅt 1 Correctbiasinsecondmoment:ÀÜr‚Üêr 1 ‚àí œÅt 2 Computeupdate: ‚àÜ= Œ∏‚àí ÓÄèÀÜs‚àö ÀÜ r +</div>
        </div>
    </div>

    <div class="question-card" id="q8">
        <div class="question-header">
            <span class="question-number">Question 8</span>
            <span class="question-type">multiple-choice</span>
        </div>

        <div class="question-text">Linear algebra provides foundational tools for solving systems of equations and analyzing data in mathematics and machine learning. Understanding matrix properties such as invertibility, norms, and decomposition is crucial for both theoretical and practical applications.

Which statement is true regarding orthogonal matrices in linear algebra?

1) Their columns must be linearly dependent and have zero norm.   
2) Their inverse does not exist unless the matrix is diagonal.   
3) Multiplying by an orthogonal matrix changes the length of any vector.   
4) They are always singular and cannot be used in eigendecomposition.   
5) Their eigenvalues are always zero.   
6) Their inverse is equal to their transpose, and they preserve lengths and angles.   
7) They must have only nonzero entries on the diagonal.</div>

        <div class="answer-section">
            <div class="answer-label">‚úì Correct Answer:</div>
            <div class="answer-text">The correct answer is 6) Their inverse is equal to their transpose, and they preserve lengths and angles..</div>
        </div>

        <div class="reference-section">
            <div class="reference-label">üìö Reference Text:</div>
            <button class="toggle-button" onclick="toggleReference(8)">
                Show/Hide Reference
            </button>
            <div id="ref8" class="reference-text hidden">m , n x n= b m . (2.19) Matrix-vectorproductnotationprovidesamorecompactrepresentationfor equationsofthisform. 2.3IdentityandInverseMatrices LinearalgebraoÔ¨Äersapowerfultoolcalledmatrixinversionthatallowsusto analyticallysolveequationformanyvaluesof. 2.11 A Todescribematrixinversion,weÔ¨ÅrstneedtodeÔ¨Ånetheconceptofanidentity matrix.Anidentitymatrixisamatrixthatdoesnotchangeanyvectorwhenwe multiplythatvectorbythatmatrix.Wedenotetheidentitymatrixthatpreserves n-dimensionalvectorsasI n.Formally,I n‚àà Rn n √ó,and ‚àÄ‚ààx Rn,I nxx= . (2.20) Thestructureoftheidentitymatrixissimple:alloftheentriesalongthemain diagonalare1,whilealloftheotherentriesarezero.SeeÔ¨Ågureforanexample.2.2 ThematrixinverseofAisdenotedasA‚àí 1,anditisdeÔ¨Ånedasthematrix suchthat A‚àí 1AI= n . (2.21) Wecannowsolveequationbythefollowingsteps: 2.11 Axb= (2.22) A‚àí 1AxA= ‚àí 1b (2.23) I nxA= ‚àí 1b (2.24) 3 6 CHAPTER2.LINEARALGEBRA xA= ‚àí 1b . (2.25) Ofcourse,thisprocessdependsonitbeingpossibletoÔ¨ÅndA‚àí 1.Wediscuss theconditionsfortheexistenceofA‚àí 1inthefollowingsection. WhenA‚àí 1exists,severaldiÔ¨ÄerentalgorithmsexistforÔ¨Åndingitinclosedform. Intheory,thesameinversematrixcanthenbeusedtosolvetheequationmany timesfordiÔ¨Äerentvaluesofb.However,A‚àí 1isprimarilyusefulasatheoretical tool,andshouldnotactuallybeusedinpracticeformostsoftwareapplications. BecauseA‚àí 1canberepresentedwithonlylimitedprecisiononadigitalcomputer, algorithmsthatmakeuseofthevalueofbcanusuallyobtainmoreaccurate estimatesof.x 2.4LinearDependenceandSpan InorderforA‚àí 1toexist,equationmusthaveexactlyonesolutionforevery 2.11 valueofb.However,itisalsopossibleforthesystemofequationstohaveno solutionsorinÔ¨Ånitelymanysolutionsforsomevaluesofb. Itisnotpossibleto havemorethanonebutlessthaninÔ¨Ånitelymanysolutionsforaparticularb;if bothandaresolutionsthen xy zxy = Œ±+(1 )‚àí Œ± (2.26) isalsoasolutionforanyreal. Œ± Toanalyzehowmanysolutionstheequationhas,wecanthinkofthecolumns ofAasspecifyingdiÔ¨Äerentdirectionswecantravelfromtheorigin(thepoint speciÔ¨Åedbythevectorofallzeros),anddeterminehowmanywaysthereareof reachingb.Inthisview,eachelementofxspeciÔ¨Åeshowfarweshouldtravelin eachofthesedirections,with x ispecifyinghowfartomoveinthedirectionof column: i Ax=ÓÅò ix iA : , i . (2.27) Ingeneral,thiskindofoperationiscalledalinearcombination.Formally,a linearcombinationofsomesetofvectors{v( 1 ), . . . ,v( ) n}isgivenbymultiplying eachvectorv( ) ibyacorrespondingscalarcoeÔ¨Écientandaddingtheresults: ÓÅò ic iv( ) i. (2.28) Thespanofasetofvectorsisthesetofallpointsobtainablebylinearcombination oftheoriginalvectors. 3 7 CHAPTER2.LINEARALGEBRA DeterminingwhetherAx=bhasasolutionthusamountstotestingwhetherb isinthespanofthecolumnsofA.Thisparticularspanisknownasthecolumn spacerangeortheof.A InorderforthesystemAx=btohaveasolutionforallvaluesofb‚àà Rm, wethereforerequirethatthecolumnspaceofAbeallof Rm.Ifanypointin Rm isexcludedfromthecolumnspace,thatpointisapotentialvalueofbthathas nosolution.TherequirementthatthecolumnspaceofAbeallof Rmimplies immediately thatAmusthaveatleast mcolumns,i.e., n m‚â•. Otherwise, the dimensionalityofthecolumnspacewouldbelessthan m.Forexample,considera 3√ó2matrix.Thetargetbis3-D,butxisonly2-D,somodifyingthevalueofx atbestallowsustotraceouta2-Dplanewithin R3.Theequationhasasolution ifandonlyifliesonthatplane.b Having n m‚â•isonlyanecessaryconditionforeverypointtohaveasolution. ItisnotasuÔ¨Écientcondition,becauseitispossibleforsomeofthecolumnsto beredundant.Considera2√ó2matrixwherebothofthecolumnsareidentical. Thishasthesamecolumnspaceasa2√ó1matrixcontainingonlyonecopyofthe replicatedcolumn.Inotherwords,thecolumnspaceisstilljustaline,andfailsto encompassallof R2,eventhoughtherearetwocolumns. Formally,thiskindofredundancyisknownaslineardependence.Asetof vectorsislinearlyindependentifnovectorinthesetisalinearcombination oftheothervectors. Ifweaddavectortoasetthatisalinearcombinationof theothervectorsintheset,thenewvectordoesnotaddanypointstotheset‚Äôs span.Thismeansthatforthecolumnspaceofthematrixtoencompassallof Rm, thematrixmustcontainatleastonesetof mlinearlyindependentcolumns.This conditionisbothnecessaryandsuÔ¨Écientforequationtohaveasolutionfor 2.11 everyvalueofb.Notethattherequirementisforasettohaveexactly mlinear independentcolumns,notatleast m.Nosetof m-dimensionalvectorscanhave morethan mmutuallylinearlyindependentcolumns,butamatrixwithmorethan mcolumnsmayhavemorethanonesuchset. Inorderforthematrixtohaveaninverse,weadditionallyneedtoensurethat equationhasonesolutionforeachvalueof 2.11 atmost b.Todoso,weneedto ensurethatthematrixhasatmost mcolumns.Otherwisethereismorethanone wayofparametrizing eachsolution. Together,thismeansthatthematrixmustbesquare,thatis,werequirethat m= nandthatallofthecolumnsmustbelinearlyindependent.Asquarematrix withlinearlydependentcolumnsisknownas.singular IfAisnotsquareorissquarebutsingular,itcanstillbepossibletosolvethe equation.However,wecannotusethemethodofmatrixinversiontoÔ¨Åndthe 3 8 CHAPTER2.LINEARALGEBRA solution. Sofarwehavediscussedmatrixinversesasbeingmultipliedontheleft.Itis alsopossibletodeÔ¨Åneaninversethatismultipliedontheright: AA‚àí 1= I . (2.29) Forsquarematrices,theleftinverseandrightinverseareequal. 2.5Norms Sometimesweneedtomeasurethesizeofavector.Inmachinelearning,weusually measurethesizeofvectorsusingafunctioncalledanorm.Formally,the Lpnorm isgivenby ||||x p=ÓÄ†ÓÅò i| x i|pÓÄ° 1 p (2.30) for p , p . ‚àà R‚â•1 Norms,includingthe Lpnorm,arefunctionsmappingvectorstonon-negative values.Onanintuitivelevel,thenormofavectorxmeasuresthedistancefrom theorigintothepointx.Morerigorously,anormisanyfunction fthatsatisÔ¨Åes thefollowingproperties: ‚Ä¢ ‚áí f() = 0 xx= 0 ‚Ä¢ ‚â§ f(+) xy f f ()+x ()y(thetriangleinequality) ‚Ä¢‚àÄ‚àà || Œ± R , f Œ±(x) = Œ± f()x The L2norm,with p= 2,isknownastheEuclideannorm.Itissimplythe EuclideandistancefromtheorigintothepointidentiÔ¨Åedbyx.The L2normis usedsofrequentlyinmachinelearningthatitisoftendenotedsimplyas||||x,with thesubscriptomitted.Itisalsocommontomeasurethesizeofavectorusing 2 thesquared L2norm,whichcanbecalculatedsimplyasxÓÄæx. Thesquared L2normismoreconvenienttoworkwithmathematically and computationally thanthe L2normitself.Forexample,thederivativesofthe squared L2normwithrespecttoeachelementofxeachdependonlyonthe correspondingelementofx,whileallofthederivativesofthe L2normdepend ontheentirevector.Inmanycontexts,thesquared L2normmaybeundesirable becauseitincreasesveryslowlyneartheorigin.Inseveralmachinelearning 3 9 CHAPTER2.LINEARALGEBRA applications,itisimportanttodiscriminatebetweenelementsthatareexactly zeroandelementsthataresmallbutnonzero.Inthesecases,weturntoafunction thatgrowsatthesamerateinalllocations,butretainsmathematical simplicity: the L1norm.The L1normmaybesimpliÔ¨Åedto ||||x 1=ÓÅò i| x i| . (2.31) The L1normiscommonlyusedinmachinelearningwhenthediÔ¨Äerencebetween zeroandnonzeroelementsisveryimportant.Everytimeanelementofxmoves awayfrom0by,the ÓÄè L1normincreasesby. ÓÄè Wesometimesmeasurethesizeofthevectorbycountingitsnumberofnonzero elements.Someauthorsrefertothisfunctionasthe‚Äú L0norm,‚Äùbutthisisincorrect terminology.Thenumberofnon-zeroentriesinavectorisnotanorm,because scalingthevectorby Œ±doesnotchangethenumberofnonzeroentries. The L1 normisoftenusedasasubstituteforthenumberofnonzeroentries. Oneothernormthatcommonlyarisesinmachinelearningisthe L‚àûnorm, alsoknownasthemaxnorm.ThisnormsimpliÔ¨Åestotheabsolutevalueofthe elementwiththelargestmagnitudeinthevector, ||||x ‚àû= max i| x i| . (2.32) Sometimeswemayalsowishtomeasurethesizeofamatrix.Inthecontext ofdeeplearning,themostcommonwaytodothisiswiththeotherwiseobscure Frobeniusnorm: |||| A F=ÓÅ≥ÓÅò i , jA2 i , j , (2.33) whichisanalogoustothe L2normofavector. Thedotproductoftwovectorscanberewrittenintermsofnorms.SpeciÔ¨Åcally, xÓÄæyx= |||| 2||||y 2cos Œ∏ (2.34) whereistheanglebetweenand. Œ∏ xy 2.6SpecialKindsofMatricesandVectors Somespecialkindsofmatricesandvectorsareparticularlyuseful. Diagonalmatricesconsistmostlyofzerosandhavenon-zeroentriesonlyalong themaindiagonal. Formally,amatrixDisdiagonalifandonlyif D i , j=0for 4 0 CHAPTER2.LINEARALGEBRA all iÓÄ∂= j. Wehavealreadyseenoneexampleofadiagonalmatrix: theidentity matrix,whereallofthediagonalentriesare1.Wewritediag(v) todenoteasquare diagonalmatrixwhosediagonalentriesaregivenbytheentriesofthevectorv. Diagonalmatricesareofinterestinpartbecausemultiplyingbyadiagonalmatrix isverycomputationally eÔ¨Écient.Tocomputediag(v)x,weonlyneedtoscaleeach element x iby v i.Inotherwords,diag(v)x=vxÓÄå.Invertingasquarediagonal matrixisalsoeÔ¨Écient.Theinverseexistsonlyifeverydiagonalentryisnonzero, andinthatcase,diag(v)‚àí 1=diag([1 /v 1 , . . . ,1 /v n]ÓÄæ).Inmanycases,wemay derivesomeverygeneralmachinelearningalgorithmintermsofarbitrarymatrices, butobtainalessexpensive(andlessdescriptive)algorithmbyrestrictingsome matricestobediagonal. Notalldiagonalmatricesneedbesquare.Itispossibletoconstructarectangular diagonalmatrix.Non-squarediagonalmatricesdonothaveinversesbutitisstill possibletomultiplybythemcheaply.Foranon-squarediagonalmatrixD,the productDxwillinvolvescalingeachelementofx,andeitherconcatenating some zerostotheresultifDistallerthanitiswide,ordiscardingsomeofthelast elementsofthevectorifiswiderthanitistall. D A matrixisanymatrixthatisequaltoitsowntranspose: symmetric AA= ÓÄæ. (2.35) Symmetricmatricesoftenarisewhentheentriesaregeneratedbysomefunctionof twoargumentsthatdoesnotdependontheorderofthearguments.Forexample, ifAisamatrixofdistancemeasurements,withA i , jgivingthedistancefrompoint itopoint,then jA i , j= A j , ibecausedistancefunctionsaresymmetric. A isavectorwith : unitvectorunitnorm ||||x 2= 1 . (2.36) AvectorxandavectoryareorthogonaltoeachotherifxÓÄæy= 0.Ifboth vectorshavenonzeronorm,thismeansthattheyareata90degreeangletoeach other.In Rn,atmost nvectorsmaybemutuallyorthogonalwithnonzeronorm. Ifthevectorsarenotonlyorthogonalbutalsohaveunitnorm,wecallthem orthonormal. Anorthogonalmatrixisasquarematrixwhoserowsaremutuallyorthonor- malandwhosecolumnsaremutuallyorthonormal: AÓÄæAAA= ÓÄæ= I . (2.37) 4 1 CHAPTER2.LINEARALGEBRA Thisimpliesthat A‚àí 1= AÓÄæ, (2.38) soorthogonalmatricesareofinterestbecausetheirinverseisverycheaptocompute. PaycarefulattentiontothedeÔ¨Ånitionoforthogonalmatrices.Counterintuitively, theirrowsarenotmerelyorthogonalbutfullyorthonormal. Thereisnospecial termforamatrixwhoserowsorcolumnsareorthogonalbutnotorthonormal. 2.7Eigendecomposition Manymathematical objectscanbeunderstoodbetterbybreakingtheminto constituentparts,orÔ¨Åndingsomepropertiesofthemthatareuniversal,notcaused bythewaywechoosetorepresentthem. Forexample,integerscanbedecomposedintoprimefactors.Thewaywe representthenumberwillchangedependingonwhetherwewriteitinbaseten 12 orinbinary,butitwillalwaysbetruethat12 = 2√ó2√ó3.Fromthisrepresentation wecanconcludeusefulproperties,suchasthatisnotdivisibleby,orthatany 12 5 integermultipleofwillbedivisibleby. 12 3 Muchaswecandiscoversomethingaboutthetruenatureofanintegerby decomposingitintoprimefactors,wecanalsodecomposematricesinwaysthat showusinformationabouttheirfunctionalpropertiesthatisnotobviousfromthe representationofthematrixasanarrayofelements. Oneofthemostwidelyusedkindsofmatrixdecompositioniscalledeigen- decomposition,inwhichwedecomposeamatrixintoasetofeigenvectorsand eigenvalues. AneigenvectorofasquarematrixAisanon-zerovectorvsuchthatmulti- plicationbyaltersonlythescaleof: A v Avv= Œª .</div>
        </div>
    </div>

    <div class="question-card" id="q9">
        <div class="question-header">
            <span class="question-number">Question 9</span>
            <span class="question-type">multiple-choice</span>
        </div>

        <div class="question-text">Monte Carlo estimation is a fundamental technique in computational statistics and machine learning for approximating expectations by averaging function evaluations over samples. Importance sampling is used when direct sampling from the base distribution is challenging, and its effectiveness relies heavily on the choice of the proposal distribution.

Which property of the optimal importance sampling distribution guarantees the lowest variance for estimating an expected value, and why is it generally impractical to use directly in real-world applications?

1) It matches the mode of \( p(x) \) only, making sampling computationally efficient.   
2) It is always uniform over the support of \( f(x) \), simplifying normalization.   
3) It maximizes entropy, ensuring broad coverage of the sample space.   
4) It minimizes the number of samples required by matching the support of \( p(x) \).   
5) It is proportional to \( p(x)^2 \), optimizing coverage in high-density regions.   
6) It closely matches the shape of \( |p(x)f(x)| \), but requires knowledge of the expectation being estimated, which is typically unavailable.   
7) It equals the marginal likelihood, allowing direct calculation of confidence intervals.</div>

        <div class="answer-section">
            <div class="answer-label">‚úì Correct Answer:</div>
            <div class="answer-text">The correct answer is 6) It closely matches the shape of \( |p(x)f(x)| \), but requires knowledge of the expectation being estimated, which is typically unavailable..</div>
        </div>

        <div class="reference-section">
            <div class="reference-label">üìö Reference Text:</div>
            <button class="toggle-button" onclick="toggleReference(9)">
                Show/Hide Reference
            </button>
            <div id="ref9" class="reference-text hidden">x( 1 ), . . . , x( ) nfrom pandthen formingtheempiricalaverage ÀÜ s n=1 nn ÓÅò i = 1f( x( ) i) . (17.3) Thisapproximation isjustiÔ¨ÅedbyafewdiÔ¨Äerentproperties.TheÔ¨Årsttrivial observationisthattheestimator ÀÜ sisunbiased,since E[ÀÜ s n] =1 nn ÓÅò i = 1E[( f x( ) i)] =1 nn ÓÅò i = 1s s .= (17.4) Butinaddition,thelawoflargenumbersstatesthatifthesamples x( ) iare i.i.d.,thentheaverageconvergesalmostsurelytotheexpectedvalue: limn‚Üí‚àûÀÜ s n= s , (17.5) 5 9 1 CHAPTER17.MONTECARLOMETHODS providedthatthevarianceoftheindividualterms,Var[ f( x( ) i)],isbounded.Tosee thismoreclearly,considerthevarianceofÀÜ s nas nincreases.Thevariance Var[ÀÜ s n] decreasesandconvergesto0,solongasVar[( f x( ) i)] <‚àû: Var[ÀÜ s n] =1 n2nÓÅò i = 1Var[()] f x (17.6) =Var[()] f x n. (17.7) ThisconvenientresultalsotellsushowtoestimatetheuncertaintyinaMonte CarloaverageorequivalentlytheamountofexpectederroroftheMonteCarlo approximation.Wecomputeboththeempiricalaverageofthe f( x( ) i)andtheir empiricalvariance,1andthendividetheestimatedvariancebythenumberof samples ntoobtainanestimatorofVar[ÀÜ s n]. Thecentrallimittheoremtells usthatthedistributionoftheaverage, ÀÜ s n,convergestoanormaldistribution withmean sandvarianceV a r [ ( ) ] f x n.ThisallowsustoestimateconÔ¨Ådenceintervals aroundtheestimate ÀÜ s n,usingthecumulativedistributionofthenormaldensity. However,allthisreliesonourabilitytoeasilysamplefromthebasedistribution p( x),butdoingsoisnotalwayspossible.Whenitisnotfeasibletosamplefrom p,analternativeistouseimportancesampling,presentedinsection.A17.2 moregeneralapproachistoformasequenceofestimatorsthatconvergetowards thedistributionofinterest.ThatistheapproachofMonteCarloMarkovchains (section).17.3 17.2ImportanceSampling Animportantstepinthedecompositionoftheintegrand(orsummand)usedbythe MonteCarlomethodinequationisdecidingwhichpartoftheintegrandshould 17.2 playtheroletheprobability p( x)andwhichpartoftheintegrandshouldplaythe roleofthequantity f( x) whoseexpectedvalue(underthatprobabilitydistribution) istobeestimated.Thereisnouniquedecompositionbecause p( x) f( x)canalways berewrittenas p f q () x() = x () xp f() x() x q() x, (17.8) wherewenowsamplefrom qandaveragep f q.Inmanycases,wewishtocompute anexpectationforagiven pandan f,andthefactthattheproblemisspeciÔ¨Åed 1Th e u n b i a s e d e s t i m a t o r o f t h e v a ria n c e i s o f t e n p re f e rre d , i n wh i c h t h e s u m o f s q u a re d d i Ô¨Ä e re n c e s i s d i v i d e d b y i n s t e a d o f . n ‚àí 1 n 5 9 2 CHAPTER17.MONTECARLOMETHODS fromthestartasanexpectationsuggeststhatthis pand fwouldbeanatural choiceofdecomposition.However,theoriginalspeciÔ¨Åcationoftheproblemmay notbethetheoptimalchoiceintermsofthenumberofsamplesrequiredtoobtain agivenlevelofaccuracy. Fortunately,theformoftheoptimalchoice q‚àócanbe derivedeasily.Theoptimal q‚àócorrespondstowhatiscalledoptimalimportance sampling. Becauseoftheidentityshowninequation,anyMonteCarloestimator 17.8 ÀÜ s p=1 nn ÓÅò i , = 1 x( ) i‚àº pf( x( ) i) (17.9) canbetransformedintoanimportancesamplingestimator ÀÜ s q=1 nn ÓÅò i , = 1 x( ) i‚àº qp( x( ) i)( f x( ) i) q( x( ) i). (17.10) Weseereadilythattheexpectedvalueoftheestimatordoesnotdependon: q E q[ÀÜ s q] = E q[ÀÜ s p] = s . (17.11) However,thevarianceofanimportancesamplingestimatorcanbegreatlysensitive tothechoiceof.Thevarianceisgivenby q Var[ÀÜ s q] = Var[p f() x() x q() x] /n . (17.12) Theminimumvarianceoccurswhenis q q‚àó() = xp f() x|() x| Z, (17.13) where Zisthenormalization constant,chosensothat q‚àó( x)sumsorintegratesto 1asappropriate.Betterimportancesamplingdistributionsputmoreweightwhere theintegrandislarger.Infact,when f( x)doesnotchangesign,Var[ÀÜ s q‚àó]=0, meaningthat whentheoptimaldistributionisused. a s i ng l e s a m p l e i s s u Ô¨É c i e nt Ofcourse,thisisonlybecausethecomputationof q‚àóhasessentiallysolvedthe originalproblem,soitisusuallynotpracticaltousethisapproachofdrawinga singlesamplefromtheoptimaldistribution. Anychoiceofsamplingdistribution qisvalid(inthesenseofyieldingthe correctexpectedvalue)and q‚àóistheoptimalone(inthesenseofyieldingminimum variance).Samplingfrom q‚àóisusuallyinfeasible,butotherchoicesof qcanbe feasiblewhilestillreducingthevariancesomewhat. 5 9 3 CHAPTER17.MONTECARLOMETHODS Anotherapproachistousebiasedimportancesampling,whichhasthe advantageofnotrequiringnormalized por q.Inthecaseofdiscretevariables,the biasedimportancesamplingestimatorisgivenby ÀÜ s B I S=ÓÅên i = 1p ( x( ) i) q ( x( ) i )f( x( ) i) ÓÅên i = 1p ( x( ) i ) q ( x( ) i )(17.14) =ÓÅên i = 1p ( x( ) i) Àú q ( x( ) i)f( x( ) i) ÓÅên i = 1p ( x( ) i) Àú q ( x( ) i)(17.15)</div>
        </div>
    </div>

    <div class="question-card" id="q10">
        <div class="question-header">
            <span class="question-number">Question 10</span>
            <span class="question-type">multiple-choice</span>
        </div>

        <div class="question-text">In machine learning, representation learning involves training models to automatically extract useful features from raw data, which can benefit subsequent tasks such as classification, transfer learning, and handling multiple domains. Unsupervised pretraining and modern approaches to representation learning offer various advantages and limitations depending on the data and application.

Which statement best characterizes the primary benefit of greedy layer-wise unsupervised pretraining in early deep neural network training?

1) It enables the direct optimization of the final layer for improved classification accuracy.   
2) It ensures that each layer independently maximizes sparsity in its learned features.   
3) It provides effective parameter initialization and regularization, helping deep networks avoid poor local minima and vanishing gradients when labeled data is limited.   
4) It guarantees that all layers of the network remain fully independent of one another during training.   
5) It eliminates the need for any supervised fine-tuning after unsupervised training is complete.   
6) It is most effective when input data is already richly represented, such as image vectors.   
7) It always improves model performance regardless of the task or data characteristics.</div>

        <div class="answer-section">
            <div class="answer-label">‚úì Correct Answer:</div>
            <div class="answer-text">The correct answer is 3) It provides effective parameter initialization and regularization, helping deep networks avoid poor local minima and vanishing gradients when labeled data is limited..</div>
        </div>

        <div class="reference-section">
            <div class="reference-label">üìö Reference Text:</div>
            <button class="toggle-button" onclick="toggleReference(10)">
                Show/Hide Reference
            </button>
            <div id="ref10" class="reference-text hidden">C h a p t e r 1 5 Represen t at i on L e ar n i n g Inthischapter,weÔ¨Årstdiscusswhatitmeanstolearnrepresentationsandhow thenotionofrepresentationcanbeusefultodesigndeeparchitectures.Wediscuss howlearningalgorithmssharestatisticalstrengthacrossdiÔ¨Äerenttasks,including usinginformationfromunsupervisedtaskstoperformsupervisedtasks.Shared representationsareusefultohandlemultiplemodalitiesordomains,ortotransfer learnedknowledgetotasksforwhichfewornoexamplesaregivenbutatask representationexists.Finally,westepbackandargueaboutthereasonsforthe successofrepresentationlearning,startingwiththetheoreticaladvantagesof distributedrepresentations(Hinton1986etal.,)anddeeprepresentationsand endingwiththemoregeneralideaofunderlyingassumptionsaboutthedata generatingprocess,inparticularaboutunderlyingcausesoftheobserveddata. ManyinformationprocessingtaskscanbeveryeasyorverydiÔ¨Écultdepending onhowtheinformationisrepresented.Thisisageneralprincipleapplicableto dailylife,computerscienceingeneral,andtomachinelearning.Forexample,it isstraightforwardforapersontodivide210by6usinglongdivision. Thetask becomesconsiderablylessstraightforwardifitisinsteadposedusingtheRoman numeralrepresentationofthenumbers.MostmodernpeopleaskedtodivideCCX byVIwouldbeginbyconvertingthenumberstotheArabicnumeralrepresentation, permittinglongdivisionproceduresthatmakeuseoftheplacevaluesystem.More concretely,wecanquantifytheasymptoticruntimeofvariousoperationsusing appropriateorinappropriate representations.Forexample,insertinganumber intothecorrectpositioninasortedlistofnumbersisanO(n)operationifthe listisrepresentedasalinkedlist,butonlyO(logn)ifthelistisrepresentedasa red-blacktree. Inthecontextofmachinelearning,whatmakesonerepresentationbetterthan 526 CHAPTER15.REPRESENTATIONLEARNING another?Generallyspeaking,agoodrepresentationisonethatmakesasubsequent learningtaskeasier.Thechoiceofrepresentationwillusuallydependonthechoice ofthesubsequentlearningtask. Wecanthinkoffeedforwardnetworkstrainedbysupervisedlearningasper- formingakindofrepresentationlearning.SpeciÔ¨Åcally,thelastlayerofthenetwork istypicallyalinearclassiÔ¨Åer,suchasasoftmaxregressionclassiÔ¨Åer.Therestof thenetworklearnstoprovidearepresentationtothisclassiÔ¨Åer.Trainingwitha supervisedcriterionnaturallyleadstotherepresentationateveryhiddenlayer(but moresonearthetophiddenlayer)takingonpropertiesthatmaketheclassiÔ¨Åcation taskeasier.Forexample,classesthatwerenotlinearlyseparableintheinput featuresmaybecomelinearlyseparableinthelasthiddenlayer.Inprinciple,the lastlayercouldbeanotherkindofmodel,suchasanearestneighborclassiÔ¨Åer (SalakhutdinovandHinton2007a,).Thefeaturesinthepenultimatelayershould learndiÔ¨Äerentpropertiesdependingonthetypeofthelastlayer. Supervisedtrainingoffeedforwardnetworksdoesnotinvolveexplicitlyimposing anyconditiononthelearnedintermediatefeatures.Otherkindsofrepresentation learningalgorithmsareoftenexplicitlydesignedtoshapetherepresentationin someparticularway.Forexample,supposewewanttolearnarepresentationthat makesdensityestimationeasier.Distributionswithmoreindependencesareeasier tomodel,sowecoulddesignanobjectivefunctionthatencouragestheelements oftherepresentationvectorhtobeindependent.Justlikesupervisednetworks, unsuperviseddeeplearningalgorithmshaveamaintrainingobjectivebutalso learnarepresentationasasideeÔ¨Äect.Regardlessofhowarepresentationwas obtained,itcanbeusedforanothertask.Alternatively,multipletasks(some supervised,someunsupervised)canbelearnedtogetherwithsomesharedinternal representation. MostrepresentationlearningproblemsfaceatradeoÔ¨Äbetweenpreservingas muchinformationabouttheinputaspossibleandattainingniceproperties(such asindependence). Representationlearningisparticularlyinterestingbecauseitprovidesone waytoperformunsupervisedandsemi-supervisedlearning.Weoftenhavevery largeamountsofunlabeledtrainingdataandrelativelylittlelabeledtraining data.Trainingwithsupervisedlearningtechniquesonthelabeledsubsetoften resultsinsevereoverÔ¨Åtting.Semi-supervisedlearningoÔ¨Äersthechancetoresolve thisoverÔ¨Åttingproblembyalsolearningfromtheunlabeleddata.SpeciÔ¨Åcally, wecanlearngoodrepresentationsfortheunlabeleddata,andthenusethese representationstosolvethesupervisedlearningtask. Humansandanimalsareabletolearnfromveryfewlabeledexamples.Wedo 5 2 7 CHAPTER15.REPRESENTATIONLEARNING notyetknowhowthisispossible.Manyfactorscouldexplainimprovedhuman performance‚Äîforexample,thebrainmayuseverylargeensemblesofclassiÔ¨Åers orBayesianinferencetechniques.Onepopularhypothesisisthatthebrainis abletoleverageunsupervisedorsemi-supervisedlearning.Therearemanyways toleverageunlabeleddata.Inthischapter,wefocusonthehypothesisthatthe unlabeleddatacanbeusedtolearnagoodrepresentation. 15. 1 Greed y L a y er-Wi s e Un s u p ervi s ed Pret ra i n i n g Unsupervisedlearningplayedakeyhistoricalroleintherevivalofdeepneural networks,enablingresearchersfortheÔ¨Årsttimetotrainadeepsupervisednetwork withoutrequiringarchitectural specializationslikeconvolutionorrecurrence.We callthisprocedure unsup e r v i se d pr e t r ai ni n g,ormoreprecisely, g r e e dy l a y e r - wi se unsup e r v i se d pr e t r ai ni n g.Thisprocedureisacanonicalexampleofhow arepresentationlearnedforonetask(unsupervisedlearning,tryingtocapture theshapeoftheinputdistribution)cansometimesbeusefulforanothertask (supervisedlearningwiththesameinputdomain). Greedylayer-wiseunsupervisedpretrainingreliesonasingle-layerrepresen- tationlearningalgorithmsuchasanRBM,asingle-layerautoencoder,asparse codingmodel,oranothermodelthatlearnslatentrepresentations.Eachlayeris pretrainedusingunsupervisedlearning,takingtheoutputofthepreviouslayer andproducingasoutputanewrepresentationofthedata,whosedistribution(or itsrelationtoothervariablessuchascategoriestopredict)ishopefullysimpler. Seealgorithm foraformaldescription. 15.1 Greedylayer-wisetrainingproceduresbasedonunsupervisedcriteriahavelong beenusedtosidestepthediÔ¨Écultyofjointlytrainingthelayersofadeepneuralnet forasupervisedtask.ThisapproachdatesbackatleastasfarastheNeocognitron (Fukushima1975,).Thedeeplearningrenaissanceof2006beganwiththediscovery thatthisgreedylearningprocedurecouldbeusedtoÔ¨Åndagoodinitialization for ajointlearningprocedureoverallthelayers,andthatthisapproachcouldbeused tosuccessfullytrainevenfullyconnectedarchitectures (Hinton2006Hinton etal.,; andSalakhutdinov2006Hinton2006Bengio2007Ranzato 2007a ,;,; etal.,; etal.,). Priortothisdiscovery,onlyconvolutionaldeepnetworksornetworkswhosedepth resultedfromrecurrencewereregardedasfeasibletotrain.Today,wenowknow thatgreedylayer-wisepretrainingisnotrequiredtotrainfullyconnecteddeep architectures,buttheunsupervisedpretrainingapproachwastheÔ¨Årstmethodto succeed. Greedylayer-wisepretrainingiscalled g r e e dybecauseitisa g r e e dy al g o - 5 2 8 CHAPTER15.REPRESENTATIONLEARNING r i t hm,meaningthatitoptimizeseachpieceofthesolutionindependently,one pieceatatime,ratherthanjointlyoptimizingallpieces.Itiscalled l a y e r - wi se becausetheseindependentpiecesarethelayersofthenetwork.SpeciÔ¨Åcally,greedy layer-wisepretrainingproceedsonelayeratatime,trainingthek-thlayerwhile keepingthepreviousonesÔ¨Åxed.Inparticular,thelowerlayers(whicharetrained Ô¨Årst)arenotadaptedaftertheupperlayersareintroduced.Itiscalled unsup e r - v i se dbecauseeachlayeristrainedwithanunsupervisedrepresentationlearning algorithm.Howeveritisalsocalled pr e t r ai ni n g,becauseitissupposedtobe onlyaÔ¨Årststepbeforeajointtrainingalgorithmisappliedto Ô¨Åne-t uneallthe layerstogether.Inthecontextofasupervisedlearningtask,itcanbeviewed asaregularizer(insomeexperiments,pretrainingdecreasestesterrorwithout decreasingtrainingerror)andaformofparameterinitialization. Itiscommontousetheword‚Äúpretraining‚Äùtorefernotonlytothepretraining stageitselfbuttotheentiretwophaseprotocolthatcombinesthepretraining phaseandasupervisedlearningphase.Thesupervisedlearningphasemayinvolve trainingasimpleclassiÔ¨Åerontopofthefeatureslearnedinthepretrainingphase, oritmayinvolvesupervisedÔ¨Åne-tuningoftheentirenetworklearnedinthe pretrainingphase.Nomatterwhatkindofunsupervisedlearningalgorithmor whatmodeltypeisemployed,inthevastmajorityofcases,theoveralltraining schemeisnearlythesame.Whilethechoiceofunsupervisedlearningalgorithm willobviouslyimpactthedetails,mostapplicationsofunsupervisedpretraining followthisbasicprotocol. Greedylayer-wiseunsupervisedpretrainingcanalsobeusedasinitialization forotherunsupervisedlearningalgorithms,suchasdeepautoencoders(Hinton andSalakhutdino v2006,)andprobabilisticmodelswithmanylayersoflatent variables.Suchmodelsincludedeepbeliefnetworks( ,)anddeep Hintonetal.2006 Boltzmannmachines(SalakhutdinovandHinton2009a,).Thesedeepgenerative modelswillbedescribedinchapter.20 Asdiscussedinsection, itisalsopossibletohavegreedylayer-wise 8.7.4 supervisedpretraining.Thisbuildsonthepremisethattrainingashallownetwork iseasierthantrainingadeepone,whichseemstohavebeenvalidatedinseveral contexts(,). Erhanetal.2010 1 5 . 1 . 1 Wh en a n d Wh y D o es Un s u p ervi s ed P ret ra i n i n g W o rk? Onmanytasks,greedylayer-wiseunsupervisedpretrainingcanyieldsubstantial improvementsintesterrorforclassiÔ¨Åcationtasks.Thisobservationwasresponsible fortherenewedinterestedindeepneuralnetworksstartingin2006(Hintonetal., 5 2 9 CHAPTER15.REPRESENTATIONLEARNING Al g o r i t hm 1 5 . 1Greedylayer-wiseunsupervisedpretrainingprotocol. Giventhefollowing: Unsupervisedfeaturelearningalgorithm L,whichtakesa trainingsetofexamplesandreturnsanencoderorfeaturefunctionf.Theraw inputdataisX,withonerowperexampleandf( 1 )(X)istheoutputoftheÔ¨Årst stageencoderonX.InthecasewhereÔ¨Åne-tuningisperformed,weusealearner Twhichtakesaninitialfunctionf,inputexamplesX(andinthesupervised Ô¨Åne-tuningcase,associatedtargetsY),andreturnsatunedfunction.Thenumber ofstagesis.m f‚ÜêIdentityfunction ÀúXX= f o r dok,...,m = 1 f( ) k= (LÀúX) ff‚Üê( ) k‚ó¶f ÀúX‚Üêf( ) k(ÀúX) e nd f o r i fÔ¨Åne-tuning t he n ff,, ‚ÜêT(XY) e nd i f Ret ur nf 2006Bengio2007Ranzato 2007a ; etal.,; etal.,).Onmanyothertasks,however, unsupervisedpretrainingeitherdoesnotconferabeneÔ¨Åtorevencausesnoticeable harm. ()studiedtheeÔ¨Äectofpretrainingonmachinelearning Maetal.2015 modelsforchemicalactivitypredictionandfoundthat,onaverage,pretrainingwas slightlyharmful,butformanytaskswassigniÔ¨Åcantlyhelpful.Becauseunsupervised pretrainingissometimeshelpfulbutoftenharmfulitisimportanttounderstand whenandwhyitworksinordertodeterminewhetheritisapplicabletoaparticular task. Attheoutset,itisimportanttoclarifythatmostofthisdiscussionisrestricted togreedyunsupervisedpretraininginparticular.Thereareother,completely diÔ¨Äerentparadigmsforperformingsemi-supervisedlearningwithneuralnetworks, suchasvirtualadversarialtrainingdescribedinsection.Itisalsopossibleto 7.13 trainanautoencoderorgenerativemodelatthesametimeasthesupervisedmodel. Examplesofthissingle-stageapproachincludethediscriminativeRBM(Larochelle andBengio2008,)andtheladdernetwork( ,),inwhichthetotal Rasmusetal.2015 objectiveisanexplicitsumofthetwoterms(oneusingthelabelsandoneonly usingtheinput). UnsupervisedpretrainingcombinestwodiÔ¨Äerentideas.First,itmakesuseof 5 3 0 CHAPTER15.REPRESENTATIONLEARNING theideathatthechoiceofinitialparametersforadeepneuralnetworkcanhave asigniÔ¨ÅcantregularizingeÔ¨Äectonthemodel(and,toalesserextent,thatitcan improveoptimization). Second,itmakesuseofthemoregeneralideathatlearning abouttheinputdistributioncanhelptolearnaboutthemappingfrominputsto outputs. Bothoftheseideasinvolvemanycomplicatedinteractionsbetweenseveral partsofthemachinelearningalgorithmthatarenotentirelyunderstood. TheÔ¨Årstidea,thatthechoiceofinitialparametersforadeepneuralnetwork canhaveastrongregularizingeÔ¨Äectonitsperformance, istheleastwellunderstood. Atthetimethatpretrainingbecamepopular,itwasunderstoodasinitializingthe modelinalocationthatwouldcauseittoapproachonelocalminimumratherthan another. Today,localminimaarenolongerconsideredtobeaseriousproblem forneuralnetworkoptimization. Wenowknowthatourstandardneuralnetwork trainingproceduresusuallydonotarriveatacriticalpointofanykind.Itremains possiblethatpretraininginitializesthemodelinalocationthatwouldotherwise beinaccessible‚Äîforexample,aregionthatissurroundedbyareaswherethecost functionvariessomuchfromoneexampletoanotherthatminibatchesgiveonly averynoisyestimateofthegradient,oraregionsurroundedbyareaswherethe Hessianmatrixissopoorlyconditionedthatgradientdescentmethodsmustuse verysmallsteps.However,ourabilitytocharacterizeexactlywhataspectsofthe pretrainedparametersareretainedduringthesupervisedtrainingstageislimited. Thisisonereasonthatmodernapproachestypicallyusesimultaneousunsupervised learningandsupervisedlearningratherthantwosequentialstages.Onemay alsoavoidstrugglingwiththesecomplicatedideasabouthowoptimization inthe supervisedlearningstagepreservesinformationfromtheunsupervisedlearning stagebysimplyfreezingthe parameters for thefeature extractorsand using supervisedlearningonlytoaddaclassiÔ¨Åerontopofthelearnedfeatures. Theotheridea,thatalearningalgorithmcanuseinformationlearnedinthe unsupervisedphasetoperformbetterinthesupervisedlearningstage,isbetter understood.Thebasicideaisthatsomefeaturesthatareusefulfortheunsupervised taskmayalsobeusefulforthesupervisedlearningtask.Forexample,ifwetrain agenerativemodelofimagesofcarsandmotorcycles,itwillneedtoknowabout wheels,andabouthowmanywheelsshouldbeinanimage.Ifwearefortunate, therepresentationofthewheelswilltakeonaformthatiseasyforthesupervised learnertoaccess.Thisisnotyetunderstoodatamathematical, theoreticallevel, soitisnotalwayspossibletopredictwhichtaskswillbeneÔ¨Åtfromunsupervised learninginthisway.Manyaspectsofthisapproacharehighlydependenton thespeciÔ¨Åcmodelsused.Forexample,ifwewishtoaddalinearclassiÔ¨Åeron 5 3 1 CHAPTER15.REPRESENTATIONLEARNING topofpretrainedfeatures,thefeaturesmustmaketheunderlyingclasseslinearly separable.Thesepropertiesoftenoccurnaturallybutdonotalwaysdoso.This isanotherreasonthatsimultaneoussupervisedandunsupervisedlearningcanbe preferable‚Äîtheconstraintsimposedbytheoutputlayerarenaturallyincluded fromthestart. Fromthepointofviewofunsupervisedpretrainingaslearningarepresentation, wecanexpectunsupervisedpretrainingtobemoreeÔ¨Äectivewhentheinitial representationispoor. Onekeyexampleofthisistheuseofwordembeddings. Wordsrepresentedbyone-hotvectorsarenotveryinformativebecauseeverytwo distinctone-hotvectorsarethesamedistancefromeachother(squaredL2distance of).Learnedwordembeddingsnaturallyencodesimilaritybetweenwordsbytheir 2 distancefromeachother.Becauseofthis,unsupervisedpretrainingisespecially usefulwhenprocessingwords.Itislessusefulwhenprocessingimages,perhaps becauseimagesalreadylieinarichvectorspacewheredistancesprovidealow qualitysimilaritymetric. Fromthepointofviewofunsupervisedpretrainingasaregularizer,wecan expectunsupervisedpretrainingtobemosthelpfulwhenthenumberoflabeled examplesisverysmall.Becausethesourceofinformationaddedbyunsupervised pretrainingistheunlabeleddata,wemayalsoexpectunsupervisedpretraining toperformbest whenthe number ofunlabeled examples is very large.The</div>
        </div>
    </div>

    <div class="question-card" id="q11">
        <div class="question-header">
            <span class="question-number">Question 11</span>
            <span class="question-type">multiple-choice</span>
        </div>

        <div class="question-text">Machine learning is built on a foundation of concepts from linear algebra, probability, statistics, and optimization, which underpin the development and evaluation of intelligent systems. Understanding the relationships between these concepts is critical for effective model design and analysis.

Which technique is specifically used for uncovering latent variables in mixture models by iteratively estimating hidden assignments and model parameters?

1) Principal component analysis (PCA)   
2) Gradient descent   
3) Logistic regression   
4) K-means clustering   
5) Ensemble methods   
6) Hyperparameter optimization   
7) Expectation-maximization (EM) algorithm </div>

        <div class="answer-section">
            <div class="answer-label">‚úì Correct Answer:</div>
            <div class="answer-text">The correct answer is 7) Expectation-maximization (EM) algorithm.</div>
        </div>

        <div class="reference-section">
            <div class="reference-label">üìö Reference Text:</div>
            <button class="toggle-button" onclick="toggleReference(11)">
                Show/Hide Reference
            </button>
            <div id="ref11" class="reference-text hidden">Designmatrix, 1 0 5 Detectorlayer,336 Determinant,xii Diagonalmatrix,40 DiÔ¨Äerentialentropy,,73641 Diracdeltafunction,64 Directedgraphicalmodel,,,, 76503559685 Directionalderivative,84 DiscriminativeÔ¨Åne-tuning, s e esupervised Ô¨Åne-tuning DiscriminativeRBM,680 Distributedrepresentation,,,17149542 Domainadaptation,532 7 7 9 INDEX Dotproduct,,33139 Doublebackprop,268 Doublyblockcirculantmatrix,330 Dreamsleep,,605647 DropConnect,263 Dropout,,,,,, 2 5 5422427428666683 Dynamicstructure,445 E-step,629 Earlystopping,,,,, 244246270271422 EBM, s e eenergy-basedmodel Echostatenetwork,,,2326401 EÔ¨Äectivecapacity,113 Eigendecomposition,41 Eigenvalue,41 Eigenvector,41 ELBO, s e eevidencelowerbound Element-wiseproduct, s e eHadamardprod- uct, s e eHadamardproduct EM, s e eexpectationmaximization Embedding,512 Empiricaldistribution,65 Empiricalrisk,274 Empiricalriskminimization,274 Encoder,4 Energyfunction,565 Energy-basedmodel,,,, 565591648657 Ensemblemethods,252 Epoch,244 Equalityconstraint,93 Equivariance,335 Errorfunction, s e eobjectivefunction ESN, s e eechostatenetwork Euclideannorm,38 Euler-Lagrangeequation,641 Evidencelowerbound,,628655 Example,98 Expectation,59 Expectationmaximization,629 Expectedvalue, s e eexpectation Explainingaway,,,570626639 Exploitation,477 Exploration,477 Exponentialdistribution, 6 4F-score,420 Factor(graphicalmodel),563 Factoranalysis,486 Factorgraph,575 Factorsofvariation,4 Feature,98 Featureselection,234 Feedforwardneuralnetwork,166 Fine-tuning,321 FinitediÔ¨Äerences,436 Forgetgate,304 Forwardpropagation,201 Fouriertransform,,357359 Fovea,363 FPCD,610 Freeenergy,, 5 6 7674 Freebase,479 Frequentistprobability,54 Frequentiststatistics, 1 3 4 Frobeniusnorm,45 Fully-visibleBayesnetwork,699 Functionalderivatives,640 FVBN, s e efully-visibleBayesnetwork Gaborfunction,365 GANs, s e egenerativeadversarialnetworks Gatedrecurrentunit,422 Gaussiandistribution, s e enormaldistribu- tion Gaussiankernel,140 Gaussianmixture,,66187 GCN, s e eglobalcontrastnormalization GeneOntology,479 Generalization,109 GeneralizedLagrangefunction, s e egeneral- izedLagrangian GeneralizedLagrangian,93 Generativeadversarialnetworks,,683693 Generativemomentmatchingnetworks,696 Generatornetwork,687 Gibbsdistribution,564 Gibbssampling,,577595 Globalcontrastnormalization,451 GPU, s e egraphicsprocessingunit Gradient,83 7 8 0 INDEX Gradientclipping,,287411 Gradientdescent,,8284 Graph,xii Graphicalmodel, s e estructuredprobabilis- ticmodel Graphicsprocessingunit,441 Greedyalgorithm,321 Greedylayer-wiseunsupervisedpretraining, 524 Greedysupervisedpretraining,321 Gridsearch,429 Hadamardproduct,,xii33 Hard,tanh195 Harmonium, s e erestrictedBoltzmannma- chine Harmonytheory,567 Helmholtz freeenergy, s e eevidencelower bound Hessian,221 Hessianmatrix,,xiii86 Heteroscedastic,186 Hiddenlayer,,6166 Hillclimbing,85 Hyperparameteroptimization,429 Hyperparameters,,119427 Hypothesisspace,,111117 i.i.d.assumptions,,,110121265 Identitymatrix,35 ILSVRC, s e eImageNetLargeScaleVisual RecognitionChallenge ImageNetLargeScaleVisualRecognition Challenge,22 Immorality,573 Importancesampling,,,588620691 Importanceweightedautoencoder,691 Independence,,xiii59 Independentandidenticallydistributed, s e e i.i.d.assumptions Independentcomponentanalysis,487 Independentsubspaceanalysis,489 Inequalityconstraint,93 Inference,,,,,,,, 558579626628630633643 646Informationretrieval,520 Initialization,298 Integral,xiii Invariance,339 Isotropic,64 Jacobianmatrix,,,xiii7185 Jointprobability,56 k-means,,361542 k-nearestneighbors,, 1 4 1544 Karush-Kuhn-Tuckerconditions,,94235 Karush‚ÄìKuhn‚ÄìTucker,93 Kernel(convolution),,328329 Kernelmachine,544 Kerneltrick,139 KKT, s e eKarush‚ÄìKuhn‚ÄìTucker KKTconditions, s e eKarush-Kuhn-Tucker conditions KLdivergence, s e eKullback-Leiblerdiver- gence Knowledgebase,,2479 Krylovmethods,222 Kullback-Leiblerdivergence,,xiii 7 3 Labelsmoothing,241 Lagrangemultipliers,,93641 Lagrangian, s e egeneralizedLagrangian LAPGAN,695 Laplacedistribution,, 6 4492 Latentvariable,66 Layer(neuralnetwork),166 LCN, s e elocalcontrastnormalization LeakyReLU,191 Leakyunits,404 Learningrate,84 Linesearch,,,848592 Linearcombination,36 Lineardependence,37 Linearfactormodels,485 Linearregression,,, 1 0 6109138 Linkprediction,480 Lipschitzconstant,91 Lipschitzcontinuous,91 Liquidstatemachine,401 7 8 1 INDEX Localconditionalprobabilitydistribution, 560 Localcontrastnormalization,452 Logisticregression,,,3 1 3 8139 Logisticsigmoid,,766 Longshort-termmemory,,,,1824304 4 0 7, 422 Loop,575 Loopybeliefpropagation,581 Lossfunction, s e eobjectivefunction Lpnorm,38 LSTM, s e elongshort-termmemory M-step,629 Machinelearning,2 Machinetranslation,100 Maindiagonal,32 Manifold,159 Manifoldhypothesis,160 Manifoldlearning,160 ManifoldtangentclassiÔ¨Åer,268 MAPapproximation,,137501 Marginalprobability,57 Markovchain,591 MarkovchainMonteCarlo,591 Markovnetwork, s e eundirectedmodel MarkovrandomÔ¨Åeld, s e eundirectedmodel Matrix,,,xixii31 Matrixinverse,35 Matrixproduct,33 Maxnorm,39 Maxpooling,336 Maximumlikelihood, 1 3 0 Maxout,,191422 MCMC, s e eMarkovchainMonteCarlo MeanÔ¨Åeld,,,633634666 Meansquarederror,107 Measuretheory,70 Measurezero,70 Memorynetwork,,413415 Methodof steepestdescent, s e egradient descent Minibatch,277 Missinginputs,99 Mixing(Markovchain),597Mixturedensitynetworks,187 Mixturedistribution,65 Mixturemodel,,187506 Mixtureofexperts,,446544 MLP, s e emultilayerperception MNIST,,,2021666 Modelaveraging,252 Modelcompression,444 ModelidentiÔ¨Åability,282 Modelparallelism,444 Momentmatching,696 Moore-Penrosepseudoinverse,,44237 Moralizedgraph,573 MP-DBM, s e emulti-predictionDBM MRF(Markov RandomField), s e eundi- rectedmodel MSE, s e emeansquarederror Multi-modallearning,535 Multi-predictionDBM,668 Multi-tasklearning,,242533 Multilayerperception,5 Multilayerperceptron,26 Multinomialdistribution,61 Multinoullidistribution,61 n-gram, 4 5 8 NADE,702 NaiveBayes,3 Nat,72 Naturalimage,555 Naturallanguageprocessing,457 Nearestneighborregression, 1 1 4 NegativedeÔ¨Ånite,88 Negativephase,,,466602604 Neocognitron,,,,162326364 Nesterovmomentum,298 NetÔ¨ÇixGrandPrize,,255475 Neurallanguagemodel,,460472 Neuralnetwork,13 NeuralTuringmachine,415 Neuroscience,15 Newton‚Äôsmethod,,88309 NLM, s e eneurallanguagemodel NLP, s e enaturallanguageprocessing Nofreelunchtheorem,115 7 8 2 INDEX Noise-contrastiveestimation,616 Non-parametricmodel, 1 1 3 Norm,,xiv38 Normaldistribution,,,6263124 Normalequations,,,, 1 0 8108111232 Normalizedinitialization,301 NumericaldiÔ¨Äerentiation, s e eÔ¨ÅnitediÔ¨Äer- ences Objectdetection,449 Objectrecognition,449 Objectivefunction,81 OMP-, k s e eorthogonalmatchingpursuit One-shotlearning,534 Operation,202 Optimization,,7981 Orthodoxstatistics, s e efrequentiststatistics Orthogonalmatchingpursuit,,26 2 5 2 Orthogonalmatrix,41 Orthogonality,40 Outputlayer,166 Paralleldistributedprocessing,17 Parameterinitialization,,298403 Parametersharing,,,,, 249332370372386 Parametertying, s e eParametersharing Parametricmodel, 1 1 3 ParametricReLU,191 Partialderivative,83 Partitionfunction,,,564601663 PCA, s e</div>
        </div>
    </div>

    <div class="question-card" id="q12">
        <div class="question-header">
            <span class="question-number">Question 12</span>
            <span class="question-type">multiple-choice</span>
        </div>

        <div class="question-text">Deep feedforward neural networks are widely used in modern machine learning for tasks such as image classification and natural language processing. Their architecture and training algorithms allow them to learn complex mappings from input to output data.

Which of the following is a key reason why deep feedforward neural networks can solve the XOR problem, while linear models cannot?

1) Deep networks use random weight initialization to generate all possible outputs   
2) Linear models are more prone to overfitting than deep networks   
3) Deep networks require less data to achieve perfect generalization   
4) Linear models can only approximate polynomial functions   
5) Deep networks employ nonlinear activation functions and hidden layers, enabling them to learn nonlinearly separable mappings   
6) Linear models automatically discover hierarchical features in data   
7) Deep networks always contain feedback connections that improve learning</div>

        <div class="answer-section">
            <div class="answer-label">‚úì Correct Answer:</div>
            <div class="answer-text">The correct answer is 5) Deep networks employ nonlinear activation functions and hidden layers, enabling them to learn nonlinearly separable mappings.</div>
        </div>

        <div class="reference-section">
            <div class="reference-label">üìö Reference Text:</div>
            <button class="toggle-button" onclick="toggleReference(12)">
                Show/Hide Reference
            </button>
            <div id="ref12" class="reference-text hidden">P a rt I I D e e p N e t w orks: Mo d e rn Practices 166 Thispartofthebooksummarizesthestateofmoderndeeplearningasitis usedtosolvepracticalapplications. Deeplearninghasalonghistoryandmanyaspirations.Severalapproaches havebeenproposedthathaveyettoentirelybearfruit.Severalambitiousgoals haveyettoberealized.Theseless-developedbranchesofdeeplearningappearin theÔ¨Ånalpartofthebook. Thispartfocusesonlyonthoseapproachesthatareessentiallyworkingtech- nologiesthatarealreadyusedheavilyinindustry. Modern deeplearning provides avery powerful framework forsupervised learning.Byaddingmorelayersandmoreunitswithinalayer,adeepnetworkcan representfunctionsofincreasingcomplexity.Mosttasksthatconsistofmappingan inputvectortoanoutputvector,andthatareeasyforapersontodorapidly,can beaccomplishedviadeeplearning,givensuÔ¨ÉcientlylargemodelsandsuÔ¨Éciently largedatasetsoflabeledtrainingexamples.Othertasks,thatcannotbedescribed asassociatingonevectortoanother,orthatarediÔ¨Écultenoughthataperson wouldrequiretimetothinkandreÔ¨Çectinordertoaccomplishthetask,remain beyondthescopeofdeeplearningfornow. Thispartofthebookdescribesthecoreparametricfunctionapproximation technologythatisbehindnearlyallmodernpracticalapplicationsofdeeplearning. We begin by describingthe feedforward deepnetworkmodelthatisusedto representthesefunctions.Next,wepresentadvancedtechniquesforregularization andoptimization ofsuchmodels.Scalingthesemodelstolargeinputssuchashigh resolutionimagesorlongtemporalsequencesrequiresspecialization.Weintroduce theconvolutionalnetworkforscalingtolargeimagesandtherecurrentneural networkforprocessingtemporalsequences.Finally,wepresentgeneralguidelines forthepracticalmethodologyinvolvedindesigning,building,andconÔ¨Åguringan applicationinvolvingdeeplearning,andreviewsomeoftheapplicationsofdeep learning. Thesechaptersarethemostimportantforapractitioner‚Äîsomeone whowants tobeginimplementingandusingdeeplearningalgorithmstosolvereal-world problemstoday. 1 6 7 C h a p t e r 6 D e e p F e e d f orw ard N e t w orks Deepfeedforwardnetworks,alsooftencalledfeedforwardneuralnetworks, ormultilayerperceptrons(MLPs),arethequintessentialdeeplearningmodels. Thegoalofafeedforwardnetworkistoapproximatesomefunction f‚àó.Forexample, foraclassiÔ¨Åer, y= f‚àó(x)mapsaninputxtoacategory y.Afeedforwardnetwork deÔ¨Ånesamappingy= f(x;Œ∏)andlearnsthevalueoftheparametersŒ∏thatresult inthebestfunctionapproximation. ThesemodelsarecalledfeedforwardbecauseinformationÔ¨Çowsthroughthe functionbeingevaluatedfromx,throughtheintermediate computations usedto deÔ¨Åne f,andÔ¨Ånallytotheoutputy.Therearenofeedbackconnectionsinwhich outputsofthemodelarefedbackintoitself.Whenfeedforwardneuralnetworks areextendedtoincludefeedbackconnections,theyarecalledrecurrentneural networks,presentedinchapter.10 Feedforwardnetworksareofextremeimportancetomachinelearningpracti- tioners.Theyformthebasisofmanyimportantcommercialapplications.For example,theconvolutionalnetworksusedforobjectrecognitionfromphotosarea specializedkindoffeedforwardnetwork.Feedforwardnetworksareaconceptual steppingstoneonthepathtorecurrentnetworks,whichpowermanynatural languageapplications. Feedforwardneuralnetworksarecallednetworksbecausetheyaretypically representedbycomposingtogethermanydiÔ¨Äerentfunctions.Themodelisasso- ciatedwithadirectedacyclicgraphdescribinghowthefunctionsarecomposed together.Forexample,wemighthavethreefunctions f( 1 ), f( 2 ),and f( 3 )connected inachain,toform f(x) = f( 3 )( f( 2 )( f( 1 )(x))).Thesechainstructuresarethemost commonlyusedstructuresofneuralnetworks.Inthiscase, f( 1 )iscalledtheÔ¨Årst layerofthenetwork, f( 2 )iscalledthesecondlayer,andsoon.Theoverall 168 CHAPTER6.DEEPFEEDFORWARDNETWORKS lengthofthechaingivesthedepthofthemodel.Itisfromthisterminologythat thename‚Äúdeeplearning‚Äùarises.TheÔ¨Ånallayerofafeedforwardnetworkiscalled theoutputlayer.Duringneuralnetworktraining,wedrive f(x)tomatch f‚àó(x). Thetrainingdataprovidesuswithnoisy,approximateexamplesof f‚àó(x) evaluated atdiÔ¨Äerenttrainingpoints.Eachexamplexisaccompanied byalabel y f‚âà‚àó(x). Thetrainingexamplesspecifydirectlywhattheoutputlayermustdoateachpoint x;itmustproduceavaluethatiscloseto y.Thebehavioroftheotherlayersis notdirectlyspeciÔ¨Åedbythetrainingdata. Thelearningalgorithmmustdecide howtousethoselayerstoproducethedesiredoutput,butthetrainingdatadoes notsaywhateachindividuallayershoulddo.Instead,thelearningalgorithmmust decidehowtousetheselayerstobestimplementanapproximation of f‚àó.Because thetrainingdatadoesnotshowthedesiredoutputforeachoftheselayers,these layersarecalledhiddenlayers. Finally,thesenetworksarecalled ne u r a lbecausetheyarelooselyinspiredby neuroscience.Eachhiddenlayerofthenetworkistypicallyvector-valued.The dimensionalityofthesehiddenlayersdeterminesthewidthofthemodel.Each elementofthevectormaybeinterpretedasplayingaroleanalogoustoaneuron. Ratherthanthinkingofthelayerasrepresentingasinglevector-to-vectorfunction, wecanalsothinkofthelayerasconsistingofmanyunitsthatactinparallel, eachrepresentingavector-to-scalarfunction.Eachunitresemblesaneuronin thesensethatitreceivesinputfrommanyotherunitsandcomputesitsown activationvalue. Theideaofusingmanylayersofvector-valuedrepresentation isdrawnfromneuroscience.Thechoiceofthefunctions f( ) i(x)usedtocompute theserepresentationsisalsolooselyguidedbyneuroscientiÔ¨Åcobservationsabout thefunctionsthatbiologicalneuronscompute.However,modernneuralnetwork researchisguidedbymanymathematical andengineeringdisciplines,andthe goalofneuralnetworksisnottoperfectlymodelthebrain.Itisbesttothinkof feedforwardnetworksasfunctionapproximation machinesthataredesignedto achievestatisticalgeneralization, occasionallydrawingsomeinsightsfromwhatwe knowaboutthebrain,ratherthanasmodelsofbrainfunction. Onewaytounderstandfeedforwardnetworksistobeginwithlinearmodels andconsiderhowtoovercometheirlimitations. Linearmodels,suchaslogistic regressionandlinearregression,areappealingbecausetheymaybeÔ¨ÅteÔ¨Éciently andreliably,eitherinclosedformorwithconvexoptimization. Linearmodelsalso havetheobviousdefectthatthemodelcapacityislimitedtolinearfunctions,so themodelcannotunderstandtheinteractionbetweenanytwoinputvariables. Toextendlinearmodelstorepresentnonlinearfunctionsofx,wecanapply thelinearmodelnottoxitselfbuttoatransformedinput œÜ(x),where œÜisa 1 6 9 CHAPTER6.DEEPFEEDFORWARDNETWORKS nonlineartransformation.Equivalently,wecanapplythekerneltrickdescribedin section,toobtainanonlinearlearningalgorithmbasedonimplicitlyapplying 5.7.2 the œÜmapping.Wecanthinkof œÜasprovidingasetoffeaturesdescribingx,or asprovidinganewrepresentationfor.x Thequestionisthenhowtochoosethemapping. œÜ 1.Oneoptionistouseaverygeneric œÜ,suchastheinÔ¨Ånite-dimens ional œÜthat isimplicitlyusedbykernelmachinesbasedontheRBFkernel. If œÜ(x)is ofhighenoughdimension,wecanalwayshaveenoughcapacitytoÔ¨Åtthe trainingset,butgeneralization tothetestsetoftenremainspoor.Very genericfeaturemappingsareusuallybasedonlyontheprincipleoflocal smoothnessanddonotencodeenoughpriorinformationtosolveadvanced problems. 2.Anotheroptionistomanuallyengineer œÜ.Untiltheadventofdeeplearning, thiswasthedominantapproach.Thisapproachrequiresdecadesofhuman eÔ¨Äortfor eachseparate task, withpractitioners specializing in diÔ¨Äerent domainssuchasspeech recognition or computer vision, and with little transferbetweendomains. 3.Thestrategyofdeeplearningistolearn œÜ.Inthisapproach,wehaveamodel y= f(x;Œ∏w ,) = œÜ(x;Œ∏)ÓÄæw.WenowhaveparametersŒ∏thatweusetolearn œÜfromabroadclassoffunctions,andparameterswthatmapfrom œÜ(x)to thedesiredoutput.Thisisanexampleofadeepfeedforwardnetwork,with œÜdeÔ¨Åningahiddenlayer. Thisapproachistheonlyoneofthethreethat givesupontheconvexityofthetrainingproblem,butthebeneÔ¨Åtsoutweigh theharms.Inthisapproach,weparametrizetherepresentationas œÜ(x;Œ∏) andusetheoptimization algorithmtoÔ¨ÅndtheŒ∏thatcorrespondstoagood representation.Ifwewish,thisapproachcancapturethebeneÔ¨ÅtoftheÔ¨Årst approachbybeinghighlygeneric‚Äîwedosobyusingaverybroadfamily œÜ(x;Œ∏).ThisapproachcanalsocapturethebeneÔ¨Åtofthesecondapproach. Humanpractitioners canencodetheirknowledgetohelpgeneralization by designingfamilies œÜ(x;Œ∏)thattheyexpectwillperformwell.Theadvantage isthatthehumandesigneronlyneedstoÔ¨Åndtherightgeneralfunction familyratherthanÔ¨Åndingpreciselytherightfunction. Thisgeneralprincipleofimprovingmodelsbylearningfeaturesextendsbeyond thefeedforwardnetworksdescribedinthischapter.Itisarecurringthemeofdeep learningthatappliestoallofthekindsofmodelsdescribedthroughoutthisbook. Feedforwardnetworksaretheapplicationofthisprincipletolearningdeterministic 1 7 0 CHAPTER6.DEEPFEEDFORWARDNETWORKS mappingsfromxtoythatlackfeedbackconnections. Othermodelspresented laterwillapplytheseprinciplestolearningstochasticmappings,learningfunctions withfeedback,andlearningprobabilitydistributionsoverasinglevector. Webeginthischapterwithasimpleexampleofafeedforwardnetwork.Next, weaddresseachofthedesigndecisionsneededtodeployafeedforwardnetwork. First,trainingafeedforwardnetworkrequiresmakingmanyofthesamedesign decisionsasarenecessaryforalinearmodel:choosingtheoptimizer,thecost function,andtheformoftheoutputunits.Wereviewthesebasicsofgradient-based learning,thenproceedtoconfrontsomeofthedesigndecisionsthatareunique tofeedforwardnetworks.Feedforwardnetworkshaveintroducedtheconceptofa hiddenlayer,andthisrequiresustochoosetheactivationfunctionsthatwill beusedtocomputethehiddenlayervalues.Wemustalsodesignthearchitecture ofthenetwork,includinghowmanylayersthenetworkshouldcontain,howthese layersshould beconnectedto each other, and howmanyunitsshould bein eachlayer.Learningindeepneuralnetworksrequirescomputingthegradients ofcomplicatedfunctions.Wepresenttheback-propagationalgorithmandits moderngeneralizations ,whichcanbeusedtoeÔ¨Écientlycomputethesegradients. Finally,weclosewithsomehistoricalperspective. 6. 1 E x am p l e: L earni n g X O R Tomaketheideaofafeedforwardnetworkmoreconcrete,webeginwithan exampleofafullyfunctioningfeedforwardnetworkonaverysimpletask:learning theXORfunction. TheXORfunction(‚Äúexclusiveor‚Äù)isanoperationontwobinaryvalues, x 1 and x 2.Whenexactlyoneofthesebinaryvaluesisequalto,theXORfunction 1 returns.Otherwise,itreturns0.TheXORfunctionprovidesthetargetfunction 1 y= f‚àó(x)thatwewanttolearn.Ourmodelprovidesafunction y= f(x;Œ∏)and ourlearningalgorithmwilladapttheparametersŒ∏tomake fassimilaraspossible to f‚àó. Inthissimpleexample,wewillnotbeconcernedwithstatisticalgeneralization. Wewantournetworktoperformcorrectlyonthefourpoints X={[0 ,0]ÓÄæ,[0 ,1]ÓÄæ, [1 ,0]ÓÄæ,and[1 ,1]ÓÄæ}. Wewilltrainthenetworkonallfourofthesepoints. The onlychallengeistoÔ¨Åtthetrainingset. Wecantreatthisproblemasaregressionproblemanduseameansquared errorlossfunction.Wechoosethislossfunctiontosimplifythemathforthis exampleasmuchaspossible.Inpracticalapplications,MSEisusuallynotan 1 7 1 CHAPTER6.DEEPFEEDFORWARDNETWORKS appropriatecostfunctionformodelingbinarydata.Moreappropriateapproaches aredescribedinsection.6.2.2.2 Evaluatedonourwholetrainingset,theMSElossfunctionis J() =Œ∏1 4ÓÅò x‚àà X( f‚àó() (;))x‚àí fxŒ∏2. (6.1) Nowwemustchoosetheformofourmodel, f(x;Œ∏).Supposethatwechoose alinearmodel,withconsistingofand.OurmodelisdeÔ¨Ånedtobe Œ∏w b f , b (;xw) = xÓÄæw+ b . (6.2) Wecanminimize J(Œ∏)inclosedformwithrespecttowand busingthenormal equations. Aftersolvingthenormalequations,weobtainw= 0and b=1 2. Thelinear modelsimplyoutputs 0 .5everywhere.Whydoesthishappen?Figureshows6.1 howalinearmodelisnotabletorepresenttheXORfunction.Onewaytosolve thisproblemistouseamodelthatlearnsadiÔ¨Äerentfeaturespaceinwhicha linearmodelisabletorepresentthesolution. SpeciÔ¨Åcally,wewillintroduceaverysimplefeedforwardnetworkwithone hiddenlayercontainingtwohiddenunits.SeeÔ¨Ågureforanillustrationof 6.2 thismodel.Thisfeedforwardnetworkhasavectorofhiddenunitshthatare computedbyafunction f( 1 )(x;Wc ,).Thevaluesofthesehiddenunitsarethen usedastheinputforasecondlayer.Thesecondlayeristheoutputlayerofthe network.Theoutputlayerisstilljustalinearregressionmodel,butnowitis appliedtohratherthantox.Thenetworknowcontainstwofunctionschained together:h= f( 1 )(x;Wc ,)and y= f( 2 )(h;w , b),withthecompletemodelbeing f , , , b f (;xWcw) = ( 2 )( f( 1 )())x . Whatfunctionshould f( 1 )compute?Linearmodelshaveserveduswellsofar, anditmaybetemptingtomake f( 1 )belinearaswell.Unfortunately,if f( 1 )were linear,thenthefeedforwardnetworkasawholewouldremainalinearfunctionof itsinput.Ignoringtheintercepttermsforthemoment,suppose f( 1 )(x) =WÓÄæx and f( 2 )(h) =hÓÄæw.Then f(x) =wÓÄæWÓÄæx.Wecouldrepresentthisfunctionas f() = xxÓÄæwÓÄ∞wherewÓÄ∞= Ww. Clearly,wemustuseanonlinearfunctiontodescribethefeatures.Mostneural networksdosousinganaÔ¨Énetransformationcontrolledbylearnedparameters, followedbyaÔ¨Åxed,nonlinearfunctioncalledanactivationfunction.Weusethat strategyhere,bydeÔ¨Åningh= g(WÓÄæx+c) ,whereWprovidestheweightsofa lineartransformationandcthebiases.Previously,todescribealinearregression 1 7 2 CHAPTER6.DEEPFEEDFORWARDNETWORKS 0 1 x 101x 2O r</div>
        </div>
    </div>

    <div class="question-card" id="q13">
        <div class="question-header">
            <span class="question-number">Question 13</span>
            <span class="question-type">multiple-choice</span>
        </div>

        <div class="question-text">In deep learning, the design of layers within convolutional neural networks affects model efficiency, parameter count, and ability to learn spatial features. Several layer types exist, including convolutional, locally connected, and fully connected layers, each with distinct properties regarding parameter sharing and connectivity.

Which layer type enables location-specific feature extraction by assigning a unique set of weights to every spatial position, resulting in no weight sharing across locations?

1) Fully connected layer   
2) Group convolutional layer   
3) Depthwise separable convolutional layer   
4) Standard convolutional layer   
5) Tiled convolutional layer   
6) Locally connected layer   
7) Dilated convolutional layer</div>

        <div class="answer-section">
            <div class="answer-label">‚úì Correct Answer:</div>
            <div class="answer-text">The correct answer is 6) Locally connected layer.</div>
        </div>

        <div class="reference-section">
            <div class="reference-label">üìö Reference Text:</div>
            <button class="toggle-button" onclick="toggleReference(13)">
                Show/Hide Reference
            </button>
            <div id="ref13" class="reference-text hidden">z 1 z 1 z 3 z 3 x 4 x 4z 4 z 4 x 5 x 5z 5 z 5s 1 s 1 s 2 s 2 s 3 s 3St r i de d c onv ol ut i on D ow nsampl i n g C onv ol ut i on Figure 9.12:Convolution witha stride.Inthisexample,we use astride oftwo. ( T o p )Convolutionwithastridelengthoftwoimplementedinasingleoperation. ( Bot- t o m )Convolutionwithastridegreaterthanonepixelismathematicallyequivalentto convolutionwithunitstridefollowedbydownsampling.Obviously,thetwo-stepapproach involvingdownsamplingiscomputationallywasteful,becauseitcomputesmanyvalues thatarethendiscarded. 3 5 0 CHAPTER9.CONVOLUTIONALNETWORKS . . . . . .. . . . . . . . .. . . . . .. . . . . . Figure9.13: T h e e Ô¨Ä e c t o f z e r o p a d d i n g o n n e t w o r k s i z e:Consideraconvolutionalnetwork withakernelofwidthsixateverylayer.Inthisexample,wedonotuseanypooling,so onlytheconvolutionoperationitselfshrinksthenetworksize. ( T o p )Inthisconvolutional network,wedonotuseanyimplicitzeropadding.Thiscausestherepresentationto shrinkbyÔ¨Åvepixelsateachlayer.Startingfromaninputofsixteenpixels,weareonly abletohavethreeconvolutionallayers,andthelastlayerdoesnotevermovethekernel, soarguablyonlytwoofthelayersaretrulyconvolutional.Therateofshrinkingcan bemitigatedbyusingsmallerkernels,butsmallerkernelsarelessexpressiveandsome shrinkingisinevitableinthiskindofarchitecture. ByaddingÔ¨Åveimplicitzeroes ( Bottom ) toeachlayer,wepreventtherepresentationfromshrinkingwithdepth.Thisallowsusto makeanarbitrarilydeepconvolutionalnetwork. 3 5 1 CHAPTER9.CONVOLUTIONALNETWORKS Insomecases,wedonotactuallywanttouseconvolution,butratherlocally connectedlayers(,,).Inthiscase,theadjacencymatrixinthe LeCun19861989 graphofourMLPisthesame,buteveryconnectionhasitsownweight,speciÔ¨Åed bya6-Dtensor W. Theindicesinto Warerespectively: i,theoutputchannel, j,theoutputrow, k,theoutputcolumn, l,theinputchannel, m,therowoÔ¨Äset withintheinput,and n,thecolumnoÔ¨Äsetwithintheinput.Thelinearpartofa locallyconnectedlayeristhengivenby Z i , j , k=ÓÅò l , m , n[ V l , j m , k n + ‚àí 1 + ‚àí 1 w i , j , k, l , m , n] . (9.9) Thisissometimesalsocalled unshar e d c o nv o l ut i o n,becauseitisasimilaroper- ationtodiscreteconvolutionwithasmallkernel,butwithoutsharingparameters acrosslocations.Figurecompareslocalconnections,convolution,andfull 9.14 connections. Locallyconnectedlayersareusefulwhenweknowthateachfeatureshouldbe afunctionofasmallpartofspace,butthereisnoreasontothinkthatthesame featureshouldoccuracrossallofspace.Forexample,ifwewanttotellifanimage isapictureofaface,weonlyneedtolookforthemouthinthebottomhalfofthe image. Itcanalsobeusefultomakeversionsofconvolutionorlocallyconnectedlayers inwhichtheconnectivityisfurtherrestricted,forexampletoconstraineachoutput channel itobeafunctionofonlyasubsetoftheinputchannels l.Acommon waytodothisistomaketheÔ¨Årst moutputchannelsconnecttoonlytheÔ¨Årst ninputchannels,thesecond moutputchannelsconnecttoonlythesecond n inputchannels,andsoon.SeeÔ¨Ågureforanexample.Modelinginteractions 9.15 betweenfewchannelsallowsthenetworktohavefewerparametersinorderto reducememoryconsumptionandincreasestatisticaleÔ¨Éciency,andalsoreduces theamountofcomputationneededtoperformforwardandback-propagation. It accomplishesthesegoalswithoutreducingthenumberofhiddenunits. T i l e d c o n v o l ut i o n( ,;,)oÔ¨Äersacom- GregorandLeCun2010aLeetal.2010 promisebetweenaconvolutionallayerandalocallyconnectedlayer.Ratherthan learningaseparatesetofweightsatspatiallocation,welearnasetofkernels every thatwerotatethroughaswemovethroughspace.Thismeansthatimmediately neighboringlocationswillhavediÔ¨ÄerentÔ¨Ålters,likeinalocallyconnectedlayer, butthememoryrequirementsforstoringtheparameterswillincreaseonlybya factorofthesizeofthissetofkernels,ratherthanthesizeoftheentireoutput featuremap.SeeÔ¨Ågureforacomparisonoflocallyconnectedlayers,tiled 9.16 convolution,andstandardconvolution. 3 5 2 CHAPTER9.CONVOLUTIONALNETWORKS x 1 x 1 x 2 x 2 x 3 x 3s 2 s 2 s 1 s 1 s 3 s 3 x 4 x 4s 4 s 4 x 5 x 5s 5 s 5 x 1 x 1 x 2 x 2s 1 s 1 s 3 s 3 x 5 x 5s 5 s 5x 1 x 1 x 2 x 2 x 3 x 3s 2 s 2 s 1 s 1 s 3 s 3 x 4 x 4s 4 s 4 x 5 x 5s 5 s 5 a b a b a b a b a a b c d e f g h i x 4 x 4 x 3 x 3s 4 s 4 s 2 s 2 Figure9.14:Comparisonoflocalconnections,convolution,andfullconnections. ( T o p )Alocallyconnectedlayerwithapatchsizeoftwopixels.Eachedgeislabeledwith auniquelettertoshowthateachedgeisassociatedwithitsownweightparameter. ( C e n t e r )Aconvolutionallayerwithakernelwidthoftwopixels.Thismodelhasexactly thesameconnectivityasthelocallyconnectedlayer.ThediÔ¨Äerenceliesnotinwhichunits interactwitheachother,butinhowtheparametersareshared.Thelocallyconnectedlayer hasnoparametersharing.Theconvolutionallayerusesthesametwoweightsrepeatedly acrosstheentireinput,asindicatedbytherepetitionoftheletterslabelingeachedge. ( Bottom )Afullyconnectedlayerresemblesalocallyconnectedlayerinthesensethateach edgehasitsownparameter(therearetoomanytolabelexplicitlywithlettersinthis diagram).However,itdoesnothavetherestrictedconnectivityofthelocallyconnected layer. 3 5 3 CHAPTER9.CONVOLUTIONALNETWORKS I nput T e nsorO ut put T e nsor S p a t i a l c o o r d i n a t e sC h a n n e l c o o r d i n a t e s Figure9.15: AconvolutionalnetworkwiththeÔ¨Årsttwooutputchannelsconnectedto onlytheÔ¨Årsttwoinputchannels,andthesecondtwooutputchannelsconnectedtoonly thesecondtwoinputchannels. 3 5 4 CHAPTER9.CONVOLUTIONALNETWORKS x 1 x 1</div>
        </div>
    </div>

    <div class="question-card" id="q14">
        <div class="question-header">
            <span class="question-number">Question 14</span>
            <span class="question-type">multiple-choice</span>
        </div>

        <div class="question-text">Recurrent Neural Networks (RNNs) are widely used for modeling sequences in domains such as natural language processing, speech recognition, and time series analysis. Various architectural modifications have been introduced to overcome challenges related to context dependency and sequence length.

Which architectural modification to RNNs enables the model to incorporate information from both past and future inputs at each timestep, thereby improving performance in tasks where context from both directions is crucial?

1) Incorporating teacher forcing during training   
2) Using a conditional independence assumption for outputs   
3) Connecting previous outputs directly to the current hidden state   
4) Employing bidirectional RNNs that process the sequence both forward and backward   
5) Restricting input and output sequences to the same length   
6) Replacing RNNs with Transformer-based models   
7) Applying dropout regularization to RNN hidden states</div>

        <div class="answer-section">
            <div class="answer-label">‚úì Correct Answer:</div>
            <div class="answer-text">The correct answer is 4) Employing bidirectional RNNs that process the sequence both forward and backward.</div>
        </div>

        <div class="reference-section">
            <div class="reference-label">üìö Reference Text:</div>
            <button class="toggle-button" onclick="toggleReference(14)">
                Show/Hide Reference
            </button>
            <div id="ref14" class="reference-text hidden">h( t ‚àí 1 )h( t ‚àí 1 )h( ) th( ) th( + 1 ) th( + 1 ) tW W W W s( ) . . .s( ) . . .h( ) . . .h( ) . . .V V VU U U x xy( ) . . .y( ) . . . R R R R R Figure10.9:AnRNNthatmapsaÔ¨Åxed-lengthvectorxintoadistributionoversequences Y.ThisRNNisappropriatefortaskssuchasimagecaptioning,whereasingleimageis usedasinputtoamodelthatthenproducesasequenceofwordsdescribingtheimage. Eachelementy( ) toftheobservedoutputsequenceservesbothasinput(forthecurrent timestep)and,duringtraining,astarget(fortheprevioustimestep). Ratherthanreceivingonlyasinglevectorxasinput,theRNNmayreceive asequenceofvectorsx( ) tasinput.TheRNNdescribedinequationcorre-10.8 spondstoaconditionaldistribution P(y( 1 ), . . . ,y( ) œÑ|x( 1 ), . . . ,x( ) œÑ)thatmakesa conditionalindependence assumptionthatthisdistributionfactorizesas ÓÅô tP(y( ) t|x( 1 ), . . . ,x( ) t) . (10.35) Toremovetheconditionalindependenceassumption,wecanaddconnectionsfrom theoutputattime ttothehiddenunitattime t+1,asshowninÔ¨Ågure.The10.10 modelcanthenrepresentarbitraryprobabilitydistributionsovertheysequence. Thiskindofmodelrepresentingadistributionoverasequencegivenanother 3 9 2 CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS o( t ‚àí 1 )o( t ‚àí 1 )o( ) to( ) to( + 1 ) to( + 1 ) tL( t ‚àí 1 )L( t ‚àí 1 )L( ) tL( ) tL( + 1 ) tL( + 1 ) ty( t ‚àí 1 )y( t ‚àí 1 )y( ) ty( ) ty( +1 ) ty( +1 ) t h( t ‚àí 1 )h( t ‚àí 1 )h( ) th( ) th( + 1 ) th( + 1 ) tW W W W h( ) . . .h( ) . . .h( ) . . .h( ) . . .V V V U U U x( t ‚àí 1 )x( t ‚àí 1 )R x( ) tx( ) tx( + 1 ) tx( + 1 ) tR R Figure10.10: Aconditionalrecurrentneuralnetworkmappingavariable-lengthsequence ofxvaluesintoadistributionoversequencesofyvaluesofthesamelength.Comparedto Ô¨Ågure,thisRNNcontainsconnectionsfromthepreviousoutputtothecurrentstate. 10.3 TheseconnectionsallowthisRNNtomodelanarbitrarydistributionoversequencesofy givensequencesofxofthesamelength.TheRNNofÔ¨Ågureisonlyabletorepresent 10.3 distributionsinwhichtheyvaluesareconditionallyindependentfromeachothergiven thevalues.x 3 9 3 CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS sequencestillhasonerestriction,whichisthatthelengthofbothsequencesmust bethesame.Wedescribehowtoremovethisrestrictioninsection.10.4 o( t ‚àí 1 )o( t ‚àí 1 )o( ) to( ) to( + 1 ) to( + 1 ) tL( t ‚àí 1 )L( t ‚àí 1 )L( ) tL( ) tL( + 1 ) tL( + 1 ) ty( t ‚àí 1 )y( t ‚àí 1 )y( ) ty( ) ty( +1 ) ty( +1 ) t h( t ‚àí 1 )h( t ‚àí 1 )h( ) th( ) th( + 1 ) th( + 1 ) t x( t ‚àí 1 )x( t ‚àí 1 )x( ) tx( ) tx( + 1 ) tx( + 1 ) tg( t ‚àí 1 )g( t ‚àí 1 )g( ) tg( ) tg( +1 ) tg( +1 ) t Figure10.11: Computation ofatypicalbidirectionalrecurrentneuralnetwork,meant tolearntomapinputsequencesxtotargetsequencesy,withloss L( ) tateachstep t. Thehrecurrencepropagatesinformationforwardintime(towardstheright)whilethe grecurrencepropagatesinformationbackwardintime(towardstheleft).Thusateach point t,theoutputunitso( ) tcanbeneÔ¨Åtfromarelevantsummaryofthepastinitsh( ) t inputandfromarelevantsummaryofthefutureinitsg( ) tinput. 10.3BidirectionalRNNs Alloftherecurrentnetworkswehaveconsidereduptonowhavea‚Äúcausal‚Äùstruc- ture,meaningthatthestateattime tonlycapturesinformationfromthepast, x( 1 ), . . . ,x( 1 ) t ‚àí,andthepresentinputx( ) t.Someofthemodelswehavediscussed alsoallowinformationfrompastyvaluestoaÔ¨Äectthecurrentstatewhenthey valuesareavailable. However,inmanyapplicationswewanttooutputapredictionofy( ) twhichmay 3 9 4 CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS dependon t h e w h o l e i npu t s e q u e nc e.Forexample,inspeechrecognition,thecorrect interpretationofthecurrentsoundasaphonememaydependonthenextfew phonemesbecauseofco-articulationandpotentiallymayevendependonthenext fewwordsbecauseofthelinguisticdependenciesbetweennearbywords:ifthere aretwointerpretationsofthecurrentwordthatarebothacousticallyplausible,we mayhavetolookfarintothefuture(andthepast)todisambiguatethem.Thisis alsotrueofhandwritingrecognitionandmanyothersequence-to-sequencelearning tasks,describedinthenextsection. Bidirectionalrecurrentneuralnetworks(orbidirectional RNNs)wereinvented toaddressthatneed(SchusterandPaliwal1997,).Theyhavebeenextremelysuc- cessful(Graves2012,)inapplicationswherethatneedarises,suchashandwriting recognition(Graves2008GravesandSchmidhuber2009 e t a l .,; ,),speechrecogni- tion(GravesandSchmidhuber2005Graves2013 Baldi ,; e t a l .,)andbioinformatics ( e t a l .,).1999 Asthenamesuggests,bidirectionalRNNscombineanRNNthatmovesforward throughtimebeginningfromthestartofthesequencewithanotherRNNthat movesbackwardthroughtimebeginningfromtheendofthesequence.Figure10.11 illustratesthetypicalbidirectional RNN,withh( ) tstandingforthestateofthe sub-RNNthatmovesforwardthroughtimeandg( ) tstandingforthestateofthe sub-RNNthatmovesbackwardthroughtime. Thisallowstheoutputunitso( ) t tocomputearepresentationthatdependson b o t h t h e p a s t</div>
        </div>
    </div>

    <div class="question-card" id="q15">
        <div class="question-header">
            <span class="question-number">Question 15</span>
            <span class="question-type">multiple-choice</span>
        </div>

        <div class="question-text">Efficient deployment of deep learning models across diverse hardware platforms requires careful consideration of computational strategies and hardware capabilities. Specialized techniques and hardware are increasingly used to address challenges in scaling, speed, and energy consumption.

Which hardware approach offers post-production reconfigurability for adapting to evolving neural network architectures while maintaining efficient inference performance?

1) Graphics Processing Units (GPUs)   
2) Central Processing Units (CPUs)   
3) Application-Specific Integrated Circuits (ASICs)   
4) Digital Signal Processors (DSPs)   
5) System-on-Chip (SoC) designs   
6) Analog computing circuits   
7) Field-Programmable Gate Arrays (FPGAs) </div>

        <div class="answer-section">
            <div class="answer-label">‚úì Correct Answer:</div>
            <div class="answer-text">The correct answer is 7) Field-Programmable Gate Arrays (FPGAs).</div>
        </div>

        <div class="reference-section">
            <div class="reference-label">üìö Reference Text:</div>
            <button class="toggle-button" onclick="toggleReference(15)">
                Show/Hide Reference
            </button>
            <div id="ref15" class="reference-text hidden">CHAPTER12.APPLICATIONS bandwidth,GP-GPUsnowoÔ¨Äeranidealplatformforneuralnetworkprogramming. Thisplatformwasrapidlyadoptedbydeeplearningresearcherssoonafteritbecame available(,; ,). Raina e t a l .2009Ciresan e t a l .2010 WritingeÔ¨ÉcientcodeforGP-GPUsremainsadiÔ¨Éculttaskbestlefttospe- cialists. ThetechniquesrequiredtoobtaingoodperformanceonGPUarevery diÔ¨ÄerentfromthoseusedonCPU.Forexample,goodCPU-basedcodeisusually designedtoreadinformationfromthecacheasmuchaspossible.OnGPU,most writablememorylocationsarenotcached,soitcanactuallybefastertocompute thesamevaluetwice,ratherthancomputeitonceandreaditbackfrommemory. GPUcodeisalsoinherentlymulti-threaded andthediÔ¨Äerentthreadsmustbe coordinatedwitheachothercarefully.Forexample,memoryoperationsarefasterif theycanbecoalesced.Coalescedreadsorwritesoccurwhenseveralthreadscan eachreadorwriteavaluethattheyneedsimultaneously,aspartofasinglememory transaction.DiÔ¨ÄerentmodelsofGPUsareabletocoalescediÔ¨Äerentkindsofread orwritepatterns.Typically,memoryoperationsareeasiertocoalesceifamong n threads,thread iaccessesbyte i+ jofmemory,and jisamultipleofsomepower of2. TheexactspeciÔ¨ÅcationsdiÔ¨ÄerbetweenmodelsofGPU.Anothercommon considerationforGPUsismakingsurethateachthreadinagroupexecutesthe sameinstructionsimultaneously.ThismeansthatbranchingcanbediÔ¨Éculton GPU.Threadsaredividedintosmallgroupscalledwarps.Eachthreadinawarp executesthesameinstructionduringeachcycle,soifdiÔ¨Äerentthreadswithinthe samewarpneedtoexecutediÔ¨Äerentcodepaths,thesediÔ¨Äerentcodepathsmust betraversedsequentiallyratherthaninparallel. DuetothediÔ¨ÉcultyofwritinghighperformanceGPUcode,researchersshould structuretheirworkÔ¨ÇowtoavoidneedingtowritenewGPUcodeinordertotest newmodelsoralgorithms.Typically,onecandothisbybuildingasoftwarelibrary ofhighperformanceoperationslikeconvolutionandmatrixmultiplication, then specifyingmodelsintermsofcallstothislibraryofoperations.Forexample,the machinelearninglibraryPylearn2(Goodfellow2013c e t a l .,)speciÔ¨Åesallofits machinelearningalgorithmsintermsofcallstoTheano( ,; Bergstra e t a l .2010 Bastien2012 e t a l .,)andcuda-convnet(,),whichprovidethese Krizhevsky2010 high-performanceoperations.Thisfactoredapproachcanalsoeasesupportfor multiplekindsofhardware.Forexample,thesameTheanoprogramcanrunon eitherCPUorGPU,withoutneedingtochangeanyofthecallstoTheanoitself. OtherlibrarieslikeTensorFlow(,)andTorch( , Abadi e t a l .2015 Collobert e t a l . 2011b)providesimilarfeatures. 4 4 6 CHAPTER12.APPLICATIONS 12.1.3Large-ScaleDistributedImplementations Inmanycases,thecomputational resourcesavailableonasinglemachineare insuÔ¨Écient.Wethereforewanttodistributetheworkloadoftrainingandinference acrossmanymachines. Distributinginferenceissimple,becauseeachinputexamplewewanttoprocess canberunbyaseparatemachine.Thisisknownas .dataparallelism Itisalsopossibletogetmodelparallelism,wheremultiplemachineswork togetheronasingledatapoint,witheachmachinerunningadiÔ¨Äerentpartofthe model.Thisisfeasibleforbothinferenceandtraining. Dataparallelismduringtrainingissomewhatharder.Wecanincreasethesize oftheminibatchusedforasingleSGDstep,butusuallywegetlessthanlinear returnsintermsofoptimization performance.Itwouldbebettertoallowmultiple machinestocomputemultiplegradientdescentstepsinparallel.Unfortunately, thestandarddeÔ¨Ånitionofgradientdescentisasacompletelysequentialalgorithm: thegradientatstepisafunctionoftheparametersproducedbystep. t t‚àí1 Thiscanbesolvedusingasynchronousstochasticgradientdescent(Ben- gio2001Recht2011 e t a l .,; e t a l .,).Inthisapproach,severalprocessorcoresshare thememoryrepresentingtheparameters.Eachcorereadsparameterswithouta lock,thencomputesagradient,thenincrementstheparameterswithoutalock. Thisreducestheaverageamountofimprovementthateachgradientdescentstep yields,becausesomeofthecoresoverwriteeachother‚Äôsprogress,buttheincreased rateofproductionofstepscausesthelearningprocesstobefasteroverall.Dean e t a l .()pioneeredthemulti-machineimplementationofthislock-freeapproach 2012 togradientdescent,wheretheparametersaremanagedbyaparameterserver ratherthanstoredinsharedmemory.Distributedasynchronousgradientdescent remainstheprimarystrategyfortraininglargedeepnetworksandisusedby mostmajordeeplearninggroupsinindustry( ,; Chilimbi e t a l .2014Wu e t a l ., 2015).AcademicdeeplearningresearcherstypicallycannotaÔ¨Äordthesamescale ofdistributedlearningsystemsbutsomeresearchhasfocusedonhowtobuild distributednetworkswithrelativelylow-costhardwareavailableintheuniversity setting( ,). Coates e t a l .2013 12.1.4ModelCompression Inmanycommercialapplications,itismuchmoreimportantthatthetimeand memorycostofrunninginferenceinamachinelearningmodelbelowthanthat thetimeandmemorycostoftrainingbelow.Forapplicationsthatdonotrequire 4 4 7 CHAPTER12.APPLICATIONS personalization,itispossibletotrainamodelonce,thendeployittobeusedby billionsofusers.Inmanycases,theenduserismoreresource-constrainedthan thedeveloper.Forexample,onemighttrainaspeechrecognitionnetworkwitha powerfulcomputercluster,thendeployitonmobilephones. Akeystrategyforreducingthecostofinferenceismodelcompression(Bu- ciluÀáa2006 e t a l .,).Thebasicideaofmodelcompressionistoreplacetheoriginal, expensivemodelwithasmallermodelthatrequireslessmemoryandruntimeto storeandevaluate. Modelcompressionisapplicablewhenthesizeoftheoriginalmodelisdriven primarilybyaneedtopreventoverÔ¨Åtting.Inmostcases,themodelwiththe lowestgeneralization errorisanensembleofseveralindependentlytrainedmodels. Evaluatingall nensemblemembersisexpensive.Sometimes,evenasinglemodel generalizesbetterifitislarge(forexample,ifitisregularizedwithdropout). Theselargemodelslearnsomefunction f(x),butdosousingmanymore parametersthanarenecessaryforthetask.Theirsizeisnecessaryonlydueto thelimitednumberoftrainingexamples.AssoonaswehaveÔ¨Åtthisfunction f(x),wecangenerateatrainingsetcontaininginÔ¨Ånitelymanyexamples,simply byapplying ftorandomlysampledpointsx.Wethentrainthenew,smaller, modeltomatch f(x)onthesepoints.InordertomosteÔ¨Écientlyusethecapacity ofthenew,smallmodel,itisbesttosamplethenewxpointsfromadistribution resemblingtheactualtestinputsthatwillbesuppliedtothemodellater.Thiscan bedonebycorruptingtrainingexamplesorbydrawingpointsfromagenerative modeltrainedontheoriginaltrainingset. Alternatively,onecantrainthesmallermodelonlyontheoriginaltraining points,buttrainittocopyotherfeaturesofthemodel,suchasitsposterior distributionovertheincorrectclasses(Hinton20142015 e t a l .,,). 12.1.5DynamicStructure Onestrategyforacceleratingdataprocessingsystemsingeneralistobuildsystems thathavedynamicstructureinthegraphdescribingthecomputationneeded toprocessaninput.Dataprocessingsystemscandynamicallydeterminewhich subsetofmanyneuralnetworksshouldberunonagiveninput.Individualneural networkscanalsoexhibitdynamicstructureinternallybydeterminingwhichsubset offeatures(hiddenunits)tocomputegiveninformationfromtheinput.This formofdynamicstructureinsideneuralnetworksissometimescalledconditional computation(,; ,). Sincemanycomponentsof Bengio2013Bengio e t a l .2013b thearchitecturemayberelevantonlyforasmallamountofpossibleinputs,the 4 4 8 CHAPTER12.APPLICATIONS systemcanrunfasterbycomputingthesefeaturesonlywhentheyareneeded. Dynamicstructureofcomputationsisabasiccomputerscienceprincipleapplied generallythroughoutthesoftwareengineeringdiscipline. Thesimplestversions ofdynamicstructureappliedtoneuralnetworksarebasedondeterminingwhich subsetofsomegroupofneuralnetworks(orothermachinelearningmodels)should beappliedtoaparticularinput. AvenerablestrategyforacceleratinginferenceinaclassiÔ¨Åeristouseacascade ofclassiÔ¨Åers.Thecascadestrategymaybeappliedwhenthegoalistodetectthe presenceofarareobject(orevent).Toknowforsurethattheobjectispresent, wemustuseasophisticatedclassiÔ¨Åerwithhighcapacity,thatisexpensivetorun. However,becausetheobjectisrare,wecanusuallyusemuchlesscomputation torejectinputsasnotcontainingtheobject.Inthesesituations,wecantrain asequenceofclassiÔ¨Åers.TheÔ¨ÅrstclassiÔ¨Åersinthesequencehavelowcapacity, andaretrainedtohavehighrecall.Inotherwords,theyaretrainedtomakesure wedonotwronglyrejectaninputwhentheobjectispresent.TheÔ¨ÅnalclassiÔ¨Åer istrainedtohavehighprecision.Attesttime,weruninferencebyrunningthe classiÔ¨Åersinasequence,abandoninganyexampleassoonasanyoneelementin thecascaderejectsit.Overall,thisallowsustoverifythepresenceofobjectswith highconÔ¨Ådence,usingahighcapacitymodel,butdoesnotforceustopaythecost offullinferenceforeveryexample.TherearetwodiÔ¨Äerentwaysthatthecascade canachievehighcapacity.Onewayistomakethelatermembersofthecascade individuallyhavehighcapacity.Inthiscase,thesystemasawholeobviouslyhas highcapacity,becausesomeofitsindividualmembersdo. Itisalsopossibleto makeacascadeinwhicheveryindividualmodelhaslowcapacitybutthesystem asawholehashighcapacityduetothecombinationofmanysmallmodels.Viola andJones2001()usedacascadeofboosteddecisiontreestoimplementafastand robustfacedetectorsuitableforuseinhandhelddigitalcameras.TheirclassiÔ¨Åer localizesafaceusingessentiallyaslidingwindowapproachinwhichmanywindows areexaminedandrejectediftheydonotcontainfaces.Anotherversionofcascades usestheearliermodelstoimplementasortofhardattentionmechanism:the earlymembersofthecascadelocalizeanobjectandlatermembersofthecascade performfurtherprocessinggiventhelocationoftheobject.Forexample,Google transcribesaddressnumbersfromStreetViewimageryusingatwo-stepcascade thatÔ¨Årstlocatestheaddressnumberwithonemachinelearningmodelandthen transcribesitwithanother(Goodfellow2014d e t a l .,). Decisiontreesthemselvesareanexampleofdynamicstructure,becauseeach nodeinthetreedetermineswhichofitssubtreesshouldbeevaluatedforeachinput. Asimplewaytoaccomplishtheunionofdeeplearninganddynamicstructure 4 4 9 CHAPTER12.APPLICATIONS istotrainadecisiontreeinwhicheachnodeusesaneuralnetworktomakethe splittingdecision( ,),thoughthishastypicallynotbeen GuoandGelfand1992 donewiththeprimarygoalofacceleratinginferencecomputations. Inthesamespirit,onecanuseaneuralnetwork,calledthegatertoselect whichoneoutofseveralexpertnetworkswillbeusedtocomputetheoutput, giventhecurrentinput.TheÔ¨Årstversionofthisideaiscalledthemixtureof experts(Nowlan1990Jacobs 1991 ,; e t a l .,),inwhichthegateroutputsaset ofprobabilities orweights(obtainedviaasoftmaxnonlinearity), oneperexpert, andtheÔ¨Ånaloutputisobtainedbytheweightedcombinationoftheoutputof theexperts.Inthatcase, theuseofthegaterdoesnotoÔ¨Äerareductionin computational cost,butifasingleexpertischosenbythegaterforeachexample, weobtainthehardmixtureofexperts( ,,),which Collobert e t a l .20012002 canconsiderablyacceleratetrainingandinferencetime.Thisstrategyworkswell whenthenumberofgatingdecisionsissmallbecauseitisnotcombinatorial. But whenwewanttoselectdiÔ¨Äerentsubsetsofunitsorparameters,itisnotpossible tousea‚Äúsoftswitch‚Äùbecauseitrequiresenumerating(andcomputingoutputsfor) allthegaterconÔ¨Ågurations. Todealwiththisproblem,severalapproacheshave beenexploredtotraincombinatorialgaters. ()experimentwith Bengio e t a l .2013b severalestimatorsofthegradientonthegatingprobabilities, whileBacon e t a l . ()and ()usereinforcementlearningtechniques(policy 2015Bengio e t a l .2015a gradient)tolearnaformofconditionaldropoutonblocksofhiddenunitsandget anactualreductionincomputational costwithoutimpactingnegativelyonthe qualityoftheapproximation. Another kindof dynamicstructure isa switch, where ahidden unit can receiveinputfromdiÔ¨Äerentunitsdependingonthecontext.Thisdynamicrouting approachcanbeinterpretedasanattentionmechanism( ,). Olshausen e t a l .1993 Sofar,theuseofahardswitchhasnotproveneÔ¨Äectiveonlarge-scaleapplications. Contemporaryapproachesinsteaduseaweightedaverageovermanypossibleinputs, andthusdonotachieveallofthepossiblecomputational beneÔ¨Åtsofdynamic structure.Contemporaryattentionmechanismsaredescribedinsection.12.4.5.1 Onemajorobstacletousingdynamicallystructuredsystemsisthedecreased degreeofparallelismthatresultsfromthesystemfollowingdiÔ¨Äerentcodebranches fordiÔ¨Äerentinputs.Thismeansthatfewoperationsinthenetworkcanbedescribed asmatrixmultiplication orbatchconvolutiononaminibatchofexamples.We canwritemorespecializedsub-routinesthatconvolveeachexamplewithdiÔ¨Äerent kernelsormultiplyeachrowofadesignmatrixbyadiÔ¨Äerentsetofcolumns ofweights.Unfortunately, thesemorespecializedsubroutinesarediÔ¨Écultto implementeÔ¨Éciently.CPUimplementations willbeslowduetothelackofcache 4 5 0 CHAPTER12.APPLICATIONS coherenceandGPUimplementations willbeslowduetothelackofcoalesced memorytransactionsandtheneedtoserializewarpswhenmembersofawarptake diÔ¨Äerentbranches.Insomecases,theseissuescanbemitigatedbypartitioningthe examplesintogroupsthatalltakethesamebranch,andprocessingthesegroups ofexamplessimultaneously. Thiscanbeanacceptablestrategyforminimizing thetimerequiredtoprocessaÔ¨ÅxedamountofexamplesinanoÔ¨Ñinesetting.In areal-timesettingwhereexamplesmustbeprocessedcontinuously,partitioning theworkloadcanresultinload-balancing issues.Forexample,ifweassignone machinetoprocesstheÔ¨Årststepinacascadeandanothermachinetoprocess thelaststepinacascade,thentheÔ¨Årstwilltendtobeoverloadedandthelast willtendtobeunderloaded. Similarissuesariseifeachmachineisassignedto implementdiÔ¨Äerentnodesofaneuraldecisiontree. 12.1.6SpecializedHardwareImplementationsofDeepNetworks Sincetheearlydaysofneuralnetworksresearch,hardwaredesignershaveworked onspecializedhardwareimplementations thatcouldspeeduptrainingand/or inferenceofneuralnetworkalgorithms.Seeearlyandmorerecentreviewsof specializedhardwarefordeepnetworks( ,;, LindseyandLindblad1994Beiu e t a l . 2003MisraandSaha2010 ; ,). DiÔ¨Äerentformsofspecializedhardware(GrafandJackel1989Meadand ,; Ismail2012Kim2009Pham2012Chen 2014ab ,; e t a l .,; e t a l .,; e t a l .,,)have beendevelopedoverthelastdecades,eitherwithASICs(application-speciÔ¨Åcinte- gratedcircuit),eitherwithdigital(basedonbinaryrepresentationsofnumbers), analog(GrafandJackel1989MeadandIsmail2012 ,; ,)(basedonphysicalimple- mentationsofcontinuousvaluesasvoltagesorcurrents)orhybridimplementations (combiningdigitalandanalogcomponents).InrecentyearsmoreÔ¨ÇexibleFPGA (Ô¨Åeldprogrammable gatedarray)implementations(wheretheparticularsofthe circuitcanbewrittenonthechipafterithasbeenbuilt)havebeendeveloped. Thoughsoftwareimplementationsongeneral-purposeprocessingunits(CPUs andGPUs)typicallyuse32or64bitsofprecisiontorepresentÔ¨Çoatingpoint numbers,ithaslongbeenknownthatitwaspossibletouselessprecision,at leastatinferencetime(HoltandBaker1991HoliandHwang1993Presley ,; ,; andHaggard1994SimardandGraf1994Wawrzynek 1996Savich ,; ,; e t a l .,; e t a l ., 2007).Thishasbecomeamorepressingissueinrecentyearsasdeeplearning hasgainedinpopularityinindustrialproducts,andasthegreatimpactoffaster hardwarewasdemonstratedwithGPUs.Anotherfactorthatmotivatescurrent researchonspecializedhardwarefordeepnetworksisthattherateofprogressof asingleCPUorGPUcorehassloweddown,andmostrecentimprovementsin 4 5 1 CHAPTER12.APPLICATIONS computingspeedhavecomefromparallelization acrosscores(eitherinCPUsor GPUs).ThisisverydiÔ¨Äerentfromthesituationofthe1990s(thepreviousneural networkera)wherethehardwareimplementations ofneuralnetworks(whichmight taketwoyearsfrominceptiontoavailabilityofachip)couldnotkeepupwith therapidprogressandlowpricesofgeneral-purposeCPUs.Buildingspecialized hardwareisthusawaytopushtheenvelopefurther,atatimewhennewhardware designsarebeingdevelopedforlow-powerdevicessuchasphones,aimingfor general-public applicationsofdeeplearning(e.g.,withspeech,computervisionor naturallanguage). Recentworkonlow-precisionimplementationsofbackprop-based neuralnets (Vanhoucke2011Courbariaux 2015Gupta2015 e t a l .,; e</div>
        </div>
    </div>

    <div class="question-card" id="q16">
        <div class="question-header">
            <span class="question-number">Question 16</span>
            <span class="question-type">multiple-choice</span>
        </div>

        <div class="question-text">Regularization techniques are crucial in machine learning for improving model generalization and preventing overfitting. Understanding the mathematical relationships among different methods such as early stopping, L2 regularization, and parameter sharing helps in designing efficient models.

Which statement accurately describes the relationship between early stopping and L2 regularization in linear models under quadratic loss assumptions?

1) Early stopping and L2 regularization always produce completely different parameter values.   
2) L2 regularization adapts automatically to validation performance, while early stopping requires manual tuning of its strength.   
3) The number of training steps in early stopping is directly proportional to the L2 regularization parameter.   
4) Early stopping regularizes directions of high curvature more strongly than directions of low curvature.   
5) Under certain assumptions, early stopping and L2 regularization can have equivalent effects on final parameter values.   
6) L2 regularization penalizes small weights more than large weights.   
7) Early stopping is mostly ineffective in controlling overfitting in linear models with quadratic loss.</div>

        <div class="answer-section">
            <div class="answer-label">‚úì Correct Answer:</div>
            <div class="answer-text">The correct answer is 5) Under certain assumptions, early stopping and L2 regularization can have equivalent effects on final parameter values..</div>
        </div>

        <div class="reference-section">
            <div class="reference-label">üìö Reference Text:</div>
            <button class="toggle-button" onclick="toggleReference(16)">
                Show/Hide Reference
            </button>
            <div id="ref16" class="reference-text hidden">regularization. Inordertocomparewithclassical L2regularization, weexamineasimple settingwheretheonlyparametersarelinearweights(Œ∏=w).Wecanmodel thecostfunction Jwithaquadraticapproximationintheneighborhoodofthe empiricallyoptimalvalueoftheweightsw‚àó: ÀÜ J J () = Œ∏ (w‚àó)+1 2(ww‚àí‚àó)ÓÄæHww (‚àí‚àó) , (7.33) whereHistheHessianmatrixof Jwithrespecttowevaluatedatw‚àó.Giventhe assumptionthatw‚àóisaminimumof J(w),weknowthatHispositivesemideÔ¨Ånite. UnderalocalTaylorseriesapproximation,thegradientisgivenby: ‚àá wÀÜ J() = (wHww‚àí‚àó) . (7.34) 2 5 0 CHAPTER7.REGULARIZATIONFORDEEPLEARNING w 1w 2w‚àó Àú w w 1w 2w‚àó Àú w Figure7.4:AnillustrationoftheeÔ¨Äectofearlystopping. ( L e f t )Thesolidcontourlines indicatethecontoursofthenegativelog-likelihood.Thedashedlineindicatesthetrajectory takenbySGDbeginningfromtheorigin.Ratherthanstoppingatthepointw‚àóthat minimizesthecost,earlystoppingresultsinthetrajectorystoppingatanearlierpointÀúw. ( R i g h t )AnillustrationoftheeÔ¨Äectof L2regularizationforcomparison.Thedashedcircles indicatethecontoursofthe L2penalty,whichcausestheminimumofthetotalcosttolie nearertheoriginthantheminimumoftheunregularizedcost. Wearegoingtostudythetrajectoryfollowedbytheparametervectorduring training.Forsimplicity,letussettheinitialparametervectortotheorigin,3that isw( 0 )= 0.Letusstudytheapproximatebehaviorofgradientdescenton Jby analyzinggradientdescentonÀÜ J: w( ) œÑ= w( 1 ) œÑ‚àí‚àí‚àá ÓÄè wÀÜ J(w( 1 ) œÑ‚àí) (7.35) = w( 1 ) œÑ‚àí‚àí ÓÄèHw(( 1 ) œÑ‚àí‚àíw‚àó) (7.36) w( ) œÑ‚àíw‚àó= ( )(IH‚àí ÓÄèw( 1 ) œÑ‚àí‚àíw‚àó) . (7.37) LetusnowrewritethisexpressioninthespaceoftheeigenvectorsofH,exploiting theeigendecompositionofH:H=QQ ŒõÓÄæ,where ŒõisadiagonalmatrixandQ isanorthonormalbasisofeigenvectors. w( ) œÑ‚àíw‚àó= (IQQ ‚àí ÓÄè ŒõÓÄæ)(w( 1 ) œÑ‚àí‚àíw‚àó)(7.38) QÓÄæ(w( ) œÑ‚àíw‚àó) = ( )I‚àí ÓÄè ŒõQÓÄæ(w( 1 ) œÑ‚àí‚àíw‚àó) (7.39) 3F o r n e u ra l n e t w o rk s , t o o b t a i n s y m m e t ry b re a k i n g b e t w e e n h i d d e n u n i t s , w e c a n n o t i n i t i a l i z e a l l t h e p a ra m e t e rs t o 0 , a s d i s c u s s e d i n s e c t i o n . Ho w e v e r, t h e a rg u m e n t h o l d s f o r a n y o t h e r 6 . 2 i n i t i a l v a l u e w( 0 ). 2 5 1 CHAPTER7.REGULARIZATIONFORDEEPLEARNING Assumingthatw( 0 )=0andthat ÓÄèischosentobesmallenoughtoguarantee |1‚àí ÓÄè Œª i| <1,theparametertrajectoryduringtrainingafter œÑparameterupdates isasfollows: QÓÄæw( ) œÑ= [ ( )I‚àíI‚àí ÓÄè ŒõœÑ]QÓÄæw‚àó. (7.40) Now,theexpressionforQÓÄæÀúwinequationfor7.13 L2regularizationcanberear- rangedas: QÓÄæÀúwI = (+ Œõ Œ±)‚àí 1ŒõQÓÄæw‚àó(7.41) QÓÄæÀúwII = [‚àí(+ Œõ Œ±)‚àí 1Œ±]QÓÄæw‚àó(7.42) Comparingequationandequation,weseethatifthehyperparameters 7.40 7.42 ÓÄè, Œ± œÑ,andarechosensuchthat ( )I‚àí ÓÄè ŒõœÑ= (+ ) Œõ Œ±I‚àí 1Œ± , (7.43) then L2regularizationandearlystoppingcanbeseentobeequivalent(atleast underthequadraticapproximation oftheobjectivefunction).Goingevenfurther, bytakinglogarithmsandusingtheseriesexpansionforlog(1+ x),wecanconclude thatifall Œª iaresmall(thatis, ÓÄè Œª iÓÄú1and Œª i /Œ±ÓÄú1)then œÑ‚âà1 ÓÄè Œ±, (7.44) Œ±‚âà1 œÑ ÓÄè. (7.45) Thatis,undertheseassumptions,thenumberoftrainingiterations œÑplaysarole inverselyproportionaltothe L2regularizationparameter,andtheinverseof œÑ ÓÄè playstheroleoftheweightdecaycoeÔ¨Écient. ParametervaluescorrespondingtodirectionsofsigniÔ¨Åcantcurvature(ofthe objectivefunction)areregularizedlessthandirectionsoflesscurvature.Ofcourse, inthecontextofearlystopping,thisreallymeansthatparametersthatcorrespond todirectionsofsigniÔ¨Åcantcurvaturetendtolearnearlyrelativetoparameters correspondingtodirectionsoflesscurvature. Thederivationsinthissectionhaveshownthatatrajectoryoflength œÑends atapointthatcorrespondstoaminimumofthe L2-regularizedobjective.Early stoppingisofcoursemorethanthemererestrictionofthetrajectorylength; instead,earlystoppingtypicallyinvolvesmonitoringthevalidationseterrorin ordertostopthetrajectoryataparticularlygoodpointinspace.Earlystopping thereforehastheadvantageoverweightdecaythatearlystoppingautomatically determinesthecorrectamountofregularizationwhileweightdecayrequiresmany trainingexperimentswithdiÔ¨Äerentvaluesofitshyperparameter. 2 5 2 CHAPTER7.REGULARIZATIONFORDEEPLEARNING 7.9ParameterTyingandParameterSharing Thusfar,inthischapter,whenwehavediscussedaddingconstraintsorpenalties totheparameters,wehavealwaysdonesowithrespecttoaÔ¨Åxedregionorpoint. Forexample, L2regularization(orweightdecay)penalizesmodelparametersfor deviatingfromtheÔ¨Åxedvalueofzero.However,sometimeswemayneedother waystoexpressourpriorknowledgeaboutsuitablevaluesofthemodelparameters. Sometimeswemightnotknowpreciselywhatvaluestheparametersshouldtake butweknow,fromknowledgeofthedomainandmodelarchitecture, thatthere shouldbesomedependencies betweenthemodelparameters. Acommontypeofdependencythatweoftenwanttoexpressisthatcertain parametersshouldbeclosetooneanother.Considerthefollowingscenario:we havetwomodelsperformingthesameclassiÔ¨Åcationtask(withthesamesetof classes)butwithsomewhatdiÔ¨Äerentinputdistributions.Formally,wehavemodel Awithparametersw( ) Aandmodel Bwithparametersw( ) B.Thetwomodels maptheinput totwo diÔ¨Äerent, but related outputs:ÀÜ y( ) A= f(w( ) A,x)and ÀÜ y( ) B= ( gw( ) B,x). Letusimaginethatthetasksaresimilarenough(perhapswithsimilarinput andoutputdistributions)thatwebelievethemodelparametersshouldbeclose toeachother: ‚àÄ i, w( ) A ishouldbecloseto w( ) B i.Wecanleveragethisinformation throughregularization. SpeciÔ¨Åcally,wecanuseaparameternormpenaltyofthe form: ‚Ñ¶(w( ) A,w( ) B)=ÓÅ´w( ) A‚àíw( ) BÓÅ´2 2. Hereweusedan L2penalty,butother choicesarealsopossible. Thiskindofapproachwasproposedby (),whoregularized Lasserreetal.2006 theparametersofonemodel,trainedasaclassiÔ¨Åerinasupervisedparadigm,to beclosetotheparametersofanothermodel,trainedinanunsupervisedparadigm (tocapturethedistributionoftheobservedinputdata).Thearchitectures were constructedsuchthatmanyoftheparametersintheclassiÔ¨Åermodelcouldbe pairedtocorrespondingparametersintheunsupervisedmodel. Whileaparameternormpenaltyisonewaytoregularizeparameterstobe closetooneanother,themorepopularwayistouseconstraints:toforcesets ofparameterstobeequal.Thismethodofregularizationisoftenreferredtoas parametersharing,becauseweinterpretthevariousmodelsormodelcomponents assharingauniquesetofparameters.AsigniÔ¨Åcantadvantageofparametersharing overregularizingtheparameterstobeclose(viaanormpenalty)isthatonlya subsetoftheparameters(theuniqueset)needtobestoredinmemory.Incertain models‚Äîsuchastheconvolutionalneuralnetwork‚ÄîthiscanleadtosigniÔ¨Åcant reductioninthememoryfootprintofthemodel. 2 5 3 CHAPTER7.REGULARIZATIONFORDEEPLEARNING ConvolutionalNeuralNetworksByfarthemostpopularandextensiveuse</div>
        </div>
    </div>

    <div class="question-card" id="q17">
        <div class="question-header">
            <span class="question-number">Question 17</span>
            <span class="question-type">multiple-choice</span>
        </div>

        <div class="question-text">Sequence models such as recurrent neural networks (RNNs) are trained to process and generate data in a step-wise manner. Training techniques and gradient computation methods are crucial for ensuring these models learn effectively and generalize well during deployment.

Which training strategy specifically addresses the mismatch between training and inference by gradually increasing the probability that an RNN uses its own generated outputs as inputs during training, thereby enabling the model to learn to recover from its own mistakes?

1) Gradient clipping   
2) Early stopping   
3) Layer normalization   
4) Scheduled sampling   
5) Residual connections   
6) Dropout regularization   
7) Weight tying</div>

        <div class="answer-section">
            <div class="answer-label">‚úì Correct Answer:</div>
            <div class="answer-text">The correct answer is 4) Scheduled sampling.</div>
        </div>

        <div class="reference-section">
            <div class="reference-label">üìö Reference Text:</div>
            <button class="toggle-button" onclick="toggleReference(17)">
                Show/Hide Reference
            </button>
            <div id="ref17" class="reference-text hidden">o r r e c t o u t p u ty( ) tdrawnfromthetrain setasinputtoh( + 1 ) t.Whenthemodelisdeployed,thetrueoutputisgenerally ( R i g h t ) notknown.Inthiscase,weapproximatethecorrectoutputy( ) twiththemodel‚Äôsoutput o( ) t,andfeedtheoutputbackintothemodel. 3 8 3 CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS likelihoodcriterionis log pÓÄê y( 1 ),y( 2 )|x( 1 ),x( 2 )ÓÄë (10.15) =log pÓÄê y( 2 )|y( 1 ),x( 1 ),x( 2 )ÓÄë +log pÓÄê y( 1 )|x( 1 ),x( 2 )ÓÄë (10.16) Inthisexample,weseethatattime t= 2,themodelistrainedtomaximizethe conditionalprobabilityofy( 2 )given b o t hthexsequencesofarandthepreviousy valuefromthetrainingset.MaximumlikelihoodthusspeciÔ¨Åesthatduringtraining, ratherthanfeedingthemodel‚Äôsownoutputbackintoitself,theseconnections shouldbefedwiththetargetvaluesspecifyingwhatthecorrectoutputshouldbe. ThisisillustratedinÔ¨Ågure.10.6 Weoriginallymotivatedteacherforcingasallowingustoavoidback-propagation throughtimeinmodelsthatlackhidden-to-hidden connections.Teacherforcing maystillbeappliedtomodelsthathavehidden-to-hidden connectionssolongas theyhaveconnectionsfromtheoutputatonetimesteptovaluescomputedinthe nexttimestep.However,assoonasthehiddenunitsbecomeafunctionofearlier timesteps,theBPTTalgorithmisnecessary.Somemodelsmaythusbetrained withbothteacherforcingandBPTT. Thedisadvantageofstrictteacherforcingarisesifthenetworkisgoingtobe laterusedinanopen-loopmode,withthenetworkoutputs(orsamplesfrom theoutputdistribution)fedbackasinput. Inthiscase,thekindofinputsthat thenetworkseesduringtrainingcouldbequitediÔ¨Äerentfromthekindofinputs thatitwillseeattesttime. Onewaytomitigatethisproblemistotrainwith bothteacher-forcedinputsandwithfree-runninginputs,forexamplebypredicting thecorrecttargetanumberofstepsinthefuturethroughtheunfoldedrecurrent output-to-input paths.Inthisway,thenetworkcanlearntotakeintoaccount inputconditions(suchasthoseitgeneratesitselfinthefree-runningmode)not seenduringtrainingandhowtomapthestatebacktowardsonethatwillmake thenetworkgenerateproperoutputsafterafewsteps.Anotherapproach(Bengio e t a l .,)tomitigatethegapbetweentheinputsseenattraintimeandthe 2015b inputsseenattesttimerandomlychoosestousegeneratedvaluesoractualdata valuesasinput.Thisapproachexploitsacurriculumlearningstrategytogradually usemoreofthegeneratedvaluesasinput. 10.2.2ComputingtheGradientinaRecurrentNeuralNetwork Computingthegradientthrougharecurrentneuralnetworkisstraightforward. Onesimplyappliesthegeneralizedback-propagationalgorithmofsection6.5.6 3 8 4 CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS totheunrolledcomputational graph.Nospecializedalgorithmsarenecessary. Gradientsobtainedbyback-propagation maythenbeusedwithanygeneral-purpose gradient-basedtechniquestotrainanRNN. TogainsomeintuitionforhowtheBPTTalgorithmbehaves,weprovidean exampleofhowtocomputegradientsbyBPTTfortheRNNequationsabove (equationandequation).Thenodesofourcomputational graphinclude 10.8 10.12 theparametersU,V,W,bandcaswellasthesequenceofnodesindexedby tforx( ) t,h( ) t,o( ) tand L( ) t. Foreachnode Nweneedtocomputethegradient ‚àá N Lrecursively,basedonthegradientcomputedatnodesthatfollowitinthe graph.WestarttherecursionwiththenodesimmediatelyprecedingtheÔ¨Ånalloss ‚àÇ L ‚àÇ L( ) t= 1 . (10.17) Inthisderivationweassumethattheoutputso( ) tareusedastheargumenttothe softmaxfunctiontoobtainthevectorÀÜyofprobabilitiesovertheoutput.Wealso assumethatthelossisthenegativelog-likelihoodofthetruetarget y( ) tgiventhe inputsofar.Thegradient‚àáo( ) t Lontheoutputsattimestep t,forall i , t,isas follows: (‚àáo( ) t L)i=‚àÇ L ‚àÇ o( ) t i=‚àÇ L ‚àÇ L( ) t‚àÇ L( ) t ‚àÇ o( ) t i=ÀÜ y( ) t i‚àí 1i , y( ) t .(10.18) Weworkourwaybackwards,startingfromtheendofthesequence.AttheÔ¨Ånal timestep, œÑh( ) œÑonlyhaso( ) œÑasadescendent,soitsgradientissimple: ‚àáh( ) œÑ L= VÓÄæ‚àáo( ) œÑ L. (10.19) Wecantheniteratebackwardsintimetoback-propagate gradientsthroughtime, from t= œÑ‚àí1downto t= 1,notingthath( ) t(for t < œÑ)hasasdescendentsboth o( ) tandh( + 1 ) t.Itsgradientisthusgivenby ‚àáh( ) t L=ÓÄ† ‚àÇh( + 1 ) t ‚àÇh( ) tÓÄ°ÓÄæ (‚àáh( +1) t L)+ÓÄ† ‚àÇo( ) t ‚àÇh( ) tÓÄ°ÓÄæ (‚àáo( ) t L) (10.20) = WÓÄæ(‚àáh( +1) t L)diagÓÄí 1‚àíÓÄê h( + 1 ) tÓÄë2ÓÄì +VÓÄæ(‚àáo( ) t L)(10.21) where diagÓÄê 1‚àíÓÄÄ h( + 1 ) tÓÄÅ2ÓÄë indicatesthediagonalmatrixcontainingtheelements 1‚àí( h( + 1 ) t i)2.ThisistheJacobianofthehyperbolictangentassociatedwiththe hiddenunitattime. i t+1 3 8 5</div>
        </div>
    </div>

    <div class="question-card" id="q18">
        <div class="question-header">
            <span class="question-number">Question 18</span>
            <span class="question-type">multiple-choice</span>
        </div>

        <div class="question-text">Probabilistic modeling in machine learning often involves estimating models whose normalization constants (partition functions) are computationally intractable. Several estimation techniques, such as score matching, ratio matching, and noise-contrastive estimation, have been developed to address these challenges and enable unsupervised learning in high-dimensional data.

Which estimation technique reframes unsupervised density estimation as a supervised binary classification task by distinguishing real data from samples drawn from a known noise distribution, and introduces an additional parameter to estimate the partition function?

1) Stochastic Maximum Likelihood   
2) Generalized Score Matching   
3) Pseudolikelihood Estimation   
4) Denoising Score Matching   
5) Contrastive Divergence   
6) Noise-Contrastive Estimation   
7) Variational Autoencoding</div>

        <div class="answer-section">
            <div class="answer-label">‚úì Correct Answer:</div>
            <div class="answer-text">The correct answer is 6) Noise-Contrastive Estimation.</div>
        </div>

        <div class="reference-section">
            <div class="reference-label">üìö Reference Text:</div>
            <button class="toggle-button" onclick="toggleReference(18)">
                Show/Hide Reference
            </button>
            <div id="ref18" class="reference-text hidden">Marlin e t a l .2010 foundthatratiomatchingoutperformsSML,pseudolikelihoodandGSMinterms oftheabilityofmodelstrainedwithratiomatchingtodenoisetestsetimages. Likethepseudolikelihoodestimator,ratiomatchingrequires nevaluationsofÀú p perdatapoint,makingitscomputational costperupdateroughly ntimeshigher thanthatofSML. Aswiththepseudolikelihoodestimator,ratiomatchingcanbethoughtofas pushingdownonallfantasystatesthathaveonlyonevariablediÔ¨Äerentfroma trainingexample.SinceratiomatchingappliesspeciÔ¨Åcallytobinarydata,this meansthatitactsonallfantasystateswithinHammingdistance1ofthedata. Ratiomatchingcanalsobeusefulasthebasisfordealingwithhigh-dimensional sparsedata,suchaswordcountvectors.Thiskindofdataposesachallengefor MCMC-basedmethodsbecausethedataisextremelyexpensivetorepresentin denseformat,yettheMCMCsamplerdoesnotyieldsparsevaluesuntilthemodel haslearnedtorepresentthesparsityinthedatadistribution.DauphinandBengio ()overcamethisissuebydesigninganunbiasedstochasticapproximation to 2013 ratiomatching.Theapproximation evaluatesonlyarandomlyselectedsubsetof thetermsoftheobjective,anddoesnotrequirethemodeltogeneratecomplete fantasysamples. SeeMarlinanddeFreitas2011()foratheoreticalanalysisoftheasymptotic eÔ¨Éciencyofratiomatching. 18.5DenoisingScoreMatching Insomecaseswemaywishtoregularizescorematching,byÔ¨Åttingadistribution psmoothed() = xÓÅö pdata()( ) y q x y| d y (18.27) ratherthanthetrue pdata.Thedistribution q( x y|) isacorruptionprocess,usually onethatformsbyaddingasmallamountofnoiseto. x y Denoisingscorematchingisespeciallyusefulbecauseinpracticeweusuallydo nothaveaccesstothetrue pdatabutratheronlyanempiricaldistributiondeÔ¨Åned bysamplesfromit.Anyconsistentestimatorwill,givenenoughcapacity,make p m o de lintoasetofDiracdistributionscenteredonthetrainingpoints.Smoothing by qhelpstoreducethisproblem,atthelossoftheasymptoticconsistencyproperty 619 CHAPTER18.CONFRONTINGTHEPARTITIONFUNCTION describedinsection. ()introducedaprocedurefor 5.4.5KingmaandLeCun2010 performingregularizedscorematchingwiththesmoothingdistribution qbeing normallydistributednoise. Recallfromsectionthatseveralautoencodertrainingalgorithmsare 14.5.1 equivalenttoscorematchingordenoisingscorematching.Theseautoencoder trainingalgorithmsare therefore a wayof overcomingthe partition function problem. 18.6Noise-ContrastiveEstimation Mosttechniquesforestimatingmodelswithintractablepartitionfunctionsdonot provideanestimateofthepartitionfunction.SMLandCDestimateonlythe gradientofthelogpartitionfunction,ratherthanthepartitionfunctionitself. Scorematchingandpseudolikelihoodavoidcomputingquantitiesrelatedtothe partitionfunctionaltogether. Noi se - c o n t r ast i v e e st i m a t i o n ( N C E )(Gutmann and Hy varinen2010, ) takesadiÔ¨Äerentstrategy.Inthisapproach,theprobabilitydistributionestimated bythemodelisrepresentedexplicitlyas log p m o de l() = log Àú x pmodel(;)+x Œ∏ c , (18.28) where cisexplicitlyintroducedasanapproximationof‚àílog Z( Œ∏).Ratherthan estimatingonly Œ∏,thenoisecontrastiveestimationproceduretreats casjust anotherparameterandestimates Œ∏and csimultaneously,usingthesamealgorithm forboth.Theresulting log p m o de l(x)thusmaynotcorrespondexactlytoavalid probabilitydistribution,butwillbecomecloserandclosertobeingvalidasthe estimateofimproves. c1 Suchanapproachwouldnotbepossibleusingmaximumlikelihoodasthe criterionfortheestimator.Themaximumlikelihoodcriterionwouldchoosetoset c c arbitrarilyhigh,ratherthansettingtocreateavalidprobabilitydistribution. NCEworksbyreducingtheunsupervisedlearningproblemofestimating p(x) tothatoflearningaprobabilisticbinaryclassiÔ¨Åerinwhichoneofthecategories correspondstothedatageneratedbythemodel.Thissupervisedlearningproblem isconstructedinsuchawaythatmaximumlikelihoodestimationinthissupervised 1NCEisalsoapplicabletoproblemswithatractablepartitionfunction,wherethereisno needtointroducetheextraparameter c.However,ithasgeneratedthemostinterestasameans ofestimatingmodelswithdiÔ¨Écultpartitionfunctions. 620 CHAPTER18.CONFRONTINGTHEPARTITIONFUNCTION learningproblemdeÔ¨Ånesanasymptoticallyconsistentestimatoroftheoriginal problem. SpeciÔ¨Åcally,weintroduceaseconddistribution,the noi se di st r i but i o n pnoise(x). Thenoisedistributionshouldbetractabletoevaluateandtosamplefrom. We cannowconstructamodeloverbothxandanew,binaryclassvariable y.Inthe newjointmodel,wespecifythat pjoint(= 1) = y1 2, (18.29) pjoint( = 1) = x| y p m o de l()x , (18.30) and pjoint( = 0) = x| y pnoise()x . (18.31) Inotherwords, yisaswitchvariablethatdetermineswhetherwewillgenerate x fromthemodelorfromthenoisedistribution. Wecanconstructasimilarjointmodeloftrainingdata.Inthiscase,the switchvariabledetermineswhetherwedraw xfromthe dat aorfromthenoise distribution.Formally, ptrain( y=1)=1 2, ptrain(x| y=1)= pdata(x), and ptrain( = 0) = x| y pnoise()x. Wecannowjustusestandardmaximumlikelihoodlearningonthe sup e r v i se d learningproblemofÔ¨Åtting pjointto ptrain: Œ∏ , c= argmax Œ∏ , cE x , py ‚àºtrainlog pjoint( ) y|x . (18.32) Thedistribution pjointisessentiallyalogisticregressionmodelappliedtothe diÔ¨Äerenceinlogprobabilities ofthemodelandthenoisedistribution: pjoint(= 1 ) = y |xp m o de l()x p m o de l()+x pnoise()x(18.33) =1 1+pnoise ( ) x pmodel ( ) x(18.34) =1 1+expÓÄê logpnoise ( ) x pmodel ( ) xÓÄë (18.35) = œÉÓÄí ‚àílogpnoise()x p m o de l()xÓÄì (18.36) = (log œÉ p m o de l()log x‚àí pnoise())x . (18.37) 621 CHAPTER18.CONFRONTINGTHEPARTITIONFUNCTION NCEisthussimpletoapplysolongaslog Àú pmodeliseasytoback-propagate through,and,asspeciÔ¨Åedabove, pnoiseiseasytoevaluate(inordertoevaluate pjoint)andsamplefrom(inordertogeneratethetrainingdata). NCEismostsuccessfulwhenappliedtoproblemswithfewrandomvariables, butcanworkwellevenifthoserandomvariablescantakeonahighnumberof values.Forexample,ithasbeensuccessfullyappliedtomodelingtheconditional distributionoverawordgiventhecontextoftheword(MnihandKavukcuoglu, 2013).Thoughthewordmaybedrawnfromalargevocabulary,thereisonlyone word. WhenNCEisappliedtoproblemswithmanyrandomvariables,itbecomesless eÔ¨Écient.ThelogisticregressionclassiÔ¨Åercanrejectanoisesamplebyidentifying anyonevariablewhosevalueisunlikely.Thismeansthatlearningslowsdown greatlyafter p m o de lhaslearnedthebasicmarginalstatistics.Imaginelearninga modelofimagesoffaces,usingunstructuredGaussiannoiseas pnoise.If p m o de l learnsabouteyes,itcanrejectalmostallunstructurednoisesampleswithout havinglearnedanythingaboutotherfacialfeatures,suchasmouths. Theconstraintthat pnoisemustbeeasytoevaluateandeasytosamplefrom canbeoverlyrestrictive.When pnoiseissimple,mostsamplesarelikelytobetoo obviouslydistinctfromthedatatoforce p m o de ltoimprovenoticeably. Likescorematchingandpseudolikelihood,NCEdoesnotworkifonlyalower boundonÀú pisavailable.Suchalowerboundcouldbeusedtoconstructalower boundon pjoint( y= 1|x),butitcanonlybeusedtoconstructanupperboundon pjoint( y= 0|x),whichappearsinhalfthetermsoftheNCEobjective.Likewise, alowerboundon pnoiseisnotuseful,becauseitprovidesonlyanupperboundon pjoint(= 1 ) y |x. WhenthemodeldistributioniscopiedtodeÔ¨Åneanewnoisedistributionbefore eachgradientstep,NCEdeÔ¨Ånesaprocedurecalled se l f - c o n t r ast i v e e st i m at i o n, whose expected gradientis equivalentto the expected gradientofmaximum likelihood(,).ThespecialcaseofNCEwherethenoisesamples Goodfellow2014 are thosegenerated by themodel suggests thatmaximumlikelihood can be interpretedasaprocedurethatforcesamodeltoconstantlylearntodistinguish realityfromitsownevolvingbeliefs,whilenoisecontrastiveestimationachieves somereducedcomputational costbyonlyforcingthemodeltodistinguishreality fromaÔ¨Åxedbaseline(thenoisemodel). Usingthesupervisedtaskofclassifyingbetweentrainingsamplesandgenerated samples(withthemodelenergyfunctionusedindeÔ¨ÅningtheclassiÔ¨Åer)toprovide agradientonthemodelwasintroducedearlierinvariousforms(Welling e t a l ., 2003bBengio2009;,). 622 CHAPTER18.CONFRONTINGTHEPARTITIONFUNCTION Noise contrastiveestimation is basedon the idea that agood generative modelshould be abletodistinguish datafromnoise.Aclosely relatedidea isthat agood generativemodelshould beabletogenerate samples thatno classiÔ¨Åercandistinguishfromdata.Thisideayieldsgenerativeadversarialnetworks (section).20.10.4 18.7EstimatingthePartitionFunction Whilemuchofthischapterisdedicatedtodescribingmethodsthatavoidneeding tocomputetheintractablepartitionfunction Z( Œ∏)associatedwithanundirected graphicalmodel,inthissectionwediscussseveralmethodsfordirectlyestimating thepartitionfunction. Estimatingthepartitionfunctioncanbeimportantbecausewerequireitif wewishtocomputethenormalizedlikelihoodofdata.Thisisoftenimportantin e v a l u a t i ngthemodel,monitoringtrainingperformance,andcomparingmodelsto eachother. Forexample,imaginewehavetwomodels:model M AdeÔ¨Åningaprobabil- itydistribution p A(x; Œ∏ A)=1 Z</div>
        </div>
    </div>

    <div class="question-card" id="q19">
        <div class="question-header">
            <span class="question-number">Question 19</span>
            <span class="question-type">multiple-choice</span>
        </div>

        <div class="question-text">Deep learning models rely on both architectural innovations and training strategies to achieve effective optimization, especially in the presence of challenges like vanishing gradients or complex data structures. Understanding how different techniques contribute to easier training is essential for designing successful neural networks.

Which approach directly combats vanishing gradients in very deep neural networks by enabling error signals to bypass several layers and reach earlier stages during backpropagation?

1) Increasing network width with more neurons per layer   
2) Using only linear activation functions   
3) Employing dropout regularization   
4) Initializing weights with small random values   
5) Applying pooling operations after convolutions   
6) Incorporating skip connections between non-adjacent layers   
7) Decreasing learning rates gradually during training</div>

        <div class="answer-section">
            <div class="answer-label">‚úì Correct Answer:</div>
            <div class="answer-text">The correct answer is 6) Incorporating skip connections between non-adjacent layers.</div>
        </div>

        <div class="reference-section">
            <div class="reference-label">üìö Reference Text:</div>
            <button class="toggle-button" onclick="toggleReference(19)">
                Show/Hide Reference
            </button>
            <div id="ref19" class="reference-text hidden">CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS 8.7.5DesigningModelstoAidOptimization Toimproveoptimization, thebeststrategyisnotalwaystoimprovetheoptimization algorithm.Instead,manyimprovementsintheoptimization ofdeepmodelshave comefromdesigningthemodelstobeeasiertooptimize. Inprinciple,wecoulduseactivationfunctionsthatincreaseanddecreasein jaggednon-monotonic patterns.However,thiswouldmakeoptimization extremely diÔ¨Écult.Inpractice, itismoreimportanttochooseamodelfamilythatiseasyto optimizethantouseapowerfuloptimizationalgorithm.Mostoftheadvancesin neuralnetworklearningoverthepast30yearshavebeenobtainedbychanging themodelfamilyratherthanchangingtheoptimization procedure.Stochastic gradientdescentwithmomentum,whichwasusedtotrainneuralnetworksinthe 1980s,remainsinuseinmodernstateoftheartneuralnetworkapplications. SpeciÔ¨Åcally,modernneuralnetworksreÔ¨Çectadesignchoicetouselineartrans- formationsbetweenlayersandactivationfunctionsthatarediÔ¨Äerentiable almost everywhereandhavesigniÔ¨Åcantslopeinlargeportionsoftheirdomain. Inpar- ticular,modelinnovationsliketheLSTM,rectiÔ¨Åedlinearunitsandmaxoutunits haveallmovedtowardusingmorelinearfunctionsthanpreviousmodelslikedeep networksbasedonsigmoidalunits.Thesemodelshavenicepropertiesthatmake optimization easier.ThegradientÔ¨Çowsthroughmanylayersprovidedthatthe Jacobianofthelineartransformationhasreasonablesingularvalues. Moreover, linearfunctionsconsistentlyincreaseinasingledirection,soevenifthemodel‚Äôs outputisveryfarfromcorrect,itisclearsimplyfromcomputingthegradient whichdirectionitsoutputshouldmovetoreducethelossfunction.Inotherwords, modernneuralnetshavebeendesignedsothattheirlocalgradientinformation correspondsreasonablywelltomovingtowardadistantsolution. Othermodeldesignstrategiescanhelptomakeoptimization easier.For example,linearpathsorskipconnectionsbetweenlayersreducethelengthof theshortestpathfromthelower layer‚Äôsparameters totheoutput, and thus mitigatethevanishinggradientproblem(Srivastava2015etal.,).Arelatedidea toskipconnectionsisaddingextracopiesoftheoutputthatareattachedtothe intermediatehiddenlayersofthenetwork,asinGoogLeNet( ,) Szegedy etal.2014a anddeeply-supervisednets(,).These‚Äúauxiliaryheads‚Äùaretrained Leeetal.2014 toperformthesametaskastheprimaryoutputatthetopofthenetworkinorder toensurethatthelowerlayersreceivealargegradient.Whentrainingiscomplete theauxiliaryheadsmaybediscarded. Thisisanalternativetothepretraining strategies,whichwereintroducedintheprevioussection.Inthisway,onecan trainjointlyallthelayersinasinglephasebutchangethearchitecture, sothat intermediatelayers(especiallythelowerones)cangetsomehintsaboutwhatthey 3 2 6 CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS shoulddo,viaashorterpath.Thesehintsprovideanerrorsignaltolowerlayers. 8.7.6ContinuationMethodsandCurriculumLearning Asarguedinsection,manyofthechallengesinoptimization arisefromthe 8.2.7 globalstructureofthecostfunctionandcannotberesolvedmerelybymakingbetter estimatesoflocalupdatedirections.Thepredominant strategyforovercomingthis problemistoattempttoinitializetheparametersinaregionthatisconnected tothesolutionbyashortpaththroughparameterspacethatlocaldescentcan discover. Continuationmethodsareafamilyofstrategiesthatcanmakeoptimization easierbychoosinginitialpointstoensurethatlocaloptimization spendsmostof itstimeinwell-behavedregionsofspace.Theideabehindcontinuationmethodsis toconstructaseriesofobjectivefunctionsoverthesameparameters.Inorderto minimizeacostfunction J(Œ∏),wewillconstructnewcostfunctions { J( 0 ), . . . , J( ) n}. ThesecostfunctionsaredesignedtobeincreasinglydiÔ¨Écult,with J( 0 )beingfairly easytominimize,and J( ) n,themostdiÔ¨Écult,being J(Œ∏),thetruecostfunction motivatingtheentireprocess.Whenwesaythat J( ) iiseasierthan J( + 1 ) i,we meanthatitiswellbehavedovermoreofŒ∏space.Arandominitialization ismore likelytolandintheregionwherelocaldescentcanminimizethecostfunction successfullybecausethisregionislarger.Theseriesofcostfunctionsaredesigned sothatasolutiontooneisagoodinitialpointofthenext.Wethusbeginby solvinganeasyproblemthenreÔ¨Ånethesolutiontosolveincrementally harder problemsuntilwearriveatasolutiontothetrueunderlyingproblem. Traditionalcontinuationmethods(predatingtheuseofcontinuationmethods forneuralnetworktraining)areusuallybasedonsmoothingtheobjectivefunction. SeeWu1997()foranexampleofsuchamethodandareviewofsomerelated methods.Continuationmethodsarealsocloselyrelatedtosimulatedannealing, whichaddsnoisetotheparameters(Kirkpatrick 1983etal.,).Continuation methodshavebeenextremelysuccessfulinrecentyears.SeeMobahiandFisher ()foranoverviewofrecentliterature,especiallyforAIapplications. 2015 Continuationmethodstraditionallyweremostlydesignedwiththegoalof overcomingthechallengeoflocalminima.SpeciÔ¨Åcally,theyweredesignedto reachaglobalminimumdespitethepresenceofmanylocalminima.Todoso, thesecontinuationmethodswouldconstructeasiercostfunctionsby‚Äúblurring‚Äùthe originalcostfunction.Thisblurringoperationcanbedonebyapproximating J( ) i() = Œ∏ EŒ∏ÓÄ∞‚àº N ( Œ∏ÓÄ∞; Œ∏ , œÉ()2 i) J(Œ∏ÓÄ∞) (8.40) viasampling.Theintuitionforthisapproachisthatsomenon-convexfunctions 3 2 7 CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS becomeapproximately convexwhenblurred.Inmanycases,thisblurringpreserves enoughinformationaboutthelocationofaglobalminimumthatwecanÔ¨Åndthe globalminimumbysolvingprogressivelylessblurredversionsoftheproblem.This approachcanbreakdowninthreediÔ¨Äerentways.First,itmightsuccessfullydeÔ¨Åne aseriesofcostfunctionswheretheÔ¨Årstisconvexandtheoptimumtracksfrom onefunctiontothenextarrivingattheglobalminimum,butitmightrequireso manyincrementalcostfunctionsthatthecostoftheentireprocedureremainshigh. NP-hardoptimization problemsremainNP-hard,evenwhencontinuationmethods areapplicable.Theothertwowaysthatcontinuationmethodsfailbothcorrespond tothemethodnotbeingapplicable.First,thefunctionmightnotbecomeconvex, nomatterhowmuchitisblurred.Considerforexamplethefunction J(Œ∏) =‚àíŒ∏ÓÄæŒ∏. Second,thefunctionmaybecomeconvexasaresultofblurring,buttheminimum ofthisblurredfunctionmaytracktoalocalratherthanaglobalminimumofthe originalcostfunction. Thoughcontinuationmethodsweremostlyoriginallydesignedtodealwiththe problemoflocalminima,localminimaarenolongerbelievedtobetheprimary problemforneuralnetworkoptimization. Fortunately,continuationmethodscan stillhelp.Theeasierobjectivefunctionsintroducedbythecontinuationmethodcan eliminateÔ¨Çatregions,decreasevarianceingradientestimates,improveconditioning oftheHessianmatrix,ordoanythingelsethatwilleithermakelocalupdates easiertocomputeorimprovethecorrespondencebetweenlocalupdatedirections andprogresstowardaglobalsolution. Bengio2009etal.()observedthatanapproachcalledcurriculumlearning orshapingcanbeinterpretedasacontinuationmethod.Curriculumlearningis basedontheideaofplanningalearningprocesstobeginbylearningsimpleconcepts andprogresstolearningmorecomplexconceptsthatdependonthesesimpler concepts.Thisbasicstrategywaspreviouslyknowntoaccelerateprogressinanimal training(,;,; Skinner1958Peterson2004KruegerandDayan2009,)andmachine learning(,;,;,). () SolomonoÔ¨Ä1989Elman1993Sanger1994Bengioetal.2009 justiÔ¨Åedthisstrategyasacontinuationmethod,whereearlier J( ) iaremadeeasierby increasingtheinÔ¨Çuenceofsimplerexamples(eitherbyassigningtheircontributions tothecostfunctionlargercoeÔ¨Écients,orbysamplingthemmorefrequently),and experimentallydemonstratedthatbetterresultscouldbeobtainedbyfollowinga curriculumonalarge-scaleneurallanguagemodelingtask.Curriculumlearning hasbeensuccessfulonawiderangeofnaturallanguage(Spitkovsky2010etal.,; Collobert2011aMikolov2011bTuandHonavar2011 etal.,; etal.,; ,)andcomputer vision( ,; ,; ,) Kumaretal.2010LeeandGrauman2011SupancicandRamanan2013 tasks.CurriculumlearningwasalsoveriÔ¨Åedasbeingconsistentwiththewayin whichhumans teach(,):teachersstartbyshowingeasierand Khanetal.2011 3 2 8 CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS moreprototypicalexamplesandthenhelpthelearnerreÔ¨Ånethedecisionsurface withthelessobviouscases.Curriculum-based strategiesaremoreeÔ¨Äectivefor teachinghumansthanstrategiesbasedonuniformsamplingofexamples,andcan alsoincreasetheeÔ¨Äectivenessofotherteachingstrategies( , BasuandChristensen 2013). Anotherimportantcontributiontoresearchoncurriculumlearningaroseinthe contextoftrainingrecurrentneuralnetworkstocapturelong-termdependencies: ZarembaandSutskever2014()foundthatmuchbetterresultswereobtainedwitha stochasticcurriculum,inwhicharandommixofeasyanddiÔ¨Écultexamplesisalways presentedtothelearner,butwheretheaverageproportionofthemorediÔ¨Écult examples(here,thosewithlonger-termdependencies)isgraduallyincreased.With adeterministiccurriculum,noimprovementoverthebaseline(ordinarytraining fromthefulltrainingset)wasobserved. Wehavenowdescribedthebasicfamilyofneuralnetworkmodelsandhowto regularizeandoptimizethem.Inthechaptersahead,weturntospecializationsof theneuralnetworkfamily,thatallowneuralnetworkstoscaletoverylargesizesand processinputdatathathasspecialstructure.Theoptimization methodsdiscussed inthischapterareoftendirectlyapplicabletothesespecializedarchitectures with littleornomodiÔ¨Åcation. 3 2 9 C h a p t e r 9 C on v ol u t i on al N e t w orks Con v o l ut i o na l net w o r k s(,),alsoknownas LeCun1989 c o n v o l ut i o na l neur al net w o r k sorCNNs,areaspecializedkindofneuralnetworkforprocessingdata thathasaknown,grid-liketopology.Examplesincludetime-seriesdata,whichcan bethoughtofasa1Dgridtakingsamplesatregulartimeintervals,andimagedata, whichcanbethoughtofasa2Dgridofpixels.Convolutionalnetworkshavebeen tremendouslysuccessfulinpracticalapplications.Thename‚Äúconvolutionalneural network‚Äùindicatesthatthenetworkemploysamathematical operationcalled c o n v o l ut i o n.Convolutionisaspecializedkindoflinearoperation.Convolutional networksaresimplyneuralnetworksthatuseconvolutioninplaceofgeneralmatrix multiplicationinatleastoneoftheirlayers. Inthis chapter, wewillÔ¨Årst describewhatconvolutionis.Next, wewill explainthemotivationbehindusingconvolutioninaneuralnetwork.Wewillthen describeanoperationcalled p o o l i ng,whichalmostallconvolutionalnetworks employ.Usually,theoperationusedinaconvolutionalneuralnetworkdoesnot correspondpreciselytothedeÔ¨ÅnitionofconvolutionasusedinotherÔ¨Åeldssuch asengineeringorpuremathematics.Wewilldescribeseveralvariantsonthe convolutionfunctionthatarewidelyusedinpracticeforneuralnetworks.We willalso show how convolutionmaybeappliedtomanykindsofdata, with diÔ¨Äerentnumbersofdimensions.Wethendiscussmeansofmakingconvolution moreeÔ¨Écient.ConvolutionalnetworksstandoutasanexampleofneuroscientiÔ¨Åc principlesinÔ¨Çuencingdeeplearning.WewilldiscusstheseneuroscientiÔ¨Åcprinciples, thenconcludewithcommentsabouttheroleconvolutionalnetworkshaveplayed inthehistoryofdeeplearning.Onetopicthischapterdoesnotaddressishowto choosethearchitectureofyourconvolutionalnetwork.Thegoalofthischapteris todescribethekindsoftoolsthatconvolutionalnetworksprovide,whilechapter11 330 CHAPTER9.CONVOLUTIONALNETWORKS describesgeneralguidelinesforchoosingwhichtoolstouseinwhichcircumstances. Researchintoconvolutionalnetworkarchitecturesproceedssorapidlythatanew bestarchitectureforagivenbenchmarkisannouncedeveryfewweekstomonths, renderingitimpracticaltodescribethebestarchitectureinprint.However,the bestarchitectureshaveconsistentlybeencomposedofthebuildingblocksdescribed here. 9.1TheConvolutionOperation Initsmostgeneralform,convolutionisanoperationontwofunctionsofareal- valuedargument.TomotivatethedeÔ¨Ånitionofconvolution,westartwithexamples oftwofunctionswemightuse. Supposewearetrackingthelocationofaspaceshipwithalasersensor.Our lasersensorprovidesasingleoutput x( t),thepositionofthespaceshipattime t.Both xand tarereal-valued,i.e.,wecangetadiÔ¨Äerentreadingfromthelaser sensoratanyinstantintime. Nowsupposethatourlasersensorissomewhatnoisy.Toobtainalessnoisy estimateofthespaceship‚Äôsposition,wewouldliketoaveragetogetherseveral measurements.Ofcourse,morerecentmeasurementsaremorerelevant,sowewill wantthistobeaweightedaveragethatgivesmoreweighttorecentmeasurements. Wecandothiswithaweightingfunction w( a),where aistheageofameasurement. Ifweapplysuchaweightedaverageoperationateverymoment,weobtainanew functionprovidingasmoothedestimateofthepositionofthespaceship: s s t() =ÓÅö x a w t a d a ()( ‚àí) (9.1) Thisoperationiscalled c o n v o l ut i o n.Theconvolutionoperationistypically denotedwithanasterisk: s t x w t () = ( ‚àó)() (9.2) Inourexample, wneedstobeavalidprobabilitydensityfunction,orthe outputisnotaweightedaverage.Also, wneedstobeforallnegativearguments, 0 oritwilllookintothefuture,whichispresumablybeyondourcapabilities.These limitationsareparticulartoourexamplethough.Ingeneral,convolutionisdeÔ¨Åned foranyfunctionsforwhichtheaboveintegralisdeÔ¨Åned,andmaybeusedforother purposesbesidestakingweightedaverages. Inconvolutionalnetworkterminology,theÔ¨Årstargument(inthisexample,the function x)totheconvolutionisoftenreferredtoasthe i nputandthesecond 3 3 1 CHAPTER9.CONVOLUTIONALNETWORKS argument(inthisexample,thefunction w)asthe k e r nel.Theoutputissometimes referredtoasthe . f e at ur e m ap Inourexample,theideaofalasersensorthatcanprovidemeasurements ateveryinstantintimeisnotrealistic.Usually,whenweworkwithdataona computer,timewillbediscretized,andoursensorwillprovidedataatregular intervals.Inourexample,itmightbemorerealistictoassumethatourlaser providesameasurementoncepersecond.Thetimeindex tcanthentakeononly integervalues.Ifwenowassumethat xand waredeÔ¨Ånedonlyoninteger t,we candeÔ¨Ånethediscreteconvolution: s t x w t () = ( ‚àó)() =‚àûÓÅò a = ‚àí ‚àûx a w t a ()( ‚àí) (9.3) Inmachinelearningapplications,theinputisusuallyamultidimensional array ofdataandthekernelisusuallyamultidimensionalarrayofparametersthatare adaptedbythelearningalgorithm.Wewillrefertothesemultidimensional arrays astensors.Becauseeachelementoftheinputandkernelmustbeexplicitlystored separately,weusuallyassumethatthesefunctionsarezeroeverywherebutthe Ô¨Ånitesetofpointsforwhichwestorethevalues.Thismeansthatinpracticewe canimplementtheinÔ¨ÅnitesummationasasummationoveraÔ¨Ånitenumberof arrayelements. Finally,weoftenuseconvolutionsovermorethanoneaxisatatime.For example,ifweuseatwo-dimensionalimage Iasourinput,weprobablyalsowant touseatwo-dimensionalkernel: K S i ,</div>
        </div>
    </div>

    <div class="question-card" id="q20">
        <div class="question-header">
            <span class="question-number">Question 20</span>
            <span class="question-type">multiple-choice</span>
        </div>

        <div class="question-text">Autoencoders are neural network architectures widely used in unsupervised learning to extract meaningful representations from high-dimensional data. Various types of autoencoders employ regularization techniques to prevent overfitting and encourage the learning of robust features.

Which method explicitly penalizes the sensitivity of an encoder's output to small input perturbations using the norm of the Jacobian, thereby promoting smoothly varying representations along the data manifold?

1) Sparse autoencoder   
2) Denoising autoencoder   
3) Contractive autoencoder   
4) Variational autoencoder   
5) Locally Linear Embedding   
6) Isomap   
7) Principal Component Analysis</div>

        <div class="answer-section">
            <div class="answer-label">‚úì Correct Answer:</div>
            <div class="answer-text">The correct answer is 3) Contractive autoencoder.</div>
        </div>

        <div class="reference-section">
            <div class="reference-label">üìö Reference Text:</div>
            <button class="toggle-button" onclick="toggleReference(20)">
                Show/Hide Reference
            </button>
            <div id="ref20" class="reference-text hidden">c a l P e r spec t i v e TheideaofusingMLPsfordenoisingdatesbacktotheworkof()LeCun1987 and ().()alsousedrecurrentnetworkstodenoise Gallinari e t a l .1987Behnke2001 images.Denoisingautoencodersare,insomesense,justMLPstrainedtodenoise. However,thename‚Äúdenoisingautoencoder‚Äùreferstoamodelthatisintendednot merelytolearntodenoiseitsinputbuttolearnagoodinternalrepresentation as asideeÔ¨Äect oflearningto denoise.This ideacame muchlater (Vincent e t a l .,,).Thelearnedrepresentationmaythenbeusedtopretraina 20082010 deeperunsupervisednetworkorasupervisednetwork.Likesparseautoencoders, sparsecoding,contractiveautoencodersandotherregularizedautoencoders,the motivationforDAEswastoallowthelearningofaveryhigh-capacity encoder whilepreventingtheencoderanddecoderfromlearningauselessidentityfunction. Priortotheintroduction ofthemodernDAE,InayoshiandKurita2005() exploredsomeofthesamegoalswithsomeofthesamemethods.Theirapproach minimizesreconstructionerrorinadditiontoasupervisedobjectivewhileinjecting noiseinthehiddenlayerofasupervisedMLP,withtheobjectivetoimprove generalization byintroducing the reconstructionerror andtheinjectednoise. However,theirmethodwasbasedonalinearencoderandcouldnotlearnfunction familiesaspowerfulascanthemodernDAE. 14.6LearningManifoldswithAutoencoders Like many other machine learning algorithms, auto encoders exploittheidea thatdataconcentratesaroundalow-dimensionalmanifoldorasmallsetofsuch manifolds,asdescribedinsection.Somemachinelearningalgorithmsexploit 5.11.3 thisideaonlyinsofarasthattheylearnafunctionthatbehavescorrectlyonthe manifoldbutmayhaveunusualbehaviorifgivenaninputthatisoÔ¨Äthemanifold. 5 1 5 CHAPTER14.AUTOENCODERS Autoencoderstakethisideafurtherandaimtolearnthestructureofthemanifold. Tounderstandhowautoencodersdothis,wemustpresentsomeimportant characteristicsofmanifolds. Animportantcharacterization ofamanifoldisthesetofits t angen t pl anes. Atapoint xona d-dimensionalmanifold,thetangentplaneisgivenby dbasis vectorsthatspanthelocaldirectionsofvariationallowedonthemanifold.As illustratedinÔ¨Ågure,theselocaldirectionsspecifyhowonecanchange 14.6 x inÔ¨Ånitesimallywhilestayingonthemanifold. Allautoencodertrainingproceduresinvolveacompromisebetweentwoforces: 1.Learningarepresentation hofatrainingexample xsuchthat xcanbe approximatelyrecoveredfrom hthroughadecoder.Thefactthat xisdrawn fromthetrainingdataiscrucial,becauseitmeanstheautoencoderneed notsuccessfullyreconstructinputsthatarenotprobableunderthedata generatingdistribution. 2. Satisfyingtheconstraintorregularizationpenalty.Thiscanbeanarchitec- turalconstraintthatlimitsthecapacityoftheautoencoder,oritcanbe aregularizationtermaddedtothereconstructioncost.Thesetechniques generallyprefersolutionsthatarelesssensitivetotheinput. Clearly,neitherforcealonewouldbeuseful‚Äîcopyingtheinputtotheoutput isnotusefulonitsown,norisignoringtheinput.Instead,thetwoforcestogether areusefulbecausetheyforcethehiddenrepresentationtocaptureinformation aboutthestructureofthedatageneratingdistribution.Theimportantprinciple isthattheautoencodercanaÔ¨Äordtorepresent o nl y t h e v a r i a t i o ns t h a t a r e ne e d e d t o r e c o ns t r u c t t r a i ning e x a m p l e s.Ifthedatageneratingdistributionconcentrates nearalow-dimensional manifold,thisyieldsrepresentationsthatimplicitlycapture alocalcoordinatesystemforthismanifold:onlythevariationstangenttothe manifoldaround xneedtocorrespondtochangesin h= f( x).Hencetheencoder learnsamappingfromtheinputspace xtoarepresentationspace,amappingthat isonlysensitivetochangesalongthemanifolddirections,butthatisinsensitiveto changesorthogonaltothemanifold. Aone-dimensional exampleisillustratedinÔ¨Ågure,showingthat,bymaking 14.7 thereconstructionfunctioninsensitivetoperturbationsoftheinputaroundthe datapoints,wecausetheautoencodertorecoverthemanifoldstructure. Tounderstandwhyautoencodersareusefulformanifoldlearning,itisin- structivetocomparethemtootherapproaches.Whatismostcommonlylearned tocharacterizeamanifoldisa r e pr e se n t at i o nofthedatapointson(ornear) 5 1 6 CHAPTER14.AUTOENCODERS Figure14.6: Anillustrationoftheconceptofatangenthyperplane.Herewecreatea one-dimensionalmanifoldin784-dimensionalspace.WetakeanMNISTimagewith784 pixelsandtransformitbytranslatingitvertically. Theamountofverticaltranslation deÔ¨Ånesacoordinatealongaone-dimensionalmanifoldthattracesoutacurvedpath throughimagespace.Thisplotshowsafewpointsalongthismanifold. Forvisualization, wehaveprojectedthemanifoldintotwodimensionalspaceusingPCA.An n-dimensional manifoldhasan n-dimensionaltangentplaneateverypoint.Thistangentplanetouches themanifoldexactlyatthatpointandisorientedparalleltothesurfaceatthatpoint. ItdeÔ¨Ånesthespaceofdirectionsinwhichitispossibletomovewhileremainingon themanifold.Thisone-dimensionalmanifoldhasasingletangentline.Weindicatean exampletangentlineatonepoint,withanimageshowinghowthistangentdirection appearsinimagespace.Graypixelsindicatepixelsthatdonotchangeaswemovealong thetangentline,whitepixelsindicatepixelsthatbrighten,andblackpixelsindicatepixels thatdarken. 5 1 7 CHAPTER14.AUTOENCODERS x 0 x 1 x 2 x0 0 .0 2 .0 4 .0 6 .0 8 .1 0 .r x ( )Id e n t i t y O p t i m a l r e c o n s t r u c t i o n Figure14.7:Iftheautoencoderlearnsareconstructionfunctionthatisinvarianttosmall perturbationsnearthedatapoints,itcapturesthemanifoldstructureofthedata.Here themanifoldstructureisacollectionof-dimensionalmanifolds.Thedasheddiagonal 0 lineindicatestheidentityfunctiontargetforreconstruction.Theoptimalreconstruction functioncrossestheidentityfunctionwhereverthereisadatapoint.Thehorizontal arrowsatthebottomoftheplotindicatethe r( x)‚àí xreconstructiondirectionvector atthebaseofthearrow,ininputspace,alwayspointingtowardsthenearest‚Äúmanifold‚Äù (asingledatapoint,inthe1-Dcase).Thedenoisingautoencoderexplicitlytriestomake thederivativeofthereconstructionfunction r( x)smallaroundthedatapoints.The contractiveautoencoderdoesthesamefortheencoder.Althoughthederivativeof r( x)is askedtobesmallaroundthedatapoints,itcanbelargebetweenthedatapoints.The spacebetweenthedatapointscorrespondstotheregionbetweenthemanifolds,where thereconstructionfunctionmusthavealargederivativeinordertomapcorruptedpoints backontothemanifold. themanifold.Sucharepresentationforaparticularexampleisalsocalledits embedding.Itistypicallygivenbyalow-dimensionalvector,withlessdimensions thanthe‚Äúambient‚Äùspaceofwhichthemanifoldisalow-dimensionalsubset.Some algorithms(non-parametric manifoldlearningalgorithms,discussedbelow)directly learnanembeddingforeachtrainingexample,whileotherslearnamoregeneral mapping,sometimescalledanencoder,orrepresentationfunction,thatmapsany pointintheambientspace(theinputspace)toitsembedding. Manifoldlearninghasmostlyfocusedonunsupervisedlearningproceduresthat attempttocapturethesemanifolds.Mostoftheinitialmachinelearningresearch onlearningnonlinearmanifoldshasfocusedon non-par a m e t r i cmethodsbased onthe near e st - n e i g h b o r g r aph.Thisgraphhasonenodepertrainingexample andedgesconnectingnearneighborstoeachother.Thesemethods(Sch√∂lkopf e t a l .,;1998RoweisandSaul2000Tenenbaum2000Brand2003Belkin ,; e t a l .,;,; 5 1 8 CHAPTER14.AUTOENCODERS Figure14.8:Non-parametricmanifoldlearningproceduresbuildanearestneighborgraph inwhichnodesrepresenttrainingexamplesadirectededgesindicatenearestneighbor relationships. Variousprocedurescanthusobtainthetangentplaneassociatedwitha neighborhoodofthegraphaswellasacoordinatesystemthatassociateseachtraining examplewithareal-valuedvectorposition,or e m b e d d in g.Itispossibletogeneralize sucharepresentationtonewexamplesbyaformofinterpolation.Solongasthenumber ofexamplesislargeenoughtocoverthecurvatureandtwistsofthemanifold,these approachesworkwell.ImagesfromtheQMULMultiviewFaceDataset( , Gong e t a l . 2000). andNiyogi2003DonohoandGrimes2003WeinbergerandSaul2004Hinton ,; ,; ,; andRoweis2003vanderMaatenandHinton2008 ,; ,)associateeachofnodeswitha tangentplanethatspansthedirectionsofvariationsassociatedwiththediÔ¨Äerence vectorsbetweentheexampleanditsneighbors,asillustratedinÔ¨Ågure.14.8 Aglobalcoordinatesystemcanthenbeobtainedthroughanoptimization or solvingalinearsystem.Figureillustrateshowamanifoldcanbetiledbya 14.9 largenumberoflocallylinearGaussian-likepatches(or‚Äúpancakes,‚Äùbecausethe GaussiansareÔ¨Çatinthetangentdirections). However,thereisafundamentaldiÔ¨Écultywithsuchlocalnon-parametric approachestomanifoldlearning,raisedin ():ifthe BengioandMonperrus2005 manifoldsarenotverysmooth(theyhavemanypeaksandtroughsandtwists), onemayneedaverylargenumberoftrainingexamplestocovereachoneof 5 1 9 CHAPTER14.AUTOENCODERS Figure14.9:Ifthetangentplanes(seeÔ¨Ågure)ateachlocationareknown,thenthey 14.6 canbetiledtoformaglobalcoordinatesystemoradensityfunction.Eachlocalpatch canbethoughtofasalocalEuclideancoordinatesystemorasalocallyÔ¨ÇatGaussian,or ‚Äúpancake,‚Äùwithaverysmallvarianceinthedirectionsorthogonaltothepancakeanda verylargevarianceinthedirectionsdeÔ¨Åningthecoordinatesystemonthepancake.A mixtureoftheseGaussiansprovidesanestimateddensityfunction,asinthemanifold Parzenwindowalgorithm( ,)oritsnon-localneural-netbased VincentandBengio2003 variant( ,). Bengio e t a l .2006c thesevariations,withnochancetogeneralizetounseenvariations.Indeed,these methodscanonlygeneralizetheshapeofthemanifoldbyinterpolating between neighboringexamples.Unfortunately,themanifoldsinvolvedinAIproblemscan haveverycomplicatedstructurethatcanbediÔ¨Éculttocapturefromonlylocal interpolation.Considerforexamplethemanifoldresultingfromtranslationshown inÔ¨Ågure.Ifwewatchjustonecoordinatewithintheinputvector, 14.6 x i,asthe imageistranslated,wewillobservethatonecoordinateencountersapeakora troughinitsvalueonceforeverypeakortroughinbrightnessintheimage. In otherwords,thecomplexityofthepatternsofbrightnessinanunderlyingimage templatedrivesthecomplexityofthemanifoldsthataregeneratedbyperforming simpleimagetransformations.Thismotivatestheuseofdistributedrepresentations anddeeplearningforcapturingmanifoldstructure. 5 2 0 CHAPTER14.AUTOENCODERS 14.7ContractiveAutoencoders Thecontractiveautoencoder(,,)introducesanexplicitregularizer Rifai e t a l .2011ab onthecode h= f( x),encouragingthederivativesof ftobeassmallaspossible: ‚Ñ¶() = h ŒªÓÄçÓÄçÓÄçÓÄç‚àÇ f() x ‚àÇ xÓÄçÓÄçÓÄçÓÄç2 F. (14.18) Thepenalty‚Ñ¶( h)isthesquaredFrobeniusnorm(sumofsquaredelements)ofthe Jacobianmatrixofpartialderivativesassociatedwiththeencoderfunction. Thereisaconnectionbetweenthedenoisingautoencoderandthecontractive autoencoder: ()showedthatinthelimitofsmallGaussian AlainandBengio2013 input noise, the denoising reconstruction erroris equivalent toacontractive penaltyonthereconstructionfunctionthatmaps xto r= g( f( x)).Inother words,denoisingautoencodersmakethereconstructionfunctionresistsmallbut Ô¨Ånite-sizedperturbationsoftheinput,whilecontractiveautoencodersmakethe featureextractionfunctionresistinÔ¨Ånitesimalperturbationsoftheinput.When usingtheJacobian-basedcontractivepenaltytopretrainfeatures f( x)foruse withaclassiÔ¨Åer,thebestclassiÔ¨Åcationaccuracyusuallyresultsfromapplyingthe contractivepenaltyto f( x)ratherthanto g( f( x)).Acontractivepenaltyon f( x) alsohascloseconnectionstoscorematching,asdiscussedinsection.14.5.1 Thename c o n t r ac</div>
        </div>
    </div>

    <div class="question-card" id="q21">
        <div class="question-header">
            <span class="question-number">Question 21</span>
            <span class="question-type">multiple-choice</span>
        </div>

        <div class="question-text">In machine learning, the way concepts and data are represented significantly affects a model's ability to learn, generalize, and handle complex tasks, especially in high-dimensional spaces. Distributed representations are widely used in modern neural networks and have distinct statistical and computational advantages over traditional symbolic or non-distributed approaches.

Which feature of distributed representations most directly enables neural networks to efficiently generalize across exponentially many input configurations, mitigating the curse of dimensionality in high-dimensional spaces?

1) Assigning each input to a unique symbolic label   
2) Dividing input space into a fixed number of linear clusters   
3) Applying smoothness assumptions to reduce overfitting   
4) Encoding each concept with a single binary variable   
5) Using multiple independently set features to describe concepts, allowing exponential growth in representational capacity   
6) Restricting model capacity by limiting the number of parameters   
7) Relying exclusively on local kernel functions to represent data</div>

        <div class="answer-section">
            <div class="answer-label">‚úì Correct Answer:</div>
            <div class="answer-text">The correct answer is 5) Using multiple independently set features to describe concepts, allowing exponential growth in representational capacity.</div>
        </div>

        <div class="reference-section">
            <div class="reference-label">üìö Reference Text:</div>
            <button class="toggle-button" onclick="toggleReference(21)">
                Show/Hide Reference
            </button>
            <div id="ref21" class="reference-text hidden">CHAPTER15.REPRESENTATIONLEARNING beexpectedvialearningagenerativemodelthatattemptstorecoverthecausal factorsand. h p( )xh| 15. 4 D i s t ri b u t ed R ep res en t at i on Distributedrepresentationsofconcepts‚Äîrepresentationscomposedofmanyele- mentsthatcanbesetseparatelyfromeachother‚Äîareoneofthemostimportant toolsforrepresentationlearning.Distributedrepresentationsarepowerfulbecause theycanusenfeatureswithkvaluestodescribekndiÔ¨Äerentconcepts.Aswe haveseenthroughoutthisbook,bothneuralnetworkswithmultiplehiddenunits andprobabilisticmodelswithmultiplelatentvariablesmakeuseofthestrategyof distributedrepresentation. Wenowintroduceanadditionalobservation. Many deeplearningalgorithmsaremotivatedbytheassumptionthatthehiddenunits canlearntorepresenttheunderlyingcausalfactorsthatexplainthedata,as discussedinsection.Distributedrepresentationsarenaturalforthisapproach, 15.3 becauseeachdirectioninrepresentationspacecancorrespondtothevalueofa diÔ¨ÄerentunderlyingconÔ¨Ågurationvariable. Anexampleofadistributedrepresentationisavectorofnbinaryfeatures, whichcantake2nconÔ¨Ågurations, eachpotentiallycorrespondingtoadiÔ¨Äerent regionininputspace,asillustratedinÔ¨Ågure.Thiscanbecomparedwith 15.7 asymbolicrepresentation,wheretheinputisassociatedwithasinglesymbolor category.Iftherearensymbolsinthedictionary,onecanimaginenfeature detectors,eachcorrespondingtothedetectionofthepresenceoftheassociated category.InthatcaseonlyndiÔ¨ÄerentconÔ¨Ågurations oftherepresentationspace arepossible,carvingndiÔ¨Äerentregionsininputspace,asillustratedinÔ¨Ågure.15.8 Suchasymbolicrepresentationisalsocalledaone-hotrepresentation,sinceitcan becapturedbyabinaryvectorwithnbitsthataremutuallyexclusive(onlyone ofthemcanbeactive).AsymbolicrepresentationisaspeciÔ¨Åcexampleofthe broaderclassofnon-distributedrepresentations,whicharerepresentationsthat maycontainmanyentriesbutwithoutsigniÔ¨Åcantmeaningfulseparatecontrolover eachentry. Examplesoflearningalgorithms basedonnon-distributedrepresentations include: ‚Ä¢Clusteringmethods,includingthek-meansalgorithm:eachinputpointis assignedtoexactlyonecluster. ‚Ä¢k-nearestneighborsalgorithms:oneorafewtemplatesorprototypeexamples areassociatedwithagiveninput.Inthecaseofk>1,therearemultiple 5 4 6 CHAPTER15.REPRESENTATIONLEARNING h 1h 2 h 3 h = [ 1 , , 1 1 ]ÓÄ° h = [ 0 , , 1 1 ]ÓÄ°h = [ 1 , , 0 1 ]ÓÄ°h = [ 1 , , 1 0 ]ÓÄ° h = [ 0 , , 1 0 ]ÓÄ°h = [ 0 , , 0 1 ]ÓÄ°h = [ 1 , , 0 0 ]ÓÄ° Figure15.7:Illustrationofhowalearningalgorithmbasedonadistributedrepresentation breaksuptheinputspaceintoregions.Inthisexample,therearethreebinaryfeatures h 1,h 2,andh 3. EachfeatureisdeÔ¨Ånedbythresholdingtheoutputofalearned,linear transformation.Eachfeaturedivides R2intotwohalf-planes.Leth+ ibethesetofinput pointsforwhichh i=1andh‚àí ibethesetofinputpointsforwhichh i=0.Inthis illustration,eachlinerepresentsthedecisionboundaryforoneh i,withthecorresponding arrowpointingtotheh+ isideoftheboundary.Therepresentationasawholetakes onauniquevalueateachpossibleintersectionofthesehalf-planes.Forexample,the representationvalue[1,1,1]ÓÄæcorrespondstotheregionh+ 1‚à©h+ 2‚à©h+ 3.Comparethistothe non-distributedrepresentationsinÔ¨Ågure.Inthegeneralcaseof 15.8 dinputdimensions, adistributedrepresentationdivides Rdbyintersectinghalf-spacesratherthanhalf-planes. ThedistributedrepresentationwithnfeaturesassignsuniquecodestoO(nd)diÔ¨Äerent regions,whilethenearestneighboralgorithmwithnexamplesassignsuniquecodestoonly nregions.Thedistributedrepresentationisthusabletodistinguishexponentiallymany moreregionsthanthenon-distributedone.Keepinmindthatnotallhvaluesarefeasible (thereisnoh=0inthisexample)andthatalinearclassiÔ¨Åerontopofthedistributed representationisnotabletoassigndiÔ¨Äerentclassidentitiestoeveryneighboringregion; evenadeeplinear-thresholdnetworkhasaVCdimensionofonlyO(wwlog )wherew isthenumberofweights(,).Thecombinationofapowerfulrepresentation Sontag1998 layerandaweakclassiÔ¨Åerlayercanbeastrongregularizer;aclassiÔ¨Åertryingtolearn theconceptof‚Äúperson‚Äùversus‚Äúnotaperson‚ÄùdoesnotneedtoassignadiÔ¨Äerentclassto aninputrepresentedas‚Äúwomanwithglasses‚Äùthanitassignstoaninputrepresentedas ‚Äúmanwithoutglasses.‚ÄùThiscapacityconstraintencourageseachclassiÔ¨Åertofocusonfew h iandencouragestolearntorepresenttheclassesinalinearlyseparableway. h 5 4 7 CHAPTER15.REPRESENTATIONLEARNING valuesdescribingeachinput,buttheycannotbecontrolledseparatelyfrom eachother,sothisdoesnotqualifyasatruedistributedrepresentation. ‚Ä¢Decisiontrees:onlyoneleaf(andthenodesonthepathfromroottoleaf)is activatedwhenaninputisgiven. ‚Ä¢Gaussianmixturesandmixturesofexperts:thetemplates(clustercenters)or expertsarenowassociatedwithadegreeofactivation.Aswiththek-nearest neighborsalgorithm,eachinputisrepresentedwithmultiplevalues,but thosevaluescannotreadilybecontrolledseparatelyfromeachother. ‚Ä¢KernelmachineswithaGaussiankernel(orothersimilarlylocalkernel): althoughthedegreeofactivationofeach‚Äúsupportvector‚Äùortemplateexample isnowcontinuous-valued,thesameissuearisesaswithGaussianmixtures. ‚Ä¢Languageortranslationmodelsbasedonn-grams.Thesetofcontexts (sequencesofsymbols)ispartitionedaccordingtoatreestructureofsuÔ¨Éxes. Aleafmaycorrespondtothelasttwowordsbeingw 1andw 2,forexample. Separateparametersareestimatedforeachleafofthetree(withsomesharing beingpossible). Forsomeofthesenon-distributedalgorithms,theoutputisnotconstantby partsbutinsteadinterpolatesbetweenneighboringregions.Therelationship betweenthenumberofparameters(orexamples)andthenumberofregionsthey candeÔ¨Åneremainslinear. Animportantrelatedconceptthatdistinguishesadistributedrepresentation fromasymboliconeisthatgeneralizationarisesduetosharedattributesbetween diÔ¨Äerentconcepts.Aspuresymbols,‚Äúcat‚Äùand‚Äúdog‚Äùareasfarfromeachother asanyothertwosymbols.However,ifoneassociatesthemwithameaningful distributedrepresentation,thenmanyofthethingsthatcanbesaidaboutcats cangeneralizetodogsandvice-versa.Forexample,ourdistributedrepresentation maycontainentriessuchas‚Äúhas_fur‚Äùor‚Äúnumber_of_legs‚Äùthathavethesame valuefortheembeddingofboth‚Äúcat‚Äùand‚Äúdog.‚ÄùNeurallanguagemodelsthat operateondistributedrepresentationsofwordsgeneralizemuchbetterthanother modelsthatoperatedirectlyonone-hotrepresentationsofwords,asdiscussedin section.Distributedrepresentationsinducearich 12.4 similarityspace,inwhich semanticallycloseconcepts(orinputs)arecloseindistance,apropertythatis absentfrompurelysymbolicrepresentations. Whenandwhycantherebeastatisticaladvantagefromusingadistributed representationaspartofalearningalgorithm? D istributedrepresentationscan 5 4 8 CHAPTER15.REPRESENTATIONLEARNING Figure15.8:Illustrationofhowthenearestneighboralgorithmbreaksuptheinputspace intodiÔ¨Äerentregions.Thenearestneighboralgorithmprovidesanexampleofalearning algorithmbasedonanon-distributedrepresentation.DiÔ¨Äerentnon-distributedalgorithms mayhavediÔ¨Äerentgeometry, but theytypicallybreaktheinput spaceintoregions, w i t h a s e p a r a t e s e t o f p a r a m e t e r s f o r e a c h r e g i o n.Theadvantageofanon-distributed approachisthat,givenenoughparameters,itcanÔ¨Åtthetrainingsetwithoutsolvinga diÔ¨Écultoptimizationalgorithm,becauseitisstraightforwardtochooseadiÔ¨Äerentoutput i n d e p e n d e n t l yforeachregion.Thedisadvantageisthatsuchnon-distributedmodels generalizeonlylocallyviathesmoothnessprior,makingitdiÔ¨Éculttolearnacomplicated functionwithmorepeaksandtroughsthantheavailablenumberofexamples.Contrast thiswithadistributedrepresentation,Ô¨Ågure.15.7 5 4 9 CHAPTER15.REPRESENTATIONLEARNING haveastatisticaladvantagewhenanapparentlycomplicatedstructurecanbe compactlyrepresentedusingasmallnumberofparameters.Sometraditionalnon- distributedlearningalgorithmsgeneralizeonlyduetothesmoothnessassumption, whichstatesthatifuv‚âà,thenthetargetfunctionftobelearnedhasthe propertythatf(u)‚âàf(v),ingeneral.Therearemanywaysofformalizingsuchan assumption,buttheendresultisthatifwehaveanexample (x,y)forwhichwe knowthatf(x)‚âày,thenwechooseanestimator ÀÜfthatapproximatelysatisÔ¨Åes theseconstraintswhilechangingaslittleaspossiblewhenwemovetoanearby inputx+ÓÄè.Thisassumptionisclearlyveryuseful,butitsuÔ¨Äersfromthecurseof dimensionality: inordertolearnatargetfunctionthatincreasesanddecreases manytimesinmanydiÔ¨Äerentregions,1wemayneedanumberofexamplesthatis atleastaslargeasthenumberofdistinguishableregions.Onecanthinkofeachof theseregionsasacategoryorsymbol:byhavingaseparatedegreeoffreedomfor eachsymbol(orregion),wecanlearnanarbitrarydecodermappingfromsymbol tovalue. However,thisdoesnotallowustogeneralizetonewsymbolsfornew regions. Ifwearelucky,theremaybesomeregularityinthetargetfunction,besidesbeing smooth.Forexample,aconvolutionalnetworkwithmax-poolingcanrecognizean objectregardlessofitslocationintheimage,eventhoughspatialtranslationof theobjectmaynotcorrespondtosmoothtransformationsintheinputspace. Letusexamineaspecialcaseofadistributedrepresentationlearningalgorithm, thatextractsbinaryfeaturesbythresholdinglinearfunctionsoftheinput.Each binaryfeatureinthisrepresentationdivides Rdintoapairofhalf-spaces, as illustratedinÔ¨Ågure.Theexponentiallylargenumberofintersectionsof 15.7 n ofthecorrespondinghalf-spacesdetermineshowmanyregionsthisdistributed representationlearnercandistinguish.Howmanyregionsaregeneratedbyan arrangementofnhyperplanesin Rd?Byapplyingageneralresultconcerningthe intersectionofhyperplanes(,),onecanshow( Zaslavsky1975 Pascanu2014betal.,) thatthenumberofregionsthisbinaryfeaturerepresentationcandistinguishis dÓÅò j = 0ÓÄín jÓÄì = (Ond). (15.4) Therefore,weseeagrowththatisexponentialintheinputsizeandpolynomialin thenumberofhiddenunits. 1P o t e n t i a l l y , we m a y w a n t t o l e a rn a f u n c t i o n wh o s e b e h a v i o r i s d i s t i n c t i n e x p o n e n t i a l l y m a n y re g i o n s : i n a d - d i m e n s i o n a l s p a c e with a t l e a s t 2 d i Ô¨Ä e re n t v a l u e s t o d i s t i n g u i s h p e r d i m e n s i o n , w e</div>
        </div>
    </div>

    <div class="question-card" id="q22">
        <div class="question-header">
            <span class="question-number">Question 22</span>
            <span class="question-type">multiple-choice</span>
        </div>

        <div class="question-text">Performance metrics are crucial in evaluating machine learning models, especially for tasks involving rare events or class imbalance. Selecting an inappropriate metric can lead to misleading assessments of model effectiveness.

Which metric is most appropriate for evaluating the ability of a classifier to detect rare positive events in a highly imbalanced dataset, providing a balanced measure of both precision and recall?

1) Area Under the Receiver Operating Characteristic (ROC) Curve   
2) Overall accuracy   
3) F-score (F1 score)   
4) Mean squared error   
5) Log-likelihood   
6) Matthews correlation coefficient   
7) R-squared</div>

        <div class="answer-section">
            <div class="answer-label">‚úì Correct Answer:</div>
            <div class="answer-text">The correct answer is 3) F-score (F1 score).</div>
        </div>

        <div class="reference-section">
            <div class="reference-label">üìö Reference Text:</div>
            <button class="toggle-button" onclick="toggleReference(22)">
                Show/Hide Reference
            </button>
            <div id="ref22" class="reference-text hidden">zeroerror.TheBayeserrordeÔ¨Ånestheminimumerrorratethatyoucanhopeto achieve,evenifyouhaveinÔ¨Ånitetrainingdataandcanrecoverthetrueprobability distribution.This isbecause your inputfeatures maynot contain complete informationabouttheoutputvariable,orbecausethesystemmightbeintrinsically stochastic.YouwillalsobelimitedbyhavingaÔ¨Åniteamountoftrainingdata. Theamountoftrainingdatacanbelimitedforavarietyofreasons.Whenyour goalistobuildthebestpossiblereal-worldproductorservice,youcantypically collectmoredatabutmustdeterminethevalueofreducingerrorfurtherandweigh thisagainstthecostofcollectingmoredata.Datacollectioncanrequiretime, money,orhumansuÔ¨Äering(forexample,ifyourdatacollectionprocessinvolves performinginvasivemedicaltests).WhenyourgoalistoanswerascientiÔ¨Åcquestion aboutwhichalgorithmperformsbetteronaÔ¨Åxedbenchmark,thebenchmark 4 2 2 CHAPTER11.PRACTICALMETHODOLOGY speciÔ¨Åcationusuallydeterminesthetrainingsetandyouarenotallowedtocollect moredata. Howcanonedetermineareasonablelevelofperformancetoexpect?Typically, intheacademicsetting,wehavesomeestimateoftheerrorratethatisattainable basedonpreviouslypublishedbenchmarkresults.Inthereal-wordsetting,we havesomeideaoftheerrorratethatisnecessaryforanapplicationtobesafe, cost-eÔ¨Äective,orappealingtoconsumers.Onceyouhavedeterminedyourrealistic desirederrorrate,yourdesigndecisionswillbeguidedbyreachingthiserrorrate. Anotherimportantconsiderationbesidesthetargetvalueoftheperformance metricisthechoiceofwhichmetrictouse.SeveraldiÔ¨Äerentperformancemetrics maybeusedtomeasuretheeÔ¨Äectivenessofacompleteapplicationthatincludes machinelearningcomponents.TheseperformancemetricsareusuallydiÔ¨Äerent fromthecostfunctionusedtotrainthemodel.Asdescribedinsection,itis5.1.2 commontomeasuretheaccuracy,orequivalently,theerrorrate,ofasystem. However,manyapplicationsrequiremoreadvancedmetrics. Sometimesitismuchmorecostlytomakeonekindofamistakethananother. Forexample,ane-mailspamdetectionsystemcanmaketwokindsofmistakes: incorrectlyclassifyingalegitimatemessageasspam,andincorrectlyallowinga spammessagetoappearintheinbox.Itismuchworsetoblockalegitimate messagethantoallowaquestionablemessagetopassthrough.Ratherthan measuringtheerrorrateofaspamclassiÔ¨Åer,wemaywishtomeasuresomeform oftotalcost,wherethecostofblockinglegitimatemessagesishigherthanthecost ofallowingspammessages. SometimeswewishtotrainabinaryclassiÔ¨Åerthatisintendedtodetectsome rareevent.Forexample,wemightdesignamedicaltestforararedisease.Suppose thatonlyoneineverymillionpeoplehasthisdisease.Wecaneasilyachieve 99.9999%accuracyonthedetectiontask,bysimplyhard-codingtheclassiÔ¨Åer toalwaysreportthatthediseaseisabsent.Clearly,accuracyisapoorwayto characterizetheperformanceofsuchasystem.Onewaytosolvethisproblemis toinsteadmeasure pr e c i si o nand r e c al l.Precisionisthefractionofdetections reportedbythemodelthatwerecorrect,whilerecallisthefractionoftrueevents thatweredetected.Adetectorthatsaysnoonehasthediseasewouldachieve perfectprecision,butzerorecall.Adetectorthatsayseveryonehasthedisease wouldachieveperfectrecall,butprecisionequaltothepercentageofpeoplewho havethedisease(0.0001%inourexampleofadiseasethatonlyonepeopleina millionhave).Whenusingprecisionandrecall,itiscommontoplota P R c ur v e, withprecisiononthe y-axisandrecallonthe x-axis.TheclassiÔ¨Åergeneratesascore thatishigheriftheeventtobedetectedoccurred. Forexample,afeedforward 4 2 3 CHAPTER11.PRACTICALMETHODOLOGY networkdesignedtodetectadiseaseoutputs ÀÜ y= P( y=1| x),estimatingthe probabilitythatapersonwhosemedicalresultsaredescribedbyfeatures xhas thedisease.Wechoosetoreportadetectionwheneverthisscoreexceedssome threshold. Byvaryingthethreshold,wecantradeprecisionforrecall. Inmany cases,wewishtosummarizetheperformanceoftheclassiÔ¨Åerwithasinglenumber ratherthanacurve.Todoso,wecanconvertprecision pandrecall rintoan F-scor egivenby F=2 pr p r+. (11.1) AnotheroptionistoreportthetotalarealyingbeneaththePRcurve. Insomeapplications,itispossibleforthemachinelearningsystemtorefuseto makeadecision.Thisisusefulwhenthemachinelearningalgorithmcanestimate howconÔ¨Ådentitshouldbeaboutadecision,especiallyifawrongdecisioncan beharmfulandifahumanoperatorisabletooccasionallytakeover.TheStreet Viewtranscriptionsystemprovidesanexampleofthissituation.Thetaskisto transcribetheaddressnumberfromaphotographinordertoassociatethelocation wherethephotowastakenwiththecorrectaddressinamap.Becausethevalue ofthemapdegradesconsiderablyifthemapisinaccurate,itisimportanttoadd anaddressonlyifthetranscriptioniscorrect.Ifthemachinelearningsystem thinksthatitislesslikelythanahumanbeingtoobtainthecorrecttranscription, thenthebestcourseofactionistoallowahumantotranscribethephotoinstead. Ofcourse,themachinelearningsystemisonlyusefulifitisabletodramatically reducetheamountofphotosthatthehumanoperatorsmustprocess.Anatural performancemetrictouseinthissituationis c o v e r age.Coverageisthefraction ofexamplesforwhichthemachinelearningsystemisabletoproducearesponse. Itispossibletotradecoverageforaccuracy.Onecanalwaysobtain100%accuracy byrefusingtoprocessanyexample,butthisreducesthecoverageto0%.Forthe StreetViewtask,thegoalfortheprojectwastoreachhuman-leveltranscription accuracywhilemaintaining95%coverage.Human-levelperformanceonthistask is98%accuracy. Manyothermetricsarepossible.Wecanforexample,measureclick-through rates,collectusersatisfactionsurveys,andsoon. Manyspecializedapplication areashaveapplication-speciÔ¨Åccriteriaaswell. Whatisimportantistodeterminewhichperformancemetrictoimproveahead oftime,thenconcentrateonimprovingthismetric.WithoutclearlydeÔ¨Ånedgoals, itcanbediÔ¨Éculttotellwhetherchangestoamachinelearningsystemmake progressornot. 4 2 4 CHAPTER11.PRACTICALMETHODOLOGY 11.2DefaultBaselineModels Afterchoosingperformancemetricsandgoals, thenextstepinanypractical applicationistoestablishareasonableend-to-endsystemassoonaspossible.In thissection,weproviderecommendations forwhichalgorithmstouseastheÔ¨Årst baselineapproachinvarioussituations.Keepinmindthatdeeplearningresearch progressesquickly,sobetterdefaultalgorithmsarelikelytobecomeavailablesoon afterthiswriting. Dependingonthecomplexityofyourproblem,youmayevenwanttobegin withoutusingdeeplearning.Ifyourproblemhasachanceofbeingsolvedby justchoosingafewlinearweightscorrectly,youmaywanttobeginwithasimple statisticalmodellikelogisticregression. Ifyouknowthatyourproblemfallsintoan‚ÄúAI-complete‚Äùcategorylikeobject recognition,speechrecognition,machinetranslation,andsoon,thenyouarelikely todowellbybeginningwithanappropriatedeeplearningmodel. First,choosethegeneralcategoryofmodelbasedonthestructureofyour data.IfyouwanttoperformsupervisedlearningwithÔ¨Åxed-sizevectorsasinput, useafeedforwardnetworkwithfullyconnectedlayers.Iftheinputhasknown topologicalstructure(forexample,iftheinputisanimage),useaconvolutional network.Inthesecases,youshouldbeginbyusingsomekindofpiecewiselinear unit(ReLUsortheirgeneralizations likeLeakyReLUs,PreLusandmaxout).If yourinputoroutputisasequence,useagatedrecurrentnet(LSTMorGRU). Areasonablechoiceofoptimization algorithmisSGDwithmomentumwitha decayinglearningrate(populardecayschemesthatperformbetterorworseon diÔ¨ÄerentproblemsincludedecayinglinearlyuntilreachingaÔ¨Åxedminimumlearning rate,decayingexponentially,ordecreasingthelearningratebyafactorof2-10 eachtimevalidationerrorplateaus).AnotherveryreasonablealternativeisAdam. Batchnormalization canhaveadramaticeÔ¨Äectonoptimization performance, especiallyforconvolutionalnetworksandnetworkswithsigmoidalnonlinearities. Whileitisreasonabletoomitbatchnormalization fromtheveryÔ¨Årstbaseline,it shouldbeintroducedquicklyifoptimization appearstobeproblematic. Unlessyourtrainingsetcontainstensofmillionsofexamplesormore,you shouldincludesomemildformsofregularizationfromthestart.Earlystopping shouldbeusedalmostuniversally.Dropoutisanexcellentregularizerthatiseasy toimplementandcompatiblewithmanymodelsandtrainingalgorithms.Batch normalization alsosometimesreducesgeneralization errorandallowsdropoutto beomitted,duetothenoiseintheestimateofthestatisticsusedtonormalize eachvariable. 4 2 5</div>
        </div>
    </div>

    <div class="question-card" id="q23">
        <div class="question-header">
            <span class="question-number">Question 23</span>
            <span class="question-type">multiple-choice</span>
        </div>

        <div class="question-text">Annealed Importance Sampling (AIS) is a technique used in probabilistic modeling and statistical physics to estimate ratios of partition functions, especially when direct computation is infeasible. AIS utilizes a sequence of intermediate distributions and specialized sampling methods to ensure reliable estimation.

Which of the following statements accurately describes the role of intermediate distributions in AIS for estimating partition function ratios?

1) They are used to reduce computational cost by eliminating the need for Markov chain transitions.   
2) They divide the proposal distribution into independent components for parallel sampling.   
3) They enable direct sampling from the target distribution without importance weights.   
4) They bridge the proposal and target distributions through closely spaced transitions, allowing reliable importance sampling between adjacent distributions.   
5) They ensure that the partition functions of all distributions are equal, simplifying estimation.   
6) They guarantee numerical stability by preventing the use of logarithms in computations.   
7) They replace the need for an extended state space by collapsing all steps into a single sampling operation.</div>

        <div class="answer-section">
            <div class="answer-label">‚úì Correct Answer:</div>
            <div class="answer-text">The correct answer is 4) They bridge the proposal and target distributions through closely spaced transitions, allowing reliable importance sampling between adjacent distributions..</div>
        </div>

        <div class="reference-section">
            <div class="reference-label">üìö Reference Text:</div>
            <button class="toggle-button" onclick="toggleReference(23)">
                Show/Hide Reference
            </button>
            <div id="ref23" class="reference-text hidden">CHAPTER18.CONFRONTINGTHEPARTITIONFUNCTION WecannowwritetheratioZ1 Z0as Z 1 Z 0=Z 1 Z 0Z Œ∑1 Z Œ∑1¬∑¬∑¬∑Z Œ∑ n ‚àí1 Z Œ∑ n ‚àí1(18.47) =Z Œ∑1 Z 0Z Œ∑2 Z Œ∑1¬∑¬∑¬∑Z Œ∑ n ‚àí1 Z Œ∑ n ‚àí2Z 1 Z Œ∑ n ‚àí1(18.48) =n ‚àí 1ÓÅô j = 0Z Œ∑ j+1 Z Œ∑ j(18.49) Providedthedistributions p Œ∑ jand p Œ∑ j + 1,forall0‚â§‚â§‚àí j n1,aresuÔ¨Éciently close,wecanreliablyestimateeachofthefactorsZ Œ∑ j+1 Z Œ∑ jusingsimpleimportance samplingandthenusethesetoobtainanestimateofZ1 Z0. Wheredotheseintermediatedistributionscomefrom?Justastheoriginal proposaldistribution p 0isadesignchoice,soisthesequenceofdistributions p Œ∑1 . . . p Œ∑ n ‚àí1.Thatis,itcanbespeciÔ¨Åcallyconstructedtosuittheproblemdomain. Onegeneral-purposeandpopularchoicefortheintermediate distributionsisto usetheweightedgeometricaverageofthetargetdistribution p 1andthestarting proposaldistribution(forwhichthepartitionfunctionisknown) p 0: p Œ∑ j‚àù pŒ∑ j 1 p1 ‚àí Œ∑ j 0 (18.50) Inordertosamplefromtheseintermediate distributions,wedeÔ¨Åneaseriesof Markovchaintransitionfunctions T Œ∑ j( xÓÄ∞| x) thatdeÔ¨Ånetheconditionalprobability distributionoftransitioningto xÓÄ∞givenwearecurrentlyat x.Thetransition operator T Œ∑ j( xÓÄ∞| x)isdeÔ¨Ånedtoleave p Œ∑ j() xinvariant: p Œ∑ j() = xÓÅö p Œ∑ j( xÓÄ∞) T Œ∑ j( x x|ÓÄ∞) d xÓÄ∞(18.51) ThesetransitionsmaybeconstructedasanyMarkovchainMonteCarlomethod (e.g.,Metropolis-Hastings,Gibbs),includingmethodsinvolvingmultiplepasses throughalloftherandomvariablesorotherkindsofiterations. TheAISsamplingstrategyisthentogeneratesamplesfrom p 0andthenuse thetransitionoperatorstosequentiallygeneratesamplesfromtheintermediate distributionsuntilwearriveatsamplesfromthetargetdistribution p 1: ‚Ä¢for k . . . K = 1 ‚ÄìSample x( ) k Œ∑1‚àº p 0()x 626 CHAPTER18.CONFRONTINGTHEPARTITIONFUNCTION ‚ÄìSample x( ) k Œ∑2‚àº T Œ∑1(x( ) k Œ∑2| x( ) k Œ∑1) ‚Äì... ‚ÄìSample x( ) k Œ∑ n ‚àí1‚àº T Œ∑ n ‚àí2(x( ) k Œ∑ n ‚àí1| x( ) k Œ∑ n ‚àí2) ‚ÄìSample x( ) k Œ∑ n‚àº T Œ∑ n ‚àí1(x( ) k Œ∑ n| x( ) k Œ∑ n ‚àí1) ‚Ä¢end Forsample k,wecanderivetheimportanceweightbychainingtogetherthe importanceweightsforthejumpsbetweentheintermediatedistributionsgivenin equation:18.49 w( ) k=Àú p Œ∑1( x( ) k Œ∑1) Àú p 0( x( ) k Œ∑1)Àú p Œ∑2( x( ) k Œ∑2) Àú p Œ∑1( x( ) k Œ∑2). . .Àú p 1( x( ) k 1) Àú p Œ∑ n ‚àí1( x( ) k Œ∑ n). (18.52) ToavoidnumericalissuessuchasoverÔ¨Çow,itisprobablybesttocompute log w( ) kby addingandsubtractinglogprobabilities, ratherthancomputing w( ) kbymultiplying anddividingprobabilities. WiththesamplingprocedurethusdeÔ¨Ånedandtheimportanceweightsgiven inequation,theestimateoftheratioofpartitionfunctionsisgivenby: 18.52 Z 1 Z 0‚âà1 KKÓÅò k = 1w( ) k(18.53) InordertoverifythatthisproceduredeÔ¨Ånesavalidimportancesampling scheme,wecanshow(,)thattheAISprocedurecorrespondstosimple Neal2001 importancesamplingonanextendedstatespacewithpointssampledoverthe productspace [ x Œ∑1 , . . . , x Œ∑ n ‚àí1 , x 1].Todothis,wedeÔ¨Ånethedistributionoverthe extendedspaceas: Àú p( x Œ∑1 , . . . , x Œ∑ n ‚àí1 , x 1) (18.54) =Àú p 1( x 1)Àú T Œ∑ n ‚àí1( x Œ∑ n ‚àí1| x 1)Àú T Œ∑ n ‚àí2( x Œ∑ n ‚àí2| x Œ∑ n ‚àí1) . . .Àú T Œ∑1( x Œ∑1| x Œ∑2) ,(18.55) where Àú T aisthereverseofthetransitionoperatordeÔ¨Ånedby T a(viaanapplication ofBayes‚Äôrule): Àú T a( xÓÄ∞| x) =p a( xÓÄ∞) p a() xT a( x x|ÓÄ∞) =Àú p a( xÓÄ∞) Àú p a() xT a( x x|ÓÄ∞) .(18.56) Pluggingtheaboveintotheexpressionforthejointdistributionontheextended statespacegiveninequation,weget:18.55 Àú p( x Œ∑1 , . . . , x Œ∑ n ‚àí1 , x 1) (18.57) 627 CHAPTER18.CONFRONTINGTHEPARTITIONFUNCTION =Àú p 1( x 1)Àú p Œ∑ n ‚àí1( x Œ∑ n ‚àí1) Àú p Œ∑ n ‚àí1( x 1)T Œ∑ n ‚àí1( x 1| x Œ∑ n ‚àí1)n ‚àí 2ÓÅô i = 1Àú p Œ∑ i( x Œ∑ i) Àú p Œ∑ i( x Œ∑ i+1)T Œ∑ i( x Œ∑ i+1| x Œ∑ i) (18.58) =Àú p 1( x 1) Àú p Œ∑ n ‚àí1( x 1)T Œ∑ n</div>
        </div>
    </div>

    <div class="question-card" id="q24">
        <div class="question-header">
            <span class="question-number">Question 24</span>
            <span class="question-type">multiple-choice</span>
        </div>

        <div class="question-text">Deep learning models have achieved remarkable progress through improvements in data availability, computational resources, and algorithmic innovations. Regularization techniques are critical for ensuring that these large models generalize effectively to new, unseen data.

Which regularization method is most commonly implemented in deep neural networks to penalize the squared magnitude of model weights, thereby shrinking them toward zero and improving generalization?

1) L2 regularization (weight decay)   
2) Dropout   
3) Batch normalization   
4) Early stopping   
5) Data augmentation   
6) L1 regularization   
7) Ensemble averaging</div>

        <div class="answer-section">
            <div class="answer-label">‚úì Correct Answer:</div>
            <div class="answer-text">The correct answer is 1) L2 regularization (weight decay).</div>
        </div>

        <div class="reference-section">
            <div class="reference-label">üìö Reference Text:</div>
            <button class="toggle-button" onclick="toggleReference(24)">
                Show/Hide Reference
            </button>
            <div id="ref24" class="reference-text hidden">CHAPTER6.DEEPFEEDFORWARDNETWORKS approachestogradientdescentarestillinuse.Mostoftheimprovementinneural networkperformancefrom1986to2015canbeattributedtotwofactors.First, largerdatasetshavereducedthedegreetowhichstatisticalgeneralization isa challengeforneuralnetworks.Second,neuralnetworkshavebecomemuchlarger, duetomorepowerfulcomputers,andbettersoftwareinfrastructure.However,a smallnumberofalgorithmicchangeshaveimprovedtheperformance ofneural networksnoticeably. Oneofthesealgorithmicchangeswasthereplacementofmeansquarederror withthecross-entropyfamilyoflossfunctions.Meansquarederrorwaspopularin the1980sand1990s,butwasgraduallyreplacedbycross-entropylossesandthe principleofmaximumlikelihoodasideasspreadbetweenthestatisticscommunity andthemachinelearningcommunity.Theuseofcross-entropylossesgreatly improvedtheperformanceofmodelswithsigmoidandsoftmaxoutputs,which hadpreviouslysuÔ¨Äeredfromsaturationandslowlearningwhenusingthemean squarederrorloss. Theothermajoralgorithmicchangethathasgreatlyimprovedtheperformance offeedforwardnetworkswasthereplacementofsigmoidhiddenunitswithpiecewise linearhiddenunits,suchasrectiÔ¨Åedlinearunits.RectiÔ¨Åcationusingthemax{0 , z} functionwasintroducedinearlyneuralnetworkmodelsanddatesbackatleast asfarastheCognitronandNeocognitron(Fukushima19751980,,).Theseearly modelsdid notuserectiÔ¨Åed linearunits, but insteadappliedrectiÔ¨Åcation to nonlinearfunctions.DespitetheearlypopularityofrectiÔ¨Åcation,rectiÔ¨Åcationwas largelyreplacedbysigmoidsinthe1980s,perhapsbecausesigmoidsperformbetter whenneuralnetworksareverysmall.Asoftheearly2000s,rectiÔ¨Åedlinearunits wereavoidedduetoasomewhatsuperstitiousbeliefthatactivationfunctionswith non-diÔ¨Äerentiablepointsmustbeavoided.Thisbegantochangeinabout2009. Jarrett2009 e t a l .()observedthat‚Äúusingarectifyingnonlinearityisthesinglemost importantfactorinimprovingtheperformanceofarecognitionsystem‚Äùamong severaldiÔ¨Äerentfactorsofneuralnetworkarchitecturedesign. Forsmalldatasets, ()observedthatusingrectifyingnon- Jarrett e t a l .2009 linearitiesisevenmoreimportantthanlearningtheweightsofthehiddenlayers. RandomweightsaresuÔ¨ÉcienttopropagateusefulinformationthrougharectiÔ¨Åed linearnetwork,allowingtheclassiÔ¨ÅerlayeratthetoptolearnhowtomapdiÔ¨Äerent featurevectorstoclassidentities. Whenmoredataisavailable,learningbeginstoextractenoughusefulknowledge toexceedtheperformanceofrandomlychosenparameters. () Glorot e t a l .2011a showedthatlearningisfareasierindeeprectiÔ¨Åedlinearnetworksthanindeep networksthathavecurvatureortwo-sidedsaturationintheiractivationfunctions. 2 2 6 CHAPTER6.DEEPFEEDFORWARDNETWORKS RectiÔ¨Åedlinearunitsarealsoofhistoricalinterestbecausetheyshowthat neurosciencehascontinuedtohave aninÔ¨Çuenceonthe developmentofdeep learningalgorithms. ()motivaterectiÔ¨Åedlinearunitsfrom Glorot e t a l .2011a biologicalconsiderations.Thehalf-rectifying nonlinearitywasintendedtocapture thesepropertiesofbiologicalneurons:1)Forsomeinputs,biologicalneuronsare completelyinactive.2)Forsomeinputs,abiologicalneuron‚Äôsoutputisproportional toitsinput.3)Mostofthetime,biologicalneuronsoperateintheregimewhere theyareinactive(i.e.,theyshouldhavesparseactivations). Whenthemodernresurgenceofdeeplearningbeganin2006,feedforward networkscontinuedtohaveabadreputation.Fromabout2006-2012,itwaswidely believedthatfeedforwardnetworkswouldnotperformwellunlesstheywereassisted byothermodels,suchasprobabilisticmodels.Today,itisnowknownthatwiththe rightresourcesandengineeringpractices,feedforwardnetworksperformverywell. Today,gradient-basedlearninginfeedforwardnetworksisusedasatooltodevelop probabilisticmodels,suchasthevariationalautoencoderandgenerativeadversarial networks,describedinchapter.Ratherthanbeingviewedasanunreliable 20 technologythatmustbesupportedbyothertechniques,gradient-basedlearningin feedforwardnetworkshasbeenviewedsince2012asapowerfultechnologythat maybeappliedtomanyothermachinelearningtasks.In2006,thecommunity usedunsupervisedlearningtosupportsupervisedlearning,andnow,ironically,it ismorecommontousesupervisedlearningtosupportunsupervisedlearning. FeedforwardnetworkscontinuetohaveunfulÔ¨Ålledpotential.Inthefuture,we expecttheywillbeappliedtomanymoretasks,andthatadvancesinoptimization algorithmsandmodeldesignwillimprovetheirperformanceevenfurther.This chapterhasprimarilydescribedtheneuralnetworkfamilyofmodels.Inthe subsequentchapters,weturntohowtousethesemodels‚Äîhowtoregularizeand trainthem. 2 2 7 C h a p t e r 7 Regularization f or D e e p L e ar n i n g Acentralprobleminmachinelearningishowtomakeanalgorithmthatwill performwellnotjustonthetrainingdata,butalsoonnewinputs.Manystrategies usedinmachinelearningareexplicitlydesignedtoreducethetesterror,possibly attheexpenseofincreasedtrainingerror.Thesestrategiesareknowncollectively asregularization. As wewillseethereareagreatmanyformsofregularization availabletothedeeplearningpractitioner. Infact, developingmoreeÔ¨Äective regularizationstrategieshasbeenoneofthemajorresearcheÔ¨ÄortsintheÔ¨Åeld. Chapterintroducedthebasicconceptsofgeneralization, underÔ¨Åtting,overÔ¨Åt- 5 ting,bias,varianceandregularization. Ifyouarenotalreadyfamiliarwiththese notions,pleaserefertothatchapterbeforecontinuingwiththisone. Inthischapter,wedescriberegularizationinmoredetail,focusingonregular- izationstrategiesfordeepmodelsormodelsthatmaybeusedasbuildingblocks toformdeepmodels. Somesectionsofthischapterdealwithstandardconceptsinmachinelearning. Ifyouarealreadyfamiliarwiththeseconcepts, feelfreetoskiptherelevant sections.However,mostofthischapterisconcernedwiththeextensionofthese basicconceptstotheparticularcaseofneuralnetworks. Insection,wedeÔ¨Ånedregularizationas‚ÄúanymodiÔ¨Åcationwemaketo 5.2.2 alearningalgorithmthatisintendedtoreduceitsgeneralization errorbutnot itstrainingerror.‚ÄùTherearemanyregularizationstrategies.Someputextra constraints ona machine learning model, such asadding restrictionson the parametervalues.Someaddextratermsintheobjectivefunctionthatcanbe thoughtofascorrespondingtoasoftconstraintontheparametervalues.Ifchosen carefully,theseextraconstraintsandpenaltiescanleadtoimprovedperformance 228 CHAPTER7.REGULARIZATIONFORDEEPLEARNING onthetestset.Sometimestheseconstraintsandpenaltiesaredesignedtoencode speciÔ¨Åckindsofpriorknowledge.Othertimes,theseconstraintsandpenalties aredesignedtoexpressagenericpreferenceforasimplermodelclassinorderto promotegeneralization. Sometimespenaltiesandconstraintsarenecessarytomake anunderdetermined problemdetermined.Otherformsofregularization,knownas ensemblemethods,combinemultiplehypothesesthatexplainthetrainingdata. Inthecontextofdeeplearning,mostregularizationstrategiesarebasedon regularizingestimators.Regularizationofanestimatorworksbytradingincreased biasforreducedvariance.AneÔ¨ÄectiveregularizerisonethatmakesaproÔ¨Åtable trade,reducingvariancesigniÔ¨Åcantlywhilenotoverlyincreasingthebias.Whenwe discussedgeneralization andoverÔ¨Åttinginchapter,wefocusedonthreesituations, 5 wherethemodelfamilybeingtrainedeither(1)excludedthetruedatagenerating process‚ÄîcorrespondingtounderÔ¨Åttingandinducingbias,or(2)matchedthetrue datageneratingprocess,or(3)includedthegeneratingprocessbutalsomany otherpossiblegeneratingprocesses‚ÄîtheoverÔ¨Åttingregimewherevariancerather thanbiasdominatestheestimationerror.Thegoalofregularizationistotakea modelfromthethirdregimeintothesecondregime. Inpractice,anoverlycomplexmodelfamilydoesnotnecessarilyincludethe targetfunctionorthetruedatageneratingprocess,orevenacloseapproximation ofeither.Wealmostneverhaveaccesstothetruedatageneratingprocessso wecanneverknowforsureifthemodelfamilybeingestimatedincludesthe generatingprocessornot.However,mostapplicationsofdeeplearningalgorithms aretodomainswherethetruedatageneratingprocessisalmostcertainlyoutside themodelfamily.Deeplearningalgorithmsaretypicallyappliedtoextremely complicateddomainssuchasimages,audiosequencesandtext,forwhichthetrue generationprocessessentiallyinvolvessimulatingtheentireuniverse.Tosome extent,wearealwaystryingtoÔ¨Åtasquarepeg(thedatageneratingprocess)into aroundhole(ourmodelfamily). Whatthismeansisthatcontrollingthecomplexityofthemodelisnota simplematterofÔ¨Åndingthemodeloftherightsize,withtherightnumberof parameters.Instead,wemightÔ¨Ånd‚Äîandindeedinpracticaldeeplearningscenarios, wealmostalwaysdoÔ¨Ånd‚ÄîthatthebestÔ¨Åttingmodel(inthesenseofminimizing generalization error)isalargemodelthathasbeenregularizedappropriately . Wenowreviewseveralstrategiesforhowtocreatesuchalarge,deep,regularized model. 2 2 9 CHAPTER7.REGULARIZATIONFORDEEPLEARNING 7.1ParameterNormPenalties Regularizationhasbeenusedfordecadespriortotheadventofdeeplearning.Linear modelssuchaslinearregressionandlogisticregressionallowsimple,straightforward, andeÔ¨Äectiveregularizationstrategies. Manyregularizationapproachesarebasedonlimitingthecapacityofmodels, suchasneuralnetworks,linearregression,orlogisticregression,byaddingapa- rameternormpenalty ‚Ñ¶(Œ∏)totheobjectivefunction J.Wedenotetheregularized objectivefunctionbyÀú J: Àú J , J , Œ± (;Œ∏Xy) = (;Œ∏Xy)+‚Ñ¶()Œ∏ (7.1) where Œ±‚àà[0 ,‚àû)isahyperparameter thatweightstherelativecontributionofthe normpenaltyterm,,relativetothestandardobjectivefunction ‚Ñ¶ J.Setting Œ±to0 resultsinnoregularization. Largervaluesof Œ±correspondtomoreregularization. Whenourtrainingalgorithmminimizestheregularizedobjectivefunction Àú Jit willdecreaseboththeoriginalobjective Jonthetrainingdataandsomemeasure ofthesizeoftheparametersŒ∏(orsomesubsetoftheparameters).DiÔ¨Äerent choicesfortheparameternormcanresultindiÔ¨Äerentsolutionsbeingpreferred. ‚Ñ¶ Inthissection,wediscusstheeÔ¨Äectsofthevariousnormswhenusedaspenalties onthemodelparameters. BeforedelvingintotheregularizationbehaviorofdiÔ¨Äerentnorms,wenotethat forneuralnetworks,wetypicallychoosetouseaparameternormpenaltythat‚Ñ¶ penalizes oftheaÔ¨Énetransformationateachlayerandleaves onlytheweights thebiasesunregularized. ThebiasestypicallyrequirelessdatatoÔ¨Åtaccurately thantheweights. EachweightspeciÔ¨Åeshowtwovariablesinteract. Fittingthe weightwellrequiresobservingbothvariablesinavarietyofconditions.Each biascontrolsonlyasinglevariable.Thismeansthatwedonotinducetoomuch variancebyleavingthebiasesunregularized. Also,regularizingthebiasparameters canintroduceasigniÔ¨ÅcantamountofunderÔ¨Åtting. Wethereforeusethevectorw toindicatealloftheweightsthatshouldbeaÔ¨Äectedbyanormpenalty,whilethe vectorŒ∏denotesalloftheparameters,includingbothwandtheunregularized parameters. Inthecontextofneuralnetworks,itissometimesdesirabletouseaseparate penaltywithadiÔ¨Äerent Œ±coeÔ¨Écientforeachlayerofthenetwork.Becauseitcan beexpensivetosearchforthecorrectvalueofmultiplehyperparameters,itisstill reasonabletousethesameweightdecayatalllayersjusttoreducethesizeof searchspace. 2 3 0 CHAPTER7.REGULARIZATIONFORDEEPLEARNING 7 . 1 . 1 L2P a ra m et e r Regu l a ri z a t i o n Wehavealreadyseen,insection,oneofthesimplestandmostcommonkinds 5.2.2 ofparameternormpenalty:the L2parameternormpenaltycommonlyknownas weightdecay.Thisregularizationstrategydrivestheweightsclosertotheorigin1 byaddingaregularizationterm‚Ñ¶(Œ∏) =1 2ÓÅ´ÓÅ´w2 2totheobjectivefunction.Inother academiccommunities, L2regularizationisalsoknownasridgeregressionor Tikhonovregularization. Wecangainsomeinsightintothebehaviorofweightdecayregularization bystudyingthegradientoftheregularizedobjectivefunction.Tosimplifythe presentation,weassumenobiasparameter,soŒ∏isjustw.Suchamodelhasthe followingtotalobjectivefunction: Àú J , (;wXy) =Œ± 2wÓÄæwwXy +( J; ,) , (7.2) withthecorrespondingparametergradient ‚àá wÀú J , Œ± (;wXy) = w+‚àá w J , . (;wXy) (7.3) Totakeasinglegradientsteptoupdatetheweights,weperformthisupdate: www ‚Üê ‚àí ÓÄè Œ±( +‚àá w J , . (;wXy)) (7.4) Writtenanotherway,theupdateis: ww ‚Üê ‚àí(1 ÓÄè Œ±)‚àí‚àá ÓÄè w J , . (;wXy) (7.5) WecanseethattheadditionoftheweightdecaytermhasmodiÔ¨Åedthelearning ruletomultiplicativelyshrinktheweightvectorbyaconstantfactoroneachstep, justbeforeperformingtheusualgradientupdate.Thisdescribeswhathappensin asinglestep.Butwhathappensovertheentirecourseoftraining? Wewillfurthersimplifytheanalysisbymakingaquadraticapproximation totheobjectivefunctionintheneighborhoodofthevalueoftheweightsthat obtainsminimalunregularized trainingcost,w‚àó=argminw J(w).Iftheobjective functionistrulyquadratic,asinthecaseofÔ¨Åttingalinearregressionmodelwith 1M o re g e n e ra l l y , we c o u l d re g u l a riz e t h e p a ra m e t e rs t o b e n e a r a n y s p e c i Ô¨Å c p o i n t i n s p a c e a n d , s u rp ris i n g l y , s t i l l g e t a re g u l a riz a t i o n e Ô¨Ä e c t ,</div>
        </div>
    </div>

    <div class="question-card" id="q25">
        <div class="question-header">
            <span class="question-number">Question 25</span>
            <span class="question-type">multiple-choice</span>
        </div>

        <div class="question-text">Dropout is a widely used regularization technique in neural networks that mitigates overfitting by randomly deactivating units during training and approximates ensemble predictions during inference. Understanding the differences between dropout and ensemble methods like bagging, as well as the role of the weight scaling rule, is crucial for effective model implementation.

Which statement most accurately describes the role of the weight scaling rule during inference with dropout in neural networks?

1) It increases each neuron's activation by the dropout rate to match training statistics.   
2) It involves averaging the outputs from all possible sub-networks for exact prediction.   
3) It reinitializes network weights randomly to approximate the ensemble output.   
4) It requires storing separate parameters for each sampled sub-network to avoid overfitting.   
5) It scales each unit's outgoing weights by its inclusion probability, allowing ensemble prediction approximation with a single forward pass.   
6) It disables dropout masks entirely and uses the unmodified network for prediction.   
7) It applies the geometric mean to combine outputs from sampled sub-models.</div>

        <div class="answer-section">
            <div class="answer-label">‚úì Correct Answer:</div>
            <div class="answer-text">The correct answer is 5) It scales each unit's outgoing weights by its inclusion probability, allowing ensemble prediction approximation with a single forward pass..</div>
        </div>

        <div class="reference-section">
            <div class="reference-label">üìö Reference Text:</div>
            <button class="toggle-button" onclick="toggleReference(25)">
                Show/Hide Reference
            </button>
            <div id="ref25" class="reference-text hidden">withdropout. Moreformally,supposethatamaskvector¬µspeciÔ¨Åeswhichunitstoinclude, and J(Œ∏¬µ ,)deÔ¨ÅnesthecostofthemodeldeÔ¨ÅnedbyparametersŒ∏andmask¬µ. Thendropouttrainingconsistsinminimizing E ¬µ J(Œ∏¬µ ,).Theexpectationcontains exponentiallymanytermsbutwecanobtainanunbiasedestimateofitsgradient bysamplingvaluesof.¬µ Dropouttrainingisnotquitethesameasbaggingtraining.Inthecaseof bagging,themodelsareallindependent.Inthecaseofdropout,themodelsshare parameters,witheachmodelinheritingadiÔ¨Äerentsubsetofparametersfromthe parentneuralnetwork.Thisparametersharingmakesitpossibletorepresentan exponentialnumberofmodelswithatractableamountofmemory.Inthecaseof bagging,eachmodelistrainedtoconvergenceonitsrespectivetrainingset.Inthe caseofdropout,typicallymostmodelsarenotexplicitlytrainedatall‚Äîusually, themodelislargeenoughthatitwouldbeinfeasibletosampleallpossiblesub- networkswithinthelifetimeoftheuniverse.Instead,atinyfractionofthepossible sub-networksareeachtrainedforasinglestep,andtheparametersharingcauses theremainingsub-networkstoarriveatgoodsettingsoftheparameters.These aretheonlydiÔ¨Äerences.Beyondthese,dropoutfollowsthebaggingalgorithm.For example,thetrainingsetencounteredbyeachsub-networkisindeedasubsetof theoriginaltrainingsetsampledwithreplacement. 2 5 9 CHAPTER7.REGULARIZATIONFORDEEPLEARNING yy h 1 h 1 h 2 h 2 x 1 x 1 x 2 x 2yy h 1 h 1 h 2 h 2 x 1 x 1 x 2 x 2yy h 1 h 1 h 2 h 2 x 2 x 2yy h 1 h 1 h 2 h 2 x 1 x 1yy h 2 h 2 x 1 x 1 x 2 x 2 yy h 1 h 1 x 1 x 1 x 2 x 2yy h 1 h 1 h 2 h 2yy x 1 x 1 x 2 x 2yy h 2 h 2 x 2 x 2 yy h 1 h 1 x 1 x 1yy h 1 h 1 x 2 x 2yy h 2 h 2 x 1 x 1yy x 1 x 1 yy x 2 x 2yy h 2 h 2yy h 1 h 1yyB ase ne t w or k E nse m bl e of s u b n e t w or k s Figure 7.6:Dropout trainsan ensemble consistingof allsub-networks that canbe constructedbyremovingnon-outputunitsfromanunderlyingbasenetwork.Here,we beginwithabasenetworkwithtwovisibleunitsandtwohiddenunits.Therearesixteen possiblesubsetsofthesefourunits.Weshowallsixteensubnetworksthatmaybeformed bydroppingoutdiÔ¨Äerentsubsetsofunitsfromtheoriginalnetwork.Inthissmallexample, alargeproportionoftheresultingnetworkshavenoinputunitsornopathconnecting theinputtotheoutput.ThisproblembecomesinsigniÔ¨Åcantfornetworkswithwider layers,wheretheprobabilityofdroppingallpossiblepathsfrominputstooutputsbecomes smaller. 2 6 0 CHAPTER7.REGULARIZATIONFORDEEPLEARNING ÀÜ x 1ÀÜ x 1 ¬µ x 1 ¬µ x 1 x 1 x 1ÀÜ x 2ÀÜ x 2 x 2 x 2 ¬µ x 2 ¬µ x 2h 1 h 1 h 2 h 2¬µ h 1 ¬µ h 1 ¬µ h 2 ¬µ h 2ÀÜ h 1ÀÜ h 1ÀÜ h 2ÀÜ h 2yyyy h 1 h 1 h 2 h 2 x 1 x 1 x 2 x 2 Figure7.7:Anexampleofforwardpropagationthroughafeedforwardnetworkusing dropout. ( T o p )Inthisexample,weuseafeedforwardnetworkwithtwoinputunits,one hiddenlayerwithtwohiddenunits,andoneoutputunit.Toperformforward ( Bottom ) propagationwithdropout,werandomlysampleavector¬µwithoneentryforeachinput orhiddenunitinthenetwork.Theentriesof¬µarebinaryandaresampledindependently fromeachother.Theprobabilityofeachentrybeingisahyperparameter,usually 1 0 .5 forthehiddenlayersand0 .8fortheinput.Eachunitinthenetworkismultipliedby thecorrespondingmask,andthenforwardpropagationcontinuesthroughtherestofthe networkasusual.Thisisequivalenttorandomlyselectingoneofthesub-networksfrom Ô¨Ågureandrunningforwardpropagationthroughit. 7.6 2 6 1 CHAPTER7.REGULARIZATIONFORDEEPLEARNING Tomakeaprediction,abaggedensemblemustaccumulatevotesfromallof itsmembers.Werefertothisprocessasinferenceinthiscontext. Sofar,our descriptionofbagginganddropouthasnotrequiredthatthemodelbeexplicitly probabilistic.Now,weassumethatthemodel‚Äôsroleistooutputaprobability distribution.Inthecaseofbagging,eachmodel iproducesaprobabilitydistribution p( ) i( y|x).Thepredictionoftheensembleisgivenbythearithmeticmeanofall ofthesedistributions, 1 kkÓÅò i = 1p( ) i( ) y|x . (7.52) Inthecaseofdropout,eachsub-modeldeÔ¨Ånedbymaskvector¬µdeÔ¨Ånesaprob- abilitydistribution p( y ,|x¬µ).Thearithmeticmeanoverallmasksisgiven byÓÅò ¬µp p y , ()¬µ(|x¬µ) (7.53) where p(¬µ)istheprobabilitydistributionthatwasusedtosample¬µattraining time. Becausethissumincludesanexponentialnumberofterms,itisintractable toevaluateexceptincaseswherethestructureofthemodelpermitssomeform ofsimpliÔ¨Åcation.Sofar,deepneuralnetsarenotknowntopermitanytractable simpliÔ¨Åcation.Instead, wecan approximatetheinferencewithsampling, by averagingtogethertheoutputfrommanymasks.Even10-20masksareoften suÔ¨Écienttoobtaingoodperformance. However,thereisanevenbetterapproach,thatallowsustoobtainagood approximationtothepredictionsoftheentireensemble,atthecostofonlyone forwardpropagation. Todoso,wechangetousingthegeometricmeanratherthan thearithmeticmeanoftheensemblemembers‚Äôpredicteddistributions.Warde- Farley2014etal.()presentargumentsandempiricalevidencethatthegeometric meanperformscomparablytothearithmeticmeaninthiscontext. Thegeometricmeanofmultipleprobabilitydistributionsisnotguaranteedtobe aprobabilitydistribution.Toguaranteethattheresultisaprobabilitydistribution, weimposetherequirementthatnoneofthesub-modelsassignsprobability0toany event,andwerenormalizetheresultingdistribution.Theunnormalized probability distributiondeÔ¨Åneddirectlybythegeometricmeanisgivenby Àú p e nse m bl e( ) = y|x 2dÓÅ≥ÓÅô ¬µp y , (|x¬µ) (7.54) where disthenumberofunitsthatmaybedropped.Hereweuseauniform distributionover¬µtosimplifythepresentation,butnon-uniformdistributionsare 2 6 2 CHAPTER7.REGULARIZATIONFORDEEPLEARNING alsopossible.Tomakepredictionswemustre-normalizetheensemble: p e nse m bl e( ) = y|xÀú p e nse m bl e( ) y|xÓÅê yÓÄ∞Àú p e nse m bl e( yÓÄ∞|x). (7.55) Akeyinsight( ,)involvedindropoutisthatwecanapproxi- Hintonetal.2012c mate p e nse m bl ebyevaluating p( y|x)inonemodel:themodelwithallunits,but withtheweightsgoingoutofunit imultipliedbytheprobabilityofincludingunit i.ThemotivationforthismodiÔ¨Åcationistocapturetherightexpectedvalueofthe outputfromthatunit.Wecallthisapproachtheweightscalinginferencerule. Thereisnotyetanytheoreticalargumentfortheaccuracyofthisapproximate inferenceruleindeepnonlinearnetworks,butempiricallyitperformsverywell. Becauseweusuallyuseaninclusionprobabilityof1 2,theweightscalingrule usuallyamountstodividingtheweightsbyattheendoftraining,andthenusing 2 themodelasusual.Anotherwaytoachievethesameresultistomultiplythe statesoftheunitsbyduringtraining.Eitherway,thegoalistomakesurethat 2 theexpectedtotalinputtoaunitattesttimeisroughlythesameastheexpected totalinputtothatunitattraintime,eventhoughhalftheunitsattraintimeare missingonaverage. Formanyclassesofmodelsthatdonothavenonlinearhiddenunits,theweight scalinginferenceruleisexact.Forasimpleexample,considerasoftmaxregression classiÔ¨Åerwithinputvariablesrepresentedbythevector: n v P y (= y | v) = softmaxÓÄê WÓÄæv+bÓÄë y. (7.56) Wecanindexintothefamilyofsub-modelsbyelement-wisemultiplicationofthe inputwithabinaryvector: d P y (= y | v;) = dsoftmaxÓÄê WÓÄæ( )+dÓÄå vbÓÄë y.(7.57) TheensemblepredictorisdeÔ¨Ånedbyre-normalizingthegeometricmeanoverall ensemblemembers‚Äôpredictions: P e nse m bl e(= ) =y y| vÀú P</div>
        </div>
    </div>

    <div class="question-card" id="q26">
        <div class="question-header">
            <span class="question-number">Question 26</span>
            <span class="question-type">multiple-choice</span>
        </div>

        <div class="question-text">Deep learning models rely on computational graphs and efficient algorithms to calculate gradients necessary for training. The back-propagation algorithm is a foundational method for propagating derivatives through such graphs, crucial for optimizing neural network parameters.

What is the principal reason the back-propagation algorithm avoids the exponential increase in computation typically associated with repeated subexpressions when traversing a computational graph?

1) It stores and reuses intermediate values for every subexpression during the backward pass.   
2) It restricts the graph to only acyclic structures to prevent repeated computations.   
3) It combines all gradient computations into a single matrix operation at each layer.   
4) It only computes derivatives for the output node, ignoring intermediate nodes.   
5) It employs stochastic sampling of edges to approximate gradients.   
6) It recomputes every intermediate value each time it is needed, minimizing memory usage.   
7) It uses a fixed number of operations per node, regardless of graph structure.</div>

        <div class="answer-section">
            <div class="answer-label">‚úì Correct Answer:</div>
            <div class="answer-text">The correct answer is 1) It stores and reuses intermediate values for every subexpression during the backward pass..</div>
        </div>

        <div class="reference-section">
            <div class="reference-label">üìö Reference Text:</div>
            <button class="toggle-button" onclick="toggleReference(26)">
                Show/Hide Reference
            </button>
            <div id="ref26" class="reference-text hidden">) n: ‚àÇ u( ) n ‚àÇ u( ) j=ÓÅò i j P a u :‚àà (() i )‚àÇ u( ) n ‚àÇ u( ) i‚àÇ u( ) i ‚àÇ u( ) j(6.49) asspeciÔ¨Åedbyalgorithm .Thesubgraph6.2 Bcontainsexactlyoneedgeforeach edgefromnode u( ) jtonode u( ) iofG.Theedgefrom u( ) jto u( ) iisassociatedwith thecomputationof‚àÇ u() i ‚àÇ u() j.Inaddition,adotproductisperformedforeachnode, betweenthegradientalreadycomputedwithrespecttonodes u( ) ithatarechildren of u( ) jandthevectorcontainingthepartialderivatives‚àÇ u() i ‚àÇ u() jforthesamechildren nodes u( ) i.Tosummarize,theamountofcomputationrequiredforperforming theback-propagationscaleslinearlywiththenumberofedgesinG,wherethe computationforeachedgecorrespondstocomputingapartialderivative(ofone nodewithrespecttooneofitsparents)aswellasperformingonemultiplication andoneaddition.Below,wegeneralizethisanalysistotensor-valuednodes,which isjustawaytogroupmultiplescalarvaluesinthesamenodeandenablemore eÔ¨Écientimplementations. Theback-propagationalgorithmisdesignedtoreducethenumberofcommon subexpressionswithoutregardtomemory.SpeciÔ¨Åcally,itperformsontheorder ofoneJacobianproductpernodeinthegraph. Thiscanbeseenfromthefact thatbackprop(algorithm )visitseachedgefromnode 6.2 u( ) jtonode u( ) iof thegraphexactlyonceinordertoobtaintheassociatedpartialderivative‚àÇ u() i ‚àÇ u() j. 2 0 9 CHAPTER6.DEEPFEEDFORWARDNETWORKS Algorithm6.2SimpliÔ¨Åedversionoftheback-propagation algorithmforcomputing thederivativesof u( ) nwithrespecttothevariablesinthegraph.Thisexampleis intendedtofurtherunderstandingbyshowingasimpliÔ¨Åedcasewhereallvariables arescalars,andwewishtocomputethederivativeswithrespectto u( 1 ), . . . , u( n i ). ThissimpliÔ¨Åedversioncomputesthederivativesofallnodesinthegraph. The computational costofthisalgorithmisproportional tothenumberofedgesin thegraph,assumingthatthepartialderivativeassociatedwitheachedgerequires aconstanttime.Thisisofthesameorderasthenumberofcomputations for theforwardpropagation. Each‚àÇ u() i ‚àÇ u() jisafunctionoftheparents u( ) jof u( ) i,thus linkingthenodesoftheforwardgraphtothoseaddedfortheback-propagation graph. Runforwardpropagation(algorithm forthisexample)toobtaintheactiva- 6.1 tionsofthenetwork Initialize grad_table,adatastructurethatwillstorethederivativesthathave beencomputed.Theentry g r a d t a b l e_ [ u( ) i]willstorethecomputedvalueof ‚àÇ u() n ‚àÇ u() i. g r a d t a b l e_ [ u( ) n] 1‚Üê for do j n= ‚àí1downto1 Thenextlinecomputes‚àÇ u() n ‚àÇ u() j=ÓÅê i j P a u :‚àà (() i )‚àÇ u() n ‚àÇ u() i‚àÇ u() i ‚àÇ u() jusingstoredvalues: g r a d t a b l e_ [ u( ) j] ‚ÜêÓÅê i j P a u :‚àà (() i ) g r a d t a b l e_ [ u( ) i]‚àÇ u() i ‚àÇ u() j endfor return{ g r a d t a b l e_ [ u( ) i] = 1 | i , . . . , n i} Back-propagationthusavoidstheexponentialexplosioninrepeatedsubexpressions. However,otheralgorithmsmaybeabletoavoidmoresubexpressionsbyperforming simpliÔ¨Åcationsonthecomputational graph,ormaybeabletoconservememoryby recomputingratherthanstoringsomesubexpressions.Wewillrevisittheseideas afterdescribingtheback-propagation algorithmitself. 6.5.4Back-PropagationComputationinFully-ConnectedMLP ToclarifytheabovedeÔ¨Ånitionoftheback-propagation computation,letusconsider thespeciÔ¨Åcgraphassociatedwithafully-connected multi-layerMLP. AlgorithmÔ¨Årstshowstheforwardpropagation, whichmapsparametersto 6.3 thesupervisedloss L(ÀÜyy ,)associatedwithasingle(input,target) trainingexample ( )xy ,,with ÀÜytheoutputoftheneuralnetworkwhenisprovidedininput. x Algorithm then shows thecorresponding computation to be donefor 6.4 2 1 0 CHAPTER6.DEEPFEEDFORWARDNETWORKS z z xxyy w wfff Figure6.9:Acomputationalgraphthatresultsinrepeatedsubexpressionswhencomputing thegradient.Let w‚àà Rbetheinputtothegraph.Weusethesamefunction f: R R‚Üí astheoperationthatweapplyateverystepofachain: x= f( w), y= f( x), z= f( y). Tocompute‚àÇ z ‚àÇ w,weapplyequationandobtain: 6.44 ‚àÇ z ‚àÇ w(6.50) =‚àÇ z ‚àÇ y‚àÇ y ‚àÇ x‚àÇ x ‚àÇ w(6.51) = fÓÄ∞() y fÓÄ∞() x fÓÄ∞() w (6.52) = fÓÄ∞((())) f f w fÓÄ∞(()) f w fÓÄ∞() w (6.53) Equationsuggestsanimplementationinwhichwecomputethevalueof 6.52 f( w)only onceandstoreitinthevariable x.Thisistheapproachtakenbytheback-propagation algorithm.Analternativeapproachissuggestedbyequation,wherethesubexpression 6.53 f( w)appearsmorethanonce.Inthealternativeapproach, f( w)isrecomputedeachtime itisneeded.Whenthememoryrequiredtostorethevalueoftheseexpressionsislow,the back-propagationapproachofequationisclearlypreferablebecauseofitsreduced 6.52 runtime.However,equationisalsoavalidimplementationofthechainrule,andis 6.53 usefulwhenmemoryislimited. 2 1 1 CHAPTER6.DEEPFEEDFORWARDNETWORKS applyingtheback-propagation algorithmtothisgraph. Algorithms andaredemonstrationsthatarechosentobesimpleand 6.36.4 straightforwardtounderstand.However, theyarespecializedtoonespeciÔ¨Åc problem. Modernsoftwareimplementations arebasedonthegeneralizedformofback- propagationdescribedinsectionbelow,whichcanaccommodateanycompu- 6.5.6 tationalgraphbyexplicitlymanipulating adatastructureforrepresentingsymbolic computation. Algorithm6.3Forwardpropagationthroughatypicaldeepneuralnetworkand thecomputationofthecostfunction.Theloss L(ÀÜyy ,)dependsontheoutput ÀÜyandonthetargety(seesectionforexamplesoflossfunctions).To 6.2.1.1 obtainthetotalcost J,thelossmaybeaddedtoaregularizer ‚Ñ¶( Œ∏),where Œ∏ containsalltheparameters(weightsandbiases).Algorithm showshowto 6.4 computegradientsof JwithrespecttoparametersWandb.Forsimplicity,this demonstrationusesonlyasingleinputexamplex.Practicalapplicationsshould useaminibatch.Seesectionforamorerealisticdemonstration. 6.5.7 Require:Networkdepth, l Require:W( ) i, i , . . . , l , ‚àà{1 }theweightmatricesofthemodel Require:b( ) i, i , . . . , l , ‚àà{1 }thebiasparametersofthemodel Require:x,theinputtoprocess Require:y,thetargetoutput h( 0 )= x fordo k , . . . , l = 1 a( ) k= b( ) k+W( ) kh( 1 ) k‚àí h( ) k= ( fa( ) k) endfor ÀÜyh= ( ) l</div>
        </div>
    </div>

    <div class="question-card" id="q27">
        <div class="question-header">
            <span class="question-number">Question 27</span>
            <span class="question-type">multiple-choice</span>
        </div>

        <div class="question-text">In statistics and machine learning, the properties of estimators‚Äîsuch as bias, variance, and consistency‚Äîare critical in evaluating how well models generalize to new data and in guiding model selection procedures. Understanding the interplay between model capacity, estimator reliability, and error metrics is essential for robust analysis.

Which property ensures that, as the sample size increases indefinitely, an estimator will converge in probability to the true value of the parameter it estimates?

1) Unbiasedness   
2) Minimum variance   
3) High model capacity   
4) Maximum likelihood   
5) Low mean squared error   
6) Standard error   
7) Consistency </div>

        <div class="answer-section">
            <div class="answer-label">‚úì Correct Answer:</div>
            <div class="answer-text">The correct answer is 7) Consistency.</div>
        </div>

        <div class="reference-section">
            <div class="reference-label">üìö Reference Text:</div>
            <button class="toggle-button" onclick="toggleReference(27)">
                Show/Hide Reference
            </button>
            <div id="ref27" class="reference-text hidden">CHAPTER5.MACHINELEARNINGBASICS RecallthattheGaussianprobabilitydensityfunctionisgivenby px(() i;¬µ,œÉ2) =1‚àö 2œÄœÉ2expÓÄ† ‚àí1 2(x() i‚àí¬µ)2 œÉ2ÓÄ° .(5.29) AcommonestimatoroftheGaussianmeanparameterisknownasthesample mean: ÀÜ¬µ m=1 mmÓÅò i=1x() i(5.30) Todeterminethebiasofthesamplemean,weareagaininterestedincalculating itsexpectation: bias(ÀÜ¬µ m) = [ÀÜ E¬µ m]‚àí¬µ (5.31) = EÓÄ¢ 1 mmÓÅò i=1x() iÓÄ£ ‚àí¬µ (5.32) =ÓÄ† 1 mmÓÅò i=1EÓÅ® x() iÓÅ©ÓÄ° ‚àí¬µ (5.33) =ÓÄ† 1 mmÓÅò i=1¬µÓÄ° ‚àí¬µ (5.34) = = 0¬µ¬µ‚àí (5.35) ThusweÔ¨ÅndthatthesamplemeanisanunbiasedestimatorofGaussianmean parameter. Example:EstimatorsoftheVarianceofaGaussianDistributionAsan example,wecomparetwodiÔ¨ÄerentestimatorsofthevarianceparameterœÉ2ofa Gaussiandistribution.Weareinterestedinknowingifeitherestimatorisbiased. TheÔ¨ÅrstestimatorofœÉ2weconsiderisknownasthesamplevariance: ÀÜœÉ2 m=1 mmÓÅò i=1ÓÄê x() i‚àíÀÜ¬µ mÓÄë2 , (5.36) where ÀÜ¬µ misthesamplemean,deÔ¨Ånedabove.Moreformally,weareinterestedin computing bias(ÀÜœÉ2 m) = [ÀÜ EœÉ2 m]‚àíœÉ2(5.37) 1 2 6 CHAPTER5.MACHINELEARNINGBASICS Webeginbyevaluatingtheterm E[ÀÜœÉ2 m]: E[ÀÜœÉ2 m] = EÓÄ¢ 1 mmÓÅò i=1ÓÄê x() i‚àíÀÜ¬µ mÓÄë2ÓÄ£ (5.38) =m‚àí1 mœÉ2(5.39) Returningtoequation,weconcludethatthebiasof 5.37 ÀÜœÉ2 mis‚àíœÉ2/m.Therefore, thesamplevarianceisabiasedestimator. Theunbiasedsamplevarianceestimator ÀúœÉ2 m=1 m‚àí1mÓÅò i=1ÓÄê x() i‚àíÀÜ¬µ mÓÄë2 (5.40) providesanalternativeapproach.Asthenamesuggeststhisestimatorisunbiased. Thatis,weÔ¨Åndthat E[ÀúœÉ2 m] = œÉ2: E[ÀúœÉ2 m] = EÓÄ¢ 1 m‚àí1mÓÅò i=1ÓÄê x() i‚àíÀÜ¬µ mÓÄë2ÓÄ£ (5.41) =m m‚àí1E[ÀÜœÉ2 m] (5.42) =m m‚àí1ÓÄím‚àí1 mœÉ2ÓÄì (5.43) = œÉ2. (5.44) Wehavetwoestimators:oneisbiasedandtheotherisnot.Whileunbiased estimatorsareclearlydesirable,theyarenotalwaysthe‚Äúbest‚Äùestimators.Aswe willseeweoftenusebiasedestimatorsthatpossessotherimportantproperties. 5.4.3VarianceandStandardError Anotherpropertyoftheestimatorthatwemightwanttoconsiderishowmuch weexpectittovaryasafunctionofthedatasample.Justaswecomputedthe expectationoftheestimatortodetermineitsbias,wecancomputeitsvariance. Thevarianceofanestimatorissimplythevariance Var(ÀÜŒ∏) (5.45) wheretherandomvariableisthetrainingset.Alternately,thesquarerootofthe varianceiscalledthe ,denotedstandarderror SE(ÀÜŒ∏). 1 2 7 CHAPTER5.MACHINELEARNINGBASICS Thevarianceorthestandarderrorofanestimatorprovidesameasureofhow wewouldexpecttheestimatewecomputefromdatatovaryasweindependently resamplethedatasetfromtheunderlyingdatageneratingprocess.Justaswe mightlikeanestimatortoexhibitlowbiaswewouldalsolikeittohaverelatively lowvariance. WhenwecomputeanystatisticusingaÔ¨Ånitenumberofsamples,ourestimate ofthetrueunderlyingparameterisuncertain,inthesensethatwecouldhave obtainedothersamplesfromthesamedistributionandtheirstatisticswouldhave beendiÔ¨Äerent.Theexpecteddegreeofvariationinanyestimatorisasourceof errorthatwewanttoquantify. Thestandarderrorofthemeanisgivenby SE(ÀÜ¬µ m) =ÓÅ∂ÓÅµÓÅµÓÅ¥VarÓÄ¢ 1 mmÓÅò i=1x() iÓÄ£ =œÉ‚àöm, (5.46) whereœÉ2isthetruevarianceofthesamplesxi.Thestandarderrorisoften estimatedbyusinganestimateofœÉ.Unfortunately,neitherthesquarerootof thesamplevariancenorthesquarerootoftheunbiasedestimatorofthevariance provideanunbiasedestimateofthestandarddeviation.Bothapproachestend tounderestimatethetruestandarddeviation,butarestillusedinpractice.The squarerootoftheunbiasedestimatorofthevarianceislessofanunderestimate. Forlarge,theapproximation isquitereasonable. m Thestandarderrorofthemeanisveryusefulinmachinelearningexperiments. Weoftenestimatethegeneralization errorbycomputingthesamplemeanofthe erroronthetestset.Thenumberofexamplesinthetestsetdeterminesthe accuracyofthisestimate.Takingadvantageofthecentrallimittheorem,which tellsusthatthemeanwillbeapproximatelydistributedwithanormaldistribution, wecanusethestandarderrortocomputetheprobabilitythatthetrueexpectation fallsinanychoseninterval.Forexample,the95%conÔ¨Ådenceintervalcenteredon themean ÀÜ¬µ mis (ÀÜ¬µ m‚àí196SE( ÀÜ.¬µ m)ÀÜ,¬µ m+196SE( ÀÜ.¬µ m)), (5.47) underthenormaldistributionwithmean ÀÜ¬µ mandvariance SE(ÀÜ¬µ m)2.Inmachine learningexperiments,itiscommontosaythatalgorithmAisbetterthanalgorithm Biftheupperboundofthe95%conÔ¨ÅdenceintervalfortheerrorofalgorithmAis lessthanthelowerboundofthe95%conÔ¨Ådenceintervalfortheerrorofalgorithm B. 1 2 8 CHAPTER5.MACHINELEARNINGBASICS Example: BernoulliDistributionWeonceagainconsiderasetofsamples {x(1),...,x() m}drawnindependentlyandidenticallyfromaBernoullidistribution (recallP(x() i;Œ∏) =Œ∏x() i(1‚àíŒ∏)(1 ‚àí x() i)).Thistimeweareinterestedincomputing thevarianceoftheestimator ÀÜŒ∏ m=1 mÓÅêm i=1x() i. VarÓÄê ÀÜŒ∏ mÓÄë = VarÓÄ† 1 mmÓÅò i=1x() iÓÄ° (5.48) =1 m2mÓÅò i=1VarÓÄê x() iÓÄë (5.49) =1 m2mÓÅò i=1Œ∏Œ∏ (1‚àí) (5.50) =1 m2mŒ∏Œ∏ (1‚àí) (5.51) =1 mŒ∏Œ∏ (1‚àí) (5.52) Thevarianceoftheestimatordecreasesasafunctionofm,thenumberofexamples inthedataset.Thisisacommonpropertyofpopularestimatorsthatwewill returntowhenwediscussconsistency(seesection).5.4.5 5.4.4TradingoÔ¨ÄBiasandVariancetoMinimizeMeanSquared Error BiasandvariancemeasuretwodiÔ¨Äerentsourcesoferrorinanestimator.Bias measurestheexpecteddeviationfromthetruevalueofthefunctionorparameter. Varianceontheotherhand,providesameasureofthedeviationfromtheexpected estimatorvaluethatanyparticularsamplingofthedataislikelytocause. Whathappenswhenwearegivenachoicebetweentwoestimators,onewith morebiasandonewithmorevariance?Howdowechoosebetweenthem?For example,imaginethatweareinterestedinapproximating thefunctionshownin Ô¨ÅgureandweareonlyoÔ¨Äeredthechoicebetweenamodelwithlargebiasand 5.2 onethatsuÔ¨Äersfromlargevariance.Howdowechoosebetweenthem? Themostcommonwaytonegotiatethistrade-oÔ¨Äistousecross-validation. Empirically,cross-validationishighlysuccessfulonmanyreal-worldtasks.Alter- natively,wecanalsocomparethemeansquarederror(MSE)oftheestimates: MSE = [( EÀÜŒ∏ m‚àíŒ∏)2] (5.53) = Bias(ÀÜŒ∏ m)2+Var(ÀÜŒ∏ m) (5.54) 1 2 9 CHAPTER5.MACHINELEARNINGBASICS TheMSEmeasurestheoverallexpecteddeviation‚Äîin asquarederrorsense‚Äî betweentheestimatorandthetruevalueoftheparameterŒ∏.Asisclearfrom equation,evaluatingtheMSEincorporatesboththebiasandthevariance. 5.54 DesirableestimatorsarethosewithsmallMSEandtheseareestimatorsthat managetokeepboththeirbiasandvariancesomewhatincheck. C apac i t yB i as Ge ne r al i z at i on e r r orV ar i anc e O pt i m al c apac i t yO v e r Ô¨Åt t i ng z o n e U nde r Ô¨Åt t i ng z o n e Figure5.6:Ascapacityincreases(x-axis),bias(dotted)tendstodecreaseandvariance (dashed)tendstoincrease,yieldinganotherU-shapedcurveforgeneralizationerror(bold curve).Ifwevarycapacityalongoneaxis,thereisanoptimalcapacity,withunderÔ¨Åtting whenthecapacityisbelowthisoptimumandoverÔ¨Åttingwhenitisabove.Thisrelationship issimilartotherelationshipbetweencapacity,underÔ¨Åtting,andoverÔ¨Åtting,discussedin sectionandÔ¨Ågure. 5.2 5.3 Therelationshipbetweenbiasandvarianceistightlylinkedtothemachine learningconceptsofcapacity,underÔ¨ÅttingandoverÔ¨Åtting.Inthecasewheregen- eralizationerrorismeasuredbytheMSE(wherebiasandvariancearemeaningful componentsofgeneralization error),increasingcapacitytendstoincreasevariance anddecreasebias.ThisisillustratedinÔ¨Ågure,whereweseeagaintheU-shaped 5.6 curveofgeneralization errorasafunctionofcapacity. 5.4.5Consistency Sofarwehavediscussedthepropertiesofvariousestimatorsforatrainingsetof Ô¨Åxedsize.Usually,wearealsoconcernedwiththebehaviorofanestimatorasthe amountoftrainingdatagrows.Inparticular,weusuallywishthat,asthenumber ofdatapointsminourdatasetincreases,ourpointestimatesconvergetothetrue 1 3 0 CHAPTER5.MACHINELEARNINGBASICS valueofthecorrespondingparameters.Moreformally,wewouldlikethat plimm ‚Üí ‚àûÀÜŒ∏ m= Œ∏. (5.55) Thesymbolplimindicatesconvergenceinprobability,meaningthatforanyÓÄè>0, P(|ÀÜŒ∏ m‚àí|Œ∏>ÓÄè)‚Üí0asm‚Üí‚àû.Theconditiondescribedbyequationis5.55 knownasconsistency.Itissometimesreferredtoasweakconsistency,with strongconsistencyreferringtothealmostsureconvergenceofÀÜŒ∏toŒ∏.Almost sureconvergenceofasequenceofrandomvariables x(1), x(2),...toavaluex occurswhenp(lim m ‚Üí ‚àû x() m= ) = 1x. Consistencyensuresthatthebiasinducedbytheestimatordiminishesasthe numberofdataexamplesgrows.However,thereverseisnottrue‚Äîasymptotic unbiasednessdoesnotimplyconsistency. Forexample,considerestimatingthe meanparameter¬µofanormaldistributionN(x;¬µ,œÉ2),withadatasetconsisting ofmsamples:{x(1),...,x() m}.WecouldusetheÔ¨Årstsamplex(1)ofthedataset asanunbiasedestimator:ÀÜŒ∏=x(1).Inthatcase, E(ÀÜŒ∏ m)=Œ∏sotheestimator isunbiasednomatterhowmanydatapointsareseen.This,ofcourse,implies thattheestimateisasymptoticallyunbiased.However,thisisnotaconsistent estimatorasitisthecasethat not ÀÜŒ∏ m‚Üí ‚Üí‚àûŒ∏mas. 5.5MaximumLikelihoodEstimation Previously,wehaveseensomedeÔ¨Ånitionsofcommonestimatorsandanalyzed theirproperties.Butwheredidtheseestimatorscomefrom?Ratherthanguessing thatsomefunctionmightmakeagoodestimatorandthenanalyzingitsbiasand variance,wewouldliketohavesomeprinciplefromwhichwecanderivespeciÔ¨Åc functionsthataregoodestimatorsfordiÔ¨Äerentmodels. Themostcommonsuchprincipleisthemaximumlikelihoodprinciple. Considerasetofmexamples X={x(1),...,x() m}drawnindependentlyfrom thetruebutunknowndatageneratingdistributionpdata() x. Letpmodel( x;Œ∏)beaparametricfamilyofprobabilitydistributionsoverthe samespaceindexedbyŒ∏.Inotherwords,pmodel(x;Œ∏)mapsanyconÔ¨Ågurationx toarealnumberestimatingthetrueprobabilitypdata()x. ThemaximumlikelihoodestimatorforisthendeÔ¨Ånedas Œ∏ Œ∏ML= argmax Œ∏pmodel(;) XŒ∏ (5.56) = argmax Œ∏mÓÅô i=1pmodel(x() i;)Œ∏ (5.57) 1 3 1 CHAPTER5.MACHINELEARNINGBASICS Thisproductovermanyprobabilitiescanbeinconvenientforavarietyofreasons. Forexample,itispronetonumericalunderÔ¨Çow.Toobtainamoreconvenient butequivalentoptimization problem,weobservethattakingthelogarithmofthe likelihooddoesnotchangeitsargmaxbutdoesconvenientlytransformaproduct intoasum: Œ∏ML= argmax Œ∏mÓÅò i=1logpmodel(x() i;)Œ∏. (5.58) Becausetheargmaxdoesnotchangewhenwerescalethecostfunction,wecan dividebymtoobtainaversionofthecriterionthatisexpressedasanexpectation withrespecttotheempiricaldistributionÀÜpdatadeÔ¨Ånedbythetrainingdata: Œ∏ML= argmax Œ∏E x ‚àºÀÜ pdatalogpmodel(;)xŒ∏. (5.59) Onewaytointerpretmaximumlikelihoodestimationistoviewitasminimizing thedissimilaritybetweentheempiricaldistributionÀÜpdatadeÔ¨Ånedbythetraining setandthemodeldistribution,withthedegreeofdissimilaritybetweenthetwo measuredbytheKLdivergence.TheKLdivergenceisgivenby DKL(ÀÜpdataÓÅ´pmodel) = E x ‚àºÀÜ pdata[log ÀÜpdata()logx‚àípmodel()]x.(5.60) Thetermontheleftisafunctiononlyofthedatageneratingprocess,notthe model.ThismeanswhenwetrainthemodeltominimizetheKLdivergence,we needonlyminimize ‚àí E</div>
        </div>
    </div>

    <div class="question-card" id="q28">
        <div class="question-header">
            <span class="question-number">Question 28</span>
            <span class="question-type">multiple-choice</span>
        </div>

        <div class="question-text">Neural networks are able to model complex, nonlinear relationships in data that cannot be captured by linear models. Their training involves optimization methods and techniques to ensure generalization and efficient learning.

Which of the following statements correctly explains why the ReLU activation function is widely used in deep neural networks?

1) It guarantees faster convergence by always producing positive outputs.   
2) It ensures the cost function remains convex during training.   
3) It allows networks to represent only linear transformations.   
4) It eliminates the need for bias terms in neural network layers.   
5) It prevents all forms of overfitting through its nonlinear nature.   
6) It causes gradients to vanish in all regions of the network.   
7) It introduces nonlinearity while maintaining computational efficiency and mitigating vanishing gradient problems. </div>

        <div class="answer-section">
            <div class="answer-label">‚úì Correct Answer:</div>
            <div class="answer-text">The correct answer is 7) It introduces nonlinearity while maintaining computational efficiency and mitigating vanishing gradient problems..</div>
        </div>

        <div class="reference-section">
            <div class="reference-label">üìö Reference Text:</div>
            <button class="toggle-button" onclick="toggleReference(28)">
                Show/Hide Reference
            </button>
            <div id="ref28" class="reference-text hidden">i g i n a l s p a c e x 0 1 2 h 101h 2L e a r n e d s p a c e h Figure6.1:SolvingtheXORproblembylearningarepresentation.Theboldnumbers printedontheplotindicatethevaluethatthelearnedfunctionmustoutputateachpoint. ( L e f t )AlinearmodelapplieddirectlytotheoriginalinputcannotimplementtheXOR function.When x1= 0,themodel‚Äôsoutputmustincreaseas x2increases.When x1= 1, themodel‚Äôsoutputmustdecreaseas x 2increases.AlinearmodelmustapplyaÔ¨Åxed coeÔ¨Écient w 2to x 2.Thelinearmodelthereforecannotusethevalueof x 1tochange thecoeÔ¨Écienton x 2andcannotsolvethisproblem. ( R i g h t )Inthetransformedspace representedbythefeaturesextractedbyaneuralnetwork,alinearmodelcannowsolve theproblem.Inourexamplesolution,thetwopointsthatmusthaveoutputhavebeen 1 collapsedintoasinglepointinfeaturespace.Inotherwords,thenonlinearfeatureshave mappedbothx= [1 ,0]ÓÄæandx= [0 ,1]ÓÄætoasinglepointinfeaturespace,h= [1 ,0]ÓÄæ. Thelinearmodelcannowdescribethefunctionasincreasingin h1anddecreasingin h2. Inthisexample,themotivationforlearningthefeaturespaceisonlytomakethemodel capacitygreatersothatitcanÔ¨Åtthetrainingset.Inmorerealisticapplications,learned representationscanalsohelpthemodeltogeneralize. 1 7 3 CHAPTER6.DEEPFEEDFORWARDNETWORKS yy hh x xWwyy h 1 h 1 x 1 x 1h 2 h 2 x 2 x 2 Figure6.2:Anexampleofafeedforwardnetwork,drawnintwodiÔ¨Äerentstyles.SpeciÔ¨Åcally, thisisthefeedforwardnetworkweusetosolvetheXORexample.Ithasasinglehidden layercontainingtwounits. ( L e f t )Inthisstyle,wedraweveryunitasanodeinthegraph. Thisstyleisveryexplicitandunambiguousbutfornetworkslargerthanthisexample itcanconsumetoomuchspace. Inthisstyle,wedrawanodeinthegraphfor ( R i g h t ) eachentirevectorrepresentingalayer‚Äôsactivations. Thisstyleismuchmorecompact. Sometimesweannotatetheedgesinthisgraphwiththenameoftheparametersthat describetherelationshipbetweentwolayers.Here,weindicatethatamatrixWdescribes themappingfromxtoh,andavectorwdescribesthemappingfromhto y.We typicallyomittheinterceptparametersassociatedwitheachlayerwhenlabelingthiskind ofdrawing. model,weusedavectorofweightsandascalarbiasparametertodescribean aÔ¨Énetransformationfromaninputvectortoanoutputscalar.Now,wedescribe anaÔ¨Énetransformationfromavectorxtoavectorh,soanentirevectorofbias parametersisneeded.Theactivationfunction gistypicallychosentobeafunction thatisappliedelement-wise,with h i= g(xÓÄæW : , i+ c i).Inmodernneuralnetworks, thedefaultrecommendation istousetherectiÔ¨ÅedlinearunitorReLU(Jarrett e t a l . e t a l . ,; ,; 2009NairandHinton2010Glorot,)deÔ¨Ånedbytheactivation 2011a function depictedinÔ¨Ågure. g z , z () = max0{} 6.3 Wecannowspecifyourcompletenetworkas f , , , b (;xWcw) = wÓÄæmax0{ ,WÓÄæxc+}+ b . (6.3) WecannowspecifyasolutiontotheXORproblem.Let W=ÓÄî11 11ÓÄï , (6.4) c=ÓÄî 0 ‚àí1ÓÄï , (6.5) 1 7 4 CHAPTER6.DEEPFEEDFORWARDNETWORKS 0 z0g z ( ) = m a x 0{ , z} Figure6.3:TherectiÔ¨Åedlinearactivationfunction.Thisactivationfunctionisthedefault activationfunctionrecommendedforusewithmostfeedforwardneuralnetworks.Applying thisfunctiontotheoutputofalineartransformationyieldsanonlineartransformation. However,thefunctionremainsveryclosetolinear,inthesensethatisapiecewiselinear functionwithtwolinearpieces.BecauserectiÔ¨Åedlinearunitsarenearlylinear,they preservemanyofthepropertiesthatmakelinearmodelseasytooptimizewithgradient- basedmethods.Theyalsopreservemanyofthepropertiesthatmakelinearmodels generalizewell.Acommonprinciplethroughoutcomputerscienceisthatwecanbuild complicatedsystemsfromminimalcomponents. MuchasaTuringmachine‚Äôsmemory needsonlytobeabletostore0or1states,wecanbuildauniversalfunctionapproximator fromrectiÔ¨Åedlinearfunctions. 1 7 5 CHAPTER6.DEEPFEEDFORWARDNETWORKS w=ÓÄî1 ‚àí2ÓÄï , (6.6) and. b= 0 Wecannowwalkthroughthewaythatthemodelprocessesabatchofinputs. LetXbethedesignmatrixcontainingallfourpointsinthebinaryinputspace, withoneexampleperrow: X=Ô£Æ Ô£ØÔ£ØÔ£∞00 01 10 11Ô£π Ô£∫Ô£∫Ô£ª. (6.7) TheÔ¨ÅrststepintheneuralnetworkistomultiplytheinputmatrixbytheÔ¨Årst layer‚Äôsweightmatrix: XW=Ô£Æ Ô£ØÔ£ØÔ£∞00 11 11 22Ô£π Ô£∫Ô£∫Ô£ª. (6.8) Next,weaddthebiasvector,toobtainc Ô£Æ Ô£ØÔ£ØÔ£∞0 1‚àí 10 10 21Ô£π Ô£∫Ô£∫Ô£ª. (6.9) Inthisspace,alloftheexamplesliealongalinewithslope.Aswemovealong 1 thisline,theoutputneedstobeginat,thenriseto,thendropbackdownto. 0 1 0 Alinearmodelcannotimplementsuchafunction.ToÔ¨Ånishcomputingthevalue offoreachexample,weapplytherectiÔ¨Åedlineartransformation: h Ô£Æ Ô£ØÔ£ØÔ£∞00 10 10 21Ô£π Ô£∫Ô£∫Ô£ª. (6.10) Thistransformationhaschangedtherelationshipbetweentheexamples.Theyno longerlieonasingleline.AsshowninÔ¨Ågure,theynowlieinaspacewherea 6.1 linearmodelcansolvetheproblem. WeÔ¨Ånishbymultiplyingbytheweightvector:w Ô£Æ Ô£ØÔ£ØÔ£∞0 1 1 0Ô£π Ô£∫Ô£∫Ô£ª. (6.11) 1 7 6 CHAPTER6.DEEPFEEDFORWARDNETWORKS Theneuralnetworkhasobtainedthecorrectanswerforeveryexampleinthebatch. Inthisexample,wesimplyspeciÔ¨Åedthesolution,thenshowedthatitobtained zeroerror. Inarealsituation,theremightbebillionsofmodelparametersand billionsoftrainingexamples,soonecannotsimplyguessthesolutionaswedid here.Instead,agradient-basedoptimization algorithmcanÔ¨Åndparametersthat produceverylittleerror.ThesolutionwedescribedtotheXORproblemisata globalminimumofthelossfunction,sogradientdescentcouldconvergetothis point.ThereareotherequivalentsolutionstotheXORproblemthatgradient descentcouldalsoÔ¨Ånd.Theconvergencepointofgradientdescentdependsonthe initialvaluesoftheparameters.Inpractice,gradientdescentwouldusuallynot Ô¨Åndclean,easilyunderstood,integer-valuedsolutionsliketheonewepresented here. 6. 2 Gradi en t - Bas e d L earni n g DesigningandtraininganeuralnetworkisnotmuchdiÔ¨Äerentfromtrainingany othermachinelearningmodelwithgradientdescent.Insection,wedescribed 5.10 howtobuildamachinelearningalgorithmbyspecifyinganoptimizationprocedure, acostfunction,andamodelfamily. ThelargestdiÔ¨Äerencebetweenthelinearmodelswehaveseensofarandneural networksisthatthenonlinearityofaneuralnetworkcausesmostinterestingloss functionstobecomenon-convex.Thismeansthatneuralnetworksareusually trainedbyusingiterative,gradient-basedoptimizersthatmerelydrivethecost functiontoaverylowvalue,ratherthanthelinearequationsolversusedtotrain linearregressionmodelsortheconvexoptimization algorithmswithglobalconver- genceguaranteesusedtotrainlogisticregressionorSVMs.Convexoptimization convergesstartingfromanyinitialparameters(intheory‚Äîinpracticeitisvery robustbutcanencounternumericalproblems).Stochasticgradientdescentapplied tonon-convexlossfunctionshasnosuchconvergenceguarantee,andissensitive tothevaluesoftheinitialparameters.Forfeedforwardneuralnetworks,itis importanttoinitializeallweightstosmallrandomvalues.Thebiasesmaybe initializedtozeroortosmallpositivevalues.Theiterativegradient-basedopti- mizationalgorithmsusedtotrainfeedforwardnetworksandalmostallotherdeep modelswillbedescribedindetailinchapter,withparameterinitialization in 8 particulardiscussedinsection.Forthemoment,itsuÔ¨Écestounderstandthat 8.4 thetrainingalgorithmisalmostalwaysbasedonusingthegradienttodescendthe costfunctioninonewayoranother. The speciÔ¨Åcalgorithmsareimprovements andreÔ¨Ånementsontheideasofgradientdescent,introducedinsection,and,4.3 1 7 7 CHAPTER6.DEEPFEEDFORWARDNETWORKS morespeciÔ¨Åcally,aremostoftenimprovementsofthestochasticgradientdescent algorithm,introducedinsection.5.9 Wecanofcourse,trainmodelssuchaslinearregressionandsupportvector machineswithgradientdescenttoo,andinfactthisiscommonwhenthetraining setisextremelylarge.Fromthispointofview,traininganeuralnetworkisnot muchdiÔ¨Äerentfromtraininganyothermodel.Computingthegradientisslightly morecomplicatedforaneuralnetwork,butcanstillbedoneeÔ¨Écientlyandexactly. Sectionwilldescribehowtoobtainthegradientusingtheback-propagation 6.5 algorithmandmoderngeneralizations oftheback-propagationalgorithm. Aswithothermachinelearningmodels,toapplygradient-basedlearningwe mustchooseacostfunction,andwemustchoosehowtorepresenttheoutputof themodel.Wenowrevisitthesedesignconsiderationswithspecialemphasison theneuralnetworksscenario. 6.2.1CostFunctions Animportantaspectofthedesignofadeepneuralnetworkisthechoiceofthe costfunction.Fortunately,thecostfunctionsforneuralnetworksaremoreorless thesameasthoseforotherparametricmodels,suchaslinearmodels. Inmostcases,ourparametricmodeldeÔ¨Ånesadistribution p(yx|;Œ∏)and wesimplyuse theprinciple ofmaximumlikelihood.Thismeansweusethe cross-entropybetweenthetrainingdataandthemodel‚Äôspredictionsasthecost function. Sometimes,wetakeasimplerapproach,whereratherthanpredictingacomplete probabilitydistributionovery,wemerelypredictsomestatisticofyconditioned on.Specializedlossfunctionsallowustotrainapredictoroftheseestimates. x Thetotalcostfunctionusedtotrainaneuralnetworkwilloftencombineone oftheprimarycostfunctionsdescribedherewitharegularizationterm.Wehave alreadyseensomesimpleexamplesofregularizationappliedtolinearmodelsin section.Theweightdecayapproachusedforlinearmodelsisalsodirectly 5.2.2 applicabletodeepneuralnetworksandisamongthemostpopularregularization strategies.Moreadvancedregularizationstrategiesforneuralnetworkswillbe describedinchapter.7 6.2.1.1LearningConditionalDistributionswithMaximumLikelihood Mostmodernneuralnetworksaretrainedusingmaximumlikelihood.Thismeans thatthecostfunctionissimplythenegativelog-likelihood,equivalentlydescribed 1 7 8 CHAPTER6.DEEPFEEDFORWARDNETWORKS asthecross-entropybetweenthetrainingdataandthemodeldistribution.This costfunctionisgivenby J() = Œ∏ ‚àí E x y ,‚àº ÀÜ pdatalog p m o de l( )yx| . (6.12) ThespeciÔ¨Åcformofthecostfunctionchangesfrommodeltomodel,depending onthespeciÔ¨Åcformoflog p m o de l.Theexpansionoftheaboveequationtypically yieldssometermsthatdonotdependonthemodelparametersandmaybedis- carded.Forexample,aswesawinsection,if5.5.1 p m o de l(yx|) =N(y; f(x;Œ∏) ,I), thenwerecoverthemeansquarederrorcost, J Œ∏() =1 2E x y ,‚àº ÀÜ pdata||‚àí ||y f(;)xŒ∏2+const , (6.13) uptoascalingfactorof1 2andatermthatdoesnotdependon.ThediscardedŒ∏ constantisbasedonthevarianceoftheGaussiandistribution,whichinthiscase wechosenottoparametrize. Previously,wesawthattheequivalencebetween maximumlikelihoodestimationwithanoutputdistributionandminimization of meansquarederrorholdsforalinearmodel,butinfact,theequivalenceholds regardlessoftheusedtopredictthemeanoftheGaussian. f(;)xŒ∏ Anadvantageofthisapproachofderivingthecostfunctionfrommaximum likelihoodisthatitremovestheburdenofdesigningcostfunctionsforeachmodel. Specifyingamodel p(yx|)automatically determinesacostfunction log p(yx|). Onerecurringthemethroughoutneuralnetworkdesignisthatthegradientof thecostfunctionmustbelargeandpredictableenoughtoserveasagoodguide forthelearningalgorithm.Functionsthatsaturate(becomeveryÔ¨Çat)undermine thisobjectivebecausetheymakethegradientbecomeverysmall.Inmanycases thishappensbecausetheactivationfunctionsusedtoproducetheoutputofthe hiddenunitsortheoutputunitssaturate. Thenegativelog-likelihoodhelpsto avoidthisproblemformanymodels.Manyoutputunitsinvolveanexpfunction thatcansaturatewhenitsargumentisverynegative.The logfunctioninthe negativelog-likelihoodcostfunctionundoestheexpofsomeoutputunits.Wewill discusstheinteractionbetweenthecostfunctionandthechoiceofoutputunitin section.6.2.2 Oneunusualpropertyofthecross-entropycostusedtoperformmaximum likelihoodestimationisthatitusuallydoesnothaveaminimumvaluewhenapplied tothemodelscommonlyusedinpractice.Fordiscreteoutputvariables,most modelsareparametrized insuchawaythattheycannotrepresentaprobability ofzeroorone,butcancomearbitrarilyclosetodoingso.Logisticregression isanexampleofsuchamodel.Forreal-valuedoutputvariables,ifthemodel 1 7 9 CHAPTER6.DEEPFEEDFORWARDNETWORKS cancontrolthedensityoftheoutputdistribution(forexample,bylearningthe varianceparameterofaGaussianoutputdistribution)thenitbecomespossible toassignextremelyhighdensitytothecorrecttrainingsetoutputs,resultingin cross-entropyapproachingnegativeinÔ¨Ånity.Regularizationtechniquesdescribed inchapterprovideseveraldiÔ¨Äerentwaysofmodifyingthelearningproblemso 7 thatthemodelcannotreapunlimitedrewardinthisway. 6.2.1.2LearningConditionalStatistics Insteadoflearningafullprobabilitydistribution</div>
        </div>
    </div>

    <div class="question-card" id="q29">
        <div class="question-header">
            <span class="question-number">Question 29</span>
            <span class="question-type">multiple-choice</span>
        </div>

        <div class="question-text">In machine learning and optimization, various strategies are used to efficiently train deep models and solve high-dimensional problems. Understanding when and how to apply methods like coordinate descent, Polyak averaging, supervised pretraining, and transfer learning is crucial for effective model development.

Which technique specifically enables a thinner, deeper neural network to improve generalization by matching its intermediate representations to those of a wider, shallower network during training?

1) Polyak averaging   
2) Newton‚Äôs method   
3) Unsupervised layer-wise pretraining   
4) Block coordinate descent   
5) Transfer learning using pretrained weights   
6) FitNets hint-based training   
7) Adam optimizer with exponential moving average</div>

        <div class="answer-section">
            <div class="answer-label">‚úì Correct Answer:</div>
            <div class="answer-text">The correct answer is 6) FitNets hint-based training.</div>
        </div>

        <div class="reference-section">
            <div class="reference-label">üìö Reference Text:</div>
            <button class="toggle-button" onclick="toggleReference(29)">
                Show/Hide Reference
            </button>
            <div id="ref29" class="reference-text hidden">minimizingwithrespecttoasubsetofthevariablessimultaneously.Theterm ‚Äúcoordinatedescent‚Äùisoftenusedtorefertoblockcoordinatedescentaswellas thestrictlyindividualcoordinatedescent. CoordinatedescentmakesthemostsensewhenthediÔ¨Äerentvariablesinthe optimization problemcanbeclearlyseparatedintogroupsthatplayrelatively isolatedroles,orwhenoptimization withrespecttoonegroupofvariablesis signiÔ¨ÅcantlymoreeÔ¨Écientthanoptimization withrespecttoallofthevariables. Forexample,considerthecostfunction J ,(HW) =ÓÅò i , j| H i , j|+ÓÅò i , jÓÄê XW‚àíÓÄæHÓÄë2 i , j.(8.38) Thisfunctiondescribesalearningproblemcalledsparsecoding,wherethegoalis toÔ¨ÅndaweightmatrixWthatcanlinearlydecodeamatrixofactivationvalues HtoreconstructthetrainingsetX.Mostapplicationsofsparsecodingalso involveweightdecayoraconstraintonthenormsofthecolumnsofW,inorder topreventthepathologicalsolutionwithextremelysmallandlarge.HW Thefunction Jisnotconvex.However, wecandividetheinputstothe trainingalgorithmintotwosets:thedictionaryparametersWandthecode representationsH.Minimizingtheobjectivefunctionwithrespecttoeitheroneof thesesetsofvariablesisaconvexproblem.Blockcoordinatedescentthusgives 3 2 1 CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS usanoptimization strategythatallowsustouseeÔ¨Écientconvexoptimization algorithms,byalternatingbetweenoptimizingWwithHÔ¨Åxed,thenoptimizing HWwithÔ¨Åxed. Coordinatedescentisnotaverygoodstrategywhenthevalueofonevariable stronglyinÔ¨Çuencestheoptimalvalueofanothervariable,asinthefunction f(x) = ( x 1‚àí x 2)2+ Œ±ÓÄÄ x2 1+ x2 2ÓÄÅ where Œ±isapositiveconstant.TheÔ¨Årsttermencourages thetwovariablestohavesimilarvalue,whilethesecondtermencouragesthem tobenearzero.Thesolutionistosetbothtozero.Newton‚Äôsmethodcansolve theprobleminasinglestepbecauseitisapositivedeÔ¨Ånitequadraticproblem. However,forsmall Œ±,coordinatedescentwillmakeveryslowprogressbecausethe Ô¨ÅrsttermdoesnotallowasinglevariabletobechangedtoavaluethatdiÔ¨Äers signiÔ¨Åcantlyfromthecurrentvalueoftheothervariable. 8.7.3PolyakAveraging Polyakaveraging(PolyakandJuditsky1992,)consistsofaveragingtogetherseveral points inthe trajectory through parameter spacevisited by anoptimization algorithm. If titerationsofgradientdescentvisitpointsŒ∏( 1 ), . . . ,Œ∏( ) t,thenthe outputofthePolyakaveragingalgorithmisÀÜŒ∏( ) t=1 tÓÅê iŒ∏( ) i. Onsomeproblem classes,suchasgradientdescentappliedtoconvexproblems,thisapproachhas strongconvergenceguarantees.Whenappliedtoneuralnetworks,itsjustiÔ¨Åcation ismoreheuristic,butitperformswellinpractice.Thebasicideaisthatthe optimization algorithmmayleapbackandforthacrossavalleyseveraltimes withoutevervisitingapointnearthebottomofthevalley.Theaverageofallof thelocationsoneithersideshouldbeclosetothebottomofthevalleythough. Innon-convexproblems,thepathtakenbytheoptimization trajectorycanbe verycomplicatedandvisitmanydiÔ¨Äerentregions.Includingpointsinparameter spacefromthedistantpastthatmaybeseparatedfromthecurrentpointbylarge barriersinthecostfunctiondoesnotseemlikeausefulbehavior.Asaresult, whenapplyingPolyakaveragingtonon-convexproblems,itistypicaltousean exponentiallydecayingrunningaverage: ÀÜŒ∏( ) t= Œ±ÀÜŒ∏( 1 ) t ‚àí+(1 )‚àí Œ±Œ∏( ) t. (8.39) Therunningaverageapproachisusedinnumerousapplications.SeeSzegedy etal.()forarecentexample. 2015 3 2 2 CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS 8.7.4SupervisedPretraining Sometimes,directlytrainingamodeltosolveaspeciÔ¨Åctaskcanbetooambitious ifthemodeliscomplexandhardtooptimizeorifthetaskisverydiÔ¨Écult.Itis sometimesmoreeÔ¨Äectivetotrainasimplermodeltosolvethetask,thenmake themodelmorecomplex.ItcanalsobemoreeÔ¨Äectivetotrainthemodeltosolve asimplertask,thenmoveontoconfronttheÔ¨Ånaltask.Thesestrategiesthat involvetrainingsimplemodelsonsimpletasksbeforeconfrontingthechallengeof trainingthedesiredmodeltoperformthedesiredtaskarecollectivelyknownas pretraining. Greedyalgorithmsbreakaproblemintomanycomponents,thensolvefor theoptimalversionofeachcomponentinisolation.Unfortunately,combiningthe individuallyoptimalcomponentsisnotguaranteedtoyieldanoptimalcomplete solution.However,greedyalgorithmscanbecomputationally muchcheaperthan algorithmsthatsolveforthebestjointsolution,andthequalityofagreedysolution isoftenacceptableifnotoptimal.Greedyalgorithmsmayalsobefollowedbya Ô¨Åne-tuningstageinwhichajointoptimization algorithmsearchesforanoptimal solutiontothefullproblem.Initializingthejointoptimization algorithmwitha greedysolutioncangreatlyspeeditupandimprovethequalityofthesolutionit Ô¨Ånds. Pretraining,andespeciallygreedypretraining,algorithmsareubiquitousin deeplearning.Inthissection,wedescribespeciÔ¨Åcallythosepretrainingalgorithms thatbreaksupervisedlearningproblemsintoothersimplersupervisedlearning problems.Thisapproachisknownas . greedysupervisedpretraining Intheoriginal( ,)versionofgreedysupervisedpretraining, Bengioetal.2007 eachstageconsistsofasupervisedlearningtrainingtaskinvolvingonlyasubsetof thelayersintheÔ¨Ånalneuralnetwork.Anexampleofgreedysupervisedpretraining isillustratedinÔ¨Ågure,inwhicheachaddedhiddenlayerispretrainedaspart 8.7 ofashallowsupervisedMLP,takingasinputtheoutputofthepreviouslytrained hiddenlayer.Insteadofpretrainingonelayeratatime,SimonyanandZisserman ()pretrainadeepconvolutionalnetwork(elevenweightlayers)andthenuse 2015 theÔ¨Årstfourandlastthreelayersfromthisnetworktoinitializeevendeeper networks(withuptonineteenlayersofweights).Themiddlelayersofthenew, verydeepnetworkareinitializedrandomly.Thenewnetworkisthenjointlytrained. Anotheroption,exploredbyYu2010etal.()istousetheofthepreviously outputs trainedMLPs,aswellastherawinput,asinputsforeachaddedstage. Why would greedy sup ervised pretraining help?The hypothesis initially discussedby ()isthatithelpstoprovidebetterguidancetothe Bengioetal.2007 3 2 3 CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS y y h( 1 )h( 1 ) x x ( a )U( 1 )U( 1 ) W( 1 )W( 1 ) y yh( 1 )h( 1 ) x x ( b )U( 1 )U( 1 )W( 1 )W( 1 ) y yh( 1 )h( 1 ) x x ( c )U( 1 )U( 1 )W( 1 )W( 1 )h( 2 )h( 2 ) y y U( 2 )U( 2 ) W( 2 )W( 2 ) y yh( 1 )h( 1 ) x x ( d )U( 1 )U( 1 )W( 1 )W( 1 )h( 2 )h( 2 )y U( 2 )U( 2 ) W( 2 )W( 2 ) Figure8.7:Illustrationofoneformofgreedysupervisedpretraining( ,). Bengio e t a l .2007 ( a )WestartbytrainingasuÔ¨Écientlyshallowarchitecture.Anotherdrawingofthe ( b ) samearchitecture.Wekeeponlytheinput-to-hiddenlayeroftheoriginalnetworkand ( c ) discardthehidden-to-outputlayer.WesendtheoutputoftheÔ¨Årsthiddenlayerasinput toanothersupervisedsinglehiddenlayerMLPthatistrainedwiththesameobjective astheÔ¨Årstnetworkwas,thusaddingasecondhiddenlayer.Thiscanberepeatedforas manylayersasdesired.Anotherdrawingoftheresult,viewedasafeedforwardnetwork. ( d ) Tofurtherimprovetheoptimization,wecanjointlyÔ¨Åne-tuneallthelayers,eitheronlyat theendorateachstageofthisprocess. 3 2 4 CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS intermediatelevelsofadeephierarchy.Ingeneral,pretrainingmayhelpbothin termsofoptimization andintermsofgeneralization. Anapproachrelatedtosupervisedpretrainingextendstheideatothecontext oftransferlearning:Yosinski2014etal.()pretrainadeepconvolutionalnetwith8 layersofweightsonasetoftasks(asubsetofthe1000ImageNetobjectcategories) andtheninitializeasame-sizenetworkwiththeÔ¨Årst klayersoftheÔ¨Årstnet.All thelayersofthesecondnetwork(withtheupperlayersinitializedrandomly)are thenjointlytrainedtoperformadiÔ¨Äerentsetoftasks(anothersubsetofthe1000 ImageNetobjectcategories),withfewertrainingexamplesthanfortheÔ¨Årstsetof tasks.Otherapproachestotransferlearningwithneuralnetworksarediscussedin section.15.2 AnotherrelatedlineofworkistheFitNets( ,)approach. Romeroetal.2015 Thisapproachbeginsbytraininganetworkthathaslowenoughdepthandgreat enoughwidth(numberofunitsperlayer)tobeeasytotrain.Thisnetworkthen becomesateacherforasecondnetwork,designatedthestudent.Thestudent networkismuchdeeperandthinner(eleventonineteenlayers)andwouldbe diÔ¨ÉculttotrainwithSGDundernormalcircumstances.Thetrainingofthe studentnetworkismadeeasierbytrainingthestudentnetworknotonlytopredict theoutputfortheoriginaltask,butalsotopredictthevalueofthemiddlelayer oftheteachernetwork.Thisextrataskprovidesasetofhintsabouthowthe hiddenlayersshouldbeusedandcansimplifytheoptimizationproblem.Additional parametersareintroducedtoregressthemiddlelayerofthe5-layerteachernetwork fromthemiddlelayerofthedeeperstudentnetwork.However,insteadofpredicting theÔ¨ÅnalclassiÔ¨Åcationtarget,theobjectiveistopredictthemiddlehiddenlayer oftheteachernetwork. Thelowerlayersofthestudentnetworksthushavetwo objectives:tohelptheoutputsofthestudentnetworkaccomplishtheirtask,as wellastopredicttheintermediatelayeroftheteachernetwork.Althoughathin anddeepnetworkappearstobemorediÔ¨Éculttotrainthanawideandshallow network,thethinanddeepnetworkmaygeneralizebetterandcertainlyhaslower computational costifitisthinenoughtohavefarfewerparameters.Without thehintsonthehiddenlayer,thestudentnetworkperformsverypoorlyinthe experiments,bothonthetrainingandtestset.Hintsonmiddlelayersmaythus beoneofthetoolstohelptrainneuralnetworksthatotherwiseseemdiÔ¨Écultto train,butotheroptimization techniquesorchangesinthearchitecturemayalso solvetheproblem. 3 2 5</div>
        </div>
    </div>

    <div class="question-card" id="q30">
        <div class="question-header">
            <span class="question-number">Question 30</span>
            <span class="question-type">multiple-choice</span>
        </div>

        <div class="question-text">Deep learning models have revolutionized computer vision and sequence processing by leveraging architectures inspired by biological neural systems. Two prominent families of neural networks, convolutional neural networks (CNNs) and recurrent neural networks (RNNs), are specialized for spatial and sequential data, respectively.

Which architectural feature most directly enables recurrent neural networks (RNNs) to generalize over variable-length input sequences in tasks like language modeling and time series analysis?

1) Use of convolutional filters with local receptive fields   
2) Training with extremely large labeled datasets   
3) Organizing neurons in hierarchical fully connected layers   
4) Sharing parameters across time steps during sequence processing   
5) Incorporation of batch normalization in every layer   
6) Application of dropout regularization at each timestep   
7) Employing max pooling to aggregate sequential features</div>

        <div class="answer-section">
            <div class="answer-label">‚úì Correct Answer:</div>
            <div class="answer-text">The correct answer is 4) Sharing parameters across time steps during sequence processing.</div>
        </div>

        <div class="reference-section">
            <div class="reference-label">üìö Reference Text:</div>
            <button class="toggle-button" onclick="toggleReference(30)">
                Show/Hide Reference
            </button>
            <div id="ref30" class="reference-text hidden">y.Gaborfunctionsarearrangedinincreasing width(decreasing Œ≤ x)aswemovelefttorightthroughthegrid,andincreasingheight (decreasing Œ≤ y)aswemovetoptobottom.Fortheothertwoplots,the Œ≤valuesareÔ¨Åxed to1.5 √ótheimagewidth.GaborfunctionswithdiÔ¨Äerentsinusoidparameters ( R i g h t ) f and œÜ.Aswemovetoptobottom, fincreases,andaswemovelefttoright, œÜincreases. Fortheothertwoplots,isÔ¨Åxedto0andisÔ¨Åxedto5theimagewidth. œÜ f √ó (replacingblackwithwhiteandviceversa). Someofthemoststrikingcorrespondencesbetweenneuroscienceandmachine learningcomefromvisuallycomparingthefeatureslearnedbymachinelearning modelswiththoseemployedbyV1. ()showedthat OlshausenandField1996 asimpleunsupervisedlearningalgorithm, sparse coding,learnsfeatureswith receptiveÔ¨Åeldssimilartothoseofsimplecells.Sincethen,wehavefoundthat anextremelywidevarietyofstatisticallearningalgorithmslearnfeatureswith Gabor-likefunctionswhenappliedtonaturalimages.Thisincludesmostdeep learningalgorithms,whichlearnthesefeaturesintheirÔ¨Årstlayer.Figure9.19 showssomeexamples.BecausesomanydiÔ¨Äerentlearningalgorithmslearnedge detectors,itisdiÔ¨ÉculttoconcludethatanyspeciÔ¨Åclearningalgorithmisthe ‚Äúright‚Äùmodelofthebrainjustbasedonthefeaturesthatitlearns(thoughitcan certainlybeabadsignifanalgorithmdoeslearnsomesortofedgedetector not whenappliedtonaturalimages).Thesefeaturesareanimportantpartofthe statisticalstructureofnaturalimagesandcanberecoveredbymanydiÔ¨Äerent approachestostatisticalmodeling.SeeHyv√§rinen 2009etal.()forareviewofthe Ô¨Åeldofnaturalimagestatistics. 3 7 0 CHAPTER9.CONVOLUTIONALNETWORKS Figure9.19:ManymachinelearningalgorithmslearnfeaturesthatdetectedgesorspeciÔ¨Åc colorsofedgeswhenappliedtonaturalimages.Thesefeaturedetectorsarereminiscentof theGaborfunctionsknowntobepresentinprimaryvisualcortex. ( L e f t )Weightslearned byanunsupervisedlearningalgorithm(spikeandslabsparsecoding)appliedtosmall imagepatches. ( R i g h t )ConvolutionkernelslearnedbytheÔ¨Årstlayerofafullysupervised convolutionalmaxoutnetwork.NeighboringpairsofÔ¨Åltersdrivethesamemaxoutunit. 9.11ConvolutionalNetworksandtheHistoryofDeep Learning Convolutionalnetworkshaveplayedanimportantroleinthehistoryofdeep learning.Theyareakeyexampleofasuccessfulapplicationofinsightsobtained bystudyingthebraintomachinelearningapplications.Theywerealsosomeof theÔ¨Årstdeepmodelstoperformwell,longbeforearbitrarydeepmodelswere consideredviable.ConvolutionalnetworkswerealsosomeoftheÔ¨Årstneural networkstosolveimportantcommercialapplicationsandremainattheforefront ofcommercialapplicationsofdeeplearningtoday.Forexample,inthe1990s,the neuralnetworkresearchgroupatAT&Tdevelopedaconvolutionalnetworkfor readingchecks(,).Bytheendofthe1990s,thissystemdeployed LeCunetal.1998b byNECwasreadingover10%ofallthechecksintheUS.Later,severalOCR andhandwritingrecognitionsystemsbasedonconvolutionalnetsweredeployedby Microsoft( ,).Seechapterformoredetailsonsuchapplications Simardetal.2003 12 andmoremodernapplicationsofconvolutionalnetworks.See () LeCunetal.2010 foramorein-depthhistoryofconvolutionalnetworksupto2010. Convolutionalnetworkswerealsousedtowinmanycontests.Thecurrent intensityofcommercialinterestindeeplearningbeganwhenKrizhevskyetal. ()wontheImageNetobjectrecognitionchallenge,butconvolutionalnetworks 2012 3 7 1 CHAPTER9.CONVOLUTIONALNETWORKS hadbeenusedtowinothermachinelearningandcomputervisioncontestswith lessimpactforyearsearlier. ConvolutionalnetsweresomeoftheÔ¨Årstworkingdeepnetworkstrainedwith back-propagation.Itisnotentirelyclearwhyconvolutionalnetworkssucceeded whengeneralback-propagationnetworkswereconsideredtohavefailed.Itmay simplybethatconvolutionalnetworksweremorecomputationally eÔ¨Écientthan fullyconnectednetworks,soitwaseasiertorunmultipleexperimentswiththem andtunetheirimplementation andhyperparameters.Largernetworksalsoseem tobeeasiertotrain.Withmodernhardware,largefullyconnectednetworks appeartoperformreasonablyonmanytasks,evenwhenusingdatasetsthatwere availableandactivationfunctionsthatwerepopularduringthetimeswhenfully connectednetworkswerebelievednottoworkwell.Itmaybethattheprimary barrierstothesuccessofneuralnetworkswerepsychological(practitioners did notexpectneuralnetworkstowork,sotheydidnotmakeaseriouseÔ¨Äorttouse neuralnetworks).Whateverthecase,itisfortunatethatconvolutionalnetworks performedwelldecadesago.Inmanyways,theycarriedthetorchfortherestof deeplearningandpavedthewaytotheacceptanceofneuralnetworksingeneral. Convolutionalnetworksprovideawaytospecializeneuralnetworkstowork withdatathathasacleargrid-structuredtopologyandtoscalesuchmodelsto verylargesize.Thisapproachhasbeenthemostsuccessfulonatwo-dimensional, imagetopology.Toprocessone-dimensional, sequentialdata,weturnnextto anotherpowerfulspecializationoftheneuralnetworksframework:recurrentneural networks. 3 7 2 C h a p t e r 1 0 S e qu e n ce Mo d e l i n g: Recurren t an d Recursiv e N e t s RecurrentneuralnetworksorRNNs( ,)areafamilyof Rumelhart e t a l .1986a neuralnetworksforprocessingsequentialdata.Muchasaconvolutionalnetwork isaneuralnetworkthatisspecializedforprocessingagridofvalues Xsuchas animage,arecurrentneuralnetworkisaneuralnetworkthatisspecializedfor processingasequenceofvaluesx( 1 ), . . . ,x( ) œÑ.Justasconvolutionalnetworks canreadilyscaletoimageswithlargewidthandheight,andsomeconvolutional networkscanprocessimagesofvariablesize,recurrentnetworkscanscaletomuch longersequencesthanwouldbepracticalfornetworkswithoutsequence-based specialization.Mostrecurrentnetworkscanalsoprocesssequencesofvariable length. Togofrommulti-layernetworkstorecurrentnetworks,weneedtotakeadvan- tageofoneoftheearlyideasfoundinmachinelearningandstatisticalmodelsof the1980s:sharingparametersacrossdiÔ¨Äerentpartsofamodel.Parametersharing makesitpossibletoextendandapplythemodeltoexamplesofdiÔ¨Äerentforms (diÔ¨Äerentlengths,here)andgeneralizeacrossthem.Ifwehadseparateparameters foreachvalueofthetimeindex,wecouldnotgeneralizetosequencelengthsnot seenduringtraining,norsharestatisticalstrengthacrossdiÔ¨Äerentsequencelengths andacrossdiÔ¨Äerentpositionsintime.Suchsharingisparticularlyimportantwhen aspeciÔ¨Åcpieceofinformationcanoccuratmultiplepositionswithinthesequence. Forexample,considerthetwosentences‚ÄúIwenttoNepalin2009‚Äùand‚ÄúIn2009, IwenttoNepal.‚ÄùIfweaskamachinelearningmodeltoreadeachsentenceand extracttheyearinwhichthenarratorwenttoNepal,wewouldlikeittorecognize theyear2009astherelevantpieceofinformation,whetheritappearsinthesixth 373 CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS wordorthesecondwordofthesentence.Supposethatwetrainedafeedforward networkthatprocessessentencesofÔ¨Åxedlength.Atraditionalfullyconnected feedforwardnetworkwouldhaveseparateparametersforeachinputfeature,soit wouldneedtolearnalloftherulesofthelanguageseparatelyateachpositionin thesentence.Bycomparison,arecurrentneuralnetworksharesthesameweights acrossseveraltimesteps. Arelatedideaistheuseofconvolutionacrossa1-Dtemporalsequence.This convolutionalapproachisthebasisfortime-delayneuralnetworks(Langand Hinton1988Waibel1989Lang1990 ,; e t a l .,; e t a l .,).Theconvolutionoperation allowsanetworktoshareparametersacrosstime,butisshallow.Theoutput ofconvolutionisasequencewhereeachmemberoftheoutputisafunctionof asmallnumberofneighboringmembersoftheinput.Theideaofparameter sharingmanifestsintheapplicationofthesameconvolutionkernelateachtime step.RecurrentnetworksshareparametersinadiÔ¨Äerentway.Eachmemberofthe outputisafunctionofthepreviousmembersoftheoutput.Eachmemberofthe outputisproducedusingthesameupdateruleappliedtothepreviousoutputs. Thisrecurrentformulationresultsinthesharingofparametersthroughavery deepcomputational graph. Forthesimplicityofexposition,werefertoRNNsasoperatingonasequence thatcontainsvectorsx( ) twiththetimestepindex trangingfromto1 œÑ.In practice,recurrentnetworksusuallyoperateonminibatchesofsuchsequences, withadiÔ¨Äerentsequencelength œÑforeachmemberoftheminibatch.Wehave omittedtheminibatchindicestosimplifynotation.Moreover,thetimestepindex neednotliterallyrefertothepassageoftimeintherealworld.Sometimesitrefers onlytothepositioninthesequence.RNNsmayalsobeappliedintwodimensions acrossspatialdatasuchasimages,andevenwhenappliedtodatainvolvingtime, thenetworkmayhaveconnectionsthatgobackwardsintime,providedthatthe entiresequenceisobservedbeforeitisprovidedtothenetwork. Thischapterextendstheideaofacomputational graphtoincludecycles.These cyclesrepresenttheinÔ¨Çuenceofthepresentvalueofavariableonitsownvalue atafuturetimestep.Suchcomputational graphsallowustodeÔ¨Ånerecurrent neuralnetworks.WethendescribemanydiÔ¨Äerentwaystoconstruct,train,and userecurrentneuralnetworks. Formoreinformationonrecurrentneuralnetworksthanisavailableinthis chapter,wereferthereadertothetextbookofGraves2012(). 3 7 4 CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS 10.1UnfoldingComputationalGraphs Acomputational graphisawaytoformalizethestructureofasetofcomputations, suchasthoseinvolvedinmappinginputsandparameterstooutputsandloss. Pleaserefertosectionforageneralintroduction. Inthissectionweexplain 6.5.1 theideaofunfoldingarecursiveorrecurrentcomputationintoacomputational graphthathasarepetitivestructure,typicallycorrespondingtoachainofevents. Unfoldingthisgraphresultsinthesharingofparametersacrossadeepnetwork structure. Forexample,considertheclassicalformofadynamicalsystem: s( ) t= ( fs( 1 ) t ‚àí;)Œ∏ , (10.1) wheres( ) tiscalledthestateofthesystem. EquationisrecurrentbecausethedeÔ¨Ånitionof 10.1 sattime trefersbackto thesamedeÔ¨Ånitionattime. t‚àí1 ForaÔ¨Ånitenumberoftimesteps œÑ,thegraphcanbeunfoldedbyapplying thedeÔ¨Ånition œÑ‚àí1times.Forexample,ifweunfoldequationfor10.1 œÑ= 3time steps,weobtain s( 3 )=( fs( 2 );)Œ∏ (10.2) =(( f fs( 1 ););)Œ∏Œ∏ (10.3) UnfoldingtheequationbyrepeatedlyapplyingthedeÔ¨Ånitioninthiswayhas yieldedanexpressionthatdoesnotinvolverecurrence.Suchanexpressioncan nowberepresentedbyatraditionaldirectedacycliccomputational graph. The unfoldedcomputational graphofequationandequationisillustratedin 10.1 10.3 Ô¨Ågure.10.1 s( t ‚àí 1 )s( t ‚àí 1 )s( ) ts( ) ts( + 1 ) ts( + 1 ) t f fs( ) . . .s( ) . . .s( ) . . .s( ) . . . f f f f f f Figure10.1:Theclassicaldynamicalsystemdescribedbyequation,illustratedasan 10.1 unfoldedcomputationalgraph. Eachnoderepresentsthestateatsometime tandthe function fmapsthestateat ttothestateat t+1.Thesameparameters(thesamevalue ofusedtoparametrize)areusedforalltimesteps. Œ∏ f Asanotherexample,letusconsideradynamicalsystemdrivenbyanexternal signalx( ) t, s( ) t= ( fs( 1 ) t ‚àí,x( ) t;)Œ∏ , (10.4) 3 7 5 CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS whereweseethatthestatenowcontainsinformationaboutthewholepastsequence. RecurrentneuralnetworkscanbebuiltinmanydiÔ¨Äerentways.Muchas almostanyfunctioncanbeconsideredafeedforwardneuralnetwork,essentially anyfunctioninvolvingrecurrencecanbeconsideredarecurrentneuralnetwork. Manyrecurrentneuralnetworksuseequationorasimilarequationto 10.5 deÔ¨Ånethevaluesoftheirhiddenunits. Toindicatethatthestateisthehidden unitsofthenetwork,wenowrewriteequationusingthevariable 10.4 htorepresent thestate: h( ) t= ( fh( 1 ) t ‚àí,x( ) t;)Œ∏ , (10.5) illustratedinÔ¨Ågure,typicalRNNswilladdextraarchitecturalfeaturessuch 10.2 asoutputlayersthatreadinformationoutofthestatetomakepredictions.h Whentherecurrentnetworkistrainedtoperformataskthatrequirespredicting thefuturefromthepast,thenetworktypicallylearnstouseh( ) tasakindoflossy summaryofthetask-relevantaspectsofthepastsequenceofinputsupto t.This summaryisingeneralnecessarilylossy,sinceitmapsanarbitrarylengthsequence (x( ) t,x( 1 ) t ‚àí,x( 2 ) t ‚àí, . . . ,x( 2 ),x( 1 ))toaÔ¨Åxedlengthvectorh( ) t.Dependingonthe trainingcriterion,thissummarymightselectivelykeepsomeaspectsofthepast sequencewithmoreprecisionthanotheraspects.Forexample,iftheRNNisused instatisticallanguagemodeling,typicallytopredictthenextwordgivenprevious words,itmaynotbenecessarytostorealloftheinformationintheinputsequence uptotime t,butratheronlyenoughinformationtopredicttherestofthesentence. Themostdemandingsituationiswhenweaskh( ) ttoberichenoughtoallow onetoapproximately recovertheinputsequence,asinautoencoderframeworks (chapter).14 f fhh x xh( t ‚àí 1 )h( t ‚àí 1 )h( ) th( ) th( + 1 ) th( + 1 ) t x( t ‚àí 1 )x(</div>
        </div>
    </div>

    <div class="question-card" id="q31">
        <div class="question-header">
            <span class="question-number">Question 31</span>
            <span class="question-type">multiple-choice</span>
        </div>

        <div class="question-text">Deep learning has revolutionized artificial intelligence by enabling computers to achieve or surpass human-level performance in complex tasks. Advances in neural network architectures and software infrastructure have fueled progress in fields ranging from image recognition to natural language processing.

Which deep learning architecture introduced the concept of combining neural networks with external memory, allowing the system to learn simple algorithms and manipulate data structures?

1) Long Short-Term Memory (LSTM) networks   
2) Adaptive Linear Element (ADALINE)   
3) Convolutional Neural Networks (CNNs)   
4) Deep Boltzmann Machines   
5) Neural Turing Machines   
6) GoogLeNet   
7) Distributed Autoencoders</div>

        <div class="answer-section">
            <div class="answer-label">‚úì Correct Answer:</div>
            <div class="answer-text">The correct answer is 5) Neural Turing Machines.</div>
        </div>

        <div class="reference-section">
            <div class="reference-label">üìö Reference Text:</div>
            <button class="toggle-button" onclick="toggleReference(31)">
                Show/Hide Reference
            </button>
            <div id="ref31" class="reference-text hidden">l .2013Farabet2013Couprie e t a l .,; e t a l ., 2013)andyieldedsuperhumanperformanceintraÔ¨ÉcsignclassiÔ¨Åcation(Ciresan 2 3 CHAPTER1.INTRODUCTION 1 9 5 0 1 9 8 5 2 0 0 0 2 0 1 5 Y e a r1 011 021 031 04C o nne c t i o ns p e r ne ur o n 12 34 567 89 1 0 F r ui t Ô¨ÇyMo useC a tH um a n Figure1.10:Initially,thenumberofconnectionsbetweenneuronsinartiÔ¨Åcialneural networkswaslimitedbyhardwarecapabilities.Today,thenumberofconnectionsbetween neuronsismostlyadesignconsideration.SomeartiÔ¨Åcialneuralnetworkshavenearlyas manyconnectionsperneuronasacat,anditisquitecommonforotherneuralnetworks tohaveasmanyconnectionsperneuronassmallermammalslikemice.Eventhehuman braindoesnothaveanexorbitantamountofconnectionsperneuron.Biologicalneural networksizesfrom (). Wikipedia2015 1.Adaptivelinearelement( ,) WidrowandHoÔ¨Ä1960 2.Neocognitron(Fukushima1980,) 3.GPU-acceleratedconvolutionalnetwork( ,) Chellapilla e t al.2006 4.DeepBoltzmannmachine(SalakhutdinovandHinton2009a,) 5.Unsupervisedconvolutionalnetwork( ,) Jarrett e t al.2009 6.GPU-acceleratedmultilayerperceptron( ,) Ciresan e t al.2010 7.Distributedautoencoder(,) Le e t al.2012 8.Multi-GPUconvolutionalnetwork( ,) Krizhevsky e t al.2012 9.COTSHPCunsupervisedconvolutionalnetwork( ,) Coates e t al.2013 10.GoogLeNet( ,) Szegedy e t al.2014a 2 4 CHAPTER1.INTRODUCTION e t a l .,).2012 Atthesametimethatthescaleandaccuracyofdeepnetworkshasincreased, sohasthecomplexityofthetasksthattheycansolve. () Goodfellow e t a l .2014d showedthatneuralnetworkscouldlearntooutputanentiresequenceofcharacters transcribedfromanimage,ratherthanjustidentifyingasingleobject.Previously, itwaswidelybelievedthatthiskindoflearningrequiredlabelingoftheindividual elementsofthesequence( ,).Recurrentneuralnetworks, G√ºl√ßehreandBengio2013 suchastheLSTMsequencemodelmentionedabove,arenowusedtomodel relationshipsbetween s e q u e nc e s s e q u e nc e s andother ratherthanjustÔ¨Åxedinputs. Thissequence-to-sequencelearningseemstobeonthecuspofrevolutionizing anotherapplication:machinetranslation(Sutskever2014Bahdanau e t a l .,; e t a l ., 2015). Thistrendofincreasingcomplexityhasbeenpushedtoitslogicalconclusion withtheintroductionofneuralTuringmachines(Graves2014a e t a l .,)thatlearn toreadfrommemorycellsandwritearbitrarycontenttomemorycells.Such neuralnetworkscanlearnsimpleprogramsfromexamplesofdesiredbehavior.For example,theycanlearntosortlistsofnumbersgivenexamplesofscrambledand sortedsequences.Thisself-programming technologyisinitsinfancy,butinthe futurecouldinprinciplebeappliedtonearlyanytask. Anothercrowningachievementofdeeplearningisitsextensiontothedomainof r e i nf o r c e m e n t l e ar ni ng.Inthecontextofreinforcementlearning,anautonomous agentmustlearntoperformataskbytrialanderror,withoutanyguidancefrom thehumanoperator.DeepMinddemonstratedthatareinforcementlearningsystem basedondeeplearningiscapableoflearningtoplayAtarivideogames,reaching human-levelperformanceonmanytasks(,).Deeplearninghas Mnih e t a l .2015 alsosigniÔ¨Åcantlyimprovedtheperformanceofreinforcementlearningforrobotics (,). Finn e t a l .2015 ManyoftheseapplicationsofdeeplearningarehighlyproÔ¨Åtable.Deeplearning isnowused bymanytoptechnologycompanies includi ngGoogle, Microsoft, Facebook,IBM,Baidu,Apple,Adobe,NetÔ¨Çix,NVIDIAandNEC. Advancesindeeplearninghavealsodependedheavilyonadvancesinsoftware infrastructure.SoftwarelibrariessuchasTheano( ,; Bergstra e t a l .2010Bastien e t a l . e t a l . ,),PyLearn2( 2012 Goodfellow,),Torch( ,), 2013c Collobert e t a l .2011b DistBelief(,),CaÔ¨Äe(,),MXNet(,),and Dean e t a l .2012 Jia2013 Chen e t a l .2015 TensorFlow(,)haveallsupportedimportantresearchprojectsor Abadi e t a l .2015 commercialproducts. Deeplearninghasalsomadecontributionsbacktoothersciences.Modern convolutionalnetworksforobjectrecognitionprovideamodelofvisualprocessing 2 5</div>
        </div>
    </div>

    <div class="question-card" id="q32">
        <div class="question-header">
            <span class="question-number">Question 32</span>
            <span class="question-type">multiple-choice</span>
        </div>

        <div class="question-text">Deep learning optimization involves navigating complex, high-dimensional loss landscapes with unique geometric features such as saddle points, flat regions, and steep cliffs. The choice of optimization algorithm can profoundly impact the effectiveness and stability of neural network training in these environments.

Which property of first-order optimization methods like stochastic gradient descent (SGD) makes them particularly suitable for training deep neural networks compared to second-order methods such as Newton's method?

1) Their tendency to escape saddle points due to noise and geometric properties of the loss surface   
2) Their ability to directly find global minima by following curvature information   
3) Their requirement for exact gradient computation at every step   
4) Their reliance on the Hessian matrix to accelerate convergence in flat regions   
5) Their susceptibility to getting trapped at saddle points   
6) Their optimal performance only in convex optimization problems   
7) Their inability to handle noisy or inexact gradient estimates</div>

        <div class="answer-section">
            <div class="answer-label">‚úì Correct Answer:</div>
            <div class="answer-text">The correct answer is 1) Their tendency to escape saddle points due to noise and geometric properties of the loss surface.</div>
        </div>

        <div class="reference-section">
            <div class="reference-label">üìö Reference Text:</div>
            <button class="toggle-button" onclick="toggleReference(32)">
                Show/Hide Reference
            </button>
            <div id="ref32" class="reference-text hidden">CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS beheads.See ()forareviewoftherelevanttheoreticalwork. Dauphinetal.2014 Anamazingpropertyofmanyrandomfunctionsisthattheeigenvaluesofthe Hessianbecomemorelikelytobepositiveaswereachregionsoflowercost. In ourcointossinganalogy,thismeanswearemorelikelytohaveourcoincomeup heads ntimesifweareatacriticalpointwithlowcost. Thismeansthatlocal minimaaremuchmorelikelytohavelowcostthanhighcost.Criticalpointswith highcostarefarmorelikelytobesaddlepoints.Criticalpointswithextremely highcostaremorelikelytobelocalmaxima. Thishappensformanyclassesofrandomfunctions.Doesithappenforneural networks? ()showedtheoreticallythatshallowautoencoders BaldiandHornik1989 (feedforwardnetworkstrainedtocopytheirinputtotheiroutput,describedin chapter)withnononlinearities haveglobalminimaandsaddlepointsbutno 14 localminimawithhighercostthantheglobalminimum.Theyobservedwithout proofthattheseresultsextendtodeepernetworkswithoutnonlinearities. The outputofsuchnetworksisalinearfunctionoftheirinput,buttheyareuseful tostudyasamodelofnonlinearneuralnetworksbecausetheirlossfunctionis anon-convexfunctionoftheirparameters.Suchnetworksareessentiallyjust multiplematricescomposedtogether. ()providedexactsolutions Saxeetal.2013 tothecompletelearningdynamicsinsuchnetworksandshowedthatlearningin thesemodelscapturesmanyofthequalitativefeaturesobservedinthetrainingof deepmodelswithnonlinearactivationfunctions. ()showed Dauphinetal.2014 experimentallythatrealneuralnetworksalsohavelossfunctionsthatcontainvery manyhigh-costsaddlepoints.Choromanska2014etal.()providedadditional theoreticalarguments,showingthatanotherclassofhigh-dimensionalrandom functionsrelatedtoneuralnetworksdoessoaswell. Whataretheimplicationsoftheproliferationofsaddlepointsfortrainingalgo- rithms?ForÔ¨Årst-orderoptimization algorithmsthatuseonlygradientinformation, thesituationisunclear.Thegradientcanoftenbecomeverysmallnearasaddle point.Ontheotherhand,gradientdescentempiricallyseemstobeabletoescape saddlepointsinmanycases. ()providedvisualizationsof Goodfellowetal.2015 severallearningtrajectoriesofstate-of-the-art neuralnetworks,withanexample giveninÔ¨Ågure.ThesevisualizationsshowaÔ¨Çatteningofthecostfunctionnear 8.2 aprominentsaddlepointwheretheweightsareallzero,buttheyalsoshowthe gradientdescenttrajectoryrapidlyescapingthisregion. () Goodfellowetal.2015 alsoarguethatcontinuous-timegradientdescentmaybeshownanalyticallytobe repelledfrom,ratherthanattractedto,anearbysaddlepoint,butthesituation maybediÔ¨Äerentformorerealisticusesofgradientdescent. ForNewton‚Äôsmethod, itisclearthatsaddlepointsconstituteaproblem. 2 8 6 CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS P r o j e c t i o n2o f Œ∏ P r o j e c t i o n 1 o f Œ∏J( )Œ∏ Figure8.2:Avisualizationofthecostfunctionofaneuralnetwork.Imageadapted withpermissionfromGoodfellow2015 e t a l .(). Thesevisualizationsappearsimilarfor feedforwardneuralnetworks,convolutionalnetworks,andrecurrentnetworksapplied torealobjectrecognition andnaturallanguageprocessingtasks.Surprisingly,these visualizationsusuallydonotshowmanyconspicuousobstacles. Priortothesuccessof stochasticgradientdescentfortrainingverylargemodelsbeginninginroughly2012, neuralnetcostfunctionsurfacesweregenerallybelievedtohavemuchmorenon-convex structurethanisrevealedbytheseprojections. Theprimaryobstaclerevealedbythis projectionisasaddlepointofhighcostnearwheretheparametersareinitialized,but,as indicatedbythebluepath,theSGDtrainingtrajectoryescapesthissaddlepointreadily. MostoftrainingtimeisspenttraversingtherelativelyÔ¨Çatvalleyofthecostfunction, whichmaybeduetohighnoiseinthegradient,poorconditioningoftheHessianmatrix inthisregion,orsimplytheneedtocircumnavigatethetall‚Äúmountain‚Äùvisibleinthe Ô¨Ågureviaanindirectarcingpath. 2 8 7 CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS Gradientdescentisdesignedtomove‚Äúdownhill‚Äùandisnotexplicitlydesigned toseekacriticalpoint.Newton‚Äôsmethod,however,isdesignedtosolvefora pointwherethegradientiszero.WithoutappropriatemodiÔ¨Åcation,itcanjump toasaddlepoint.Theproliferation ofsaddlepointsinhighdimensionalspaces presumablyexplainswhysecond-ordermethodshavenotsucceededinreplacing gradientdescentforneuralnetworktraining. ()introduceda Dauphinetal.2014 saddle-freeNewtonmethodforsecond-orderoptimization andshowedthatit improvessigniÔ¨Åcantlyoverthetraditionalversion.Second-order methodsremain diÔ¨Éculttoscaletolargeneuralnetworks,butthissaddle-freeapproachholds promiseifitcouldbescaled. Thereareotherkindsofpointswithzerogradientbesidesminimaandsaddle points.Therearealsomaxima, whic haremuchlikesaddlepointsfromthe perspectiveofoptimization‚Äîmany algorithmsarenotattractedtothem, but unmodiÔ¨ÅedNewton‚Äôsmethodis.Maximaofmanyclassesofrandomfunctions becomeexponentiallyrareinhighdimensionalspace,justlikeminimado. Theremayalsobewide,Ô¨Çatregionsofconstantvalue.Intheselocations,the gradientandalsotheHessianareallzero.Suchdegeneratelocationsposemajor problemsforallnumericaloptimization algorithms.Inaconvexproblem,awide, Ô¨Çatregionmustconsistentirelyofglobalminima,butinageneraloptimization problem,sucharegioncouldcorrespondtoahighvalueoftheobjectivefunction. 8.2.4CliÔ¨ÄsandExplodingGradients Neuralnetworkswithmanylayersoftenhaveextremelysteepregionsresembling cliÔ¨Äs,asillustratedinÔ¨Ågure.Theseresultfromthemultiplicationofseveral 8.3 largeweightstogether.OnthefaceofanextremelysteepcliÔ¨Ästructure,the gradientupdatestepcanmovetheparametersextremelyfar,usuallyjumpingoÔ¨Ä ofthecliÔ¨Ästructurealtogether. 2 8 8 CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS ÓÅ∑ ÓÅ¢ÓÅäÓÅ∑ÓÄª ÓÅ¢ ÓÄ®ÓÄ© Figure8.3:Theobjectivefunctionforhighlynonlineardeepneuralnetworksorfor recurrentneuralnetworksoftencontainssharpnonlinearitiesinparameterspaceresulting fromthemultiplicationofseveralparameters.Thesenonlinearitiesgiverisetovery highderivativesinsomeplaces.WhentheparametersgetclosetosuchacliÔ¨Äregion,a gradientdescentupdatecancatapulttheparametersveryfar,possiblylosingmostofthe optimizationworkthathadbeendone. FigureadaptedwithpermissionfromPascanu e t a l .().2013 ThecliÔ¨Äcanbedangerouswhetherweapproachitfromaboveorfrombelow, butfortunatelyitsmostseriousconsequencescanbeavoidedusingthegradient clippingheuristicdescribedinsection.Thebasicideaistorecallthat 10.11.1 thegradientdoesnotspecifytheoptimalstepsize,butonlytheoptimaldirection withinaninÔ¨Ånitesimalregion.Whenthetraditionalgradientdescentalgorithm proposestomakeaverylargestep,thegradientclippingheuristicintervenesto reducethestepsizetobesmallenoughthatitislesslikelytogooutsidetheregion wherethegradientindicatesthedirectionofapproximately steepestdescent.CliÔ¨Ä structuresaremostcommoninthecostfunctionsforrecurrentneuralnetworks, becausesuchmodelsinvolveamultiplication ofmanyfactors,withonefactor foreachtimestep.Longtemporalsequencesthusincuranextremeamountof multiplication. 8.2.5Long-TermDependencies AnotherdiÔ¨Écultythatneuralnetworkoptimization algorithmsmustovercome arises when thecomputational gra ph becomes extremely deep.Feedforward networkswithmanylayershavesuchdeepcomputational graphs.Sodorecurrent networks,describedinchapter,whichconstructverydeepcomputational graphs 10 2 8 9 CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS byrepeatedlyapplyingthesameoperationateachtimestepofalongtemporal sequence.Repeatedapplicationofthesameparametersgivesrisetoespecially pronounceddiÔ¨Éculties. Forexample,supposethatacomputational graphcontainsapaththatconsists ofrepeatedlymultiplyingbyamatrixW.After tsteps,thisisequivalenttomul- tiplyingbyWt.SupposethatWhasaneigendecompositionW=Vdiag(Œª)V‚àí 1. Inthissimplecase,itisstraightforwardtoseethat Wt=ÓÄÄ VŒªVdiag()‚àí 1ÓÄÅt= ()VdiagŒªtV‚àí 1. (8.11) Anyeigenvalues Œª ithatarenotnearanabsolutevalueofwilleitherexplodeifthey 1 aregreaterthaninmagnitudeorvanishiftheyarelessthaninmagnitude.The 1 1 vanishingandexplodinggradientproblemreferstothefactthatgradients throughsuchagrapharealsoscaledaccordingtodiag(Œª)t.Vanishinggradients makeitdiÔ¨Éculttoknowwhichdirectiontheparametersshouldmovetoimprove thecostfunction,whileexplodinggradientscanmakelearningunstable.ThecliÔ¨Ä structuresdescribedearlierthatmotivategradientclippingareanexampleofthe explodinggradientphenomenon. Therepeatedmultiplication byWateachtimestepdescribedhereisvery similartothepowermethodalgorithmusedtoÔ¨Åndthelargesteigenvalueof amatrixWandthecorrespondingeigenvector.Fromthispointofviewitis notsurprisingthatxÓÄæWtwilleventuallydiscardallcomponentsofxthatare orthogonaltotheprincipaleigenvectorof.W RecurrentnetworksusethesamematrixWateachtimestep,butfeedforward networksdonot,soevenverydeepfeedforwardnetworkscanlargelyavoidthe vanishingandexplodinggradientproblem(,). Sussillo2014 Wedeferafurtherdiscussionofthechallengesoftrainingrecurrentnetworks untilsection,afterrecurrentnetworkshavebeendescribedinmoredetail. 10.7 8.2.6InexactGradients Mostoptimization algorithmsaredesignedwiththeassumptionthatwehave accesstotheexactgradientorHessianmatrix.Inpractice,weusuallyonlyhave anoisyorevenbiasedestimateofthesequantities. Nearlyeverydeeplearning algorithmreliesonsampling-basedestimatesatleastinsofarasusingaminibatch oftrainingexamplestocomputethegradient. Inothercases,theobjectivefunctionwewanttominimizeisactuallyintractable. Whentheobjectivefunctionisintractable,typicallyitsgradientisintractableas well.Insuchcaseswecanonlyapproximatethegradient.Theseissuesmostlyarise 2 9 0 CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS withthemoreadvancedmodelsinpart.Forexample,contrastivedivergence III givesatechniqueforapproximatingthegradientoftheintractablelog-likelihood ofaBoltzmannmachine. Variousneuralnetworkoptimization algorithmsaredesignedtoaccountfor imperfectionsinthegradientestimate.Onecanalsoavoidtheproblembychoosing asurrogatelossfunctionthatiseasiertoapproximate thanthetrueloss. 8.2.7PoorCorrespondencebetweenLocalandGlobalStructure Manyoftheproblemswehavediscussedsofarcorrespondtopropertiesofthe lossfunctionatasinglepoint‚ÄîitcanbediÔ¨Éculttomakeasinglestepif J(Œ∏)is poorlyconditionedatthecurrentpointŒ∏,orifŒ∏liesonacliÔ¨Ä,orifŒ∏isasaddle pointhidingtheopportunitytomakeprogressdownhillfromthegradient. Itispossibletoovercomealloftheseproblemsatasinglepointandstill performpoorlyifthedirectionthatresultsinthemostimprovementlocallydoes notpointtowarddistantregionsofmuchlowercost. Goodfellow2015etal.()arguethatmuchoftheruntimeoftrainingisdueto thelengthofthetrajectoryneededtoarriveatthesolution.Figureshowsthat8.2 thelearningtrajectoryspendsmostofitstimetracingoutawidearcarounda mountain-shapedstructure. MuchofresearchintothediÔ¨Écultiesofoptimization hasfocusedonwhether trainingarrivesataglobalminimum,alocalminimum,orasaddlepoint,butin practiceneuralnetworksdonotarriveatacriticalpointofanykind.Figure8.1 showsthatneuralnetworksoftendonotarriveataregionofsmallgradient.Indeed, suchcriticalpointsdonotevennecessarilyexist.Forexample,thelossfunction ‚àílog p( y|x;Œ∏)canlackaglobalminimumpointandinsteadasymptotically approachsomevalueasthemodelbecomesmoreconÔ¨Ådent.ForaclassiÔ¨Åerwith discrete yand p( y|x)providedbyasoftmax,thenegativelog-likelihoodcan becomearbitrarilyclosetozeroifthemodelisabletocorrectlyclassifyevery exampleinthetrainingset,butitisimpossibletoactuallyreachthevalueof zero.Likewise,amodelofrealvalues p( y|x) =N( y; f(Œ∏) , Œ≤‚àí 1)canhavenegative log-likelihoodthatasymptotestonegativeinÔ¨Ånity‚Äîif f(Œ∏)isabletocorrectly predictthevalueofalltrainingset ytargets,thelearningalgorithmwillincrease Œ≤withoutbound.SeeÔ¨Ågureforanexampleofafailureoflocaloptimization to 8.4 Ô¨Åndagoodcostfunctionvalueevenintheabsenceofanylocalminimaorsaddle points. Futureresearchwillneedtodevelopfurtherunderstandingofthefactorsthat inÔ¨Çuencethelengthofthelearningtrajectoryandbettercharacterizetheoutcome 2 9 1 CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS Œ∏J ( ) Œ∏ Figure8.4:Optimizationbasedonlocaldownhillmovescanfailifthelocalsurfacedoes notpointtowardtheglobalsolution.Hereweprovideanexampleofhowthiscanoccur, eveniftherearenosaddlepointsandnolocalminima.Thisexamplecostfunction containsonlyasymptotestowardlowvalues,notminima.ThemaincauseofdiÔ¨Écultyin thiscaseisbeinginitializedonthewrongsideofthe‚Äúmountain‚Äùandnotbeingableto traverseit. Inhigherdimensionalspace,learningalgorithmscanoftencircumnavigate suchmountainsbutthetrajectoryassociatedwithdoingsomaybelongandresultin excessivetrainingtime,asillustratedinÔ¨Ågure.8.2 oftheprocess. ManyexistingresearchdirectionsareaimedatÔ¨Åndinggoodinitialpointsfor problemsthathavediÔ¨Écultglobalstructure,ratherthandevelopingalgorithms thatusenon-localmoves. GradientdescentandessentiallyalllearningalgorithmsthatareeÔ¨Äectivefor trainingneuralnetworksarebasedonmakingsmall,localmoves.Theprevious sectionshaveprimarilyfocusedonhowthecorrectdirectionoftheselocalmoves canbediÔ¨Éculttocompute.Wemaybeabletocomputesomepropertiesofthe objectivefunction,suchasitsgradient,onlyapproximately ,withbiasorvariance inourestimateofthecorrectdirection.Inthesecases,localdescentmayormay notdeÔ¨Åneareasonablyshortpathtoavalidsolution,butwearenotactually abletofollowthelocaldescentpath.Theobjectivefunctionmayhaveissues suchaspoorconditioningordiscontinuousgradients,causingtheregionwhere thegradientprovidesagoodmodeloftheobjectivefunctiontobeverysmall.In thesecases,localdescentwithstepsofsize ÓÄèmaydeÔ¨Åneareasonablyshortpath tothesolution,butweareonlyabletocomputethelocaldescentdirectionwith stepsofsize Œ¥ ÓÄèÓÄú.Inthesecases,localdescentmayormaynotdeÔ¨Åneapath tothesolution,butthepathcontainsmanysteps,sofollowingthepathincursa 2 9 2 CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS highcomputational cost.Sometimeslocalinformationprovidesusnoguide,when thefunctionhasawideÔ¨Çatregion,orifwemanagetolandexactlyonacritical point(usuallythislatterscenarioonlyhappenstomethodsthatsolveexplicitly forcriticalpoints,suchasNewton‚Äôsmethod).Inthesecases,localdescentdoes notdeÔ¨Åneapathtoasolutionatall.Inothercases,localmovescanbetoogreedy andleadusalongapaththatmovesdownhillbutawayfromanysolution,asin Ô¨Ågure,oralonganunnecessarilylongtrajectorytothesolution,asinÔ¨Ågure. 8.4 8.2 Currently,wedonotunderstandwhichoftheseproblemsaremostrelevantto makingneuralnetworkoptimization diÔ¨Écult,andthisisanactiveareaofresearch. RegardlessofwhichoftheseproblemsaremostsigniÔ¨Åcant,allofthemmightbe avoidedifthereexistsaregionofspaceconnectedreasonablydirectlytoasolution byapaththatlocaldescentcanfollow,andifweareabletoinitializelearning withinthatwell-behavedregion. Thislastviewsuggestsresearchintochoosing goodinitialpointsfortraditionaloptimization algorithmstouse. 8.2.8TheoreticalLimitsofOptimization Severaltheoreticalresultsshowthattherearelimitsontheperformanceofany optimization algorithmwemightdesignforneuralnetworks(BlumandRivest, 1992Judd1989WolpertandMacReady1997 ;,; ,).Typicallytheseresultshave littlebearingontheuseofneuralnetworksinpractice. Sometheoreticalresultsapplyonlytothecasewheretheunitsofaneural networkoutput discretevalues.However, most neuralnetworkunitsoutput smoothlyincreasingvaluesthatmakeoptimization vialocalsearchfeasible.Some theoreticalresultsshowthatthereexistproblemclassesthatareintractable,but itcanbediÔ¨Éculttotellwhetheraparticularproblemfallsintothatclass.Other resultsshowthatÔ¨Åndingasolutionforanetworkofagivensizeisintractable,but inpracticewecanÔ¨Åndasolutioneasilybyusingalargernetworkforwhichmany moreparametersettingscorrespondtoanacceptablesolution.Moreover,inthe contextofneuralnetworktraining,weusuallydonotcareaboutÔ¨Åndingtheexact minimumofafunction,butseekonlytoreduceitsvaluesuÔ¨Écientlytoobtaingood generalization error. Theoretical analysisofwhetheranoptimization algorithm canaccomplishthisgoalisextremelydiÔ¨Écult.Developingmorerealisticbounds ontheperformanceofoptimization algorithmsthereforeremainsanimportant goalformachinelearningresearch. 2 9 3 CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS 8.3BasicAlgorithms Wehavepreviouslyintroducedthegradientdescent(section)algorithmthat 4.3 followsthegradientofanentiretrainingsetdownhill.Thismaybeaccelerated considerablybyusingstochasticgradientdescenttofollowthegradientofrandomly selectedminibatchesdownhill,asdiscussedinsectionandsection. 5.9 8.1.3 8.3.1StochasticGradientDescent Stochasticgradientdescent(SGD)anditsvariantsareprobablythemostused optimization algorithmsformachinelearningingeneralandfordeeplearning inparticular. As discussedinsection,itispossibletoobtainanunbiased 8.1.3 estimateofthegradientbytakingtheaveragegradientonaminibatchof m examplesdrawni.i.dfromthedatageneratingdistribution. Algorithmshowshowtofollowthisestimateofthegradientdownhill. 8.1 Algorithm8.1Stochasticgradientdescent(SGD)updateattrainingiteration k Require:Learningrate ÓÄè k. Require:InitialparameterŒ∏ while do stoppingcriterionnotmet Sampleaminibatchof mexamplesfromthetrainingset{x( 1 ), . . . ,x( ) m}with correspondingtargetsy( ) i. Computegradientestimate: ÀÜg‚Üê+1 m‚àá Œ∏ÓÅê i L f((x( ) i;)Œ∏ ,y( ) i) Applyupdate:Œ∏Œ∏‚Üê ‚àí ÓÄèÀÜg endwhile AcrucialparameterfortheSGDalgorithmisthelearningrate.Previously,we havedescribedSGDasusingaÔ¨Åxedlearningrate ÓÄè.Inpractice,itisnecessaryto graduallydecreasethelearningrateovertime,sowenowdenotethelearningrate atiterationas k ÓÄè k.</div>
        </div>
    </div>

    <div class="question-card" id="q33">
        <div class="question-header">
            <span class="question-number">Question 33</span>
            <span class="question-type">multiple-choice</span>
        </div>

        <div class="question-text">Optimization algorithms for machine learning leverage various forms of gradient and curvature information to efficiently minimize complex objective functions, particularly in deep learning. Different methods balance convergence speed, robustness to non-convexity, and computational cost when dealing with large numbers of parameters.

Which of the following statements correctly identifies a key computational advantage of the conjugate gradient method over standard Newton‚Äôs method in training large neural networks?

1) Conjugate gradient requires explicit calculation and inversion of the full Hessian matrix.   
2) Conjugate gradient only works for strictly convex quadratic objective functions without modification.   
3) Conjugate gradient avoids direct Hessian inversion by iteratively searching along conjugate directions, making it more computationally efficient for large problems.   
4) Conjugate gradient is always slower than steepest descent due to additional line searches.   
5) Conjugate gradient utilizes stochastic updates for every parameter in each iteration.   
6) Conjugate gradient mandates regularization of the Hessian at every step to ensure convergence.   
7) Conjugate gradient builds a full quasi-Newton Hessian approximation before starting optimization.</div>

        <div class="answer-section">
            <div class="answer-label">‚úì Correct Answer:</div>
            <div class="answer-text">The correct answer is 3) Conjugate gradient avoids direct Hessian inversion by iteratively searching along conjugate directions, making it more computationally efficient for large problems..</div>
        </div>

        <div class="reference-section">
            <div class="reference-label">üìö Reference Text:</div>
            <button class="toggle-button" onclick="toggleReference(33)">
                Show/Hide Reference
            </button>
            <div id="ref33" class="reference-text hidden">Œ¥(operationsappliedelement-wise) Applyupdate:Œ∏Œ∏Œ∏ ‚Üê +‚àÜ endwhile Newton‚Äôsmethodisanoptimization schemebasedonusingasecond-orderTay- lorseriesexpansiontoapproximate J(Œ∏)nearsomepointŒ∏ 0,ignoringderivatives ofhigherorder: J J () Œ∏‚âà(Œ∏ 0)+(Œ∏Œ∏‚àí 0)ÓÄæ‚àá Œ∏ J(Œ∏ 0)+1 2(Œ∏Œ∏‚àí 0)ÓÄæHŒ∏Œ∏ (‚àí 0) ,(8.26) whereHistheHessianof JwithrespecttoŒ∏evaluatedatŒ∏ 0.Ifwethensolvefor thecriticalpointofthisfunction,weobtaintheNewtonparameterupdaterule: Œ∏‚àó= Œ∏ 0‚àíH‚àí 1‚àá Œ∏ J(Œ∏ 0) (8.27) Thusforalocallyquadraticfunction(withpositivedeÔ¨ÅniteH),byrescaling thegradientbyH‚àí 1,Newton‚Äôsmethodjumpsdirectlytotheminimum. If the objectivefunctionisconvexbutnotquadratic(therearehigher-orderterms),this updatecanbeiterated,yieldingthetrainingalgorithmassociatedwithNewton‚Äôs method,giveninalgorithm .8.8 3 1 1 CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS Algorithm8.8Newton‚Äôsmethodwithobjective J(Œ∏) = 1 mÓÅêm i = 1 L f((x( ) i;)Œ∏ , y( ) i). Require:InitialparameterŒ∏ 0 Require:Trainingsetofexamples m while do stoppingcriterionnotmet Computegradient:g‚Üê1 m‚àá Œ∏ÓÅê i L f((x( ) i;)Œ∏ ,y( ) i) ComputeHessian:H‚Üê1 m‚àá2 Œ∏ÓÅê i L f((x( ) i;)Œ∏ ,y( ) i) ComputeHessianinverse:H‚àí 1 Computeupdate: ‚àÜ= Œ∏‚àíH‚àí 1g Applyupdate:Œ∏Œ∏Œ∏ = +‚àÜ endwhile Forsurfacesthatarenotquadratic,aslongastheHessianremainspositive deÔ¨Ånite,Newton‚Äôsmethodcanbeappliediteratively.Thisimpliesatwo-step iterativeprocedure.First,updateorcomputetheinverseHessian(i.e.byupdat- ingthequadraticapproximation). Second, updatetheparametersaccordingto equation.8.27 Insection,wediscussedhowNewton‚Äôsmethodisappropriateonlywhen 8.2.3 theHessianispositivedeÔ¨Ånite.Indeeplearning,thesurfaceoftheobjective functionistypicallynon-convexwithmanyfeatures,suchassaddlepoints,that areproblematicforNewton‚Äôsmethod. IftheeigenvaluesoftheHessianarenot allpositive,forexample,nearasaddlepoint,thenNewton‚Äôsmethodcanactually causeupdatestomoveinthewrongdirection.Thissituationcanbeavoided byregularizingtheHessian.Commonregularizationstrategiesincludeaddinga constant,,alongthediagonaloftheHessian.Theregularizedupdatebecomes Œ± Œ∏‚àó= Œ∏ 0‚àí[(( H fŒ∏ 0))+ ] Œ±I‚àí 1‚àá Œ∏ f(Œ∏ 0) . (8.28) Thisregularizationstrategyisusedinapproximations toNewton‚Äôsmethod,such astheLevenberg‚ÄìMarquardt algorithm(Levenberg1944Marquardt1963 ,;,),and worksfairlywellaslongasthenegativeeigenvaluesoftheHessianarestillrelatively closetozero.Incaseswheretherearemoreextremedirectionsofcurvature,the valueof Œ±wouldhavetobesuÔ¨ÉcientlylargetooÔ¨Äsetthenegativeeigenvalues. However,as Œ±increasesinsize,theHessianbecomesdominatedbythe Œ±Idiagonal andthedirectionchosenbyNewton‚Äôsmethodconvergestothestandardgradient dividedby Œ±. Whenstrongnegativecurvatureispresent, Œ±mayneedtobeso largethatNewton‚Äôsmethodwouldmakesmallerstepsthangradientdescentwith aproperlychosenlearningrate. Beyondthechallengescreatedbycertainfeaturesoftheobjectivefunction, 3 1 2 CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS suchassaddlepoints,theapplicationofNewton‚Äôsmethodfortraininglargeneural networksislimitedbythesigniÔ¨Åcantcomputational burdenitimposes.The numberofelementsintheHessianissquaredinthenumberofparameters,sowith kparameters(andforevenverysmallneuralnetworksthenumberofparameters kcanbeinthemillions),Newton‚Äôsmethodwouldrequiretheinversionofa k k√ó matrix‚Äîwith computational complexityof O( k3).Also,sincetheparameterswill changewitheveryupdate,theinverseHessianhastobecomputed ateverytraining iteration.Asaconsequence,onlynetworkswithaverysmallnumberofparameters canbepracticallytrainedviaNewton‚Äôsmethod.Intheremainderofthissection, wewilldiscussalternativesthatattempttogainsomeoftheadvantagesofNewton‚Äôs methodwhileside-steppingthecomputational hurdles. 8.6.2ConjugateGradients ConjugategradientsisamethodtoeÔ¨Écientlyavoidthecalculationoftheinverse Hessianbyiterativelydescendingconjugatedirections.Theinspirationforthis approachfollowsfromacarefulstudyoftheweaknessofthemethodofsteepest descent(seesectionfordetails),wherelinesearchesareappliediterativelyin 4.3 thedirectionassociatedwiththegradient.Figureillustrateshowthemethodof 8.6 steepestdescent,whenappliedinaquadraticbowl,progressesinaratherineÔ¨Äective back-and-forth,zig-zagpattern.Thishappensbecauseeachlinesearchdirection, whengivenbythegradient,isguaranteedtobeorthogonaltothepreviousline searchdirection. Lettheprevioussearchdirectionbed t ‚àí 1.Attheminimum,wheretheline searchterminates,thedirectionalderivativeiszeroindirectiond t ‚àí 1:‚àá Œ∏ J(Œ∏)¬∑ d t ‚àí 1=0.SincethegradientatthispointdeÔ¨Ånesthecurrentsearchdirection, d t=‚àá Œ∏ J(Œ∏) willhavenocontributioninthedirectiond t ‚àí 1.Thusd tisorthogonal tod t ‚àí 1.Thisrelationshipbetweend t ‚àí 1andd tisillustratedinÔ¨Ågurefor8.6 multipleiterationsofsteepestdescent.AsdemonstratedintheÔ¨Ågure,thechoiceof orthogonaldirectionsofdescentdonotpreservetheminimumalongtheprevious searchdirections.Thisgivesrisetothezig-zagpatternofprogress,whereby descendingtotheminimuminthecurrentgradientdirection,wemustre-minimize theobjectiveinthepreviousgradientdirection.Thus,byfollowingthegradientat theendofeachlinesearchweare,inasense,undoingprogresswehavealready madeinthedirectionofthepreviouslinesearch.Themethodofconjugategradients seekstoaddressthisproblem. Inthemethodofconjugategradients,weseektoÔ¨Åndasearchdirectionthat isconjugatetothepreviouslinesearchdirection,i.e.itwillnotundoprogress madeinthatdirection.Attrainingiteration t,thenextsearchdirectiond ttakes 3 1 3 CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS  ÓÄ≥ ÓÄ∞  ÓÄ≤ ÓÄ∞  ÓÄ± ÓÄ∞ ÓÄ∞ ÓÄ± ÓÄ∞ ÓÄ≤ ÓÄ∞ ÓÄ≥ ÓÄ∞ ÓÄ≤ ÓÄ∞ ÓÄ± ÓÄ∞ÓÄ∞ÓÄ± ÓÄ∞ÓÄ≤ ÓÄ∞ Figure8.6:Themethodofsteepestdescentappliedtoaquadraticcostsurface.The methodofsteepestdescentinvolvesjumpingtothepointoflowestcostalongtheline deÔ¨Ånedbythegradientattheinitialpointoneachstep.Thisresolvessomeoftheproblems seenwithusingaÔ¨ÅxedlearningrateinÔ¨Ågure,butevenwiththeoptimalstepsize 4.6 thealgorithmstillmakesback-and-forthprogresstowardtheoptimum.BydeÔ¨Ånition,at theminimumoftheobjectivealongagivendirection,thegradientattheÔ¨Ånalpointis orthogonaltothatdirection. theform: d t= ‚àá Œ∏ J Œ≤ ()+Œ∏ td t ‚àí 1 (8.29) where Œ≤ tisacoeÔ¨Écientwhosemagnitudecontrolshowmuchofthedirection,d t ‚àí 1, weshouldaddbacktothecurrentsearchdirection. Twodirections,d tandd t ‚àí 1,aredeÔ¨ÅnedasconjugateifdÓÄæ tHd t ‚àí 1= 0,where HistheHessianmatrix. Thestraightforwardwaytoimposeconjugacywouldinvolvecalculationofthe eigenvectorsofHtochoose Œ≤ t,whichwouldnotsatisfyourgoalofdeveloping amethodthatismorecomputationally viablethanNewton‚Äôsmethodforlarge problems. Canwecalculatetheconjugatedirectionswithoutresortingtothese calculations?Fortunatelytheanswertothatisyes. Twopopularmethodsforcomputingthe Œ≤ tare: 1. Fletcher-Reeves: Œ≤ t=‚àá Œ∏ J(Œ∏ t)ÓÄæ‚àá Œ∏ J(Œ∏ t) ‚àá Œ∏ J(Œ∏ t ‚àí 1)ÓÄæ‚àá Œ∏ J(Œ∏ t ‚àí 1)(8.30) 3 1 4 CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS 2. Polak-Ribi√®re: Œ≤ t=(‚àá Œ∏ J(Œ∏ t)‚àí‚àá Œ∏ J(Œ∏ t ‚àí 1))ÓÄæ‚àá Œ∏ J(Œ∏ t) ‚àá Œ∏ J(Œ∏ t ‚àí 1)ÓÄæ‚àá Œ∏ J(Œ∏ t ‚àí 1)(8.31) Foraquadraticsurface,theconjugatedirectionsensurethatthegradientalong thepreviousdirectiondoesnotincreaseinmagnitude.Wethereforestayatthe minimumalongthepreviousdirections.Asaconsequence,ina k-dimensional parameterspace,theconjugategradientmethodrequiresatmost klinesearchesto achievetheminimum.Theconjugategradientalgorithmisgiveninalgorithm .8.9 Algorithm8.9Theconjugategradientmethod Require:InitialparametersŒ∏ 0 Require:Trainingsetofexamples m InitializeœÅ 0= 0 Initialize g 0= 0 Initialize t= 1 while do stoppingcriterionnotmet Initializethegradientg t= 0 Computegradient:g t‚Üê1 m‚àá Œ∏ÓÅê i L f((x( ) i;)Œ∏ ,y( ) i) Compute Œ≤ t=( g t ‚àí g t ‚àí1 )ÓÄæg t gÓÄæ t ‚àí1g t ‚àí1(Polak-Ribi√®re) (Nonlinearconjugategradient:optionallyreset Œ≤ ttozero,forexampleif tis amultipleofsomeconstant,suchas) k k= 5 Computesearchdirection:œÅ t= ‚àíg t+ Œ≤ tœÅ t ‚àí 1 PerformlinesearchtoÔ¨Ånd: ÓÄè‚àó= argmin ÓÄè1 mÓÅêm i = 1 L f((x( ) i;Œ∏ t+ ÓÄèœÅ t) ,y( ) i) (Onatrulyquadraticcostfunction,analyticallysolvefor ÓÄè‚àóratherthan explicitlysearchingforit) Applyupdate:Œ∏ t + 1= Œ∏ t+ ÓÄè‚àóœÅ t t t‚Üê+1 endwhile NonlinearConjugateGradients:Sofarwehavediscussedthemethodof conjugategradientsasitisappliedtoquadraticobjectivefunctions. Ofcourse, ourprimaryinterestinthischapteristoexploreoptimization methodsfortraining neuralnetworksandotherrelateddeeplearningmodelswherethecorresponding objectivefunctionisfarfromquadratic.Perhapssurprisingly,themethodof conjugategradientsisstillapplicableinthissetting,thoughwithsomemodiÔ¨Åcation. Withoutanyassurancethattheobjectiveisquadratic,theconjugatedirections 3 1 5 CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS arenolongerassuredtoremainattheminimumoftheobjectiveforprevious directions.Asaresult,thenonlinearconjugategradientsalgorithmincludes occasionalresetswherethemethodofconjugategradientsisrestartedwithline searchalongtheunalteredgradient. Practitionersreportreasonableresultsinapplicationsofthenonlinearconjugate gradientsalgorithmtotrainingneuralnetworks,thoughitisoftenbeneÔ¨Åcialto initializetheoptimizationwithafewiterationsofstochasticgradientdescentbefore commencingnonlinearconjugategradients.Also,whilethe(nonlinear)conjugate gradientsalgorithmhastraditionallybeencastasabatchmethod,minibatch versionshavebeenusedsuccessfullyforthetrainingofneuralnetworks(,Leetal. 2011). AdaptationsofconjugategradientsspeciÔ¨Åcallyforneuralnetworkshave beenproposedearlier,suchasthescaledconjugategradientsalgorithm(,Moller 1993). 8.6.3BFGS TheBroyden‚ÄìFletcher‚ÄìGoldfarb‚ÄìShanno(BFGS)algorithmattemptsto bringsomeoftheadvantagesofNewton‚Äôsmethodwithoutthecomputational</div>
        </div>
    </div>

    <div class="question-card" id="q34">
        <div class="question-header">
            <span class="question-number">Question 34</span>
            <span class="question-type">multiple-choice</span>
        </div>

        <div class="question-text">The normal (Gaussian) distribution is fundamental in probability theory, statistics, and machine learning, especially for modeling real-valued variables and uncertainties. Understanding its properties and generalizations is crucial for applying statistical models to both univariate and multivariate data.

Which of the following statements accurately describes a property unique to the normal distribution among all distributions with a specified mean and variance?

1) It maximizes entropy, representing the highest possible uncertainty given the constraints.   
2) It is always symmetric about the median, regardless of parameterization.   
3) Its probability density function is always bounded above by 1 for any variance.   
4) It only models independent random variables with zero covariance.   
5) The sum of any two normal distributions is always a standard normal distribution.   
6) It is always the most robust to outliers among continuous distributions.   
7) Its cumulative distribution function can be expressed in closed form using elementary functions.</div>

        <div class="answer-section">
            <div class="answer-label">‚úì Correct Answer:</div>
            <div class="answer-text">The correct answer is 1) It maximizes entropy, representing the highest possible uncertainty given the constraints..</div>
        </div>

        <div class="reference-section">
            <div class="reference-label">üìö Reference Text:</div>
            <button class="toggle-button" onclick="toggleReference(34)">
                Show/Hide Reference
            </button>
            <div id="ref34" class="reference-text hidden">norinversevarianceofthedistribution: N(;x¬µ,Œ≤‚àí 1) =ÓÅ≤ Œ≤ 2œÄexpÓÄí ‚àí1 2Œ≤x¬µ (‚àí)2ÓÄì . (3.22) Normaldistributionsareasensiblechoiceformanyapplications.Intheabsence ofpriorknowledgeaboutwhatformadistributionovertherealnumbersshould take,thenormaldistributionisagooddefaultchoicefortwomajorreasons. 63 CHAPTER3.PROBABILITYANDINFORMATIONTHEORY ‚àí ‚àí ‚àí ‚àí 20 . 15 . 10 . 05 00 05 10 15 20 . . . . . . x000 .005 .010 .015 .020 .025 .030 .035 .040 .p(x)Maximumat= x ¬µ InÔ¨Çectionpointsat x ¬µ œÉ = ¬± Figure3.1:Thenormaldistribution:ThenormaldistributionN(x;¬µ,œÉ2)exhibits aclassic‚Äúbellcurve‚Äùshape,withthexcoordinateofitscentralpeakgivenby¬µ,and thewidthofitspeakcontrolledbyœÉ.Inthisexample,wedepictthestandardnormal distribution,withand. ¬µ= 0œÉ= 1 First,manydistributionswewishtomodelaretrulyclosetobeingnormal distributions.The c e n t r al l i m i t t heor e mshowsthatthesumofmanyindepen- dentrandomvariablesisapproximatelynormallydistributed.Thismeansthat inpractice,manycomplicatedsystemscanbemodeledsuccessfullyasnormally distributednoise,evenifthesystemcanbedecomposedintopartswithmore structuredbehavior. Second,outofallpossibleprobabilitydistributionswiththesamevariance, thenormaldistributionencodesthemaximumamountofuncertaintyoverthe realnumbers.Wecanthusthinkofthenormaldistributionasbeingtheone thatinsertstheleastamountofpriorknowledgeintoamodel.Fullydeveloping andjustifyingthisidearequiresmoremathematical tools,andispostponedto section.19.4.2 Thenormaldistributiongeneralizesto Rn,inwhichcaseitisknownasthe m ul t i v ar i at e nor m al di st r i but i o n.Itmaybeparametrized withapositive deÔ¨Ånitesymmetricmatrix: Œ£ N(; ) = x ¬µ, Œ£ÓÅ≥ 1 (2)œÄndet() Œ£expÓÄí ‚àí1 2( ) x ¬µ‚àíÓÄæŒ£‚àí 1( ) x ¬µ‚àíÓÄì .(3.23) 64 CHAPTER3.PROBABILITYANDINFORMATIONTHEORY Theparameter ¬µstillgivesthemeanofthedistribution,thoughnowitis vector-valued.Theparameter Œ£givesthecovariancematrixofthedistribution. Asintheunivariatecase,whenwewishtoevaluatethePDFseveraltimesfor manydiÔ¨Äerentvaluesoftheparameters,thecovarianceisnotacomputationally eÔ¨Écientwaytoparametrizethedistribution,sinceweneedtoinvert Œ£toevaluate thePDF.Wecaninsteadusea : pr e c i si o n m at r i x Œ≤ N(; x ¬µ Œ≤,‚àí 1) =ÓÅ≥ det() Œ≤ (2)œÄnexpÓÄí ‚àí1 2( ) x ¬µ‚àíÓÄæŒ≤ x ¬µ (‚àí)ÓÄì .(3.24) WeoftenÔ¨Åxthecovariancematrixtobeadiagonalmatrix.Anevensimpler versionisthe i sot r o pi cGaussiandistribution,whosecovariancematrixisascalar timestheidentitymatrix. 3.9.4ExponentialandLaplaceDistributions Inthecontextofdeeplearning,weoftenwanttohaveaprobabilitydistribution withasharppointatx=0.Toaccomplishthis,wecanusethe e x p o nen t i al di st r i but i o n: pxŒªŒª (;) = 1 x ‚â• 0exp( )‚àíŒªx. (3.25) Theexponentialdistributionusestheindicatorfunction 1 x ‚â• 0toassignprobability zerotoallnegativevaluesof.x Acloselyrelatedprobabilitydistributionthatallowsustoplaceasharppeak ofprobabilitymassatanarbitrarypointisthe¬µ L apl ac e di st r i but i o n Laplace(;) =x¬µ,Œ≥1 2Œ≥expÓÄí ‚àí|‚àí|x¬µ Œ≥ÓÄì . (3.26) 3.9.5TheDiracDistributionandEmpiricalDistribution Insomecases,wewishtospecifythatallofthemassinaprobabilitydistribution clustersaroundasinglepoint.ThiscanbeaccomplishedbydeÔ¨ÅningaPDFusing theDiracdeltafunction,:Œ¥x() pxŒ¥x¬µ. () = (‚àí) (3.27) TheDiracdeltafunctionisdeÔ¨Ånedsuchthatitiszero-valuedeverywhereexcept 0,yetintegratesto1.TheDiracdeltafunctionisnotanordinaryfunctionthat associateseachvaluexwithareal-valuedoutput,insteaditisadiÔ¨Äerentkindof 65</div>
        </div>
    </div>

    <div class="question-card" id="q35">
        <div class="question-header">
            <span class="question-number">Question 35</span>
            <span class="question-type">multiple-choice</span>
        </div>

        <div class="question-text">Deep learning model training involves complex optimization challenges due to the high dimensionality and non-convex nature of neural network cost functions. Understanding the sources of slow convergence and generalization error is crucial for effective model development.

Which of the following statements most accurately describes the primary cause of slow optimization progress in high-dimensional neural network training, especially when the gradient norm remains substantial?

1) Most local minima have high cost and trap the optimizer early in training   
2) Overfitting due to excessive epochs prevents the optimizer from converging quickly   
3) Ill-conditioning of the Hessian forces smaller step sizes even with large gradients   
4) Minibatch sampling inherently introduces large bias in gradient estimation   
5) Saddle points always have a higher cost than global minima in neural networks   
6) Non-convexity guarantees that only poor minima are encountered   
7) Weight space symmetry eliminates the need for monitoring gradient norms</div>

        <div class="answer-section">
            <div class="answer-label">‚úì Correct Answer:</div>
            <div class="answer-text">The correct answer is 3) Ill-conditioning of the Hessian forces smaller step sizes even with large gradients.</div>
        </div>

        <div class="reference-section">
            <div class="reference-label">üìö Reference Text:</div>
            <button class="toggle-button" onclick="toggleReference(35)">
                Show/Hide Reference
            </button>
            <div id="ref35" class="reference-text hidden">a(x , y). Inthisscenario,examplesareneverrepeated;everyexperienceisafairsample from p da t a. Theequivalenceiseasiesttoderivewhenbothxand yarediscrete. Inthis case,thegeneralization error(equation)canbewrittenasasum 8.2 J‚àó() =Œ∏ÓÅò xÓÅò yp da t a()((;)) x , y L fxŒ∏ , y , (8.7) withtheexactgradient g= ‚àá Œ∏ J‚àó() =Œ∏ÓÅò xÓÅò yp da t a()x , y‚àá Œ∏ L f , y . ((;)xŒ∏)(8.8) Wehavealreadyseenthesamefactdemonstratedforthelog-likelihoodinequa- tionandequation;weobservenowthatthisholdsforotherfunctions 8.5 8.6 L besidesthelikelihood.Asimilarresultcanbederivedwhenxand yarecontinuous, undermildassumptionsregarding p da t aand. L Hence, wecanobtainanunbiasedestimatoroftheexactgradientof the generalization errorbysamplingaminibatchofexamples {x( 1 ), . . .x( ) m}withcor- respondingtargets y( ) ifromthedatageneratingdistribution p da t a,andcomputing thegradientofthelosswithrespecttotheparametersforthatminibatch: ÀÜg=1 m‚àá Œ∏ÓÅò iL f((x( ) i;)Œ∏ , y( ) i) . (8.9) Updatinginthedirectionof Œ∏ ÀÜgperformsSGDonthegeneralization error. Ofcourse, thisinterpretation only applies whenexamplesarenotreused. Nonetheless,itisusuallybesttomakeseveralpassesthroughthetrainingset, unlessthetrainingsetisextremelylarge. When multiplesuchepochsareused, onlytheÔ¨Årstepochfollowstheunbiasedgradientofthegeneralization error,but 2 8 1 CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS ofcourse,theadditionalepochsusuallyprovideenoughbeneÔ¨Åtduetodecreased trainingerrortooÔ¨Äsettheharmtheycausebyincreasingthegapbetweentraining errorandtesterror. Withsomedatasetsgrowingrapidlyinsize,fasterthancomputingpower,it isbecomingmorecommonformachinelearningapplicationstouseeachtraining exampleonlyonceoreventomakeanincompletepassthroughthetraining set.Whenusinganextremelylargetrainingset,overÔ¨Åttingisnotanissue,so underÔ¨Åttingandcomputational eÔ¨Éciencybecomethepredominant concerns.See also ()foradiscussionoftheeÔ¨Äectofcomputational BottouandBousquet2008 bottlenecksongeneralization error,asthenumberoftrainingexamplesgrows. 8.2ChallengesinNeuralNetworkOptimization Optimization ingeneralisanextremelydiÔ¨Éculttask.Traditionally,machine learninghasavoidedthediÔ¨Écultyofgeneraloptimization bycarefullydesigning theobjectivefunctionandconstraintstoensurethattheoptimization problemis convex.Whentrainingneuralnetworks,wemustconfrontthegeneralnon-convex case.Evenconvexoptimization isnotwithoutitscomplications. Inthissection, wesummarizeseveralofthemostprominentchallengesinvolvedinoptimization fortrainingdeepmodels. 8.2.1Ill-Conditioning Somechallengesariseevenwhenoptimizingconvexfunctions.Ofthese,themost prominentisill-conditioning oftheHessianmatrixH.Thisisaverygeneral probleminmostnumericaloptimization, convexorotherwise,andisdescribedin moredetailinsection.4.3.1 Theill-conditioning problemisgenerallybelievedtobepresentinneural networktrainingproblems.Ill-conditioningcanmanifestbycausingSGDtoget ‚Äústuck‚Äùinthesensethatevenverysmallstepsincreasethecostfunction. Recallfromequationthatasecond-orderTaylorseriesexpansionofthe 4.9 costfunctionpredictsthatagradientdescentstepofwilladd ‚àí ÓÄèg 1 2ÓÄè2gÓÄæHgg‚àí ÓÄèÓÄæg (8.10) tothecost.Ill-conditioningofthegradientbecomesaproblemwhen1 2ÓÄè2gÓÄæHg exceeds ÓÄègÓÄæg. Todeterminewhetherill-conditioning isdetrimentaltoaneural network training task, one canmonitorthe squaredgradientnormgÓÄægand 2 8 2 CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS ‚àí50050100150200250 Trainingtime(epochs)‚àí20246810121416Gradient norm 0 50100150200250 Trainingtime(epochs)01 .02 .03 .04 .05 .06 .07 .08 .09 .10 .ClassiÔ¨Åcationerrorrate Figure8.1:Gradientdescentoftendoesnotarriveatacriticalpointofanykind.Inthis example,thegradientnormincreasesthroughouttrainingofaconvolutionalnetworkused forobjectdetection. ( L e f t )Ascatterplotshowinghowthenormsofindividualgradient evaluationsaredistributedovertime.Toimprovelegibility,onlyonegradientnorm isplottedperepoch.Therunningaverageofallgradientnormsisplottedasasolid curve.Thegradientnormclearlyincreasesovertime,ratherthandecreasingaswewould expectifthetrainingprocessconvergedtoacriticalpoint.Despitetheincreasing ( R i g h t ) gradient,thetrainingprocessisreasonablysuccessful.ThevalidationsetclassiÔ¨Åcation errordecreasestoalowlevel. thegÓÄæHgterm.Inmanycases,thegradientnormdoesnotshrinksigniÔ¨Åcantly throughoutlearning,butthegÓÄæHgtermgrowsbymorethananorderofmagnitude. Theresultisthatlearningbecomesveryslowdespitethepresenceofastrong gradientbecausethelearningratemustbeshrunktocompensateforevenstronger curvature.FigureshowsanexampleofthegradientincreasingsigniÔ¨Åcantly 8.1 duringthesuccessfultrainingofaneuralnetwork. Thoughill-conditioning ispresentinothersettingsbesidesneuralnetwork training,someofthetechniquesusedtocombatitinothercontextsareless applicabletoneuralnetworks.Forexample,Newton‚Äôsmethodisanexcellenttool forminimizingconvexfunctionswithpoorlyconditionedHessianmatrices,butin thesubsequentsectionswewillarguethatNewton‚ÄôsmethodrequiressigniÔ¨Åcant modiÔ¨Åcationbeforeitcanbeappliedtoneuralnetworks. 8.2.2LocalMinima Oneofthemostprominentfeaturesofaconvexoptimization problemisthatit canbereducedtotheproblemofÔ¨Åndingalocalminimum.Anylocalminimumis 2 8 3 CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS guaranteedtobeaglobalminimum.SomeconvexfunctionshaveaÔ¨Çatregionat thebottomratherthanasingleglobalminimumpoint,butanypointwithinsuch aÔ¨Çatregionisanacceptablesolution.Whenoptimizingaconvexfunction,we knowthatwehavereachedagoodsolutionifweÔ¨Åndacriticalpointofanykind. Withnon-convexfunctions,suchasneuralnets,itispossibletohavemany localminima.Indeed,nearlyanydeepmodelisessentiallyguaranteedtohave anextremelylargenumberoflocalminima.However,aswewillsee,thisisnot necessarilyamajorproblem. Neuralnetworksandanymodelswithmultipleequivalentlyparametrized latent variablesallhavemultiplelocalminimabecauseofthemodelidentiÔ¨Åability problem.AmodelissaidtobeidentiÔ¨ÅableifasuÔ¨Écientlylargetrainingsetcan ruleoutallbutonesettingofthemodel‚Äôsparameters.Modelswithlatentvariables areoftennotidentiÔ¨Åablebecausewecanobtainequivalentmodelsbyexchanging latentvariableswitheachother.Forexample,wecouldtakeaneuralnetworkand modifylayer1byswappingtheincomingweightvectorforunit iwiththeincoming weightvectorforunit j,thendoingthesamefortheoutgoingweightvectors.Ifwe have mlayerswith nunitseach,thenthereare n!mwaysofarrangingthehidden units.Thiskindofnon-identiÔ¨Åabilit yisknownasweightspacesymmetry. Inadditiontoweightspacesymmetry,manykindsofneuralnetworkshave additionalcausesofnon-identiÔ¨Åabilit y.Forexample,inanyrectiÔ¨Åedlinearor maxoutnetwork,wecanscalealloftheincomingweightsandbiasesofaunitby Œ±ifwealsoscaleallofitsoutgoingweightsby1 Œ±.Thismeansthat‚Äîifthecost functiondoesnotincludetermssuchasweightdecaythatdependdirectlyonthe weightsratherthanthemodels‚Äôoutputs‚ÄîeverylocalminimumofarectiÔ¨Åedlinear ormaxoutnetworkliesonan( m n√ó)-dimensionalhyperbolaofequivalentlocal minima. ThesemodelidentiÔ¨Åabilityissuesmeanthattherecanbeanextremelylarge orevenuncountablyinÔ¨Åniteamountoflocalminimainaneuralnetworkcost function.However,alloftheselocalminimaarisingfromnon-identiÔ¨Åabilit yare equivalenttoeachotherincostfunctionvalue.Asaresult,theselocalminimaare notaproblematicformofnon-convexity. Localminimacanbeproblematiciftheyhavehighcostincomparisontothe globalminimum.Onecanconstructsmallneuralnetworks,evenwithouthidden units,thathavelocalminimawithhighercostthantheglobalminimum(Sontag andSussman1989Brady1989GoriandTesi1992 ,; etal.,; ,).Iflocalminima withhighcostarecommon,thiscouldposeaseriousproblemforgradient-based optimization algorithms. Itremainsanopenquestionwhethertherearemanylocalminimaofhighcost 2 8 4 CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS fornetworksofpracticalinterestandwhetheroptimization algorithmsencounter them.Formanyyears,mostpractitioners believedthatlocalminimawerea commonproblemplaguingneuralnetworkoptimization. Today,thatdoesnot appeartobethecase.Theproblemremainsanactiveareaofresearch,butexperts nowsuspectthat,forsuÔ¨Écientlylargeneuralnetworks,mostlocalminimahavea lowcostfunctionvalue,andthatitisnotimportanttoÔ¨Åndatrueglobalminimum ratherthantoÔ¨Åndapointinparameterspacethathaslowbutnotminimalcost (,; ,; ,; Saxeetal.2013Dauphinetal.2014Goodfellow etal.2015Choromanska etal.,).2014 Manypractitioners attributenearlyalldiÔ¨Écultywithneuralnetworkoptimiza- tiontolocalminima.Weencouragepractitioners tocarefullytestforspeciÔ¨Åc problems.Atestthatcanruleoutlocalminimaastheproblemistoplotthe normofthegradientovertime.Ifthenormofthegradientdoesnotshrinkto insigniÔ¨Åcantsize,theproblemisneitherlocalminimanoranyotherkindofcritical point.Thiskindofnegativetestcanruleoutlocalminima.Inhighdimensional spaces,itcanbeverydiÔ¨Éculttopositivelyestablishthatlocalminimaarethe problem.Manystructuresotherthanlocalminimaalsohavesmallgradients. 8.2.3Plateaus,SaddlePointsandOtherFlatRegions Formanyhigh-dimensionalnon-convexfunctions,localminima(andmaxima) areinfactrarecomparedtoanotherkindofpointwithzerogradient:asaddle point.Somepointsaroundasaddlepointhavegreatercostthanthesaddlepoint, whileothershavealowercost. Atasaddlepoint,theHessianmatrixhasboth positiveandnegativeeigenvalues.Pointslyingalongeigenvectorsassociatedwith positiveeigenvalueshavegreatercostthanthesaddlepoint,whilepointslying alongnegativeeigenvalueshavelowervalue.Wecanthinkofasaddlepointas beingalocalminimumalongonecross-sectionofthecostfunctionandalocal maximumalonganothercross-section.SeeÔ¨Ågureforanillustration. 4.5 Manyclasses ofrandomfunctionsexhibitthefollowingbehavior:inlow- dimensionalspaces,localminimaarecommon.Inhigherdimensionalspaces,local minimaarerareandsaddlepointsaremorecommon.Forafunction f: Rn‚Üí Rof thistype,theexpectedratioofthenumberofsaddlepointstolocalminimagrows exponentiallywith n.Tounderstandtheintuitionbehindthisbehavior,observe thattheHessianmatrixatalocalminimumhasonlypositiveeigenvalues. The Hessianmatrixatasaddlepointhasamixtureofpositiveandnegativeeigenvalues. ImaginethatthesignofeacheigenvalueisgeneratedbyÔ¨Çippingacoin.Inasingle dimension,itiseasytoobtainalocalminimumbytossingacoinandgettingheads once.In n-dimensionalspace,itisexponentiallyunlikelythatall ncointosseswill 2 8 5</div>
        </div>
    </div>

    <div class="question-card" id="q36">
        <div class="question-header">
            <span class="question-number">Question 36</span>
            <span class="question-type">multiple-choice</span>
        </div>

        <div class="question-text">Generative neural networks such as NADE and its extensions are used for modeling complex, high-dimensional data distributions, leveraging techniques like weight-sharing, mixture models, and flexible variable orderings. These models play an important role in density estimation and sample generation within unsupervised learning.

Which feature allows NADE-based models to perform flexible inference, enabling the prediction of any subset of variables given any observed subset, while also supporting the creation of ensembles over multiple orderings?

1) Incorporating contractive penalties in the loss function   
2) Randomizing the input variable order and providing masking information   
3) Increasing the number of mean-field inference steps (k)   
4) Using independent output weights for each variable   
5) Employing Gaussian mixture models for continuous data   
6) Restricting group connectivity in hidden layers   
7) Utilizing pseudo-gradients to stabilize training</div>

        <div class="answer-section">
            <div class="answer-label">‚úì Correct Answer:</div>
            <div class="answer-text">The correct answer is 2) Randomizing the input variable order and providing masking information.</div>
        </div>

        <div class="reference-section">
            <div class="reference-label">üìö Reference Text:</div>
            <button class="toggle-button" onclick="toggleReference(36)">
                Show/Hide Reference
            </button>
            <div id="ref36" class="reference-text hidden">W k, i . (20.83) Theremainingweights,where,arezero. j < i 7 0 8 CHAPTER20.DEEPGENERATIVEMODELS x 1 x 1 x 2 x 2 x 3 x 3 x 4 x 4h 1 h 1 h 2 h 2 h 3 h 3P x ( 4| x 1 , x 2 , x 3 ) P x ( 4| x 1 , x 2 , x 3 )P x ( 3| x 1 , x 2 ) P x ( 3| x 1 , x 2 ) P x ( 2| x 1 ) P x ( 2| x 1 )P x ( 1 ) P x ( 1 ) W : 1 , W : 1 , W : 1 , W : 2 , W : 2 , W : 3 , Figure20.10:Anillustrationoftheneuralautoregressivedensityestimator(NADE).The hiddenunitsareorganizedingroups h( ) jsothatonlytheinputs x 1 , . . . , x iparticipate incomputing h( ) iandpredicting P( x j| x j‚àí 1 , . . . , x 1),for j > i.NADEisdiÔ¨Äerentiated fromearlierneuralauto-regressivenetworksbytheuseofaparticularweightsharing pattern: WÓÄ∞ j , k , i= W k , iisshared(indicatedintheÔ¨Ågurebytheuseofthesamelinepattern foreveryinstanceofareplicatedweight)foralltheweightsgoingoutfrom x itothe k-th unitofanygroup.Recallthatthevector j i‚â• ( W 1 , i , W 2 , i , . . . , W n , i)isdenoted W : , i. LarochelleandMurray2011()chosethissharingschemesothatforward propagationinaNADEmodellooselyresemblesthecomputations performedin meanÔ¨ÅeldinferencetoÔ¨ÅllinmissinginputsinanRBM.ThismeanÔ¨Åeldinference correspondstorunningarecurrentnetworkwithsharedweightsandtheÔ¨Årststep ofthatinferenceisthesameasinNADE.TheonlydiÔ¨ÄerenceisthatwithNADE, theoutputweightsconnectingthehiddenunitstotheoutputareparametrized independentlyfromtheweightsconnectingtheinputunitstothehiddenunits.In theRBM,thehidden-to-output weightsarethetransposeoftheinput-to-hidden weights.TheNADEarchitecturecanbeextendedtomimicnotjustonetimestep ofthemeanÔ¨Åeldrecurrentinferencebuttomimic ksteps.Thisapproachiscalled NADE-(,). kRaiko e t a l .2014 Asmentionedpreviously,auto-regressiv enetworksmaybeextendtoprocess continuous-valueddata.Aparticularlypowerfulandgenericwayofparametrizing acontinuousdensityisasaGaussianmixture(introducedinsection)with3.9.6 mixtureweights Œ± i(thecoeÔ¨Écientorpriorprobabilityforcomponent i),per- componentconditionalmean ¬µ iandper-componentconditionalvariance œÉ2 i. A modelcalledRNADE(,)usesthisparametrization toextendNADE Uria e t a l .2013 torealvalues.Aswithothermixturedensitynetworks,theparametersofthis 7 0 9 CHAPTER20.DEEPGENERATIVEMODELS distributionareoutputsofthenetwork,withthemixtureweightprobabilities producedbyasoftmaxunit,andthevariancesparametrized sothattheyare positive. Stochasticgradientdescentcanbenumericallyill-behavedduetothe interactionsbetweentheconditionalmeans ¬µ iandtheconditionalvariances œÉ2 i. ToreducethisdiÔ¨Éculty,()useapseudo-gradientthatreplacesthe Uria e t a l .2013 gradientonthemean,intheback-propagationphase. Anotherveryinterestingextensionoftheneuralauto-regressiv earchitectures getsridoftheneedtochooseanarbitraryorderfortheobservedvariables(Murray andLarochelle2014,).Inauto-regressive networks,theideaistotrainthenetwork tobeabletocopewithanyorderbyrandomlysamplingordersandprovidingthe informationtohiddenunitsspecifyingwhichoftheinputsareobserved(onthe rightsideoftheconditioningbar)andwhicharetobepredictedandarethus consideredmissing(ontheleftsideoftheconditioningbar).Thisisnicebecause itallowsonetouseatrainedauto-regressiv enetworkto p e r f o r m a ny i nfe r e nc e p r o b l e m(i.e.predictorsamplefromtheprobabilitydistributionoveranysubset ofvariablesgivenanysubset)extremelyeÔ¨Éciently.Finally,sincemanyordersof variablesarepossible( n!for nvariables)andeachorder oofvariablesyieldsa diÔ¨Äerent,wecanformanensembleofmodelsformanyvaluesof: p o(x|) o p e nse m bl e() =x1 kkÓÅò i = 1p o(x|( ) i) . (20.84) Thisensemblemodelusuallygeneralizesbetterandassignshigherprobabilityto thetestsetthandoesanindividualmodeldeÔ¨Ånedbyasingleordering. Inthesamepaper,theauthorsproposedeepversionsofthearchitecture, but unfortunately thatimmediatelymakescomputationasexpensiveasintheoriginal neuralauto-regressiv eneuralnetwork( ,).TheÔ¨Årstlayer BengioandBengio2000b andtheoutputlayercanstillbecomputedin O( n h)multiply-addoperations, asintheregularNADE,where histhenumberofhiddenunits(thesizeofthe groups h i,inÔ¨Åguresand),whereasitis 20.1020.9 O( n2h)inBengioandBengio ().However,fortheotherhiddenlayers,thecomputationis 2000b O( n2h2)ifevery ‚Äúprevious‚Äùgroupatlayer lparticipatesinpredictingthe‚Äúnext‚Äùgroupatlayer l+1, assuming ngroupsof hhiddenunitsateachlayer.Makingthe i-thgroupatlayer l+1onlydependonthe i-thgroup,asinMurrayandLarochelle2014()atlayer l reducesitto O n h(2),whichisstilltimesworsethantheregularNADE. h 7 1 0 CHAPTER20.DEEPGENERATIVEMODELS 20.11DrawingSamplesfromAutoencoders Inchapter,wesawthatmanykindsofautoencoderslearnthedatadistribution. 14 Therearecloseconnectionsbetweenscorematching,denoisingautoencoders,and contractiveautoencoders.Theseconnectionsdemonstratethatsomekindsof autoencoderslearnthedatadistributioninsomeway.Wehavenotyetseenhow todrawsamplesfromsuchmodels. Somekindsofautoencoders,suchasthevariationalautoencoder,explicitly representaprobabilitydistributionandadmitstraightforwardancestralsampling. MostotherkindsofautoencodersrequireMCMCsampling. Contractiveautoencodersaredesignedtorecoveranestimateofthetangent planeofthedatamanifold.Thismeansthatrepeatedencodinganddecodingwith injectednoisewillinducearandomwalkalongthesurfaceofthemanifold(Rifai e t a l . e t a l . ,;2012Mesnil,).ThismanifolddiÔ¨Äusiontechniqueisakindof 2012 Markovchain. ThereisalsoamoregeneralMarkovchainthatcansamplefromanydenoising autoencoder. 20.11.1MarkovChainAssociatedwithanyDenoisingAutoen- coder Theabovediscussionleftopenthequestionofwhatnoisetoinjectandwhere, inordertoobtainaMarkovchainthatwouldgeneratefromthedistribution estimatedbytheautoencoder. ()showedhowtoconstruct Bengio e t a l .2013c suchaMarkovchainforgeneralizeddenoisingautoencoders.Generalized denoisingautoencodersarespeciÔ¨Åedbyadenoisingdistributionforsamplingan estimateofthecleaninputgiventhecorruptedinput. EachstepoftheMarkovchainthatgeneratesfromtheestimateddistribution consistsofthefollowingsub-steps,illustratedinÔ¨Ågure:20.11 1.Startingfromthepreviousstate x,injectcorruptionnoise,sampling Àú xfrom C(Àú x x|). 2. EncodeÀú xinto h= ( fÀú x). 3. Decodetoobtaintheparameters of h œâ h= ( g) p g p ( = x | œâ ()) = h (x|Àú x). 4. Samplethenextstatefrom x p g p ( = x| œâ ()) = h (x|Àú x). 7 1 1 CHAPTER20.DEEPGENERATIVEMODELS xxÀú xÀú xh h œâœâ ÀÜ x ÀÜ xC ( Àú x x| ) p ( ) x| œâfg Figure20.11:EachstepoftheMarkovchainassociatedwithatraineddenoisingautoen- coder,thatgeneratesthesamplesfromtheprobabilisticmodelimplicitlytrainedbythe denoisinglog-likelihoodcriterion.Eachstepconsistsin(a)injectingnoiseviacorruption process Cinstate x,yieldingÀú x,(b)encodingitwithfunction</div>
        </div>
    </div>

    <div class="question-card" id="q37">
        <div class="question-header">
            <span class="question-number">Question 37</span>
            <span class="question-type">multiple-choice</span>
        </div>

        <div class="question-text">Probability theory and linear algebra are foundational in machine learning, underpinning key methods for modeling uncertainty and processing data. Understanding principal components, types of probability, and random variables is essential for interpreting and building modern AI systems.

Which of the following best describes how principal components are determined in Principal Components Analysis (PCA)?

1) They are the rows of the data matrix that have the most distinct values.   
2) They are the columns of the covariance matrix with the smallest variances.   
3) They are the diagonal elements of the data covariance matrix with the largest values.   
4) They are the orthogonal vectors obtained by decomposing the data into independent features using clustering techniques.   
5) They are the eigenvectors of the data covariance matrix corresponding to the largest eigenvalues.   
6) They are the random variables with the highest probability density under a given probability distribution.   
7) They are the conditional probabilities calculated for each feature given all others.</div>

        <div class="answer-section">
            <div class="answer-label">‚úì Correct Answer:</div>
            <div class="answer-text">The correct answer is 5) They are the eigenvectors of the data covariance matrix corresponding to the largest eigenvalues..</div>
        </div>

        <div class="reference-section">
            <div class="reference-label">üìö Reference Text:</div>
            <button class="toggle-button" onclick="toggleReference(37)">
                Show/Hide Reference
            </button>
            <div id="ref37" class="reference-text hidden">eigenvalue. ThisderivationisspeciÔ¨Åctothecaseof l=1andrecoversonlytheÔ¨Årst principalcomponent.Moregenerally,whenwewishtorecoverabasisofprincipal components,thematrixDisgivenbythe leigenvectorscorrespondingtothe largesteigenvalues.Thismaybeshownusingproofbyinduction.Werecommend writingthisproofasanexercise. Linearalgebraisoneofthefundamentalmathematical disciplinesthatis necessarytounderstanddeeplearning.Anotherkeyareaofmathematics thatis ubiquitousinmachinelearningisprobabilitytheory,presentednext. 5 2 C h a p t e r 3 ProbabilityandInformation Theory Inthischapter,wedescribeprobabilitytheoryandinformationtheory. Probabilitytheoryisamathematical frameworkforrepresentinguncertain statements.Itprovidesameansofquantifyinguncertaintyandaxiomsforderiving newuncertainstatements.InartiÔ¨Åcialintelligenceapplications,weuseprobability theoryintwomajorways.First,thelawsofprobabilitytellushowAIsystems shouldreason,sowedesignouralgorithmstocomputeorapproximate various expressionsderivedusingprobabilitytheory.Second,wecanuseprobabilityand statisticstotheoreticallyanalyzethebehaviorofproposedAIsystems. Probabilitytheoryisafundamentaltoolofmanydisciplinesofscienceand engineering.Weprovidethischaptertoensurethatreaderswhosebackgroundis primarilyinsoftwareengineeringwithlimitedexposuretoprobabilitytheorycan understandthematerialinthisbook. Whileprobabilitytheoryallowsustomakeuncertainstatementsandreasonin thepresenceofuncertainty,informationtheoryallowsustoquantifytheamount ofuncertaintyinaprobabilitydistribution. Ifyouarealreadyfamiliarwithprobabilitytheoryandinformationtheory,you maywishtoskipallofthischapterexceptforsection,whichdescribesthe 3.14 graphsweusetodescribestructuredprobabilisticmodelsformachinelearning.If youhaveabsolutelynopriorexperiencewiththesesubjects,thischaptershould besuÔ¨Écienttosuccessfullycarryoutdeeplearningresearchprojects,butwedo suggestthatyouconsultanadditionalresource,suchasJaynes2003(). 53 CHAPTER3.PROBABILITYANDINFORMATIONTHEORY 3.1WhyProbability? Manybranchesofcomputersciencedealmostlywithentitiesthatareentirely deterministicandcertain.AprogrammercanusuallysafelyassumethataCPUwill executeeachmachineinstructionÔ¨Çawlessly.Errorsinhardwaredooccur,butare rareenoughthatmostsoftwareapplicationsdonotneedtobedesignedtoaccount forthem.Giventhatmanycomputerscientistsandsoftwareengineersworkina relativelycleanandcertainenvironment,itcanbesurprisingthatmachinelearning makesheavyuseofprobabilitytheory. Thisisbecausemachinelearningmustalwaysdealwithuncertainquantities, andsometimesmayalsoneedtodealwithstochastic(non-determinis tic)quantities. Uncertaintyandstochasticitycanarisefrommanysources.Researchershavemade compellingargumentsforquantifyinguncertaintyusingprobabilitysinceatleast the1980s.Manyoftheargumentspresentedherearesummarizedfromorinspired byPearl1988(). Nearlyallactivitiesrequiresomeabilitytoreasoninthepresenceofuncertainty. Infact,beyondmathematical statementsthataretruebydeÔ¨Ånition,itisdiÔ¨Écult tothinkofanypropositionthatisabsolutelytrueoranyeventthatisabsolutely guaranteedtooccur. Therearethreepossiblesourcesofuncertainty: 1.Inherentstochasticityinthesystembeingmodeled.Forexample,most interpretationsofquantummechanicsdescribethedynamicsofsubatomic particlesasbeingprobabilistic.Wecanalsocreatetheoreticalscenariosthat wepostulatetohaverandomdynamics,suchasahypothetical cardgame whereweassumethatthecardsaretrulyshuÔ¨Ñedintoarandomorder. 2.Incompleteobservability.Evendeterministicsystemscanappearstochastic whenwecannotobserveallofthevariablesthatdrivethebehaviorofthe system.Forexample,intheMontyHallproblem,agameshowcontestantis askedtochoosebetweenthreedoorsandwinsaprizeheldbehindthechosen door.Twodoorsleadtoagoatwhileathirdleadstoacar. Theoutcome giventhecontestant‚Äôschoiceisdeterministic,butfromthecontestant‚Äôspoint ofview,theoutcomeisuncertain. 3.Incompletemodeling.Whenweuseamodelthatmustdiscardsomeof the information wehave observed, the discarded i nformationresults in uncertaintyinthemodel‚Äôspredictions. Forexample,supposewebuilda robotthatcanexactlyobservethelocationofeveryobjectaroundit.Ifthe 54 CHAPTER3.PROBABILITYANDINFORMATIONTHEORY robotdiscretizesspacewhenpredictingthefuturelocationoftheseobjects, thenthediscretizationmakestherobotimmediatelybecomeuncertainabout theprecisepositionofobjects: eachobjectcouldbeanywherewithinthe discretecellthatitwasobservedtooccupy. Inmanycases,itismorepracticaltouseasimplebutuncertainrulerather thanacomplexbutcertainone,evenifthetrueruleisdeterministicandour modelingsystemhastheÔ¨Ådelitytoaccommodateacomplexrule.Forexample,the simplerule‚ÄúMostbirdsÔ¨Çy‚Äùischeaptodevelopandisbroadlyuseful,whilearule oftheform,‚ÄúBirdsÔ¨Çy,exceptforveryyoungbirdsthathavenotyetlearnedto Ô¨Çy,sickorinjuredbirdsthathavelosttheabilitytoÔ¨Çy,Ô¨Çightlessspeciesofbirds includingthecassowary,ostrichandkiwi...‚Äù isexpensivetodevelop,maintainand communicate,andafterallofthiseÔ¨Äortisstillverybrittleandpronetofailure. Whileitshouldbeclearthatweneedameansofrepresentingandreasoning aboutuncertainty,itisnotimmediatelyobviousthatprobabilitytheorycanprovide allofthetoolswewantforartiÔ¨Åcialintelligenceapplications.Probabilitytheory wasoriginallydevelopedtoanalyzethefrequenciesofevents.Itiseasytosee howprobabilitytheorycanbeusedtostudyeventslikedrawingacertainhandof cardsinagameofpoker.Thesekindsofeventsareoftenrepeatable. Whenwe saythatanoutcomehasaprobabilitypofoccurring,itmeansthatifwerepeated theexperiment(e.g.,drawahandofcards)inÔ¨Ånitelymanytimes,thenproportion poftherepetitionswouldresultinthatoutcome.Thiskindofreasoningdoesnot seemimmediatelyapplicabletopropositionsthatarenotrepeatable.Ifadoctor analyzesapatientandsaysthatthepatienthasa40%chanceofhavingtheÔ¨Çu, thismeanssomethingverydiÔ¨Äerent‚ÄîwecannotmakeinÔ¨Ånitelymanyreplicasof thepatient,noristhereanyreasontobelievethatdiÔ¨Äerentreplicasofthepatient wouldpresentwiththesamesymptomsyethavevaryingunderlyingconditions.In thecaseofthedoctordiagnosingthepatient,weuseprobabilitytorepresenta degr e e o f b e l i e f,with1indicatingabsolutecertaintythatthepatienthastheÔ¨Çu and0indicatingabsolutecertaintythatthepatientdoesnothavetheÔ¨Çu. The formerkindofprobability,relateddirectlytotheratesatwhicheventsoccur,is knownas f r e q uen t i st pr o babili t y,whilethelatter,relatedtoqualitativelevels ofcertainty,isknownas B ay e si an pr o babili t y. Ifwelistseveralpropertiesthatweexpectcommonsensereasoningabout uncertaintytohave,thentheonlywaytosatisfythosepropertiesistotreat Bayesianprobabilities asbehavingexactlythesameasfrequentistprobabilities. Forexample,ifwewanttocomputetheprobabilitythataplayerwillwinapoker gamegiventhatshehasacertainsetofcards,weuseexactlythesameformulas aswhenwecomputetheprobabilitythatapatienthasadiseasegiventhatshe 55 CHAPTER3.PROBABILITYANDINFORMATIONTHEORY hascertainsymptoms.Formoredetailsaboutwhyasmallsetofcommonsense assumptionsimpliesthatthesameaxiomsmustcontrolbothkindsofprobability, see(). Ramsey1926 Probabilitycanbeseenastheextensionoflogictodealwithuncertainty.Logic providesasetofformalrulesfordeterminingwhatpropositionsareimpliedto betrueorfalsegiventheassumptionthatsomeothersetofpropositionsistrue orfalse.Probabilitytheoryprovidesasetofformalrulesfordeterminingthe likelihoodofapropositionbeingtruegiventhelikelihoodofotherpropositions. 3.2RandomVariables A r andom v ar i abl eisavariablethatcantakeondiÔ¨Äerentvaluesrandomly.We typicallydenotetherandomvariableitselfwithalowercaseletterinplaintypeface, andthevaluesitcantakeonwithlowercasescriptletters.Forexample,x 1andx 2 arebothpossiblevaluesthattherandomvariablexcantakeon.Forvector-valued variables,wewouldwritetherandomvariableas xandoneofitsvaluesas x.On itsown,arandomvariableisjustadescriptionofthestatesthatarepossible;it mustbecoupledwithaprobabilitydistributionthatspeciÔ¨Åeshowlikelyeachof thesestatesare. Randomvariablesmaybediscreteorcontinuous.Adiscreterandomvariable isonethathasaÔ¨ÅniteorcountablyinÔ¨Ånitenumberofstates.Notethatthese statesarenotnecessarilytheintegers;theycanalsojustbenamedstatesthat arenotconsideredtohaveanynumericalvalue.Acontinuousrandomvariableis associatedwitharealvalue. 3.3ProbabilityDistributions A pr o babili t y di st r i but i o nisadescriptionofhowlikelyarandomvariableor setofrandomvariablesistotakeoneachofitspossiblestates.Thewaywe describeprobabilitydistributionsdependsonwhetherthevariablesarediscreteor continuous. 3.3.1DiscreteVariablesandProbabilityMassFunctions Aprobabilitydistributionoverdiscretevariablesmaybedescribedusinga pr o ba- bi l i t y m ass f unc t i o n(PMF).Wetypicallydenoteprobabilitymassfunctionswith acapitalP.OftenweassociateeachrandomvariablewithadiÔ¨Äerentprobability 56 CHAPTER3.PROBABILITYANDINFORMATIONTHEORY massfunctionandthereadermustinferwhichprobabilitymassfunctiontouse basedontheidentityoftherandomvariable,ratherthanthenameofthefunction; P P ()xisusuallynotthesameas()y. Theprobabilitymassfunctionmapsfromastateofarandomvariableto theprobabilityofthatrandomvariabletakingonthatstate.Theprobability thatx=xisdenotedasP(x),withaprobabilityof1indicatingthatx=xis certainandaprobabilityof0indicatingthatx=xisimpossible.Sometimes todisambiguatewhichPMFtouse,wewritethenameoftherandomvariable explicitly:P(x=x).SometimeswedeÔ¨ÅneavariableÔ¨Årst,thenuse‚àºnotationto specifywhichdistributionitfollowslater:xx. ‚àºP() Probabilitymassfunctionscanactonmanyvariablesatthesametime.Such aprobabilitydistributionovermanyvariablesisknownasa j o i n t pr o babili t y di st r i but i o n.P(x=x,y=y)denotestheprobabilitythatx=xandy=y simultaneously.Wemayalsowrite forbrevity. Px,y() Tobeaprobabilitymassfunctiononarandomvariablex,afunctionPmust satisfythefollowingproperties: ‚Ä¢Thedomainofmustbethesetofallpossiblestatesofx. P ‚Ä¢‚àÄ‚ààxx,0‚â§P(x)‚â§1.Animpossibleeventhasprobabilityandnostatecan 0 belessprobablethanthat.Likewise,aneventthatisguaranteedtohappen hasprobability,andnostatecanhaveagreaterchanceofoccurring. 1 ‚Ä¢ÓÅê x ‚àà xP(x) = 1.Werefertothispropertyasbeing nor m al i z e d.Without thisproperty,wecouldobtainprobabilities greaterthanonebycomputing theprobabilityofoneofmanyeventsoccurring. Forexample,considerasinglediscreterandomvariablexwithkdiÔ¨Äerent states.Wecanplacea uni f o r m di st r i but i o nonx‚Äîthatis,makeeachofits statesequallylikely‚Äîbysettingitsprobabilitymassfunctionto Px (= x i) =1 k(3.1) foralli.WecanseethatthisÔ¨Åtstherequirementsforaprobabilitymassfunction. Thevalue1 kispositivebecauseisapositiveinteger.Wealsoseethat k ÓÅò iPx (= x i) =ÓÅò i1 k=k k= 1, (3.2) sothedistributionisproperlynormalized. 57 CHAPTER3.PROBABILITYANDINFORMATIONTHEORY 3.3.2ContinuousVariablesandProbabilityDensityFunctions Whenworkingwithcontinuousrandomvariables,wedescribeprobabilitydistri- butionsusinga pr o babili t y densit y f unc t i o n ( P D F)ratherthanaprobability massfunction.Tobeaprobabilitydensityfunction,afunctionpmustsatisfythe followingproperties: ‚Ä¢Thedomainofmustbethesetofallpossiblestatesofx. p ‚Ä¢‚àÄ‚àà ‚â• ‚â§ xx,px() 0 () . p Notethatwedonotrequirex 1. ‚Ä¢ÓÅí pxdx()= 1. Aprobabilitydensityfunctionp(x)doesnotgivetheprobabilityofaspeciÔ¨Åc statedirectly,insteadtheprobabilityoflandinginsideaninÔ¨Ånitesimalregionwith volumeisgivenby. Œ¥x pxŒ¥x() WecanintegratethedensityfunctiontoÔ¨Åndtheactualprobabilitymassofa setofpoints.SpeciÔ¨Åcally,theprobabilitythatxliesinsomeset Sisgivenbythe integralofp(x)overthatset.Intheunivariateexample,theprobabilitythatx liesintheintervalisgivenby []a,bÓÅí [ ] a , bpxdx(). ForanexampleofaprobabilitydensityfunctioncorrespondingtoaspeciÔ¨Åc probabilitydensityoveracontinuousrandomvariable,considerauniformdistribu- tiononanintervaloftherealnumbers.Wecandothiswithafunctionu(x;a,b), whereaandbaretheendpointsoftheinterval,withb>a.The‚Äú;‚Äùnotationmeans ‚Äúparametrized by‚Äù;weconsiderxtobetheargumentofthefunction,whileaand bareparametersthatdeÔ¨Ånethefunction.Toensurethatthereisnoprobability massoutsidetheinterval,wesayu(x;a,b)=0forallxÓÄ∂‚àà[a,b] [.Withina,b], uxa,b (;) =1 b a ‚àí.Wecanseethatthisisnonnegativeeverywhere.Additionally,it integratesto1.Weoftendenotethatxfollowstheuniformdistributionon[a,b] bywritingx. ‚àºUa,b() 3.4MarginalProbability Sometimesweknowtheprobabilitydistributionoverasetofvariablesandwewant toknowtheprobabilitydistributionoverjustasubsetofthem.Theprobability distributionoverthesubsetisknownasthe distribution. m ar g i nal pr o babili t y Forexample,supposewehavediscreterandomvariablesxandy,andweknow P,(xy.WecanÔ¨Åndxwiththe : ) P() sum r ul e ‚àÄ‚ààxxx,P(= ) =xÓÅò yPx,y. (= xy= ) (3.3) 58 CHAPTER3.PROBABILITYANDINFORMATIONTHEORY Thename‚Äúmarginalprobability‚Äùcomesfromtheprocessofcomputingmarginal probabilities onpaper.WhenthevaluesofP(xy,)arewritteninagridwith diÔ¨ÄerentvaluesofxinrowsanddiÔ¨Äerentvaluesofyincolumns,itisnaturalto sumacrossarowofthegrid,thenwriteP(x)inthemarginofthepaperjustto therightoftherow. Forcontinuousvariables,weneedtouseintegrationinsteadofsummation: px() =ÓÅö px,ydy. () (3.4) 3.5ConditionalProbability Inmanycases,weareinterestedintheprobabilityofsomeevent,giventhatsome othereventhashappened.Thisiscalleda c o ndi t i o n a l pr o babili t y.Wedenote theconditionalprobabilitythaty=ygivenx=xasP(y=y|x=x).This conditionalprobabilitycanbecomputedwiththeformula Pyx (= y |x= ) =Py,x (= yx= ) Px (=</div>
        </div>
    </div>

    <div class="question-card" id="q38">
        <div class="question-header">
            <span class="question-number">Question 38</span>
            <span class="question-type">multiple-choice</span>
        </div>

        <div class="question-text">Deep learning models in computer vision often rely on various normalization techniques during preprocessing to enhance model performance and robustness. These techniques are essential for managing variation in image data and improving feature extraction.

Which of the following statements best characterizes local contrast normalization (LCN) in deep learning for image processing?

1) LCN normalizes pixel values within local neighborhoods, often using convolution operations, to enhance edges and local details in images.   
2) LCN standardizes the global pixel intensity distribution by subtracting the mean and scaling by the standard deviation across the entire image.   
3) LCN is typically non-differentiable and unsuitable for integration into deep neural networks.   
4) LCN operates only on grayscale images and cannot be adapted for color channels.   
5) LCN mainly reduces generalization error through data augmentation techniques.   
6) LCN decorrelates features across the dataset using principal component analysis (PCA).   
7) LCN is primarily used to map images into a consistent overall brightness and contrast scale for batch processing.</div>

        <div class="answer-section">
            <div class="answer-label">‚úì Correct Answer:</div>
            <div class="answer-text">The correct answer is 1) LCN normalizes pixel values within local neighborhoods, often using convolution operations, to enhance edges and local details in images..</div>
        </div>

        <div class="reference-section">
            <div class="reference-label">üìö Reference Text:</div>
            <button class="toggle-button" onclick="toggleReference(38)">
                Show/Hide Reference
            </button>
            <div id="ref38" class="reference-text hidden">t a l .,; e t a l .,)suggests thatbetween8and16bitsofprecisioncansuÔ¨Éceforusingortrainingdeep neuralnetworkswithback-propagation. Whatisclearisthatmoreprecisionis requiredduringtrainingthanatinferencetime,andthatsomeformsofdynamic Ô¨Åxedpointrepresentationofnumberscanbeusedtoreducehowmanybitsare requiredpernumber.TraditionalÔ¨ÅxedpointnumbersarerestrictedtoaÔ¨Åxed range(whichcorrespondstoagivenexponentinaÔ¨Çoatingpointrepresentation). DynamicÔ¨Åxedpointrepresentationssharethatrangeamongasetofnumbers (suchasalltheweightsinonelayer).UsingÔ¨ÅxedpointratherthanÔ¨Çoatingpoint representationsandusinglessbitspernumberreducesthehardwaresurfacearea, powerrequirementsandcomputingtimeneededforperformingmultiplications, andmultiplications arethemostdemandingoftheoperationsneededtouseor trainamoderndeepnetworkwithbackprop. 12. 2 C om p u t er V i s i on Computervisionhastraditionallybeenoneofthemostactiveresearchareasfor deeplearningapplications,becausevisionisataskthatiseÔ¨Äortlessforhumans andmanyanimalsbutchallengingforcomputers( ,).Manyof Ballard e t a l .1983 themostpopularstandardbenchmarktasksfordeeplearningalgorithmsareforms ofobjectrecognitionoropticalcharacterrecognition. ComputervisionisaverybroadÔ¨Åeldencompassingawidevarietyofways ofprocessingimages,andanamazingdiversityofapplications. Applicationsof computervisionrangefromreproducinghumanvisualabilities,suchasrecognizing faces,tocreatingentirelynewcategoriesofvisualabilities.Asanexampleof thelattercategory,onerecentcomputervisionapplicationistorecognizesound wavesfromthevibrationstheyinduceinobjectsvisibleinavideo(,Davis e t a l . 2014).Mostdeeplearningresearchoncomputervisionhasnotfocusedonsuch 4 5 2 CHAPTER12.APPLICATIONS exoticapplicationsthatexpandtherealmofwhatispossiblewithimagerybut ratherasmallcoreofAIgoalsaimedatreplicatinghumanabilities.Mostdeep learningforcomputervisionisusedforobjectrecognitionordetectionofsome form,whetherthismeansreportingwhichobjectispresentinanimage,annotating animagewithboundingboxesaroundeachobject,transcribingasequenceof symbolsfromanimage,orlabelingeachpixelinanimagewiththeidentityofthe objectitbelongsto.Becausegenerativemodelinghasbeenaguidingprinciple ofdeeplearningresearch,thereisalsoalargebodyofworkonimagesynthesis usingdeepmodels.Whileimagesynthesisisusuallynotconsidereda e x nihil o computervisionendeavor,modelscapableofimagesynthesisareusuallyusefulfor imagerestoration,acomputervisiontaskinvolvingrepairingdefectsinimagesor removingobjectsfromimages. 12.2.1Preprocessing Manyapplicationareasrequiresophisticatedpreprocessingbecausetheoriginal inputcomesinaformthatisdiÔ¨Écultformanydeeplearningarchitecturesto represent.Computervisionusuallyrequiresrelativelylittleofthiskindofpre- processing.Theimagesshouldbestandardizedsothattheirpixelsalllieinthe same,reasonablerange,like[0,1]or[-1,1]. Mixingimagesthatliein[0,1]with imagesthatliein[0,255]willusuallyresultinfailure.Formattingimagestohave thesamescaleistheonlykindofpreprocessingthatisstrictlynecessary.Many computervisionarchitectures requireimagesofastandardsize,soimagesmustbe croppedorscaledtoÔ¨Åtthatsize.Eventhisrescalingisnotalwaysstrictlynecessary. Someconvolutionalmodelsacceptvariably-sizedinputsanddynamicallyadjust thesizeoftheirpoolingregionstokeeptheoutputsizeconstant(Waibel e t a l ., 1989).Otherconvolutionalmodelshavevariable-sizedoutputthatautomatically scalesinsizewiththeinput,suchasmodelsthatdenoiseorlabeleachpixelinan image( ,). Hadsell e t a l .2007 Datasetaugmentation maybeseenasawayofpreprocessingthetrainingset only.Datasetaugmentationisanexcellentwaytoreducethegeneralization error ofmostcomputervisionmodels.Arelatedideaapplicableattesttimeistoshow themodelmanydiÔ¨Äerentversionsofthesameinput(forexample,thesameimage croppedatslightlydiÔ¨Äerentlocations)andhavethediÔ¨Äerentinstantiationsofthe modelvotetodeterminetheoutput.Thislatterideacanbeinterpretedasan ensembleapproach,andhelpstoreducegeneralization error. Otherkindsofpreprocessingareappliedtoboththetrainandthetestsetwith thegoalofputtingeachexampleintoamorecanonicalforminordertoreducethe amountofvariationthatthemodelneedstoaccountfor.Reducingtheamountof 4 5 3 CHAPTER12.APPLICATIONS variationinthedatacanbothreducegeneralization errorandreducethesizeof themodelneededtoÔ¨Åtthetrainingset.Simplertasksmaybesolvedbysmaller models,andsimplersolutionsaremorelikelytogeneralizewell.Preprocessing ofthiskindisusuallydesignedtoremovesomekindofvariabilityintheinput datathatiseasyforahumandesignertodescribeandthatthehumandesigner isconÔ¨Ådenthasnorelevancetothetask.Whentrainingwithlargedatasetsand largemodels,thiskindofpreprocessingisoftenunnecessary,anditisbesttojust letthemodellearnwhichkindsofvariabilityitshouldbecomeinvariantto.For example,theAlexNetsystemforclassifyingImageNetonlyhasonepreprocessing step:subtractingthemeanacrosstrainingexamplesofeachpixel(Krizhevsky e t a l .,).2012 12.2.1.1ContrastNormalization Oneofthemostobvioussourcesofvariationthatcanbesafelyremoved for manytasksistheamountofcontrastintheimage.Contrastsimplyreferstothe magnitudeofthediÔ¨Äerencebetweenthebrightandthedarkpixelsinanimage. Therearemanywaysofquantifyingthecontrastofanimage.Inthecontextof deeplearning,contrastusuallyreferstothestandarddeviationofthepixelsinan imageorregionofanimage.Supposewehaveanimagerepresentedbyatensor X‚àà Rr c√ó√ó3,with X i , j ,1beingtheredintensityatrow iandcolumn j, X i , j ,2giving thegreenintensityand X i , j ,3givingtheblueintensity.Thenthecontrastofthe entireimageisgivenby ÓÅ∂ÓÅµÓÅµÓÅ¥1 3 r cr ÓÅò i=1c ÓÅò j=13 ÓÅò k=1ÓÄÄ X i , j , k‚àí¬Ø XÓÄÅ2(12.1) where ¬Ø Xisthemeanintensityoftheentireimage: ¬Ø X=1 3 r cr ÓÅò i=1c ÓÅò j=13 ÓÅò k=1X i , j , k . (12.2) Globalcontrastnormalization(GCN)aimstopreventimagesfromhaving varyingamountsofcontrastbysubtractingthemeanfromeachimage, then rescalingitsothatthe standarddeviation across its pixelsis equaltosome constant s.Thisapproachiscomplicatedbythefactthatnoscalingfactorcan changethecontrastofazero-contrastimage(onewhosepixelsallhaveequal intensity).Imageswithverylowbutnon-zerocontrastoftenhavelittleinformation content.Dividingbythetruestandarddeviationusuallyaccomplishesnothing 4 5 4 CHAPTER12.APPLICATIONS morethanamplifyingsensornoiseorcompressionartifactsinsuchcases.This motivatesintroducingasmall,positiveregularizationparameter Œªtobiasthe estimateofthestandarddeviation.Alternately,onecanconstrainthedenominator tobeatleast ÓÄè.Givenaninputimage X,GCNproducesanoutputimage XÓÄ∞, deÔ¨Ånedsuchthat XÓÄ∞ i , j , k= sX i , j , k‚àí¬Ø X maxÓÄö ÓÄè ,ÓÅ± Œª+1 3 r cÓÅêr i=1ÓÅêc j=1ÓÅê3 k=1ÓÄÄ X i , j , k‚àí¬Ø XÓÄÅ2ÓÄõ .(12.3) Datasetsconsistingoflargeimagescroppedtointerestingobjectsareunlikely tocontainanyimageswithnearlyconstantintensity.Inthesecases,itissafe topracticallyignorethesmalldenominator problembysetting Œª= 0andavoid divisionby0inextremelyrarecasesbysetting ÓÄètoanextremelylowvaluelike 10‚àí8. Thisistheapproachusedby ()ontheCIFAR-10 Goodfellow e t a l .2013a dataset.Smallimagescroppedrandomlyaremorelikelytohavenearlyconstant intensity,makingaggressiveregularizationmoreuseful. ()used Coates e t a l .2011 ÓÄè Œª = 0and = 10onsmall,randomlyselectedpatchesdrawnfromCIFAR-10. Thescaleparameter scanusuallybesetto,asdoneby (), 1 Coates e t a l .2011 orchosentomakeeachindividualpixelhavestandarddeviationacrossexamples closeto1,asdoneby (). Goodfellow e t a l .2013a Thestandarddeviationinequationisjustarescalingofthe 12.3 L2norm oftheimage(assumingthemeanoftheimagehasalreadybeenremoved).Itis preferabletodeÔ¨ÅneGCNintermsofstandarddeviationratherthan L2norm becausethestandarddeviationincludesdivisionbythenumberofpixels,soGCN basedonstandarddeviationallowsthesame stobeusedregardlessofimage size.However,theobservationthatthe L2normisproportionaltothestandard deviationcanhelpbuildausefulintuition.OnecanunderstandGCNasmapping examplestoasphericalshell.SeeÔ¨Ågureforanillustration.Thiscanbea 12.1 usefulpropertybecauseneuralnetworksareoftenbetteratrespondingtodirections inspaceratherthanexactlocations.Respondingtomultipledistancesinthe samedirectionrequireshiddenunitswithcollinearweightvectorsbutdiÔ¨Äerent biases.SuchcoordinationcanbediÔ¨Écultforthelearningalgorithmtodiscover. Additionally,manyshallowgraphicalmodelshaveproblemswithrepresenting multipleseparatedmodesalongthesameline.GCNavoidstheseproblemsby reducingeachexampletoadirectionratherthanadirectionandadistance. Counterintuitively,thereisapreprocessingoperationknownasspheringand itisnotthesameoperationasGCN.Spheringdoesnotrefertomakingthedata lieonasphericalshell,butrathertorescalingtheprincipalcomponentstohave 4 5 5 CHAPTER12.APPLICATIONS ‚àí 1 5 0 0 1 5 . . . x 0‚àí 1 5 .0 0 .1 5 .x 1Rawinput ‚àí 1 5 0 0 1 5 . . . x 0GCN, = 10 Œª‚àí 2 ‚àí 1 5 0 0 1 5 . . . x 0GCN, = 0 Œª Figure12.1:GCNmapsexamplesontoasphere. ( L e f t )Rawinputdatamayhaveanynorm. ( C e n t e r )GCNwith Œª= 0mapsallnon-zeroexamplesperfectlyontoasphere.Hereweuse s= 1and ÓÄè= 10‚àí 8.BecauseweuseGCNbasedonnormalizingthestandarddeviation ratherthanthe L2norm,theresultingsphereisnottheunitsphere. ( R i g h t )Regularized GCN,with Œª >0,drawsexamplestowardthespherebutdoesnotcompletelydiscardthe variationintheirnorm.Weleaveandthesameasbefore. s ÓÄè equalvariance,sothatthemultivariatenormaldistributionusedbyPCAhas sphericalcontours.Spheringismorecommonlyknownas .whitening Globalcontrastnormalization willoftenfailtohighlightimagefeatureswe wouldliketostandout,suchasedgesandcorners.Ifwehaveascenewithalarge darkareaandalargebrightarea(suchasacitysquarewithhalftheimagein theshadowofabuilding)thenglobalcontrastnormalization willensurethereisa largediÔ¨Äerencebetweenthebrightnessofthedarkareaandthebrightnessofthe lightarea.Itwillnot,however,ensurethatedgeswithinthedarkregionstandout. Thismotivateslocalcontrastnormalization.Localcontrastnormalization ensuresthatthecontrastisnormalizedacrosseachsmallwindow,ratherthanover theimageasawhole.SeeÔ¨Ågureforacomparisonofglobalandlocalcontrast 12.2 normalization. VariousdeÔ¨Ånitionsoflocalcontrastnormalization arepossible.Inallcases, onemodiÔ¨Åeseachpixelbysubtractingameanofnearbypixelsanddividingby astandarddeviationofnearbypixels.Insomecases,thisisliterallythemean andstandarddeviationofallpixelsinarectangularwindowcenteredonthe pixeltobemodiÔ¨Åed(,).Inothercases,thisisaweightedmean Pinto e t a l .2008 andweightedstandarddeviationusingGaussianweightscenteredonthepixelto bemodiÔ¨Åed. Inthecaseofcolorimages,somestrategiesprocessdiÔ¨Äerentcolor 4 5 6 CHAPTER12.APPLICATIONS Inputimage GCN LCN Figure12.2:Acomparisonofglobalandlocalcontrastnormalization.Visually,theeÔ¨Äects ofglobalcontrastnormalizationaresubtle.Itplacesallimagesonroughlythesame scale,whichreducestheburdenonthelearningalgorithmtohandlemultiplescales.Local contrastnormalizationmodiÔ¨Åestheimagemuchmore,discardingallregionsofconstant intensity.Thisallowsthemodeltofocusonjusttheedges.RegionsofÔ¨Ånetexture, suchasthehousesinthesecondrow,maylosesomedetailduetothebandwidthofthe normalizationkernelbeingtoohigh. channelsseparatelywhileotherscombineinformationfromdiÔ¨Äerentchannelsto normalizeeachpixel( ,). Sermanet e t a l .2012 Localcontrastnormalization canusuallybeimplemented eÔ¨Écientlybyusing separableconvolution(seesection)tocomputefeaturemapsoflocalmeansand 9.8 localstandarddeviations,thenusingelement-wisesubtractionandelement-wise divisionondiÔ¨Äerentfeaturemaps. Localcontrastnormalization isadiÔ¨Äerentiable operationandcanalsobeusedas</div>
        </div>
    </div>

    <div class="question-card" id="q39">
        <div class="question-header">
            <span class="question-number">Question 39</span>
            <span class="question-type">multiple-choice</span>
        </div>

        <div class="question-text">Recurrent neural networks (RNNs) are widely used in deep learning for modeling sequential and structured data, including language, images, and hierarchical information. Various architectural innovations have extended their flexibility and representational power.

Which innovation most directly addresses the limitation of summarizing long input sequences into a fixed-size context vector in sequence-to-sequence RNN architectures?

1) Using convolutional layers before the RNN encoder   
2) Applying dropout regularization to the context vector   
3) Increasing the number of hidden units in the encoder   
4) Stacking multiple decoder layers   
5) Incorporating attention mechanisms that allow dynamic focus on input elements   
6) Employing skip connections between encoder and decoder   
7) Reducing the output sequence length during training</div>

        <div class="answer-section">
            <div class="answer-label">‚úì Correct Answer:</div>
            <div class="answer-text">The correct answer is 5) Incorporating attention mechanisms that allow dynamic focus on input elements.</div>
        </div>

        <div class="reference-section">
            <div class="reference-label">üìö Reference Text:</div>
            <button class="toggle-button" onclick="toggleReference(39)">
                Show/Hide Reference
            </button>
            <div id="ref39" class="reference-text hidden">a nd t h e f u t u r ebut ismostsensitivetotheinputvaluesaroundtime t,withouthavingtospecifya Ô¨Åxed-sizewindowaround t(asonewouldhavetodowithafeedforwardnetwork, aconvolutionalnetwork,oraregularRNNwithaÔ¨Åxed-sizelook-aheadbuÔ¨Äer). Thisideacanbenaturallyextendedto2-dimensionalinput,suchasimages,by havingRNNs,eachonegoinginoneofthefourdirections: up, down,left, f o u r right.Ateachpoint ( i , j)ofa2-Dgrid,anoutput O i , jcouldthencomputea representationthatwouldcapturemostlylocalinformationbutcouldalsodepend on long-range inputs,ifthe RNN isable tolearn tocarry that information. Comparedtoaconvolutionalnetwork,RNNsappliedtoimagesaretypicallymore expensivebutallowforlong-rangelateralinteractionsbetweenfeaturesinthe samefeaturemap(,; Visin e t a l .2015Kalchbrenner 2015 e t a l .,).Indeed,the forwardpropagationequationsforsuchRNNsmaybewritteninaformthatshows theyuseaconvolutionthatcomputesthebottom-upinputtoeachlayer,prior totherecurrentpropagationacrossthefeaturemapthatincorporatesthelateral interactions. 3 9 5 CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS 10.4Encoder-DecoderSequence-to-SequenceArchitec- tures WehaveseeninÔ¨ÅgurehowanRNNcanmapaninputsequencetoaÔ¨Åxed-size 10.5 vector.WehaveseeninÔ¨ÅgurehowanRNNcanmapaÔ¨Åxed-sizevectortoa 10.9 sequence. WehaveseeninÔ¨Ågures,,andhowanRNNcan 10.310.410.1010.11 mapaninputsequencetoanoutputsequenceofthesamelength. E nc ode r ‚Ä¶ x( 1 )x( 1 )x( 2 )x( 2 )x( ) . . .x( ) . . .x( n x )x( n x ) D e c ode r ‚Ä¶ y( 1 )y( 1 )y( 2 )y( 2 )y( ) . . .y( ) . . .y( n y )y( n y )CC Figure10.12: Exam pleofanencoder-decoderorsequence-to-sequenceRNNarchitecture, forlearningtogenerateanoutputsequence( y( 1 ), . . . , y( n y ))givenaninputsequence ( x( 1 ), x( 2 ), . . . , x( n x )).ItiscomposedofanencoderRNNthatreadstheinputsequence andadecoderRNNthatgeneratestheoutputsequence(orcomputestheprobabilityofa givenoutputsequence).TheÔ¨ÅnalhiddenstateoftheencoderRNNisusedtocomputea generallyÔ¨Åxed-sizecontextvariable Cwhichrepresentsasemanticsummaryoftheinput sequenceandisgivenasinputtothedecoderRNN. HerewediscusshowanRNNcanbetrainedtomapaninputsequencetoan outputsequencewhichisnotnecessarilyofthesamelength. This comesupin manyapplications,suchasspeechrecognition,machinetranslationorquestion 3 9 6 CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS answering,wheretheinputandoutputsequencesinthetrainingsetaregenerally notofthesamelength(althoughtheirlengthsmightberelated). WeoftencalltheinputtotheRNNthe‚Äúcontext.‚ÄùWewanttoproducea representationofthiscontext, C.Thecontext Cmightbeavectororsequenceof vectorsthatsummarizetheinputsequenceXx= (( 1 ), . . . ,x( n x )). ThesimplestRNNarchitectureformappingavariable-length sequenceto anothervariable-length sequencewasÔ¨Årstproposedby ()and Cho e t a l .2014a shortlyafterbySutskever2014 e t a l .(),whoindependentlydevelopedthatarchi- tectureandweretheÔ¨Årsttoobtainstate-of-the-art translationusingthisapproach. Theformersystemisbasedonscoringproposalsgeneratedbyanothermachine translationsystem,whilethelatterusesastandalonerecurrentnetworktogenerate thetranslations. Theseauthorsrespectivelycalledthisarchitecture, illustrated inÔ¨Ågure,theencoder-decoderorsequence-to-sequencearchitecture.The 10.12 ideaisverysimple:(1)anencoderorreaderorinputRNNprocessestheinput sequence.Theencoderemitsthecontext C,usuallyasasimplefunctionofits Ô¨Ånalhiddenstate. (2)adecoderorwriteroroutputRNNisconditionedon thatÔ¨Åxed-lengthvector(justlikeinÔ¨Ågure)togeneratetheoutputsequence 10.9 Y=(y( 1 ), . . . ,y( n y )).Theinnovationofthiskindofarchitectureoverthose presentedinearliersectionsofthischapteristhatthelengths n xand n ycan varyfromeachother,whilepreviousarchitectures constrained n x= n y= œÑ.Ina sequence-to-sequencearchitecture,thetwoRNNsaretrainedjointlytomaximize theaverageoflog P(y( 1 ), . . . ,y( n y )|x( 1 ), . . . ,x( n x ))overallthepairsofxandy sequencesinthetrainingset.Thelaststateh n xoftheencoderRNNistypically usedasarepresentation Coftheinputsequencethatisprovidedasinputtothe decoderRNN. Ifthecontext Cisavector,thenthedecoderRNNissimplyavector-to- sequenceRNNasdescribedinsection.Aswehaveseen,thereareatleast 10.2.4 twowaysforavector-to-sequenceRNNtoreceiveinput.Theinputcanbeprovided astheinitialstateoftheRNN,ortheinputcanbeconnectedtothehiddenunits ateachtimestep.Thesetwowayscanalsobecombined. Thereisnoconstraintthattheencodermusthavethesamesizeofhiddenlayer asthedecoder. Oneclearlimitationofthisarchitectureiswhenthecontext Coutputbythe encoderRNNhasadimensionthatistoosmalltoproperlysummarizealong sequence.Thisphenomenon wasobservedby ()inthecontext Bahdanau e t a l .2015 ofmachinetranslation.Theyproposedtomake Cavariable-length sequencerather thanaÔ¨Åxed-sizevector.Additionally,theyintroducedanattentionmechanism thatlearnstoassociateelementsofthesequence Ctoelementsoftheoutput 3 9 7 CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS sequence.Seesectionformoredetails. 12.4.5.1 10.5DeepRecurrentNetworks ThecomputationinmostRNNscanbedecomposedintothreeblocksofparameters andassociatedtransformations: 1. fromtheinputtothehiddenstate, 2. fromtheprevioushiddenstatetothenexthiddenstate,and 3. fromthehiddenstatetotheoutput. WiththeRNNarchitectureofÔ¨Ågure,eachofthesethreeblocksisassociated 10.3 withasingleweightmatrix.Inotherwords,whenthenetworkisunfolded,each ofthesecorrespondstoashallowtransformation. Byashallowtransformation, wemeanatransformationthatwouldberepresentedbyasinglelayerwithin adeepMLP.TypicallythisisatransformationrepresentedbyalearnedaÔ¨Éne transformationfollowedbyaÔ¨Åxednonlinearity. Woulditbeadvantageoustointroducedepthineachoftheseoperations? Experimentalevidence(Graves2013Pascanu2014a e t a l .,; e t a l .,)stronglysuggests so.Theexperimentalevidenceisinagreementwiththeideathatweneedenough depthinordertoperformtherequiredmappings.SeealsoSchmidhuber1992(), ElHihiandBengio1996Jaeger2007a (),or()forearlierworkondeepRNNs. Graves2013 e t a l .()weretheÔ¨ÅrsttoshowasigniÔ¨ÅcantbeneÔ¨Åtofdecomposing thestateofanRNNintomultiplelayersasinÔ¨Ågure(left).Wecanthink 10.13 ofthelowerlayersinthehierarchydepictedinÔ¨Ågureaasplayingarole 10.13 intransformingtherawinputintoarepresentationthatismoreappropriate,at thehigherlevelsofthehiddenstate.Pascanu2014a e t a l .()goastepfurther andproposetohaveaseparateMLP(possiblydeep)foreachofthethreeblocks enumeratedabove,asillustratedinÔ¨Ågureb.Considerationsofrepresentational 10.13 capacitysuggesttoallocateenoughcapacityineachofthesethreesteps,butdoing sobyaddingdepthmayhurtlearningbymakingoptimization diÔ¨Écult.Ingeneral, itiseasiertooptimizeshallowerarchitectures,andaddingtheextradepthof Ô¨Ågurebmakestheshortestpathfromavariableintimestep 10.13 ttoavariable intimestep t+1becomelonger.Forexample,ifanMLPwithasinglehidden layerisusedforthestate-to-statetransition,wehavedoubledthelengthofthe shortestpathbetweenvariablesinanytwodiÔ¨Äerenttimesteps,comparedwiththe ordinaryRNNofÔ¨Ågure.However,asarguedby 10.3 Pascanu2014a e t a l .(),this 3 9 8 CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS hy xz ( a) ( b) ( c )xhy xhy Figure10.13:Arecurrentneuralnetworkcanbemadedeepinmanyways(Pascanu e t a l .,).Thehiddenrecurrentstatecanbebrokendownintogroupsorganized 2014a ( a ) hierarchically.Deepercomputation(e.g.,anMLP)canbeintroducedintheinput-to- ( b ) hidden,hidden-to-hiddenandhidden-to-outputparts. Thismaylengthentheshortest pathlinkingdiÔ¨Äerenttimesteps.Thepath-lengtheningeÔ¨Äectcanbemitigatedby ( c ) introducingskipconnections. 3 9 9 CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS canbemitigatedbyintroducingskipconnectionsinthehidden-to-hidden path,as illustratedinÔ¨Ågurec.10.13 10.6RecursiveNeuralNetworks x( 1 )x( 1 )x( 2 )x( 2 )x( 3 )x( 3 )V V Vy yL L x( 4 )x( 4 )Voo U W U WUW Figure10.14:Arecursivenetworkhasacomputationalgraphthatgeneralizesthatofthe recurrentnetworkfromachaintoatree.Avariable-sizesequencex( 1 ),x( 2 ), . . . ,x( ) tcan bemappedtoaÔ¨Åxed-sizerepresentation(theoutputo),withaÔ¨Åxedsetofparameters (theweightmatricesU,V,W).TheÔ¨Ågureillustratesasupervisedlearningcaseinwhich sometargetisprovidedwhichisassociatedwiththewholesequence. y Recursiveneuralnetworks2representyetanothergeneralization ofrecurrent networks,withadiÔ¨Äerentkindofcomputational graph,whichisstructuredasa deeptree,ratherthanthechain-likestructureofRNNs.Thetypicalcomputational graphforarecursivenetworkisillustratedinÔ¨Ågure.Recursiveneural 10.14 2W e s u g g e s t t o n o t a b b re v i a t e ‚Äú re c u</div>
        </div>
    </div>

    <div class="question-card" id="q40">
        <div class="question-header">
            <span class="question-number">Question 40</span>
            <span class="question-type">multiple-choice</span>
        </div>

        <div class="question-text">Neural language models often face computational challenges when dealing with very large vocabularies, especially during training and generation tasks. Various sampling-based approaches have been developed to make these processes more efficient and scalable.

Which technique enables efficient gradient estimation in large vocabulary neural language models by sampling a small subset of words and adjusting for bias using importance weights derived from a separate proposal distribution?

1) Ranking loss optimization   
2) Maximum entropy modeling   
3) Uniform random sampling   
4) Bag-of-words representation   
5) Noise-contrastive estimation   
6) Importance sampling   
7) Encoder-decoder initialization</div>

        <div class="answer-section">
            <div class="answer-label">‚úì Correct Answer:</div>
            <div class="answer-text">The correct answer is 6) Importance sampling.</div>
        </div>

        <div class="reference-section">
            <div class="reference-label">üìö Reference Text:</div>
            <button class="toggle-button" onclick="toggleReference(40)">
                Show/Hide Reference
            </button>
            <div id="ref40" class="reference-text hidden">4 6 9 CHAPTER12.APPLICATIONS inthenextposition.Everyincorrectwordshouldhavelowprobabilityunderthe model.Itcanbecomputationally costlytoenumerateallofthesewords.Instead, itispossibletosampleonlyasubsetofthewords.Usingthenotationintroduced inequation,thegradientcanbewrittenasfollows: 12.8 ‚àÇ P y C log(|) ‚àÇ Œ∏=‚àÇlogsoftmax y()a ‚àÇ Œ∏(12.13) =‚àÇ ‚àÇ Œ∏logea y ÓÅê i ea i(12.14) =‚àÇ ‚àÇ Œ∏( a y‚àílogÓÅò iea i) (12.15) =‚àÇ a y ‚àÇ Œ∏‚àíÓÅò iP y i C (= |)‚àÇ a i ‚àÇ Œ∏(12.16) whereaisthevectorofpre-softmaxactivations(orscores),withoneelement perword.TheÔ¨Årsttermisthepositivephaseterm(pushing a yup)whilethe secondtermisthenegativephaseterm(pushing a idownforall i,withweight P( i C|).Sincethenegativephasetermisanexpectation,wecanestimateitwith aMonteCarlosample.However,thatwouldrequiresamplingfromthemodelitself. Samplingfromthemodelrequirescomputing P( i C|)forall iinthevocabulary, whichispreciselywhatwearetryingtoavoid. Insteadofsamplingfromthemodel,onecansamplefromanotherdistribution, calledtheproposaldistribution(denoted q),anduseappropriateweightstocorrect forthebiasintroducedbysamplingfromthewrongdistribution(Bengioand S√©n√©cal2003BengioandS√©n√©cal2008 ,; ,).Thisisanapplicationofamoregeneral techniquecalledimportancesampling,whichwillbedescribedinmoredetail insection.Unfortunately,evenexactimportancesamplingisnoteÔ¨Écient 17.2 becauseitrequirescomputingweights p i /q i,where p i= P( i C|),whichcan onlybecomputedifallthescores a iarecomputed.Thesolutionadoptedfor thisapplicationiscalledbiasedimportancesampling,wheretheimportance weightsarenormalizedtosumto1.Whennegativeword n iissampled,the associatedgradientisweightedby w i=p n i /q n iÓÅêN j=1 p n j /q n j. (12.17) Theseweightsareusedtogivetheappropriateimportancetothe mnegative samplesfrom qusedtoformtheestimatednegativephasecontributiontothe 4 7 0 CHAPTER12.APPLICATIONS gradient: || VÓÅò i=1P i C(|)‚àÇ a i ‚àÇ Œ∏‚âà1 mm ÓÅò i=1w i‚àÇ a n i ‚àÇ Œ∏. (12.18) Aunigramorabigramdistributionworkswellastheproposaldistribution q.Itis easytoestimatetheparametersofsuchadistributionfromdata.Afterestimating theparameters,itisalsopossibletosamplefromsuchadistributionveryeÔ¨Éciently. Importancesamplingisnotonlyusefulforspeedingupmodelswithlarge softmaxoutputs.Moregenerally,itisusefulforacceleratingtrainingwithlarge sparseoutputlayers,wheretheoutputisasparsevectorratherthana-of-1 n choice.Anexampleisabagofwords.Abagofwordsisasparsevectorv where v iindicatesthepresenceorabsenceofword ifromthevocabularyinthe document.Alternately, v icanindicatethenumberoftimesthatword iappears. Machinelearningmodelsthatemitsuchsparsevectorscanbeexpensivetotrain foravarietyofreasons.Earlyinlearning,themodelmaynotactuallychooseto maketheoutputtrulysparse.Moreover,thelossfunctionweusefortrainingmight mostnaturallybedescribedintermsofcomparingeveryelementoftheoutputto everyelementofthetarget.Thismeansthatitisnotalwaysclearthatthereisa computational beneÔ¨Åttousingsparseoutputs,becausethemodelmaychooseto makethemajorityoftheoutputnon-zeroandallofthesenon-zerovaluesneedto becomparedtothecorrespondingtrainingtarget,evenifthetrainingtargetiszero. Dauphin 2011 e t a l .()demonstratedthatsuchmodelscanbeacceleratedusing importancesampling.TheeÔ¨Écientalgorithmminimizesthelossreconstructionfor the‚Äúpositivewords‚Äù(thosethatarenon-zerointhetarget)andanequalnumber of‚Äúnegativewords.‚ÄùThenegativewordsarechosenrandomly,usingaheuristicto samplewordsthataremorelikelytobemistaken. Thebiasintroducedbythis heuristicoversamplingcanthenbecorrectedusingimportanceweights. Inallofthesecases,thecomputational complexityofgradientestimationfor theoutputlayerisreducedtobeproportionaltothenumberofnegativesamples ratherthanproportionaltothesizeoftheoutputvector. 12.4.3.4Noise-ContrastiveEstimationandRankingLoss Otherapproachesbasedonsamplinghavebeenproposedtoreducethecomputa- tionalcostoftrainingneurallanguagemodelswithlargevocabularies.Anearly exampleistherankinglossproposedbyCollobertandWeston2008a(),which viewstheoutputoftheneurallanguagemodelforeachwordasascoreandtriesto makethescoreofthecorrectword a yberankedhighincomparisontotheother 4 7 1 CHAPTER12.APPLICATIONS scores a i.Therankinglossproposedthenis L=ÓÅò imax(01 ,‚àí a y+ a i) . (12.19) Thegradientiszeroforthe i-thtermifthescoreoftheobservedword, a y,is greaterthanthescoreofthenegativeword a ibyamarginof1.Oneissuewith thiscriterionisthatitdoesnotprovideestimatedconditionalprobabilities, which areusefulinsomeapplications,includingspeechrecognitionandtextgeneration (includingconditionaltextgenerationtaskssuchastranslation). Amorerecentlyusedtrainingobjectiveforneurallanguagemodelisnoise- contrastiveestimation,whichisintroducedinsection.Thisapproachhas 18.6 beensuccessfullyappliedtoneurallanguagemodels(MnihandTeh2012Mnih,; andKavukcuoglu2013,). 12.4.4CombiningNeuralLanguageModelswith-grams n Amajoradvantageof n-grammodelsoverneuralnetworksisthat n-grammodels achievehighmodelcapacity(bystoringthefrequenciesofverymanytuples) whilerequiringverylittlecomputationtoprocessanexample(bylookingup onlyafewtuplesthatmatchthecurrentcontext).Ifweusehashtablesortrees toaccessthecounts,thecomputationusedfor n-gramsisalmostindependent ofcapacity.Incomparison,doublinganeuralnetwork‚Äôsnumberofparameters typicallyalsoroughlydoublesitscomputationtime.Exceptionsincludemodels thatavoidusingallparametersoneachpass.Embeddinglayersindexonlyasingle embeddingineachpass,sowecanincreasethevocabularysizewithoutincreasing thecomputationtimeperexample.Someothermodels,suchastiledconvolutional networks,canaddparameterswhilereducingthedegreeofparametersharing inordertomaintainthesameamountofcomputation. However,typicalneural networklayersbasedonmatrixmultiplication useanamountofcomputation proportionaltothenumberofparameters. Oneeasywaytoaddcapacityisthustocombinebothapproachesinanensemble consistingofaneurallanguagemodelandan n-gramlanguagemodel(Bengio e t a l .,,).Aswithanyensemble,thistechniquecanreducetesterrorif 20012003 theensemblemembersmakeindependentmistakes.TheÔ¨Åeldofensemblelearning providesmanywaysofcombiningtheensemblemembers‚Äôpredictions,including uniformweightingandweightschosenonavalidationset.Mikolov2011a e t a l .() extendedtheensembletoincludenotjusttwomodelsbutalargearrayofmodels. Itisalsopossibletopairaneuralnetworkwithamaximumentropymodeland trainbothjointly(Mikolov2011b e t a l .,).Thisapproachcanbeviewedastraining 4 7 2 CHAPTER12.APPLICATIONS aneuralnetworkwithanextrasetofinputsthatareconnecteddirectlytothe output,andnotconnectedtoanyotherpartofthemodel.Theextrainputsare indicatorsforthepresenceofparticular n-gramsintheinputcontext,sothese variablesareveryhigh-dimensionalandverysparse.Theincreaseinmodelcapacity ishuge‚Äîthenewportionofthearchitecturecontainsupto|| s Vnparameters‚Äîbut theamountofaddedcomputationneededtoprocessaninputisminimalbecause theextrainputsareverysparse. 12.4.5NeuralMachineTranslation Machinetranslationisthetaskofreadingasentenceinonenaturallanguageand emittingasentencewiththeequivalentmeaninginanotherlanguage. Mac hine translationsystemsofteninvolvemanycomponents.Atahighlevel,thereis oftenonecomponentthatproposesmanycandidatetranslations.Manyofthese translationswillnotbegrammaticalduetodiÔ¨Äerencesbetweenthelanguages.For example,manylanguagesputadjectivesafternouns,sowhentranslatedtoEnglish directlytheyyieldphrasessuchas‚Äúapplered.‚ÄùTheproposalmechanismsuggests manyvariantsofthesuggestedtranslation,ideallyincluding‚Äúredapple.‚ÄùAsecond componentofthetranslationsystem,alanguagemodel,evaluatestheproposed translations,andcanscore‚Äúredapple‚Äùasbetterthan‚Äúapplered.‚Äù Theearliestuseofneuralnetworksformachinetranslationwastoupgradethe languagemodelofatranslationsystembyusinganeurallanguagemodel(Schwenk e t a l .,;2006Schwenk2010,).Previously,mostmachinetranslationsystemshad usedan n-grammodelforthiscomponent.The n-grambasedmodelsusedfor machinetranslationincludenotjusttraditionalback-oÔ¨Ä n-grammodels(Jelinek andMercer1980Katz1987ChenandGoodman1999 ,;,; ,)butalsomaximum entropylanguagemodels(,),inwhichanaÔ¨Éne-softmaxlayer Berger e t a l .1996 predictsthenextwordgiventhepresenceoffrequent-gramsinthecontext. n Traditionallanguagemodelssimplyreporttheprobabilityofanaturallanguage sentence.Becausemachinetranslationinvolvesproducinganoutputsentencegiven aninputsentence,itmakessensetoextendthenaturallanguagemodeltobe conditional.Asdescribedinsection,itisstraightforwardtoextendamodel 6.2.1.1 thatdeÔ¨ÅnesamarginaldistributionoversomevariabletodeÔ¨Åneaconditional distributionoverthatvariablegivenacontext C,where Cmightbeasinglevariable oralistofvariables. ()beatthestate-of-the-art insomestatistical Devlin e t a l .2014 machinetranslationbenchmarksbyusinganMLPtoscoreaphraset1 ,t2 , . . . ,t k inthetargetlanguagegivenaphrases1 ,s2 , . . . ,s ninthesourcelanguage.The MLPestimates P(t1 ,t2 , . . . ,t k|s1 ,s2 , . . . ,s n).TheestimateformedbythisMLP replacestheestimateprovidedbyconditional-grammodels. n 4 7 3 CHAPTER12.APPLICATIONS D e c ode rO ut put ob j e c t ( E ngl i s h s e nt e nc e ) I nt e r m e di at e , s e m a n t i c r e pr e s e nt a t i o n Sourc e ob j e c t ( F r e nc h s e n t e nc e or i m a g e )E nc ode r Figure12.5:Theencoder-decoderarchitecturetomapbackandforthbetweenasurface representation(suchasasequenceofwordsoranimage)andasemanticrepresentation. Byusingtheoutputofanencoderofdatafromonemodality(suchastheencodermapping fromFrenchsentencestohiddenrepresentationscapturingthemeaningofsentences)as theinputtoadecoderforanothermodality(suchasthedecodermappingfromhidden representationscapturingthemeaningofsentencestoEnglish),wecantrainsystemsthat translatefromonemodalitytoanother.Thisideahasbeenappliedsuccessfullynotjust tomachinetranslationbutalsotocaptiongenerationfromimages. AdrawbackoftheMLP-basedapproachisthatitrequiresthesequencestobe preprocessedtobeofÔ¨Åxedlength.TomakethetranslationmoreÔ¨Çexible,wewould liketouseamodelthatcanaccommodatevariablelengthinputsandvariable lengthoutputs.AnRNNprovidesthisability.Section describesseveralways 10.2.4 ofconstructinganRNNthatrepresentsaconditionaldistributionoverasequence givensomeinput,andsectiondescribeshowtoaccomplishthisconditioning 10.4 whentheinputisasequence.Inallcases,onemodelÔ¨Årstreadstheinputsequence andemitsadatastructurethatsummarizestheinputsequence.Wecallthis summarythe‚Äúcontext‚Äù C.Thecontext Cmaybealistofvectors,oritmaybea vectorortensor.Themodelthatreadstheinputtoproduce CmaybeanRNN (,; Cho e t a l .2014aSutskever2014Jean2014 e t a l .,; e t a</div>
        </div>
    </div>

    <div class="question-card" id="q41">
        <div class="question-header">
            <span class="question-number">Question 41</span>
            <span class="question-type">multiple-choice</span>
        </div>

        <div class="question-text">Linear algebra provides the foundation for many data science and machine learning algorithms, including methods for solving linear systems and reducing dimensionality. Key concepts include matrix operations, eigenvalues, and specialized decompositions.

Which statement correctly describes the role of the Moore-Penrose pseudoinverse when a matrix A has more columns than rows in a linear system Ax = y?

1) It guarantees a unique solution for every possible y.   
2) It finds the solution x that maximizes the determinant of A.   
3) It computes the approximation that minimizes the trace of A.   
4) It provides the solution x with the minimal Euclidean norm among all possible solutions.   
5) It always results in a zero reconstruction error regardless of y.   
6) It selects the solution x that maximizes the Frobenius norm of Ax.   
7) It yields the solution x corresponding to the largest eigenvalue of A.</div>

        <div class="answer-section">
            <div class="answer-label">‚úì Correct Answer:</div>
            <div class="answer-text">The correct answer is 4) It provides the solution x with the minimal Euclidean norm among all possible solutions..</div>
        </div>

        <div class="reference-section">
            <div class="reference-label">üìö Reference Text:</div>
            <button class="toggle-button" onclick="toggleReference(41)">
                Show/Hide Reference
            </button>
            <div id="ref41" class="reference-text hidden">CHAPTER2.LINEARALGEBRA byleft-multiplyingeachsidetoobtain xBy= . (2.45) Dependingonthestructureoftheproblem,itmaynotbepossibletodesigna uniquemappingfromto.AB IfAistallerthanitiswide, thenitispossibleforthisequationtohave nosolution.IfAiswiderthanitistall,thentherecouldbemultiplepossible solutions. TheMoore-Penrosepseudoinverseallowsustomakesomeheadwayin thesecases.ThepseudoinverseofisdeÔ¨Ånedasamatrix A A+=lim Œ± ÓÄ¶ 0(AÓÄæAI+ Œ±)‚àí 1AÓÄæ. (2.46) PracticalalgorithmsforcomputingthepseudoinversearenotbasedonthisdeÔ¨Åni- tion,butrathertheformula A+= VD+UÓÄæ, (2.47) whereU,DandVarethesingularvaluedecompositionofA,andthepseudoinverse D+ofadiagonalmatrixDisobtainedbytakingthereciprocalofitsnon-zero elementsthentakingthetransposeoftheresultingmatrix. WhenAhasmorecolumnsthanrows,thensolvingalinearequationusingthe pseudoinverseprovidesoneofthemanypossiblesolutions.SpeciÔ¨Åcally,itprovides thesolutionx=A+ywithminimalEuclideannorm ||||x 2amongallpossible solutions. WhenAhasmorerowsthancolumns,itispossiblefortheretobenosolution. Inthiscase,usingthepseudoinversegivesusthexforwhichAxisascloseas possibletointermsofEuclideannorm y ||‚àí||Axy 2. 2.10TheTraceOperator Thetraceoperatorgivesthesumofallofthediagonalentriesofamatrix: Tr() =AÓÅò iA i , i . (2.48) Thetraceoperatorisusefulforavarietyofreasons.Someoperationsthatare diÔ¨ÉculttospecifywithoutresortingtosummationnotationcanbespeciÔ¨Åedusing 4 6 CHAPTER2.LINEARALGEBRA matrixproductsandthetraceoperator.Forexample,thetraceoperatorprovides analternativewayofwritingtheFrobeniusnormofamatrix: |||| A F=ÓÅ± Tr(AAÓÄæ) . (2.49) Writinganexpressionintermsofthetraceoperatoropensupopportunitiesto manipulatetheexpressionusingmanyusefulidentities. Forexample,thetrace operatorisinvarianttothetransposeoperator: Tr() = Tr(AAÓÄæ) . (2.50) Thetraceofasquarematrixcomposedofmanyfactorsisalsoinvariantto movingthelastfactorintotheÔ¨Årstposition,iftheshapesofthecorresponding matricesallowtheresultingproducttobedeÔ¨Åned: Tr( ) = Tr( ) = Tr( ) ABCCABBCA (2.51) ormoregenerally, Tr(nÓÅô i = 1F( ) i) = Tr(F( ) nn ‚àí 1ÓÅô i = 1F( ) i) . (2.52) Thisinvariancetocyclicpermutationholdseveniftheresultingproducthasa diÔ¨Äerentshape.Forexample,forA‚àà Rm n √óandB‚àà Rn m √ó,wehave Tr( ) = Tr( )ABBA (2.53) eventhoughAB‚àà Rm m √óandBA‚àà Rn n √ó. Anotherusefulfacttokeepinmindisthatascalarisitsowntrace: a=Tr( a). 2.11TheDeterminant Thedeterminant ofa squarematrix, denoted det(A), isa functionmapping matricesto realscalars.Thedeterminant isequal totheproductof allthe eigenvaluesofthematrix.Theabsolutevalueofthedeterminantcanbethought ofasameasureofhowmuchmultiplicationbythematrixexpandsorcontracts space.Ifthedeterminantis0,thenspaceiscontractedcompletelyalongatleast onedimension,causingittoloseallofitsvolume.Ifthedeterminantis1,then thetransformationpreservesvolume. 4 7 CHAPTER2.LINEARALGEBRA 2.12Example:PrincipalComponentsAnalysis Onesimplemachinelearningalgorithm,principalcomponentsanalysisorPCA canbederivedusingonlyknowledgeofbasiclinearalgebra. Supposewehaveacollectionof mpoints{x( 1 ), . . . ,x( ) m}in Rn.Supposewe wouldliketoapplylossycompressiontothesepoints.Lossycompressionmeans storingthepointsinawaythatrequireslessmemorybutmaylosesomeprecision. Wewouldliketoloseaslittleprecisionaspossible. Onewaywecanencodethesepointsistorepresentalower-dimensionalversion ofthem.Foreachpointx( ) i‚àà RnwewillÔ¨Åndacorrespondingcodevectorc( ) i‚àà Rl. If lissmallerthan n,itwilltakelessmemorytostorethecodepointsthanthe originaldata.WewillwanttoÔ¨Åndsomeencodingfunctionthatproducesthecode foraninput, f(x) =c,andadecodingfunctionthatproducesthereconstructed inputgivenitscode, .xx ‚âà g f(()) PCAisdeÔ¨Ånedbyourchoiceofthedecodingfunction.SpeciÔ¨Åcally,tomakethe decoderverysimple,wechoosetousematrixmultiplicationtomapthecodeback into Rn.Let,where g() = cDcD‚àà Rn l √óisthematrixdeÔ¨Åningthedecoding. ComputingtheoptimalcodeforthisdecodercouldbeadiÔ¨Écultproblem.To keeptheencodingproblemeasy,PCAconstrainsthecolumnsofDtobeorthogonal toeachother.(NotethatDisstillnottechnically‚Äúanorthogonalmatrix‚Äùunless l n= ) Withtheproblemasdescribedsofar,manysolutionsarepossible,becausewe canincreasethescaleofD : , iifwedecrease c iproportionallyforallpoints.Togive theproblemauniquesolution,weconstrainallofthecolumnsoftohaveunitD norm. Inordertoturnthisbasicideaintoanalgorithmwecanimplement,theÔ¨Årst thingweneedtodoisÔ¨Ågureouthowtogeneratetheoptimalcodepointc‚àófor eachinputpointx.Onewaytodothisistominimizethedistancebetweenthe inputpointxanditsreconstruction, g(c‚àó).Wecanmeasurethisdistanceusinga norm.Intheprincipalcomponentsalgorithm,weusethe L2norm: c‚àó= argmin c||‚àí ||x g()c 2 . (2.54) Wecanswitchtothesquared L2norminsteadofthe L2normitself,because bothareminimizedbythesamevalueofc.Bothareminimizedbythesame valueofcbecausethe L2normisnon-negative andthesquaringoperationis 4 8 CHAPTER2.LINEARALGEBRA monotonically increasingfornon-negative arguments. c‚àó= argmin c||‚àí ||x g()c2 2 . (2.55) ThefunctionbeingminimizedsimpliÔ¨Åesto ( ())x‚àí gcÓÄæ( ())x‚àí gc (2.56) (bythedeÔ¨Ånitionofthe L2norm,equation)2.30 = xÓÄæxx‚àíÓÄæg g ()c‚àí()cÓÄæxc+( g)ÓÄæg()c (2.57) (bythedistributiveproperty) = xÓÄæxx‚àí2ÓÄæg g ()+c ()cÓÄæg()c (2.58) (becausethescalar g()cÓÄæxisequaltothetransposeofitself). Wecannowchangethefunctionbeingminimizedagain,toomittheÔ¨Årstterm, sincethistermdoesnotdependon:c c‚àó= argmin c‚àí2xÓÄæg g ()+c ()cÓÄæg .()c (2.59) Tomakefurtherprogress,wemustsubstituteinthedeÔ¨Ånitionof: g()c c‚àó= argmin c‚àí2xÓÄæDcc+ÓÄæDÓÄæDc (2.60) = argmin c‚àí2xÓÄæDcc+ÓÄæI lc (2.61) (bytheorthogonalityandunitnormconstraintson)D = argmin c‚àí2xÓÄæDcc+ÓÄæc (2.62) Wecansolvethisoptimization problemusingvectorcalculus(seesectionif4.3 youdonotknowhowtodothis): ‚àá c(2‚àíxÓÄæDcc+ÓÄæc) = 0 (2.63) ‚àí2DÓÄæxc+2= 0 (2.64) cD= ÓÄæx . (2.65) 4 9 CHAPTER2.LINEARALGEBRA ThismakesthealgorithmeÔ¨Écient: wecanoptimallyencodexjustusinga matrix-vectoroperation.Toencodeavector,weapplytheencoderfunction f() = xDÓÄæx . (2.66) Usingafurthermatrixmultiplication, wecanalsodeÔ¨ÅnethePCAreconstruction operation: r g f () = x (()) = xDDÓÄæx . (2.67) Next,weneedtochoosetheencodingmatrixD.Todoso,werevisittheidea ofminimizingthe L2distancebetweeninputsandreconstructions.Sincewewill usethesamematrixDtodecodeallofthepoints,wecannolongerconsiderthe pointsinisolation.Instead,wemustminimizetheFrobeniusnormofthematrix oferrorscomputedoveralldimensionsandallpoints: D‚àó= argmin DÓÅ≥ÓÅò i , jÓÄê x( ) i j‚àí r(x( ) i) jÓÄë2 subjecttoDÓÄæDI= l(2.68) ToderivethealgorithmforÔ¨ÅndingD‚àó,wewillstartbyconsideringthecase where l= 1.Inthiscase,Disjustasinglevector,d.Substitutingequation2.67 intoequationandsimplifyinginto,theproblemreducesto 2.68 Dd d‚àó= argmin dÓÅò i||x( ) i‚àíddÓÄæx( ) i||2 2subjectto||||d 2= 1 .(2.69) Theaboveformulationisthemostdirectwayofperformingthesubstitution, butisnotthemoststylisticallypleasingwaytowritetheequation.Itplacesthe scalarvaluedÓÄæx( ) iontherightofthevectord.Itismoreconventionaltowrite scalarcoeÔ¨Écientsontheleftofvectortheyoperateon.Wethereforeusuallywrite suchaformulaas d‚àó= argmin dÓÅò i||x( ) i‚àídÓÄæx( ) id||2 2subjectto||||d 2= 1 ,(2.70) or,exploitingthefactthatascalarisitsowntranspose,as d‚àó= argmin dÓÅò i||x( ) i‚àíx( ) i ÓÄædd||2 2subjectto||||d 2= 1 .(2.71) Thereadershouldaimtobecomefamiliarwithsuchcosmeticrearrangements . 5 0 CHAPTER2.LINEARALGEBRA Atthispoint,itcanbehelpfultorewritetheproblemintermsofasingle designmatrixofexamples,ratherthanasasumoverseparateexamplevectors. Thiswillallowustousemorecompactnotation.LetX‚àà Rm n √óbethematrix deÔ¨Ånedbystackingallofthevectorsdescribingthepoints,suchthatX i , :=x( ) iÓÄæ. Wecannowrewritetheproblemas d‚àó= argmin d||‚àíXXddÓÄæ||2 FsubjecttodÓÄæd= 1 .(2.72) Disregardingtheconstraintforthemoment,wecansimplifytheFrobeniusnorm portionasfollows: argmin d||‚àíXXddÓÄæ||2 F (2.73) = argmin dTrÓÄíÓÄê XXdd ‚àíÓÄæÓÄëÓÄæÓÄê XXdd ‚àíÓÄæÓÄëÓÄì (2.74) (byequation)2.49 = argmin dTr(XÓÄæXX‚àíÓÄæXddÓÄæ‚àíddÓÄæXÓÄæXdd+ÓÄæXÓÄæXddÓÄæ)(2.75) = argmin dTr(XÓÄæX)Tr(‚àíXÓÄæXddÓÄæ)Tr(‚àíddÓÄæXÓÄæX)+Tr(ddÓÄæXÓÄæXddÓÄæ) (2.76) = argmin d‚àíTr(XÓÄæXddÓÄæ)Tr(‚àíddÓÄæXÓÄæX)+Tr(ddÓÄæXÓÄæXddÓÄæ)(2.77) (becausetermsnotinvolvingdonotaÔ¨Äectthe) d argmin = argmin d‚àí2Tr(XÓÄæXddÓÄæ)+Tr(ddÓÄæXÓÄæXddÓÄæ)(2.78) (becausewecancycletheorderofthematricesinsideatrace,equation)2.52 = argmin d‚àí2Tr(XÓÄæXddÓÄæ)+Tr(XÓÄæXddÓÄæddÓÄæ)(2.79) (usingthesamepropertyagain) Atthispoint,were-introducetheconstraint: argmin d‚àí2Tr(XÓÄæXddÓÄæ)+Tr(XÓÄæXddÓÄæddÓÄæ)subjecttodÓÄæd= 1(2.80) = argmin d‚àí2Tr(XÓÄæXddÓÄæ)+Tr(XÓÄæXddÓÄæ)subjecttodÓÄæd= 1(2.81) (duetotheconstraint) = argmin d‚àíTr(XÓÄæXddÓÄæ)subjecttodÓÄæd= 1(2.82) 5 1 CHAPTER2.LINEARALGEBRA = argmax dTr(XÓÄæXddÓÄæ)subjecttodÓÄæd= 1(2.83) = argmax dTr(dÓÄæXÓÄæXdd )subjecttoÓÄæd= 1(2.84) Thisoptimizationproblemmaybesolvedusingeigendecomposition.SpeciÔ¨Åcally, theoptimaldisgivenbytheeigenvectorofXÓÄæXcorrespondingtothelargest</div>
        </div>
    </div>

    <div class="question-card" id="q42">
        <div class="question-header">
            <span class="question-number">Question 42</span>
            <span class="question-type">multiple-choice</span>
        </div>

        <div class="question-text">In machine learning, understanding how model complexity and data size impact generalization is fundamental for designing effective algorithms. Concepts like VC dimension, overfitting, and the No Free Lunch theorem are central to evaluating learning performance.

Which statement best describes the relationship between training error, generalization error, and model capacity in supervised learning?

1) Training error increases as model capacity increases, while generalization error steadily decreases.   
2) Both training error and generalization error decrease as model capacity increases indefinitely.   
3) Training error remains constant regardless of model capacity, but generalization error fluctuates randomly.   
4) Generalization error reaches its minimum at the lowest model capacity and rises as capacity increases.   
5) With higher model capacity, both training and generalization errors increase due to overfitting.   
6) Training error forms a U-shaped curve as capacity increases, while generalization error declines linearly.   
7) Training error decreases with increased model capacity, but generalization error follows a U-shaped curve, decreasing then increasing as capacity moves from underfitting to overfitting. </div>

        <div class="answer-section">
            <div class="answer-label">‚úì Correct Answer:</div>
            <div class="answer-text">The correct answer is 7) Training error decreases with increased model capacity, but generalization error follows a U-shaped curve, decreasing then increasing as capacity moves from underfitting to overfitting..</div>
        </div>

        <div class="reference-section">
            <div class="reference-label">üìö Reference Text:</div>
            <button class="toggle-button" onclick="toggleReference(42)">
                Show/Hide Reference
            </button>
            <div id="ref42" class="reference-text hidden">thetasksoitgeneralizeswelltonewdata. ÓÅ∏ÓÄ∞ÓÅπÓÅïÓÅÆ ÓÅ§ ÓÅ• ÓÅ≤ ÓÅ¶ ÓÅ© ÓÅ¥ ÓÅ¥ ÓÅ© ÓÅÆ ÓÅß ÓÅ∏ÓÄ∞ÓÅπÓÅÅÓÅ∞ ÓÅ∞ ÓÅ≤ ÓÅØ ÓÅ∞ ÓÅ≤ ÓÅ© ÓÅ° ÓÅ¥ ÓÅ• ÓÄ† ÓÅ£ ÓÅ° ÓÅ∞ ÓÅ° ÓÅ£ ÓÅ© ÓÅ¥ ÓÅπ ÓÅ∏ÓÄ∞ÓÅπÓÅè ÓÅ∂ ÓÅ• ÓÅ≤ ÓÅ¶ ÓÅ© ÓÅ¥ ÓÅ¥ ÓÅ© ÓÅÆ ÓÅß Figure5.2:WeÔ¨Åtthreemodelstothisexampletrainingset.Thetrainingdatawas generatedsynthetically,byrandomlysamplingxvaluesandchoosingydeterministically byevaluatingaquadraticfunction. ( L e f t )AlinearfunctionÔ¨ÅttothedatasuÔ¨Äersfrom underÔ¨Åtting‚Äîitcannotcapturethecurvaturethatispresentinthedata. A ( C e n t e r ) quadraticfunctionÔ¨Åttothedatageneralizeswelltounseenpoints.ItdoesnotsuÔ¨Äerfrom asigniÔ¨ÅcantamountofoverÔ¨ÅttingorunderÔ¨Åtting.Apolynomialofdegree9Ô¨Åtto ( R i g h t ) thedatasuÔ¨ÄersfromoverÔ¨Åtting.HereweusedtheMoore-Penrosepseudoinversetosolve theunderdeterminednormalequations.Thesolutionpassesthroughallofthetraining pointsexactly,butwehavenotbeenluckyenoughforittoextractthecorrectstructure. Itnowhasadeepvalleyinbetweentwotrainingpointsthatdoesnotappearinthetrue underlyingfunction.Italsoincreasessharplyontheleftsideofthedata,whilethetrue functiondecreasesinthisarea. Sofarwehavedescribedonlyonewayofchangingamodel‚Äôscapacity:by changingthenumberofinputfeaturesithas,andsimultaneouslyaddingnew parametersassociatedwiththosefeatures.Thereareinfactmanywaysofchanging amodel‚Äôscapacity.Capacityisnotdeterminedonlybythechoiceofmodel.The modelspeciÔ¨Åeswhichfamilyoffunctionsthelearningalgorithmcanchoosefrom whenvaryingtheparametersinordertoreduceatrainingobjective.Thisiscalled therepresentationalcapacityofthemodel.Inmanycases,Ô¨Åndingthebest functionwithinthisfamilyisaverydiÔ¨Écultoptimization problem.Inpractice, thelearningalgorithmdoesnotactuallyÔ¨Åndthebestfunction,butmerelyone thatsigniÔ¨Åcantlyreducesthetrainingerror.Theseadditionallimitations,suchas 1 1 3 CHAPTER5.MACHINELEARNINGBASICS theimperfectionoftheoptimization algorithm,meanthatthelearningalgorithm‚Äôs eÔ¨Äectivecapacitymaybelessthantherepresentationalcapacityofthemodel family. Ourmodernideasaboutimprovingthegeneralization ofmachinelearning modelsarereÔ¨Ånementsofthoughtdatingbacktophilosophersatleastasearly asPtolemy.Manyearlyscholarsinvokeaprincipleofparsimonythatisnow mostwidelyknownasOccam‚Äôsrazor(c.1287-1347).Thisprinciplestatesthat amongcompetinghypothesesthatexplainknownobservationsequallywell,one shouldchoosethe‚Äúsimplest‚Äùone.Thisideawasformalizedandmademoreprecise inthe20thcenturybythefoundersofstatisticallearningtheory(Vapnikand Chervonenkis1971Vapnik1982Blumer1989Vapnik1995 ,;,; etal.,;,). Statisticallearningtheoryprovidesvariousmeansofquantifyingmodelcapacity. Amongthese,themostwell-knownistheVapnik-Chervonenkisdimension,or VCdimension.TheVCdimensionmeasuresthecapacityofabinaryclassiÔ¨Åer.The VCdimensionisdeÔ¨Ånedasbeingthelargestpossiblevalueofmforwhichthere existsatrainingsetofmdiÔ¨ÄerentxpointsthattheclassiÔ¨Åercanlabelarbitrarily. Quantifyingthecapacityofthemodelallowsstatisticallearningtheoryto makequantitativepredictions.Themostimportantresultsinstatisticallearning theoryshowthatthediscrepancybetweentrainingerrorandgeneralization error isboundedfromabovebyaquantitythatgrowsasthemodelcapacitygrowsbut shrinksasthenumberoftrainingexamplesincreases(VapnikandChervonenkis, 1971Vapnik1982Blumer 1989Vapnik1995 ;,; etal.,;,).Theseboundsprovide intellectualjustiÔ¨Åcationthatmachinelearningalgorithmscanwork,buttheyare rarelyusedinpracticewhenworkingwithdeeplearningalgorithms.Thisisin partbecausetheboundsareoftenquitelooseandinpartbecauseitcanbequite diÔ¨Éculttodeterminethecapacityofdeeplearningalgorithms. Theproblemof determiningthecapacityofadeeplearningmodelisespeciallydiÔ¨Écultbecausethe eÔ¨Äectivecapacityislimitedbythecapabilitiesoftheoptimization algorithm,and wehavelittletheoreticalunderstandingoftheverygeneralnon-convexoptimization problemsinvolvedindeeplearning. Wemustrememberthatwhilesimplerfunctionsaremorelikelytogeneralize (tohaveasmallgapbetweentrainingandtesterror)wemuststillchoosea suÔ¨Écientlycomplexhypothesistoachievelowtrainingerror.Typically,training errordecreasesuntilitasymptotestotheminimumpossibleerrorvalueasmodel capacityincreases(assumingtheerrormeasurehasaminimumvalue).Typically, generalization errorhasaU-shapedcurveasafunctionofmodelcapacity.Thisis illustratedinÔ¨Ågure.5.3 Toreachthemostextremecaseofarbitrarilyhighcapacity,weintroduce 1 1 4 CHAPTER5.MACHINELEARNINGBASICS 0 O pti m a l C a pa c i t y C a pa c i t yE r r o rU nde r Ô¨Åtti ng z o ne O v e r Ô¨Åtti ng z o ne G e ne r a l i z a t i o n g a pT r a i n i n g e r r o r G e n e r a l i z a t i o n e r r o r Figure5.3:Typicalrelationshipbetweencapacityanderror.Trainingandtesterror behavediÔ¨Äerently.Attheleftendofthegraph,trainingerrorandgeneralizationerror arebothhigh.ThisistheunderÔ¨Åttingregime.Asweincreasecapacity,trainingerror decreases,butthegapbetweentrainingandgeneralizationerrorincreases.Eventually, thesizeofthisgapoutweighsthedecreaseintrainingerror,andweentertheoverÔ¨Åtting regime,wherecapacityistoolarge,abovetheoptimalcapacity. theconceptofnon-parametricmodels.Sofar,wehaveseenonlyparametric models,suchaslinearregression.Parametricmodelslearnafunctiondescribed byaparametervectorwhosesizeisÔ¨ÅniteandÔ¨Åxedbeforeanydataisobserved. Non-parametric modelshavenosuchlimitation. Sometimes,non-parametric modelsarejusttheoreticalabstractions(suchas analgorithmthatsearchesoverallpossibleprobabilitydistributions)thatcannot beimplemented inpractice.However,wecanalsodesignpracticalnon-parametric modelsbymakingtheircomplexityafunctionofthetrainingsetsize.Oneexample ofsuchanalgorithmisnearestneighborregression.Unlikelinearregression, whichhasaÔ¨Åxed-lengthvectorofweights,thenearestneighborregressionmodel simplystorestheXandyfromthetrainingset. Whenaskedtoclassifyatest pointx,themodellooksupthenearestentryinthetrainingsetandreturnsthe associatedregressiontarget.Inotherwords,ÀÜy=y iwherei=argmin||X i ,:‚àí||x2 2. ThealgorithmcanalsobegeneralizedtodistancemetricsotherthantheL2norm, suchaslearneddistancemetrics( ,).Ifthealgorithmis Goldbergeretal.2005 allowedtobreaktiesbyaveragingthey ivaluesforallX i ,:thataretiedfornearest, thenthisalgorithmisabletoachievetheminimumpossibletrainingerror(which mightbegreaterthanzero,iftwoidenticalinputsareassociatedwithdiÔ¨Äerent outputs)onanyregressiondataset. Finally,wecanalsocreateanon-parametric learningalgorithmbywrappinga 1 1 5 CHAPTER5.MACHINELEARNINGBASICS parametriclearningalgorithminsideanotheralgorithmthatincreasesthenumber ofparametersasneeded.Forexample,wecouldimagineanouterloopoflearning thatchangesthedegreeofthepolynomiallearnedbylinearregressionontopofa polynomialexpansionoftheinput. Theidealmodelisanoraclethatsimplyknowsthetrueprobabilitydistribution thatgeneratesthedata. Evensuchamodelwillstillincursomeerroronmany problems,becausetheremaystillbesomenoiseinthedistribution.Inthecase ofsupervisedlearning,themappingfromxtoymaybeinherentlystochastic, orymaybeadeterministicfunctionthatinvolvesothervariablesbesidesthose includedinx.Theerrorincurredbyanoraclemakingpredictionsfromthetrue distributioniscalledthe p,y(x)Bayeserror. Trainingandgeneralization errorvaryasthesizeofthetrainingsetvaries. Expectedgeneralization errorcanneverincreaseasthenumberoftrainingexamples increases.Fornon-parametric models,moredatayieldsbettergeneralization until thebestpossibleerrorisachieved.AnyÔ¨Åxedparametricmodelwithlessthan optimalcapacitywillasymptotetoanerrorvaluethatexceedstheBayeserror.See Ô¨Ågureforanillustration.Notethatitispossibleforthemodeltohaveoptimal 5.4 capacityandyetstillhavealargegapbetweentrainingandgeneralization error. Inthissituation,wemaybeabletoreducethisgapbygatheringmoretraining examples. 5.2.1TheNoFreeLunchTheorem Learningtheoryclaimsthatamachinelearningalgorithmcangeneralizewellfrom aÔ¨Ånitetrainingsetofexamples.Thisseemstocontradictsomebasicprinciplesof logic.Inductivereasoning,orinferringgeneralrulesfromalimitedsetofexamples, isnotlogicallyvalid. Tologicallyinferaruledescribingeverymemberofaset, onemusthaveinformationabouteverymemberofthatset. Inpart,machinelearningavoidsthisproblembyoÔ¨Äeringonlyprobabilisticrules, ratherthantheentirelycertainrulesusedinpurelylogicalreasoning. Machine learningpromisestoÔ¨Åndrulesthatareprobably most correctaboutmembersof thesettheyconcern. Unfortunately,eventhisdoesnotresolvetheentireproblem.Thenofree lunchtheoremformachinelearning(Wolpert1996,)statesthat,averagedover allpossibledatageneratingdistributions,everyclassiÔ¨Åcationalgorithmhasthe sameerrorratewhenclassifyingpreviouslyunobservedpoints.Inotherwords, insomesense,nomachinelearningalgorithmisuniversallyanybetterthanany other.Themostsophisticatedalgorithmwecanconceiveofhasthesameaverage 1 1 6 CHAPTER5.MACHINELEARNINGBASICS ÓÄ± ÓÄ∞ÓÄ∞ÓÄ± ÓÄ∞ÓÄ±ÓÄ± ÓÄ∞ÓÄ≤ÓÄ± ÓÄ∞ÓÄ≥ÓÄ± ÓÄ∞ÓÄ¥ÓÄ± ÓÄ∞ÓÄµ ÓÅéÓÅµÓÅ≠ ÓÅ¢ ÓÅ•ÓÅ≤ÓÄ† ÓÅØ ÓÅ¶ ÓÄ† ÓÅ¥ ÓÅ≤ÓÅ° ÓÅ© ÓÅÆ ÓÅ© ÓÅÆ ÓÅß ÓÄ† ÓÅ•ÓÅ∏ ÓÅ° ÓÅ≠ ÓÅ∞ ÓÅ¨ ÓÅ• ÓÅ≥ÓÄ∞ ÓÄÆ ÓÄ∞ÓÄ∞ ÓÄÆ ÓÄµÓÄ± ÓÄÆ ÓÄ∞ÓÄ± ÓÄÆ ÓÄµÓÄ≤ ÓÄÆ ÓÄ∞ÓÄ≤ ÓÄÆ ÓÄµÓÄ≥ ÓÄÆ ÓÄ∞ÓÄ≥ ÓÄÆ ÓÄµÓÅÖ ÓÅ≤ÓÅ≤ÓÅØ ÓÅ≤ÓÄ† ÓÄ® ÓÅç ÓÅì ÓÅÖ ÓÄ©ÓÅÇ ÓÅ° ÓÅπ ÓÅ• ÓÅ≥ÓÄ† ÓÅ• ÓÅ≤ ÓÅ≤ ÓÅØ ÓÅ≤ ÓÅî ÓÅ≤ ÓÅ° ÓÅ© ÓÅÆ ÓÄ† ÓÄ® ÓÅ± ÓÅµ ÓÅ° ÓÅ§ ÓÅ≤ ÓÅ° ÓÅ¥ ÓÅ© ÓÅ£ ÓÄ© ÓÅî ÓÅ• ÓÅ≥ÓÅ¥ ÓÄ† ÓÄ® ÓÅ± ÓÅµ ÓÅ° ÓÅ§ ÓÅ≤ ÓÅ° ÓÅ¥ ÓÅ© ÓÅ£ ÓÄ© ÓÅî ÓÅ• ÓÅ≥ÓÅ¥ ÓÄ† ÓÄ® ÓÅØ ÓÅ∞ ÓÅ¥ ÓÅ© ÓÅ≠ ÓÅ° ÓÅ¨ ÓÄ† ÓÅ£ ÓÅ° ÓÅ∞ ÓÅ° ÓÅ£ ÓÅ© ÓÅ¥ ÓÅπ ÓÄ© ÓÅî ÓÅ≤ ÓÅ° ÓÅ© ÓÅÆ ÓÄ† ÓÄ® ÓÅØ ÓÅ∞ ÓÅ¥ ÓÅ© ÓÅ≠ ÓÅ° ÓÅ¨ ÓÄ† ÓÅ£ ÓÅ° ÓÅ∞ ÓÅ° ÓÅ£ ÓÅ© ÓÅ¥ ÓÅπ ÓÄ© ÓÄ± ÓÄ∞ÓÄ∞ÓÄ± ÓÄ∞ÓÄ±ÓÄ± ÓÄ∞ÓÄ≤ÓÄ± ÓÄ∞ÓÄ≥ÓÄ± ÓÄ∞ÓÄ¥ÓÄ± ÓÄ∞ÓÄµ ÓÅéÓÅµÓÅ≠ ÓÅ¢ ÓÅ•ÓÅ≤ÓÄ† ÓÅØ ÓÅ¶ ÓÄ† ÓÅ¥ ÓÅ≤ÓÅ° ÓÅ© ÓÅÆ ÓÅ© ÓÅÆ ÓÅß ÓÄ† ÓÅ•ÓÅ∏ ÓÅ° ÓÅ≠ ÓÅ∞ ÓÅ¨ ÓÅ• ÓÅ≥ÓÄ∞ÓÄµÓÄ± ÓÄ∞ÓÄ± ÓÄµÓÄ≤ ÓÄ∞ÓÅè ÓÅ∞ ÓÅ¥ ÓÅ© ÓÅ≠</div>
        </div>
    </div>

    <div class="question-card" id="q43">
        <div class="question-header">
            <span class="question-number">Question 43</span>
            <span class="question-type">multiple-choice</span>
        </div>

        <div class="question-text">Boltzmann machines are a class of energy-based probabilistic models used in machine learning to capture complex distributions, particularly in unsupervised learning scenarios. Their variants, such as Restricted Boltzmann Machines (RBMs), play an important role in feature learning and deep generative architectures.

Which property allows Boltzmann machines with hidden units to serve as universal approximators of probability distributions over discrete variables?

1) The ability to model only linear interactions among observed variables   
2) The inclusion of hidden units enabling the capture of complex, nonlinear relationships   
3) The restriction of connections to a bipartite graph structure   
4) The tractability of the partition function for normalized probability computation   
5) The use of local learning rules based on Hebbian principles   
6) The absence of intra-layer connections among visible or hidden units   
7) The biological plausibility of the learning mechanism</div>

        <div class="answer-section">
            <div class="answer-label">‚úì Correct Answer:</div>
            <div class="answer-text">The correct answer is 2) The inclusion of hidden units enabling the capture of complex, nonlinear relationships.</div>
        </div>

        <div class="reference-section">
            <div class="reference-label">üìö Reference Text:</div>
            <button class="toggle-button" onclick="toggleReference(43)">
                Show/Hide Reference
            </button>
            <div id="ref43" class="reference-text hidden">‚àà{0 ,1}d. TheBoltzmannmachineisanenergy-basedmodel(section),16.2.4 654 CHAPTER20.DEEPGENERATIVEMODELS meaningwedeÔ¨Ånethejointprobabilitydistributionusinganenergyfunction: P() = xexp(()) ‚àí E x Z, (20.1) where E( x)istheenergyfunctionand Zisthepartitionfunctionthatensures thatÓÅê x P() = 1 x.TheenergyfunctionoftheBoltzmannmachineisgivenby E() = x ‚àí xÓÄæU x b‚àíÓÄæx , (20.2) where Uisthe‚Äúweight‚Äùmatrixofmodelparametersand bisthevectorofbias parameters. InthegeneralsettingoftheBoltzmannmachine,wearegivenasetoftraining examples,eachofwhichare n-dimensional.Equationdescribesthejoint 20.1 probabilitydistributionovertheobservedvariables.Whilethisscenarioiscertainly viable,itdoeslimitthekindsofinteractionsbetweentheobservedvariablesto thosedescribedbytheweightmatrix.SpeciÔ¨Åcally,itmeansthattheprobabilityof oneunitbeingonisgivenbyalinearmodel(logisticregression)fromthevaluesof theotherunits. TheBoltzmannmachinebecomesmorepowerfulwhennotallthevariablesare observed.Inthiscase,thelatentvariables,canactsimilarlytohiddenunitsina multi-layerperceptronandmodelhigher-orderinteractionsamongthevisibleunits. JustastheadditionofhiddenunitstoconvertlogisticregressionintoanMLPresults intheMLPbeingauniversalapproximatoroffunctions,aBoltzmannmachine withhiddenunitsisnolongerlimitedtomodelinglinearrelationshipsbetween variables.Instead,theBoltzmannmachinebecomesauniversalapproximator of probabilitymassfunctionsoverdiscretevariables( ,). LeRouxandBengio2008 Formally,wedecomposetheunits xintotwosubsets:thevisibleunits vand thelatent(orhidden)units.Theenergyfunctionbecomes h E ,( v h v ) = ‚àíÓÄæR v v‚àíÓÄæW h h‚àíÓÄæS h b‚àíÓÄæv c‚àíÓÄæh .(20.3) BoltzmannMachineLearningLearningalgorithmsforBoltzmannmachines areusuallybasedonmaximumlikelihood.AllBoltzmannmachineshavean intractablepartitionfunction,sothemaximumlikelihoodgradientmustbeap- proximatedusingthetechniquesdescribedinchapter.18 OneinterestingpropertyofBoltzmannmachineswhentrainedwithlearning rulesbasedonmaximumlikelihoodisthattheupdateforaparticularweight connectingtwounitsdependsonlythestatisticsofthosetwounits,collected underdiÔ¨Äerentdistributions: P m o de l( v)and ÀÜ P da t a( v) P m o de l( h v|).Therestofthe 6 5 5 CHAPTER20.DEEPGENERATIVEMODELS networkparticipatesinshapingthosestatistics,buttheweightcanbeupdated withoutknowinganythingabouttherestofthenetworkorhowthosestatisticswere produced.Thismeansthatthelearningruleis‚Äúlocal,‚ÄùwhichmakesBoltzmann machinelearningsomewhatbiologicallyplausible. Itisconceivablethatifeach neuronwerearandomvariableinaBoltzmannmachine,thentheaxonsand dendritesconnectingtworandomvariablescouldlearnonlybyobservingtheÔ¨Åring patternofthecellsthattheyactuallyphysicallytouch.Inparticular,inthe positivephase,twounitsthatfrequentlyactivatetogetherhavetheirconnection strengthened.ThisisanexampleofaHebbianlearningrule(,)oftenHebb1949 summarizedwiththemnemonic‚ÄúÔ¨Åretogether,wiretogether.‚Äù Hebbian learning rulesareamongtheoldesthypothesizedexplanationsforlearninginbiological systemsandremainrelevanttoday( ,). Giudice e t a l .2009 Otherlearningalgorithmsthatusemoreinformationthanlocalstatisticsseem torequireustohypothesizetheexistenceofmoremachinerythanthis.For example,forthebraintoimplementback-propagation inamultilayerperceptron, itseemsnecessaryforthebraintomaintainasecondarycommunication networkfor transmittinggradientinformationbackwardsthroughthenetwork.Proposalsfor biologicallyplausibleimplementations(andapproximations)ofback-propagation havebeenmade(,;,)butremaintobevalidated,and Hinton2007aBengio2015 Bengio2015()linksback-propagationofgradientstoinferenceinenergy-based modelssimilartotheBoltzmannmachine(butwithcontinuouslatentvariables). ThenegativephaseofBoltzmannmachinelearningissomewhatharderto explainfromabiologicalpointofview.Asarguedinsection,dreamsleep 18.2 maybeaformofnegativephasesampling.Thisideaismorespeculativethough. 20.2RestrictedBoltzmannMachines Inventedunderthenameharmonium(,),restrictedBoltzmann Smolensky1986 machinesaresomeofthemostcommonbuildingblocksofdeepprobabilisticmodels. WehavebrieÔ¨ÇydescribedRBMspreviously,insection.Herewereviewthe 16.7.1 previousinformationandgointomoredetail.RBMsareundirectedprobabilistic graphicalmodelscontainingalayerofobservablevariablesandasinglelayerof latentvariables.RBMsmaybestacked(oneontopoftheother)toformdeeper models.SeeÔ¨Ågureforsomeexamples.Inparticular,Ô¨Ågureashowsthe 20.1 20.1 graphstructureoftheRBMitself.Itisabipartitegraph,withnoconnections permittedbetweenanyvariablesintheobservedlayerorbetweenanyunitsinthe latentlayer. 6 5 6 CHAPTER20.DEEPGENERATIVEMODELS h 1 h 1 h 2 h 2 h 3 h 3 v 1 v 1 v 2 v 2 v 3 v 3h 4 h 4 h( 1 ) 1 h( 1 ) 1 h( 1 ) 2 h( 1 ) 2 h( 1 ) 3 h( 1 ) 3 v 1 v 1 v 2 v 2 v 3 v 3h( 2 ) 1 h( 2 ) 1 h( 2 ) 2 h( 2 ) 2h( 2 ) 3h( 2 ) 3 h( 1 ) 4 h( 1 ) 4 (a) (b) h( 1 ) 1h( 1 ) 1h( 1 ) 2h( 1 ) 2h( 1 ) 3 h( 1 ) 3 v 1 v 1 v 2 v 2 v 3 v 3h( 2 ) 1 h( 2 ) 1 h( 2 ) 2 h( 2 ) 2 h( 2 ) 3 h( 2 ) 3 h( 1 ) 4h( 1 ) 4 (c) Figure20.1:ExamplesofmodelsthatmaybebuiltwithrestrictedBoltzmannmachines. ( a )TherestrictedBoltzmannmachineitselfisanundirectedgraphicalmodelbasedon abipartitegraph,withvisibleunitsinonepartofthegraphandhiddenunitsinthe otherpart.Therearenoconnectionsamongthevisibleunits,noranyconnectionsamong thehiddenunits. Typicallyeveryvisibleunitisconnectedtoeveryhiddenunitbutit ispossibletoconstructsparselyconnectedRBMssuchasconvolutionalRBMs.A ( b ) deepbeliefnetworkisahybridgraphicalmodelinvolvingbothdirectedandundirected connections.LikeanRBM,ithasnointralayerconnections.However,aDBNhasmultiple hiddenlayers,andthusthereareconnectionsbetweenhiddenunitsthatareinseparate layers.Allofthelocalconditionalprobabilitydistributionsneededbythedeepbelief networkarecopieddirectlyfromthelocalconditionalprobabilitydistributionsofits constituentRBMs.Alternatively,wecouldalsorepresentthedeepbeliefnetworkwith acompletelyundirectedgraph,butitwouldneedintralayerconnectionstocapturethe dependenciesbetweenparents.AdeepBoltzmannmachineisanundirectedgraphical ( c ) modelwithseverallayersoflatentvariables.LikeRBMsandDBNs,DBMslackintralayer connections. DBMsarelesscloselytiedtoRBMsthanDBNsare. Wheninitializinga DBMfromastackofRBMs,itisnecessarytomodifytheRBMparametersslightly.Some kindsofDBMsmaybetrainedwithoutÔ¨ÅrsttrainingasetofRBMs. 6 5 7 CHAPTER20.DEEPGENERATIVEMODELS WebeginwiththebinaryversionoftherestrictedBoltzmannmachine,butas weseelaterthereareextensionstoothertypesofvisibleandhiddenunits. Moreformally,lettheobservedlayerconsistofasetof n vbinaryrandom variableswhichwerefertocollectivelywiththevectorv.Werefertothelatentor hiddenlayerof n hbinaryrandomvariablesas. h LikethegeneralBoltzmannmachine,therestrictedBoltzmannmachineisan energy-basedmodelwiththejointprobabilitydistributionspeciÔ¨Åedbyitsenergy function: P , (= v vh= ) = h1 Zexp(( )) ‚àí E v h , . (20.4) TheenergyfunctionforanRBMisgivenby E ,( v h b ) = ‚àíÓÄæv c‚àíÓÄæh v‚àíÓÄæW h , (20.5) andisthenormalizingconstantknownasthepartitionfunction: Z Z=ÓÅò vÓÅò hexp ( ) {‚àí E v h ,} . (20.6) ItisapparentfromthedeÔ¨Ånitionofthepartitionfunction Zthatthenaivemethod ofcomputing Z(exhaustivelysummingoverallstates)couldbecomputationally intractable,unlessacleverlydesignedalgorithmcouldexploitregularitiesinthe probabilitydistributiontocompute Zfaster.InthecaseofrestrictedBoltzmann machines, ()formallyprovedthatthepartitionfunction LongandServedio2010 Z isintractable.Theintractablepartitionfunction Zimpliesthatthenormalized jointprobabilitydistributionisalsointractabletoevaluate. P() v 20.2.1ConditionalDistributions Though P( v)isintractable,thebipartitegraphstructureoftheRBMhasthe veryspecialpropertythatitsconditionaldistributions P(hv|)and P(vh|)are factorialandrelativelysimpletocomputeandtosamplefrom. Derivingtheconditionaldistributionsfromthejointdistributionisstraightfor- ward: P( ) = h v|P ,( h v) P() v(20.7) =1 P() v1 ZexpÓÅÆ bÓÄæv c+ÓÄæh v+ÓÄæW hÓÅØ (20.8) =1 ZÓÄ∞expÓÅÆ cÓÄæh v+ÓÄæW hÓÅØ (20.9) 6 5 8 CHAPTER20.DEEPGENERATIVEMODELS =1 ZÓÄ∞expÔ£± Ô£≤ Ô£≥n hÓÅò j = 1c j h j+n hÓÅò j = 1vÓÄæW : , j h jÔ£º Ô£Ω Ô£æ(20.10) =1 ZÓÄ∞n hÓÅô j = 1expÓÅÆ c j h</div>
        </div>
    </div>

    <div class="question-card" id="q44">
        <div class="question-header">
            <span class="question-number">Question 44</span>
            <span class="question-type">multiple-choice</span>
        </div>

        <div class="question-text">In the field of machine learning, techniques such as one-shot, zero-shot, and semi-supervised learning allow models to generalize from limited labeled data by leveraging advanced forms of representation learning. These approaches are especially valuable when labeled data is scarce or when models must operate across diverse modalities.

Which of the following best describes how zero-shot learning enables a model to recognize new classes without labeled examples?

1) By training on vast amounts of labeled data covering all possible classes   
2) By using ensemble methods to combine predictions from multiple models   
3) By generating synthetic labeled data for unseen classes   
4) By clustering input data based solely on raw pixel similarities   
5) By relying on transfer learning from pre-trained supervised models   
6) By leveraging unsupervised learning to create completely disentangled representations   
7) By utilizing auxiliary information such as semantic descriptions or attribute embeddings to link unseen classes to learned representations </div>

        <div class="answer-section">
            <div class="answer-label">‚úì Correct Answer:</div>
            <div class="answer-text">The correct answer is 7) By utilizing auxiliary information such as semantic descriptions or attribute embeddings to link unseen classes to learned representations.</div>
        </div>

        <div class="reference-section">
            <div class="reference-label">üìö Reference Text:</div>
            <button class="toggle-button" onclick="toggleReference(44)">
                Show/Hide Reference
            </button>
            <div id="ref44" class="reference-text hidden">o - sho t l e ar ni ng,sometimesalsocalled z e r o - dat a l e ar ni ng.Onlyonelabeledexample ofthetransfertaskisgivenforone-shotlearning,whilenolabeledexamplesare givenatallforthezero-shotlearningtask. One-shotlearning(Fei-Fei2006etal.,)ispossiblebecausetherepresentation learnstocleanlyseparatetheunderlyingclassesduringtheÔ¨Årststage.Duringthe transferlearningstage,onlyonelabeledexampleisneededtoinferthelabelofmany possibletestexamplesthatallclusteraroundthesamepointinrepresentation space.Thisworkstotheextentthatthefactorsofvariationcorrespondingto theseinvarianceshavebeencleanlyseparatedfromtheotherfactors,inthelearned representationspace,andwehavesomehowlearnedwhichfactorsdoanddonot matterwhendiscriminatingobjectsofcertaincategories. Asanexampleofazero-shotlearningsetting,considertheproblemofhaving alearnerreadalargecollectionoftextandthensolveobjectrecognitionproblems. 5 3 8 CHAPTER15.REPRESENTATIONLEARNING ItmaybepossibletorecognizeaspeciÔ¨Åcobjectclassevenwithouthavingseenan imageofthatobject,ifthetextdescribestheobjectwellenough. Forexample, havingreadthatacathasfourlegsandpointyears,thelearnermightbeableto guessthatanimageisacat,withouthavingseenacatbefore. Zero-datalearning(Larochelle2008 Palatucci etal.,)andzero-shotlearning( etal.,;2009Socher2013betal.,)areonlypossiblebecauseadditionalinformation hasbeenexploitedduringtraining.Wecanthinkofthezero-datalearningscenario asincludingthreerandomvariables:thetraditionalinputsx,thetraditional outputsortargetsy,andanadditionalrandomvariabledescribingthetask,T. Themodelistrainedtoestimatetheconditionaldistributionp(yx|,T)where Tisadescriptionofthetaskwewishthemodeltoperform. Inourexampleof recognizingcatsafterhavingreadaboutcats,theoutputisabinaryvariabley withy= 1indicating‚Äúyes‚Äùandy= 0indicating‚Äúno.‚ÄùThetaskvariableTthen representsquestionstobeansweredsuchas‚ÄúIsthereacatinthisimage?‚ÄùIfwe haveatrainingsetcontainingunsupervisedexamplesofobjectsthatliveinthe samespaceasT,wemaybeabletoinferthemeaningofunseeninstancesofT. Inourexampleofrecognizingcatswithouthavingseenanimageofthecat,itis importantthatwehavehadunlabeledtextdatacontainingsentencessuchas‚Äúcats havefourlegs‚Äùor‚Äúcatshavepointyears.‚Äù Zero-shotlearningrequiresTtoberepresentedinawaythatallowssomesort ofgeneralization. Forexample,Tcannotbejustaone-hotcodeindicatingan objectcategory. ()provideinsteadadistributedrepresentation Socheretal.2013b ofobjectcategoriesbyusingalearnedwordembeddingforthewordassociated witheachcategory. Asimilarphenomenon happensinmachinetranslation(Klementiev2012etal.,; Mikolov2013bGouws2014 etal.,; etal.,):wehavewordsinonelanguage,and therelationshipsbetweenwordscanbelearnedfromunilingualcorpora;onthe otherhand,wehavetranslatedsentenceswhichrelatewordsinonelanguagewith wordsintheother.Eventhoughwemaynothavelabeledexamplestranslating wordAinlanguageXtowordBinlanguageY,wecangeneralizeandguessa translationforwordAbecausewehavelearnedadistributedrepresentationfor wordsinlanguageX,adistributedrepresentationforwordsinlanguageY,and createdalink(possiblytwo-way)relatingthetwospaces,viatrainingexamples consistingofmatchedpairsofsentencesinbothlanguages.Thistransferwillbe mostsuccessfulifallthreeingredients(thetworepresentationsandtherelations betweenthem)arelearnedjointly. Zero-shotlearningisaparticularformoftransferlearning.Thesameprinciple explainshowonecanperform m ul t i - m o dal l e ar ni ng,capturingarepresentation 5 3 9 CHAPTER15.REPRESENTATIONLEARNING h x = f x ( ) x x t e s t y t e s th y = f y ( ) y y ‚àí s pa ce R e l at i onshi p b e t w e e n e m be dde d p oi n t s w i t hi n one o f t h e d o m a i n s Maps be t w e e n r e p r e s e n t at i on spac e s f x f y x ‚àí s pa ce ( ) pa i r s i n t he t r a i ni ng s et x y , f x : enco der f unctio n f o r x f y : enco der f unctio n f o r y Figure15.3:Transferlearningbetweentwodomainsxandyenableszero-shotlearning. Labeledorunlabeledexamplesofxallowonetolearnarepresentationfunctionf xand similarlywithexamplesofytolearnf y.Eachapplicationofthef xandf yfunctions appearsasanupwardarrow,withthestyleofthearrowsindicatingwhichfunctionis applied.Distanceinhxspaceprovidesasimilaritymetricbetweenanypairofpoints inxspacethatmaybemoremeaningfulthandistanceinxspace.Likewise,distance inh yspaceprovidesasimilaritymetricbetweenanypairofpointsinyspace.Both ofthesesimilarityfunctionsareindicatedwithdottedbidirectionalarrows.Labeled examples(dashedhorizontallines)arepairs(xy,)whichallowonetolearnaone-way ortwo-waymap(solidbidirectionalarrow)betweentherepresentationsf x(x)andthe representationsf y(y)andanchortheserepresentationstoeachother.Zero-datalearning isthenenabledasfollows.Onecanassociateanimagex t e s ttoawordy t e s t,evenifno imageofthatwordwaseverpresented,simplybecauseword-representationsfy(yt e s t) andimage-representationsf x(x t e s t)canberelatedtoeachotherviathemapsbetween representationspaces.Itworksbecause,althoughthatimageandthatwordwerenever paired,theirrespectivefeaturevectorsf x(x t e s t)andf y(y t e s t)havebeenrelatedtoeach other.FigureinspiredfromsuggestionbyHrantKhachatrian. 5 4 0 CHAPTER15.REPRESENTATIONLEARNING inonemodality,arepresentationintheother,andtherelationship(ingeneralajoint distribution)betweenpairs (xy,)consistingofoneobservationxinonemodality andanotherobservationyintheothermodality(SrivastavaandSalakhutdino v, 2012).Bylearningallthreesetsofparameters(fromxtoitsrepresentation,from ytoitsrepresentation,andtherelationshipbetweenthetworepresentations), conceptsinonerepresentationareanchoredintheother,andvice-versa,allowing onetomeaningfully generalizeto newpairs.Theprocedureis illustratedin Ô¨Ågure.15.3 15. 3 S em i - S u p ervi s ed D i s en t a n g l i n g of C au s al F ac t ors Animportantquestionaboutrepresentationlearningis‚Äúwhatmakesonerepre- sentationbetterthananother?‚ÄùOnehypothesisisthatanidealrepresentation isoneinwhichthefeatureswithintherepresentationcorrespondtotheunder- lyingcausesoftheobserveddata,withseparatefeaturesordirectionsinfeature spacecorrespondingtodiÔ¨Äerentcauses,sothattherepresentationdisentanglesthe causesfromoneanother.ThishypothesismotivatesapproachesinwhichweÔ¨Årst seekagoodrepresentationforp(x). Sucharepresentationmayalsobeagood representationforcomputingp(yx|)ifyisamongthemostsalientcausesof x. Thisideahasguidedalargeamountofdeeplearningresearchsinceatleast the1990s(BeckerandHinton1992HintonandSejnowski1999 ,; ,),inmoredetail. Forotherargumentsaboutwhensemi-supervisedlearningcanoutperformpure supervisedlearning,wereferthereadertosection1.2of (). Chapelleetal.2006 Inotherapproachestorepresentationlearning,wehaveoftenbeenconcerned witharepresentationthatiseasytomodel‚Äîforexample,onewhoseentriesare sparse,orindependentfromeachother.Arepresentationthatcleanlyseparates theunderlyingcausalfactorsmaynotnecessarilybeonethatiseasytomodel. However,afurtherpartofthehypothesismotivatingsemi-supervisedlearning viaunsupervisedrepresentationlearningisthatformanyAItasks,thesetwo propertiescoincide: once weareabletoobtaintheunderlyingexplanationsfor whatweobserve,itgenerallybecomeseasytoisolateindividualattributesfrom theothers.SpeciÔ¨Åcally,ifarepresentationhrepresentsmanyoftheunderlying causesoftheobservedx,andtheoutputsyareamongthemostsalientcauses, thenitiseasytopredictfrom.yh First,letusseehowsemi-supervisedlearningcanfailbecauseunsupervised learningofp(x)isofnohelptolearnp(yx|).Considerforexamplethecase wherep(x)isuniformlydistributedandwewanttolearnf(x) = E[y|x].Clearly, observingatrainingsetofvaluesalonegivesusnoinformationabout. x p( )y x| 5 4 1 CHAPTER15.REPRESENTATIONLEARNING xp x ( )y = 1 y = 2 y = 3 Figure15.4:Exampleofadensityoverxthatisamixtureoverthree components. Thecomponentidentityisanunderlyingexplanatoryfactor,y.Becausethemixture components(e.g., naturalobjectclassesinimagedata)arestatisticallysalient,just modelingp(x)inanunsupervisedwaywithnolabeledexamplealreadyrevealsthefactor y. Next,letusseeasimpleexampleofhowsemi-supervisedlearningcansucceed. Considerthesituationwhere xarisesfromamixture,withonemixturecomponent pervalueofy,asillustratedinÔ¨Ågure. Ifthemixturecomponentsarewell- 15.4 separated,thenmodelingp(x)revealspreciselywhereeachcomponentis,anda singlelabeledexampleofeachclasswillthenbeenoughtoperfectlylearnp(yx|). Butmoregenerally,whatcouldmakeandbetiedtogether? p( )y x|p()x Ifyiscloselyassociatedwithoneofthecausalfactorsofx,thenp(x)and p(yx|)will bestronglytied, andunsupervisedrepresentationlearningthat triestodisentangletheunderlyingfactorsofvariationislikelytobeusefulasa semi-supervisedlearningstrategy. Considertheassumptionthatyisoneofthecausalfactorsofx,andlet hrepresentallthosefactors.Thetruegenerativeprocesscanbeconceivedas structuredaccordingtothisdirectedgraphicalmodel,withastheparentof: h x p,pp. (hx) = ( )xh|()h (15.1) Asaconsequence,thedatahasmarginalprobability p() = x E hp. ( )xh| (15.2) Fromthisstraightforwardobservation,weconcludethatthebestpossiblemodel ofx(fromageneralization pointofview)istheonethatuncoverstheabove‚Äútrue‚Äù 5 4 2 CHAPTER15.REPRESENTATIONLEARNING structure,withhasalatentvariablethatexplainstheobservedvariationsinx. The‚Äúideal‚Äùrepresentationlearningdiscussedaboveshouldthusrecovertheselatent factors.Ifyisoneofthese(orcloselyrelatedtooneofthem),thenitwillbe veryeasytolearntopredict yfromsucharepresentation.Wealsoseethatthe conditionaldistributionofygivenxistiedbyBayes‚Äôruletothecomponentsin theaboveequation: p( ) = yx|pp ( )xy|()y p()x. (15.3) Thusthemarginalp(x) isintimatelytiedtotheconditionalp(yx|) andknowledge ofthestructureoftheformershouldbehelpfultolearnthelatter.Therefore,in situationsrespectingtheseassumptions,semi-supervisedlearningshouldimprove performance. Animportantresearchproblemregardsthefactthatmostobservationsare formedbyanextremelylargenumberofunderlyingcauses.Supposey=h i,but theunsupervisedlearnerdoesnotknowwhichh i.Thebruteforcesolutionisfor anunsupervisedlearnertolearnarepresentationthatcapturesthereasonably all salientgenerativefactorsh janddisentanglesthemfromeachother,thusmaking iteasytopredictfrom,regardlessofwhichh y h iisassociatedwith.y Inpractice,thebruteforcesolutionisnotfeasiblebecauseitisnotpossible tocaptureallormostofthefactorsofvariationthatinÔ¨Çuenceanobservation. Forexample,inavisualscene,shouldtherepresentationalwaysencodeallof thesmallestobjectsinthebackground? Itisawell-documented</div>
        </div>
    </div>

    <div class="question-card" id="q45">
        <div class="question-header">
            <span class="question-number">Question 45</span>
            <span class="question-type">multiple-choice</span>
        </div>

        <div class="question-text">Recurrent neural networks (RNNs) have evolved to address challenges in learning long-term dependencies within sequential data, leading to innovations such as gated architectures and specialized connectivity patterns. Understanding these developments is crucial for designing stable and effective sequence models.

Which architectural feature in RNNs is specifically designed to dynamically regulate the flow and retention of information, enabling the network to learn when to remember or forget data over long sequences?

1) Using purely linear activation functions in hidden units   
2) Initializing recurrent weight matrices with very small values   
3) Employing skip connections between adjacent time steps only   
4) Incorporating only fixed leaky units with non-learnable time constants   
5) Removing all recurrent connections to avoid gradient issues   
6) Increasing the spectral radius far above one without gating   
7) Implementing gating mechanisms such as input, forget, and output gates in LSTM cells </div>

        <div class="answer-section">
            <div class="answer-label">‚úì Correct Answer:</div>
            <div class="answer-text">The correct answer is 7) Implementing gating mechanisms such as input, forget, and output gates in LSTM cells.</div>
        </div>

        <div class="reference-section">
            <div class="reference-label">üìö Reference Text:</div>
            <button class="toggle-button" onclick="toggleReference(45)">
                Show/Hide Reference
            </button>
            <div id="ref45" class="reference-text hidden">CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS behavior(ifthesameJacobianwasappliediteratively).Eventhoughh( ) tora smallvariationofh( ) tofinterestinback-propagation arereal-valued,theycan beexpressedinsuchacomplex-valuedbasis.Whatmattersiswhathappensto themagnitude(complexabsolutevalue)ofthesepossiblycomplex-valuedbasis coeÔ¨Écients, whenwemultiplythematrixbythevector.Aneigenvaluewith magnitudegreaterthanonecorrespondstomagniÔ¨Åcation (exponentialgrowth,if appliediteratively)orshrinking(exponentialdecay,ifappliediteratively). Withanonlinearmap, theJacobianisfreetochangeateachstep.The dynamicsthereforebecomemorecomplicated.However,itremainstruethata smallinitialvariationcanturnintoalargevariationafterseveralsteps.One diÔ¨Äerencebetweenthepurelylinearcaseandthenonlinearcaseisthattheuseof asquashingnonlinearitysuchastanhcancausetherecurrentdynamicstobecome bounded.Notethat itispossible forback-propagation to retainunbounded dynamicsevenwhenforwardpropagationhasboundeddynamics,forexample, whenasequenceoftanhunitsareallinthemiddleoftheirlinearregimeandare connectedbyweightmatriceswithspectralradiusgreaterthan.However,itis 1 rareforalloftheunitstosimultaneouslylieattheirlinearactivationpoint. tanh ThestrategyofechostatenetworksissimplytoÔ¨Åxtheweightstohavesome spectralradiussuchas,whereinformationiscarriedforwardthroughtimebut 3 doesnotexplodeduetothestabilizingeÔ¨Äectofsaturatingnonlinearities liketanh. Morerecently,ithasbeenshownthatthetechniquesusedtosettheweights inESNscouldbeusedtotheweightsinafullytrainablerecurrentnet- i nit i a l i z e work(withthehidden-to-hidden recurrentweightstrainedusingback-propagation throughtime),helpingtolearnlong-termdependencies(Sutskever2012Sutskever ,; e t a l .,).Inthissetting,aninitialspectralradiusof1.2performswell,combined 2013 withthesparseinitialization schemedescribedinsection.8.4 10.9LeakyUnitsandOtherStrategiesforMultiple TimeScales Onewaytodealwithlong-termdependencies istodesignamodelthatoperates atmultipletimescales,sothatsomepartsofthemodeloperateatÔ¨Åne-grained timescalesandcanhandlesmalldetails,whileotherpartsoperateatcoarsetime scalesandtransferinformationfromthedistantpasttothepresentmoreeÔ¨Éciently. VariousstrategiesforbuildingbothÔ¨Åneandcoarsetimescalesarepossible.These includetheadditionofskipconnectionsacrosstime,‚Äúleakyunits‚Äùthatintegrate signalswithdiÔ¨Äerenttimeconstants,andtheremovalofsomeoftheconnections 4 0 6 CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS usedtomodelÔ¨Åne-grainedtimescales. 10.9.1AddingSkipConnectionsthroughTime Onewaytoobtaincoarsetimescalesistoadddirectconnectionsfromvariablesin thedistantpasttovariablesinthepresent.Theideaofusingsuchskipconnections datesbackto()andfollowsfromtheideaofincorporatingdelaysin Lin e t a l .1996 feedforwardneuralnetworks( ,).Inanordinaryrecurrent LangandHinton1988 network,arecurrentconnectiongoesfromaunitattime ttoaunitattime t+1. Itispossibletoconstructrecurrentnetworkswithlongerdelays(,). Bengio1991 Aswehaveseeninsection,gradientsmayvanishorexplodeexponentially 8.2.5 w i t h r e s p e c t t o t h e nu m b e r o f t i m e s t e p s.()introducedrecurrent Lin e t a l .1996 connectionswithatime-delayof dtomitigatethisproblem.Gradientsnow diminishexponentiallyasafunctionofœÑ dratherthan œÑ.Sincethereareboth delayedandsinglestepconnections,gradientsmaystillexplodeexponentiallyin œÑ. Thisallowsthelearningalgorithmtocapturelongerdependenciesalthoughnotall long-termdependencies mayberepresentedwellinthisway. 10.9.2LeakyUnitsandaSpectrumofDiÔ¨ÄerentTimeScales Anotherwaytoobtainpathsonwhichtheproductofderivativesisclosetooneisto haveunitswith l i ne a rself-connectionsandaweightnearoneontheseconnections. Whenweaccumulatearunningaverage ¬µ( ) tofsomevalue v( ) tbyapplyingthe update ¬µ( ) t‚Üê Œ± ¬µ( 1 ) t ‚àí+(1‚àí Œ±) v( ) tthe Œ±parameterisanexampleofalinearself- connectionfrom ¬µ( 1 ) t ‚àíto ¬µ( ) t.When Œ±isnearone,therunningaverageremembers informationaboutthepastforalongtime,andwhen Œ±isnearzero,information aboutthepastisrapidlydiscarded.Hiddenunitswithlinearself-connectionscan behavesimilarlytosuchrunningaverages.Suchhiddenunitsarecalledleaky units. Skipconnectionsthrough dtimestepsareawayofensuringthataunitcan alwayslearntobeinÔ¨Çuencedbyavaluefrom dtimestepsearlier.Theuseofa linearself-connectionwithaweightnearoneisadiÔ¨Äerentwayofensuringthatthe unitcanaccessvaluesfromthepast.Thelinearself-connectionapproachallows thiseÔ¨ÄecttobeadaptedmoresmoothlyandÔ¨Çexiblybyadjustingthereal-valued Œ±ratherthanbyadjustingtheinteger-valuedskiplength. Theseideaswereproposedby()andby (). Mozer1992 ElHihiandBengio1996 Leakyunitswerealsofoundtobeusefulinthecontextofechostatenetworks (,). Jaeger e t a l .2007 4 0 7 CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS Therearetwobasicstrategiesforsettingthetimeconstantsusedbyleaky units. OnestrategyistomanuallyÔ¨Åxthemtovaluesthatremainconstant,for examplebysamplingtheirvaluesfromsomedistributiononceatinitialization time. Anotherstrategyistomakethetimeconstantsfreeparametersandlearnthem. HavingsuchleakyunitsatdiÔ¨Äerenttimescalesappearstohelpwithlong-term dependencies(,;Mozer1992Pascanu2013 e t a l .,). 10.9.3RemovingConnections Anotherapproachtohandlelong-termdependenciesistheideaoforganizing thestateoftheRNNatmultipletime-scales( ,),with ElHihiandBengio1996 informationÔ¨Çowingmoreeasilythroughlongdistancesattheslowertimescales. ThisideadiÔ¨Äersfromtheskipconnectionsthroughtimediscussedearlier becauseitinvolvesactively r e m o v i nglength-oneconnectionsandreplacingthem withlongerconnections.UnitsmodiÔ¨Åedinsuchawayareforcedtooperateona longtimescale.Skipconnectionsthroughtimeedges.Unitsreceivingsuch a d d newconnectionsmaylearntooperateonalongtimescalebutmayalsochooseto focusontheirothershort-termconnections. TherearediÔ¨Äerentwaysinwhichagroupofrecurrentunitscanbeforcedto operateatdiÔ¨Äerenttimescales.Oneoptionistomaketherecurrentunitsleaky, buttohavediÔ¨ÄerentgroupsofunitsassociatedwithdiÔ¨ÄerentÔ¨Åxedtimescales. Thiswastheproposalin()andhasbeensuccessfullyusedin Mozer1992 Pascanu e t a l .().Anotheroptionistohaveexplicitanddiscreteupdatestakingplace 2013 atdiÔ¨Äerenttimes,withadiÔ¨ÄerentfrequencyfordiÔ¨Äerentgroupsofunits.Thisis theapproachof ()and ElHihiandBengio1996Koutnik 2014 e t a l .().Itworked wellonanumberofbenchmarkdatasets. 10.10TheLongShort-TermMemoryandOtherGated RNNs Asofthiswriting,themosteÔ¨Äectivesequencemodelsusedinpracticalapplications arecalledgatedRNNs.Theseincludethelongshort-termmemoryand networksbasedonthe . gatedrecurrentunit Likeleakyunits,gatedRNNsarebasedontheideaofcreatingpathsthrough timethathavederivativesthatneithervanishnorexplode.Leakyunits did thiswithconnectionweightsthatwereeithermanuallychosenconstantsorwere parameters.GatedRNNsgeneralizethistoconnectionweightsthatmaychange 4 0 8 CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS ateachtimestep. √ó i nput i nput gate f or ge t gate output gateoutput s t at es e l f - l oop√ó + √ó Figure10.16:BlockdiagramoftheLSTMrecurrentnetwork‚Äúcell.‚ÄùCellsareconnected recurrentlytoeachother,replacingtheusualhiddenunitsofordinaryrecurrentnetworks. AninputfeatureiscomputedwitharegularartiÔ¨Åcialneuronunit.Itsvaluecanbe accumulatedintothestateifthesigmoidalinputgateallowsit.Thestateunithasa linearself-loopwhoseweightiscontrolledbytheforgetgate.Theoutputofthecellcan beshutoÔ¨Äbytheoutputgate.Allthegatingunitshaveasigmoidnonlinearity,whilethe inputunitcanhaveanysquashingnonlinearity. Thestateunitcanalsobeusedasan extrainputtothegatingunits.Theblacksquareindicatesadelayofasingletimestep. Leakyunitsallowthenetworkto a c c u m u l a t einformation(suchasevidence foraparticularfeatureorcategory)overalongduration.However,oncethat informationhasbeenused,itmightbeusefulfortheneuralnetworkto f o r g e tthe oldstate.Forexample,ifasequenceismadeofsub-sequencesandwewantaleaky unittoaccumulateevidenceinsideeachsub-subsequence,weneedamechanismto forgettheoldstatebysettingittozero.Insteadofmanuallydecidingwhento clearthestate,wewanttheneuralnetworktolearntodecidewhentodoit.This 4 0 9 CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS iswhatgatedRNNsdo. 10.10.1LSTM Thecleverideaofintroducingself-loopstoproducepathswherethegradient canÔ¨Çowforlongdurationsisacorecontributionoftheinitiallongshort-term memory(LSTM)model(HochreiterandSchmidhuber1997,).Acrucialaddition hasbeentomaketheweightonthisself-loopconditionedonthecontext,ratherthan Ô¨Åxed(,).Bymakingtheweightofthisself-loopgated(controlled Gers e t a l .2000 byanotherhiddenunit),thetimescaleofintegrationcanbechangeddynamically. Inthiscase,wemeanthatevenforanLSTMwithÔ¨Åxedparameters,thetimescale ofintegrationcanchangebasedontheinputsequence,becausethetimeconstants areoutputbythemodelitself.TheLSTMhasbeenfoundextremelysuccessful inmanyapplications, suchasunconstrainedhandwriting recognition(Graves e t a l .,),speechrecognition( 2009 Graves2013GravesandJaitly2014 e t a l .,; ,), handwritinggeneration(Graves2013,),machinetranslation(Sutskever2014 e t a l .,), imagecaptioning(,; Kiros e t a l .2014bVinyals2014bXu2015 e t a l .,; e t a l .,)and parsing(Vinyals2014a e t a l .,). TheLSTMblockdiagramisillustratedinÔ¨Ågure.Thecorresponding 10.16 forwardpropagationequationsaregivenbelow,inthecaseofashallowrecurrent networkarchitecture. Deeperarchitectures havealsobeensuccessfullyused(Graves e t a l .,;2013Pascanu2014a e t a l .,).Insteadofaunitthatsimplyappliesanelement- wisenonlinearitytotheaÔ¨Énetransformationofinputsandrecurrentunits,LSTM recurrentnetworkshave‚ÄúLSTMcells‚Äùthathaveaninternalrecurrence(aself-loop), inadditiontotheouterrecurrenceoftheRNN.Eachcellhasthesameinputs andoutputsasanordinaryrecurrentnetwork,buthasmoreparametersanda systemofgatingunitsthatcontrolstheÔ¨Çowofinformation. Themostimportant componentisthestateunit s( ) t ithathasalinearself-loopsimilartotheleaky unitsdescribedintheprevioussection.However,here,theself-loopweight(orthe associatedtimeconstant)iscontrolledbyaforgetgateunit f( ) t i(fortimestep t andcell),thatsetsthisweighttoavaluebetween0and1viaasigmoidunit: i f( ) t i= œÉÔ£´ Ô£≠ bf i+ÓÅò jUf i , j x( ) t j+ÓÅò jWf i , j h( 1 ) t ‚àí jÔ£∂ Ô£∏ ,(10.40) wherex( ) tisthecurrentinputvectorandh( ) tisthecurrenthiddenlayervector, containingtheoutputsofalltheLSTMcells,andbf,Uf,Wfarerespectively biases,inputweightsandrecurrentweightsfortheforgetgates.TheLSTMcell 4 1 0 CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS internalstateisthusupdatedasfollows,butwithaconditionalself-loopweight f( ) t i: s( ) t i= f( ) t i s( 1 )</div>
        </div>
    </div>

    <div class="question-card" id="q46">
        <div class="question-header">
            <span class="question-number">Question 46</span>
            <span class="question-type">multiple-choice</span>
        </div>

        <div class="question-text">Deep Boltzmann Machines (DBMs) are probabilistic graphical models used for unsupervised learning, and have been extended to handle real-valued data through models such as the Gaussian-Bernoulli Restricted Boltzmann Machine (RBM). Advanced training techniques and model architectures have improved their optimization and representation capabilities.

Which model specifically addresses the limitation of Gaussian RBMs by introducing separate hidden units for representing both the mean and covariance of observed data?

1) Variational Autoencoder (VAE)   
2) Deep Belief Network (DBN)   
3) Stochastic Maximum Likelihood (SML) RBM   
4) Multi-Prediction Deep Boltzmann Machine (MP-DBM)   
5) Mean and Covariance RBM (mcRBM)   
6) Dropout-regularized Boltzmann Machine   
7) Gaussian-Bernoulli RBM</div>

        <div class="answer-section">
            <div class="answer-label">‚úì Correct Answer:</div>
            <div class="answer-text">The correct answer is 5) Mean and Covariance RBM (mcRBM).</div>
        </div>

        <div class="reference-section">
            <div class="reference-label">üìö Reference Text:</div>
            <button class="toggle-button" onclick="toggleReference(46)">
                Show/Hide Reference
            </button>
            <div id="ref46" class="reference-text hidden">7 3 CHAPTER20.DEEPGENERATIVEMODELS inaHessianmatrixthatisbetterconditioned. ()experimentally Melchior e t a l .2013 conÔ¨ÅrmedthattheconditioningoftheHessianmatriximproves,andobservedthat thecenteringtrickisequivalenttoanotherBoltzmannmachinelearningtechnique, theenhancedgradient(,).Theimprovedconditioningofthe Cho e t a l .2011 Hessianmatrixallowslearningtosucceed,evenindiÔ¨Écultcasesliketraininga deepBoltzmannmachinewithmultiplelayers. TheotherapproachtojointlytrainingdeepBoltzmannmachinesisthemulti- predictiondeepBoltzmannmachine(MP-DBM)whichworksbyviewingthemean Ô¨ÅeldequationsasdeÔ¨Åningafamilyofrecurrentnetworksforapproximately solving everypossibleinferenceproblem( ,).Ratherthantraining Goodfellow e t a l .2013b themodeltomaximizethelikelihood,themodelistrainedtomakeeachrecurrent networkobtainanaccurateanswertothecorrespondinginferenceproblem.The trainingprocessisillustratedinÔ¨Ågure. Itconsistsofrandomlysamplinga 20.5 trainingexample,randomlysamplingasubsetofinputstotheinferencenetwork, andthentrainingtheinferencenetworktopredictthevaluesoftheremaining units. Thisgeneralprincipleofback-propagating throughthecomputational graph forapproximateinferencehasbeenappliedtoothermodels(Stoyanov2011 e t a l .,; Brakel2013 e t a l .,).InthesemodelsandintheMP-DBM,theÔ¨Ånallossisnot thelowerboundonthelikelihood.Instead,theÔ¨Ånallossistypicallybasedon theapproximateconditionaldistributionthattheapproximate inferencenetwork imposesoverthemissingvalues.Thismeansthatthetrainingofthesemodels issomewhatheuristicallymotivated.Ifweinspectthe p( v)representedbythe BoltzmannmachinelearnedbytheMP-DBM,ittendstobesomewhatdefective, inthesensethatGibbssamplingyieldspoorsamples. Back-propagationthroughtheinferencegraphhastwomainadvantages.First, ittrainsthemodelasitisreallyused‚Äîwithapproximate inference.Thismeans thatapproximateinference,forexample,toÔ¨Ållinmissinginputs,ortoperform classiÔ¨Åcationdespitethepresenceofmissinginputs,ismoreaccurateintheMP- DBMthanintheoriginalDBM.TheoriginalDBMdoesnotmakeanaccurate classiÔ¨Åeronitsown;thebestclassiÔ¨ÅcationresultswiththeoriginalDBMwere basedontrainingaseparateclassiÔ¨ÅertousefeaturesextractedbytheDBM, ratherthanbyusinginferenceintheDBMtocomputethedistributionoverthe classlabels.MeanÔ¨ÅeldinferenceintheMP-DBMperformswellasaclassiÔ¨Åer withoutspecialmodiÔ¨Åcations.Theotheradvantageofback-propagating through approximateinferenceisthatback-propagationcomputestheexactgradientof theloss.Thisisbetterforoptimization thantheapproximate gradientsofSML training,whichsuÔ¨Äerfrombothbiasandvariance.ThisprobablyexplainswhyMP- 6 7 4 CHAPTER20.DEEPGENERATIVEMODELS Figure20.5:Anillustrationofthemulti-predictiontrainingprocessforadeepBoltzmann machine.EachrowindicatesadiÔ¨Äerentexamplewithinaminibatchforthesametraining step. EachcolumnrepresentsatimestepwithinthemeanÔ¨Åeldinferenceprocess. For eachexample,wesampleasubsetofthedatavariablestoserveasinputstotheinference process.Thesevariablesareshadedblacktoindicateconditioning.Wethenrunthe meanÔ¨Åeldinferenceprocess,witharrowsindicatingwhichvariablesinÔ¨Çuencewhichother variablesintheprocess.Inpracticalapplications,weunrollmeanÔ¨Åeldforseveralsteps. Inthisillustration,weunrollforonlytwosteps.Dashedarrowsindicatehowtheprocess couldbeunrolledformoresteps.Thedatavariablesthatwerenotusedasinputstothe inferenceprocessbecometargets,shadedingray.Wecanviewtheinferenceprocessfor eachexampleasarecurrentnetwork.Weusegradientdescentandback-propagationto traintheserecurrentnetworkstoproducethecorrecttargetsgiventheirinputs.This trainsthemeanÔ¨ÅeldprocessfortheMP-DBMtoproduceaccurateestimates.Figure adaptedfrom (). Goodfellow e t a l .2013b 6 7 5 CHAPTER20.DEEPGENERATIVEMODELS DBMsmaybetrainedjointlywhileDBMsrequireagreedylayer-wisepretraining. Thedisadvantageofback-propagatingthroughtheapproximate inferencegraphis thatitdoesnotprovideawaytooptimizethelog-likelihood,butratheraheuristic approximationofthegeneralizedpseudolikelihood. TheMP-DBMinspiredtheNADE- k(Raiko2014 e t a l .,)extensiontothe NADEframework,whichisdescribedinsection.20.10.10 TheMP-DBMhassomeconnectionstodropout.Dropoutsharesthesamepa- rametersamongmanydiÔ¨Äerentcomputational graphs,withthediÔ¨Äerencebetween eachgraphbeingwhetheritincludesorexcludeseachunit.TheMP-DBMalso sharesparametersacrossmanycomputational graphs.InthecaseoftheMP-DBM, thediÔ¨Äerencebetweenthegraphsiswhethereachinputunitisobservedornot. Whenaunitisnotobserved,theMP-DBMdoesnotdeleteitentirelyasdropout does.Instead,theMP-DBMtreatsitasalatentvariabletobeinferred.Onecould imagineapplyingdropouttotheMP-DBMbyadditionallyremovingsomeunits ratherthanmakingthemlatent. 20.5BoltzmannMachinesforReal-ValuedData WhileBoltzmannmachineswereoriginallydevelopedforusewithbinarydata, manyapplicationssuchasimageandaudiomodelingseemtorequiretheability torepresentprobabilitydistributionsoverrealvalues.Insomecases,itispossible totreatreal-valueddataintheinterval[0,1]asrepresentingtheexpectationofa binaryvariable.Forexample, ()treatsgrayscaleimagesinthetraining Hinton2000 setasdeÔ¨Åning[0,1]probabilityvalues.EachpixeldeÔ¨Ånestheprobabilityofa binaryvaluebeing1,andthebinarypixelsareallsampledindependentlyfrom eachother.Thisisacommonprocedureforevaluatingbinarymodelsongrayscale imagedatasets.However,itisnotaparticularlytheoreticallysatisfyingapproach, andbinaryimagessampledindependentlyinthiswayhaveanoisyappearance.In thissection,wepresentBoltzmannmachinesthatdeÔ¨Åneaprobabilitydensityover real-valueddata. 20.5.1Gaussian-BernoulliRBMs RestrictedBoltzmannmachinesmaybedevelopedformanyexponentialfamily conditionaldistributions(Welling2005 e t a l .,).Ofthese,themostcommonisthe RBMwithbinaryhiddenunitsandreal-valuedvisibleunits,withtheconditional distributionoverthevisibleunitsbeingaGaussiandistributionwhosemeanisa functionofthehiddenunits. 6 7 6 CHAPTER20.DEEPGENERATIVEMODELS Therearemanywaysofparametrizing Gaussian-Bernoulli RBMs.Onechoice iswhethertouseacovariancematrixoraprecisionmatrixfortheGaussian distribution.Herewepresenttheprecisionformulation.ThemodiÔ¨Åcationtoobtain thecovarianceformulationisstraightforward. Wewishtohavetheconditional distribution p , ( ) = (; v h| N v W h Œ≤‚àí 1) . (20.38) WecanÔ¨Åndthetermsweneedtoaddtotheenergyfunctionbyexpandingthe unnormalized logconditionaldistribution: log (;N v W h Œ≤ ,‚àí 1) = ‚àí1 2( ) v W h ‚àíÓÄæŒ≤ v W h Œ≤ (‚àí )+( f) .(20.39) Here fencapsulatesallthetermsthatareafunctiononlyoftheparameters andnottherandomvariablesinthemodel.Wecandiscard fbecauseitsonly roleistonormalizethedistribution,andthepartitionfunctionofwhateverenergy functionwechoosewillcarryoutthatrole. Ifweincludealloftheterms(withtheirsignÔ¨Çipped)involving vfromequa- tioninourenergyfunctionanddonotaddanyothertermsinvolving 20.39 v,then ourenergyfunctionwillrepresentthedesiredconditional . p( ) v h| Wehavesomefreedomregardingtheotherconditionaldistribution, p( h v|). Notethatequationcontainsaterm 20.39 1 2hÓÄæWÓÄæŒ≤ W h . (20.40) Thistermcannotbeincludedinitsentiretybecauseitincludes h i h jterms.These correspondtoedgesbetweenthehiddenunits.Ifweincludedtheseterms,we wouldhavealinearfactormodelinsteadofarestrictedBoltzmannmachine.When designingourBoltzmannmachine,wesimplyomitthese h i h jcrossterms.Omitting themdoesnotchangetheconditional p( v h|)soequationisstillrespected. 20.39 However,westillhaveachoiceaboutwhethertoincludethetermsinvolvingonly asingle h i.Ifweassumeadiagonalprecisionmatrix,weÔ¨Åndthatforeachhidden unit h iwehaveaterm 1 2h iÓÅò jŒ≤ j W2 j , i . (20.41) Intheabove,weusedthefactthat h2 i= h ibecause h i‚àà{0 ,1}.Ifweincludethis term(withitssignÔ¨Çipped)intheenergyfunction,thenitwillnaturallybias h i tobeturnedoÔ¨Äwhentheweightsforthatunitarelargeandconnectedtovisible unitswithhighprecision.Thechoiceofwhetherornottoincludethisbiasterm doesnotaÔ¨Äectthefamilyofdistributionsthemodelcanrepresent(assumingthat 6 7 7 CHAPTER20.DEEPGENERATIVEMODELS weincludebiasparametersforthehiddenunits)butitdoesaÔ¨Äectthelearning dynamicsofthemodel.Includingthetermmayhelpthehiddenunitactivations remainreasonableevenwhentheweightsrapidlyincreaseinmagnitude. OnewaytodeÔ¨ÅnetheenergyfunctiononaGaussian-Bernoulli RBMisthus E ,( v h) =1 2vÓÄæ( )( ) Œ≤ vÓÄå ‚àí v Œ≤ÓÄåÓÄæW h b‚àíÓÄæh(20.42) butwemayalsoaddextratermsorparametrizetheenergyintermsofthevariance ratherthanprecisionifwechoose. Inthisderivation,wehavenotincludedabiastermonthevisibleunits,butone couldeasilybeadded.OneÔ¨Ånalsourceofvariabilityintheparametrization ofa Gaussian-Bernoulli RBMisthechoiceofhowtotreattheprecisionmatrix.Itmay eitherbeÔ¨Åxedtoaconstant(perhapsestimatedbasedonthemarginalprecision ofthedata)orlearned.Itmayalsobeascalartimestheidentitymatrix,orit maybeadiagonalmatrix.Typicallywedonotallowtheprecisionmatrixtobe non-diagonal inthiscontext,becausesomeoperationsontheGaussiandistribution requireinvertingthematrix,andadiagonalmatrixcanbeinvertedtrivially.In thesectionsahead,wewillseethatotherformsofBoltzmannmachinespermit modelingthecovariancestructure,usingvarioustechniquestoavoidinvertingthe precisionmatrix. 20.5.2UndirectedModelsofConditionalCovariance WhiletheGaussianRBMhasbeenthecanonicalenergymodelforreal-valued data, ()arguethattheGaussianRBMinductivebiasisnot Ranzato e t a l .2010a wellsuitedtothestatisticalvariationspresentinsometypesofreal-valueddata, especiallynaturalimages.Theproblemisthatmuchoftheinformationcontent presentinnaturalimagesisembeddedinthecovariancebetweenpixelsratherthan intherawpixelvalues.Inotherwords,itistherelationshipsbetweenpixelsand nottheirabsolutevalueswheremostoftheusefulinformationinimagesresides. SincetheGaussianRBMonlymodelstheconditionalmeanoftheinputgiventhe hiddenunits,itcannotcaptureconditionalcovarianceinformation. Inresponse tothesecriticisms,alternativemodelshavebeenproposedthatattempttobetter accountforthecovarianceofreal-valueddata.Thesemodelsincludethemeanand covarianceRBM(mcRBM1),themean-productof t-distribution(mPoT)model andthespikeandslabRBM(ssRBM). 1Th e t e rm ‚Äú m c R B M ‚Äù i s p ro n o u n c e d b y s a y i n g t h e n a m e o f t h e l e t t e rs M - C- R - B - M ; t h e ‚Äú m c ‚Äù i s n o t p ro n o u n c e d l i k e t h e ‚Äú M c ‚Äù i n ‚Äú M c D o n a l d ‚Äô s . ‚Äù 6 7 8 CHAPTER20.DEEPGENERATIVEMODELS MeanandCovarianceRBMThemcRBMusesitshiddenunitstoindepen- dentlyencodetheconditionalmeanandcovarianceofallobservedunits.The mcRBMhiddenlayerisdividedintotwogroupsofunits:meanunitsandcovariance units.ThegroupthatmodelstheconditionalmeanissimplyaGaussianRBM. TheotherhalfisacovarianceRBM( ,),alsocalledacRBM, Ranzato e t a l .2010a whosecomponentsmodeltheconditionalcovariancestructure,asdescribedbelow. SpeciÔ¨Åcally,withbinarymeanunits h( ) mandbinarycovarianceunits h( ) c,the mcRBMmodelisdeÔ¨Ånedasthecombinationoftwoenergyfunctions: E m c( x h ,( ) m, h( )</div>
        </div>
    </div>

    <div class="question-card" id="q47">
        <div class="question-header">
            <span class="question-number">Question 47</span>
            <span class="question-type">multiple-choice</span>
        </div>

        <div class="question-text">Neural network initialization is a crucial aspect of deep learning that affects optimization, generalization, and computational stability. Various initialization methods aim to balance gradient flow, symmetry breaking, and regularization in multilayer architectures.

Which initialization strategy is specifically designed to maintain the variance of activations and gradients across layers in deep neural networks with both tanh and sigmoid activations, and samples weights uniformly from the range U(-‚àö6/(m+n), ‚àö6/(m+n)) where m and n are the numbers of input and output units?

1) He initialization   
2) LeCun initialization   
3) Zero initialization   
4) Orthogonal initialization   
5) Xavier (Glorot-Bengio) initialization   
6) Sparse initialization   
7) Batch normalization initialization</div>

        <div class="answer-section">
            <div class="answer-label">‚úì Correct Answer:</div>
            <div class="answer-text">The correct answer is 5) Xavier (Glorot-Bengio) initialization.</div>
        </div>

        <div class="reference-section">
            <div class="reference-label">üìö Reference Text:</div>
            <button class="toggle-button" onclick="toggleReference(47)">
                Show/Hide Reference
            </button>
            <div id="ref47" class="reference-text hidden">algorithmsarenotiterativebynatureandsimplysolvefora solutionpoint.Otheroptimization algorithmsareiterativebynaturebut,when appliedtotherightclassofoptimization problems,convergetoacceptablesolutions inanacceptableamountoftimeregardlessofinitialization. Deeplearningtraining algorithmsusuallydonothaveeitheroftheseluxuries.Trainingalgorithmsfordeep learningmodelsareusuallyiterativeinnatureandthusrequiretheusertospecify someinitialpointfromwhichtobegintheiterations.Moreover,trainingdeep modelsisasuÔ¨ÉcientlydiÔ¨ÉculttaskthatmostalgorithmsarestronglyaÔ¨Äectedby thechoiceofinitialization. Theinitialpointcandeterminewhetherthealgorithm convergesatall,withsomeinitialpointsbeingsounstablethatthealgorithm encountersnumericaldiÔ¨Écultiesandfailsaltogether.Whenlearningdoesconverge, theinitialpointcandeterminehowquicklylearningconvergesandwhetherit convergestoapointwithhigh orlowcost.Also, pointsofcomparablecost canhavewildlyvaryinggeneralization error,andtheinitialpointcanaÔ¨Äectthe generalization aswell. Moderninitialization strategiesaresimpleandheuristic.Designingimproved initialization strategiesisadiÔ¨Éculttaskbecauseneuralnetworkoptimization is notyetwellunderstood.Mostinitialization strategiesarebasedonachievingsome nicepropertieswhenthenetworkisinitialized.However,wedonothaveagood understandingofwhichofthesepropertiesarepreservedunderwhichcircumstances afterlearningbeginstoproceed.AfurtherdiÔ¨Écultyisthatsomeinitialpoints maybebeneÔ¨Åcialfromtheviewpointofoptimization butdetrimentalfromthe viewpointofgeneralization. OurunderstandingofhowtheinitialpointaÔ¨Äects generalization isespeciallyprimitive,oÔ¨Äeringlittletonoguidanceforhowtoselect theinitialpoint. Perhapstheonlypropertyknownwithcompletecertaintyisthattheinitial parametersneedto‚Äúbreaksymmetry‚Äù betweendiÔ¨Äerentunits.Iftwohidden unitswiththesameactivationfunctionareconnectedtothesameinputs,then theseunitsmusthavediÔ¨Äerentinitialparameters. Iftheyhavethesameinitial parameters,thenadeterministiclearningalgorithmappliedtoadeterministiccost andmodelwillconstantlyupdatebothoftheseunitsinthesameway.Evenifthe modelortrainingalgorithmiscapableofusingstochasticitytocomputediÔ¨Äerent updatesfordiÔ¨Äerentunits(forexample,ifonetrainswithdropout),itisusually besttoinitializeeachunittocomputeadiÔ¨Äerentfunctionfromalloftheother units.Thismayhelptomakesurethatnoinputpatternsarelostinthenull spaceofforwardpropagationandnogradientpatternsarelostinthenullspace ofback-propagation. ThegoalofhavingeachunitcomputeadiÔ¨Äerentfunction 3 0 1 CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS motivatesrandominitialization oftheparameters.Wecouldexplicitlysearch foralargesetofbasisfunctionsthatareallmutuallydiÔ¨Äerentfromeachother, butthisoftenincursanoticeablecomputational cost.Forexample,ifwehaveat mostasmanyoutputsasinputs,wecoulduseGram-Schmidtorthogonalization onaninitialweightmatrix,andbeguaranteedthateachunitcomputesavery diÔ¨Äerentfunctionfromeachotherunit.Randominitialization fromahigh-entropy distributionoverahigh-dimensionalspaceiscomputationally cheaperandunlikely toassignanyunitstocomputethesamefunctionaseachother. Typically,wesetthebiasesforeachunittoheuristicallychosenconstants,and initializeonlytheweightsrandomly.Extraparameters,forexample,parameters encodingtheconditionalvarianceofaprediction,areusuallysettoheuristically chosenconstantsmuchlikethebiasesare. Wealmostalwaysinitializealltheweightsin themodel tovalues drawn randomly froma Gaussian oruniform distribution.Thechoiceof Gaussian oruniformdistributiondoesnotseemtomatterverymuch,buthasnotbeen exhaustivelystudied.Thescaleoftheinitialdistribution,however,doeshavea largeeÔ¨Äectonboththeoutcomeoftheoptimization procedureandontheability ofthenetworktogeneralize. LargerinitialweightswillyieldastrongersymmetrybreakingeÔ¨Äect,helping toavoidredundantunits.Theyalsohelptoavoidlosingsignalduringforwardor back-propagationthroughthelinearcomponentofeachlayer‚Äîlargervaluesinthe matrixresultinlargeroutputsofmatrixmultiplication. Initialweightsthatare toolargemay,however,resultinexplodingvaluesduringforwardpropagationor back-propagation.Inrecurrentnetworks,largeweightscanalsoresultinchaos (suchextremesensitivitytosmallperturbationsoftheinputthatthebehavior ofthedeterministicforwardpropagationprocedureappearsrandom). Tosome extent,theexplodinggradientproblemcanbemitigatedbygradientclipping (thresholdingthevaluesofthegradientsbeforeperformingagradientdescentstep). Largeweightsmayalsoresultinextremevaluesthatcausetheactivationfunction tosaturate,causingcompletelossofgradientthroughsaturatedunits.These competingfactorsdeterminetheidealinitialscaleoftheweights. Theperspectivesofregularizationandoptimization cangiveverydiÔ¨Äerent insightsintohowweshouldinitializeanetwork.Theoptimization perspective suggeststhattheweightsshouldbelargeenoughtopropagateinformationsuccess- fully,butsomeregularizationconcernsencouragemakingthemsmaller.Theuse ofanoptimization algorithmsuchasstochasticgradientdescentthatmakessmall incrementalchangestotheweightsandtendstohaltinareasthatarenearerto theinitialparameters(whetherduetogettingstuckinaregionoflowgradient,or 3 0 2 CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS duetotriggeringsomeearlystoppingcriterionbasedonoverÔ¨Åtting)expressesa priorthattheÔ¨Ånalparametersshouldbeclosetotheinitialparameters.Recall fromsectionthatgradientdescentwithearlystoppingisequivalenttoweight 7.8 decayforsomemodels.Inthegeneralcase,gradientdescentwithearlystoppingis notthesameasweightdecay,butdoesprovidealooseanalogyforthinkingabout theeÔ¨Äectofinitialization. WecanthinkofinitializingtheparametersŒ∏toŒ∏ 0as beingsimilartoimposingaGaussianprior p(Œ∏)withmeanŒ∏ 0.Fromthispoint ofview,itmakessensetochooseŒ∏ 0tobenear0.Thispriorsaysthatitismore likelythatunitsdonotinteractwitheachotherthanthattheydointeract.Units interactonlyifthelikelihoodtermoftheobjectivefunctionexpressesastrong preferenceforthemtointeract.Ontheotherhand,ifweinitializeŒ∏ 0tolarge values,thenourpriorspeciÔ¨Åeswhichunitsshouldinteractwitheachother,and howtheyshouldinteract. Someheuristicsareavailableforchoosingtheinitialscaleoftheweights.One heuristicistoinitializetheweightsofafullyconnectedlayerwith minputsand noutputsbysamplingeachweightfrom U(‚àí1‚àöm,1‚àöm),whileGlorotandBengio ()suggestusingthe 2010 normalizedinitialization W i , j‚àº UÓÄ† ‚àíÓÅ≤ 6 m n+,ÓÅ≤ 6 m n+ÓÄ° . (8.23) Thislatterheuristicisdesignedtocompromisebetweenthegoalofinitializing alllayerstohavethesameactivationvarianceandthegoalofinitializingall layerstohavethesamegradientvariance.Theformulaisderivedusingthe assumptionthatthenetworkconsistsonlyofachainofmatrixmultiplications, withnononlinearities. Realneuralnetworksobviouslyviolatethisassumption, butmanystrategiesdesignedforthelinearmodelperformreasonablywellonits nonlinearcounterparts. Saxe2013etal.()recommendinitializingtorandomorthogonalmatrices,with acarefullychosenscalingorgainfactor gthataccountsforthenonlinearityapplied ateachlayer.TheyderivespeciÔ¨ÅcvaluesofthescalingfactorfordiÔ¨Äerenttypesof nonlinearactivationfunctions.Thisinitialization schemeisalsomotivatedbya modelofadeepnetworkasasequenceofmatrixmultiplieswithoutnonlinearities. Undersuchamodel,thisinitialization schemeguaranteesthatthetotalnumberof trainingiterationsrequiredtoreachconvergenceisindependentofdepth. Increasingthescalingfactor gpushesthenetworktowardtheregimewhere activationsincreaseinnormastheypropagateforwardthroughthenetworkand gradientsincreaseinnormastheypropagatebackward. ()showed Sussillo2014 thatsettingthegainfactorcorrectlyissuÔ¨Écienttotrainnetworksasdeepas 3 0 3 CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS 1,000layers,withoutneedingtouseorthogonalinitializations. A keyinsightof thisapproachisthatinfeedforwardnetworks,activationsandgradientscangrow orshrinkoneachstepofforwardorback-propagation, followingarandomwalk behavior.ThisisbecausefeedforwardnetworksuseadiÔ¨Äerentweightmatrixat eachlayer.Ifthisrandomwalkistunedtopreservenorms,thenfeedforward networkscanmostlyavoidthevanishingandexplodinggradientsproblemthat ariseswhenthesameweightmatrixisusedateachstep,describedinsection.8.2.5 Unfortunately,theseoptimalcriteriaforinitialweightsoftendonotleadto optimalperformance.ThismaybeforthreediÔ¨Äerentreasons.First,wemay beusingthewrongcriteria‚ÄîitmaynotactuallybebeneÔ¨Åcialtopreservethe normofasignalthroughouttheentirenetwork.Second,thepropertiesimposed atinitialization maynotpersistafterlearninghasbeguntoproceed.Third,the criteriamightsucceedatimprovingthespeedofoptimization butinadvertently increasegeneralization error.Inpractice,weusuallyneedtotreatthescaleofthe weightsasahyperparameter whoseoptimalvalueliessomewhereroughlynearbut notexactlyequaltothetheoreticalpredictions. Onedrawbacktoscalingrulesthatsetalloftheinitialweightstohavethe samestandarddeviation,suchas1‚àöm,isthateveryindividualweightbecomes extremelysmallwhenthelayersbecomelarge. ()introducedan Martens2010 alternativeinitialization schemecalledsparseinitializationinwhicheachunitis initializedtohaveexactly knon-zeroweights.Theideaistokeepthetotalamount ofinputtotheunitindependentfromthenumberofinputs mwithoutmakingthe magnitudeofindividualweightelementsshrinkwith m.Sparseinitialization helps toachievemorediversityamongtheunitsatinitialization time.However,italso imposesaverystrongpriorontheweightsthatarechosentohavelargeGaussian values.Becauseittakesalongtimeforgradientdescenttoshrink‚Äúincorrect‚Äùlarge values,thisinitialization schemecancauseproblemsforunitssuchasmaxoutunits thathaveseveralÔ¨Åltersthatmustbecarefullycoordinatedwitheachother. Whencomputational resourcesallowit,itisusuallyagoodideatotreatthe initialscaleoftheweightsforeachlayerasahyperparameter, andtochoosethese scalesusingahyperparametersearchalgorithmdescribedinsection,such11.4.2 asrandomsearch.Thechoiceofwhethertousedenseorsparseinitialization canalsobemadeahyperparameter.Alternately,onecanmanuallysearchfor thebestinitialscales.Agoodruleofthumbforchoosingtheinitialscalesisto lookattherangeorstandarddeviationofactivationsorgradientsonasingle minibatchofdata.Iftheweightsaretoosmall,therangeofactivationsacrossthe minibatchwillshrinkastheactivationspropagateforwardthroughthenetwork. ByrepeatedlyidentifyingtheÔ¨Årstlayerwithunacceptably smallactivationsand 3 0 4 CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS increasingitsweights,itispossibletoeventuallyobtainanetworkwithreasonable initialactivationsthroughout.Iflearningisstilltooslowatthispoint,itcanbe usefultolookattherangeorstandarddeviationofthegradientsaswellasthe activations. Thisprocedurecaninprinciplebeautomatedandisgenerallyless computationally costlythanhyperparameter optimization basedonvalidationset errorbecauseitisbasedonfeedbackfromthebehavioroftheinitialmodelona singlebatchofdata,ratherthanonfeedbackfromatrainedmodelonthevalidation set.Whilelongusedheuristically,thisprotocolhasrecentlybeenspeciÔ¨Åedmore formallyandstudiedby (). MishkinandMatas2015 So far we have focused on the initialization ofthe weights.Fortunately, initialization ofotherparametersistypicallyeasier. Theapproachforsettingthebiasesmustbecoordinatedwiththeapproach forsettingstheweights.Settingthebiasestozeroiscompatiblewithmostweight initialization schemes.Thereareafewsituationswherewemaysetsomebiasesto non-zerovalues: ‚Ä¢Ifabiasisforanoutputunit,thenitisoftenbeneÔ¨Åcialtoinitializethebiasto obtaintherightmarginalstatisticsoftheoutput.Todothis,weassumethat theinitialweightsaresmallenoughthattheoutputoftheunitisdetermined onlybythebias.ThisjustiÔ¨Åessettingthebiastotheinverseoftheactivation functionappliedtothemarginalstatisticsoftheoutputinthetrainingset. Forexample,iftheoutputisadistributionoverclassesandthisdistribution isahighlyskeweddistributionwiththemarginalprobabilityofclass igiven byelement c iofsomevectorc,thenwecansetthebiasvectorbbysolving theequationsoftmax (b) =c.ThisappliesnotonlytoclassiÔ¨Åersbutalsoto modelswewillencounterinPart,suchasautoencodersandBoltzmann III machines.Thesemodelshavelayerswhoseoutputshouldresembletheinput datax,anditcanbeveryhelpfultoinitializethebiasesofsuchlayersto matchthemarginaldistributionover.x ‚Ä¢Sometimeswemay wanttochoosethebiastoavoidcausingtoo much saturationatinitialization. Forexample,wemaysetthebiasofaReLU hiddenunitto0.1ratherthan0toavoidsaturatingtheReLUatinitialization. Thisapproachisnotcompatiblewithweightinitialization schemesthatdo notexpectstronginputfromthebiasesthough.Forexample, itisnot recommendedforusewithrandomwalkinitialization (,). Sussillo2014 ‚Ä¢Sometimesaunitcontrolswhetherotherunitsareabletoparticipateina function.Insuchsituations,wehaveaunitwithoutput uandanotherunit h‚àà[0 ,1],andtheyaremultipliedtogethertoproduceanoutput u h. We 3 0 5</div>
        </div>
    </div>

    <div class="question-card" id="q48">
        <div class="question-header">
            <span class="question-number">Question 48</span>
            <span class="question-type">multiple-choice</span>
        </div>

        <div class="question-text">In modern machine learning, achieving high generalization often requires balancing model complexity, data availability, and effective hyperparameter tuning. Strategies differ across domains depending on resource constraints and the nature of the task.

Which strategy is most appropriate for improving model generalization error when collecting additional data is impractical due to cost or privacy concerns?

1) Increase the learning rate and retrain the model   
2) Reduce the number of hidden units to decrease model capacity   
3) Switch from supervised to unsupervised learning   
4) Remove regularization to allow the model to fit the training data better   
5) Adjust model size and increase regularization techniques such as dropout or weight decay   
6) Add more convolutional layers regardless of data constraints   
7) Replace the optimizer with a less commonly used algorithm</div>

        <div class="answer-section">
            <div class="answer-label">‚úì Correct Answer:</div>
            <div class="answer-text">The correct answer is 5) Adjust model size and increase regularization techniques such as dropout or weight decay.</div>
        </div>

        <div class="reference-section">
            <div class="reference-label">üìö Reference Text:</div>
            <button class="toggle-button" onclick="toggleReference(48)">
                Show/Hide Reference
            </button>
            <div id="ref48" class="reference-text hidden">CHAPTER11.PRACTICALMETHODOLOGY Ifyourtaskissimilartoanothertaskthathasbeenstudiedextensively,you willprobablydowellbyÔ¨Årstcopyingthemodelandalgorithmthatisalready knowntoperformbestonthepreviouslystudiedtask.Youmayevenwanttocopy atrainedmodelfromthattask.Forexample,itiscommontousethefeatures fromaconvolutionalnetworktrainedonImageNettosolveothercomputervision tasks( ,). Girshicketal.2015 Acommonquestioniswhethertobeginbyusingunsupervisedlearning,de- scribedfurtherinpart.ThisissomewhatdomainspeciÔ¨Åc.Somedomains,such III asnaturallanguageprocessing,areknowntobeneÔ¨Åttremendouslyfromunsuper- visedlearningtechniquessuchaslearningunsupervisedwordembeddings.Inother domains,suchascomputervision,currentunsupervisedlearningtechniquesdo notbringabeneÔ¨Åt,exceptinthesemi-supervisedsetting,whenthenumberof labeledexamplesisverysmall( ,; Kingma etal.2014Rasmus2015etal.,).Ifyour applicationisinacontextwhereunsupervisedlearningisknowntobeimportant, thenincludeitinyourÔ¨Årstend-to-endbaseline.Otherwise,onlyuseunsupervised learninginyourÔ¨Årstattemptifthetaskyouwanttosolveisunsupervised.You canalwaystryaddingunsupervisedlearninglaterifyouobservethatyourinitial baselineoverÔ¨Åts. 11.3DeterminingWhethertoGatherMoreData AftertheÔ¨Årstend-to-endsystemisestablished,itistimetomeasuretheperfor- manceofthealgorithmanddeterminehowtoimproveit.Manymachinelearning novicesaretemptedtomakeimprovementsbytryingoutmanydiÔ¨Äerentalgorithms. However,itisoftenmuchbettertogathermoredatathantoimprovethelearning algorithm. Howdoesonedecidewhethertogathermoredata?First,determinewhether theperformanceonthetrainingsetisacceptable.Ifperformanceonthetraining setispoor,thelearningalgorithmisnotusingthetrainingdatathatisalready available,sothereisnoreasontogathermoredata.Instead,tryincreasingthe sizeofthemodelbyaddingmorelayersoraddingmorehiddenunitstoeachlayer. Also,tryimprovingthelearningalgorithm,forexamplebytuningthelearning ratehyperparameter. Iflargemodelsandcarefullytunedoptimization algorithms donotworkwell,thentheproblemmightbetheofthetrainingdata.The quality datamaybetoonoisyormaynotincludetherightinputsneededtopredictthe desiredoutputs.Thissuggestsstartingover,collectingcleanerdataorcollectinga richersetoffeatures. Iftheperformanceonthetrainingsetisacceptable,thenmeasuretheper- 4 2 6 CHAPTER11.PRACTICALMETHODOLOGY formanceonatestset.Iftheperformanceonthetestsetisalsoacceptable, thenthereisnothinglefttobedone.Iftestsetperformanceismuchworsethan trainingsetperformance,thengatheringmoredataisoneofthemosteÔ¨Äective solutions. Thekeyconsiderationsarethecostandfeasibilityofgatheringmore data,thecostandfeasibilityofreducingthetesterrorbyothermeans,andthe amountofdatathatisexpectedtobenecessarytoimprovetestsetperformance signiÔ¨Åcantly. Atlargeinternetcompanieswithmillionsorbillionsofusers,itis feasibletogatherlargedatasets,andtheexpenseofdoingsocanbeconsiderably lessthantheotheralternatives,sotheanswerisalmostalwaystogathermore trainingdata.Forexample,thedevelopmentoflargelabeleddatasetswasoneof themostimportantfactorsinsolvingobjectrecognition.Inothercontexts,suchas medicalapplications,itmaybecostlyorinfeasibletogathermoredata.Asimple alternativetogatheringmoredataistoreducethesizeofthemodelorimprove regularization, byadjustinghyperparameters suchasweightdecaycoeÔ¨Écients, orbyaddingregularizationstrategiessuchasdropout.IfyouÔ¨Åndthatthegap betweentrainandtestperformanceisstillunacceptable evenaftertuningthe regularizationhyperparameters ,thengatheringmoredataisadvisable. Whendecidingwhethertogathermoredata,itisalsonecessarytodecide howmuchtogather.Itishelpfultoplotcurvesshowingtherelationshipbetween trainingsetsizeandgeneralization error,likeinÔ¨Ågure.Byextrapolatingsuch 5.4 curves,onecanpredicthowmuchadditionaltrainingdatawouldbeneededto achieveacertainlevelofperformance.Usually,addingasmallfractionofthetotal numberofexampleswillnothaveanoticeableimpactongeneralization error.Itis thereforerecommendedtoexperimentwithtrainingsetsizesonalogarithmicscale, forexampledoublingthenumberofexamplesbetweenconsecutiveexperiments. Ifgatheringmuchmoredataisnotfeasible,theonlyotherwaytoimprove generalization erroristoimprovethelearningalgorithmitself.Thisbecomesthe domainofresearchandnotthedomainofadviceforappliedpractitioners. 11.4SelectingHyperparameters Mostdeeplearningalgorithmscomewithmanyhyperparametersthatcontrolmany aspectsofthealgorithm‚Äôsbehavior.SomeofthesehyperparametersaÔ¨Äectthetime andmemorycostofrunningthealgorithm.Someofthesehyperparameters aÔ¨Äect thequalityofthemodelrecoveredbythetrainingprocessanditsabilitytoinfer correctresultswhendeployedonnewinputs. Therearetwobasicapproachestochoosingthesehyperparameters :choosing themmanuallyandchoosingthemautomatically .Choosingthehyperparameters 4 2 7 CHAPTER11.PRACTICALMETHODOLOGY manuallyrequiresunderstandingwhatthehyperparametersdoandhowmachine learningmodelsachievegoodgeneralization. Automatichyperparameterselection algorithmsgreatlyreducetheneedtounderstandtheseideas,buttheyareoften muchmorecomputationally costly. 1 1 . 4 . 1 Ma n u a l Hyp erp a ra m et er T u n i n g Tosethyperparameters manually,onemustunderstandtherelationshipbetween hyperparameters,trainingerror,generalization errorandcomputational resources (memoryandruntime).Thismeansestablishingasolidfoundationonthefun- damentalideasconcerningtheeÔ¨Äectivecapacityofalearningalgorithmfrom chapter.5 ThegoalofmanualhyperparametersearchisusuallytoÔ¨Åndthelowestgeneral- izationerrorsubjecttosomeruntimeandmemorybudget.Wedonotdiscusshow todeterminetheruntimeandmemoryimpactofvarioushyperparametershere becausethisishighlyplatform-dependent. TheprimarygoalofmanualhyperparametersearchistoadjusttheeÔ¨Äective capacityofthemodeltomatchthecomplexityofthetask.EÔ¨Äectivecapacity isconstrainedbythreefactors: therepresentationalcapacityofthemodel,the abilityofthelearningalgorithmtosuccessfullyminimizethecostfunctionusedto trainthemodel,andthedegreetowhichthecostfunctionandtrainingprocedure regularizethemodel.Amodelwithmorelayersandmorehiddenunitsperlayerhas higherrepresentationalcapacity‚Äîitiscapableofrepresentingmorecomplicated functions.Itcannotnecessarilyactuallylearnallofthesefunctionsthough,if thetrainingalgorithmcannotdiscoverthatcertainfunctionsdoagoodjobof minimizingthetrainingcost,orifregularizationtermssuchasweightdecayforbid someofthesefunctions. Thegeneralization errortypicallyfollowsaU-shapedcurvewhenplottedas afunctionofoneofthehyperparameters ,asinÔ¨Ågure. Atoneextreme,the 5.3 hyperparametervaluecorrespondstolowcapacity,andgeneralization errorishigh becausetrainingerrorishigh.ThisistheunderÔ¨Åttingregime.Attheotherextreme, thehyperparameter valuecorrespondstohighcapacity,andthegeneralization errorishighbecausethegapbetweentrainingandtesterrorishigh.Somewhere inthemiddleliestheoptimalmodelcapacity,whichachievesthelowestpossible generalization error,byaddingamediumgeneralization gaptoamediumamount oftrainingerror. Forsomehyperparameters,overÔ¨Åttingoccurswhenthevalueofthehyper- parameterislarge. Thenumberofhiddenunitsinalayerisonesuchexample, 4 2 8 CHAPTER11.PRACTICALMETHODOLOGY becauseincreasingthenumberofhiddenunitsincreasesthecapacityofthemodel. Forsomehyperparameters ,overÔ¨Åttingoccurswhenthevalueofthehyperparame- terissmall.Forexample,thesmallestallowableweightdecaycoeÔ¨Écientofzero correspondstothegreatesteÔ¨Äectivecapacityofthelearningalgorithm. NoteveryhyperparameterwillbeabletoexploretheentireU-shapedcurve. Manyhyperparameters arediscrete,suchasthenumberofunitsinalayerorthe numberoflinearpiecesinamaxoutunit,soitisonlypossibletovisitafewpoints alongthecurve.Somehyperparametersarebinary.Usuallythesehyperparameters areswitchesthat specify whetherornotto usesomeoptionalcomponentof thelearningalgorithm,suchasapreprocessingstepthatnormalizestheinput featuresbysubtractingtheirmeananddividingbytheirstandarddeviation.These hyperparameterscanonlyexploretwopointsonthecurve.Otherhyperparameters havesomeminimumormaximumvaluethatpreventsthemfromexploringsome partofthecurve.Forexample,theminimumweightdecaycoeÔ¨Écientiszero.This meansthatifthemodelisunderÔ¨Åttingwhenweightdecayiszero,wecannotenter theoverÔ¨ÅttingregionbymodifyingtheweightdecaycoeÔ¨Écient.Inotherwords, somehyperparameters canonlysubtractcapacity. Thelearningrateisperhapsthemostimportanthyperparameter. Ifyou have timeto tuneonly onehyperparameter, tune thelearning rate. It con- trolstheeÔ¨Äectivecapacityofthemodelinamorecomplicatedwaythanother hyperparameters‚ÄîtheeÔ¨Äectivecapacityofthemodelishighestwhenthelearning rateiscorrectfortheoptimizationproblem,notwhenthelearningrateisespecially largeorespeciallysmall.ThelearningratehasaU-shapedcurvefortrainingerror, illustratedinÔ¨Ågure.Whenthelearningrateistoolarge,gradientdescent 11.1 caninadvertentlyincreaseratherthandecreasethetrainingerror.Intheidealized quadraticcase,thisoccursifthelearningrateisatleasttwiceaslargeasits optimalvalue( ,).Whenthelearningrateistoosmall,training LeCunetal.1998a isnotonlyslower,butmaybecomepermanentlystuckwithahightrainingerror. ThiseÔ¨Äectispoorlyunderstood(itwouldnothappenforaconvexlossfunction). Tuningtheparametersotherthanthelearningraterequiresmonitoringboth trainingandtesterrortodiagnosewhetheryourmodelisoverÔ¨ÅttingorunderÔ¨Åtting, thenadjustingitscapacityappropriately . Ifyourerroronthetrainingsetishigherthanyourtargeterrorrate,youhave nochoicebuttoincreasecapacity.Ifyouarenotusingregularizationandyouare conÔ¨Ådentthatyouroptimization algorithmisperformingcorrectly,thenyoumust addmorelayerstoyournetworkoraddmorehiddenunits.Unfortunately,this increasesthecomputational costsassociatedwiththemodel. Ifyourerroronthetestsetishigherthanthanyourtargeterrorrate,youcan 4 2 9 CHAPTER11.PRACTICALMETHODOLOGY 1 0‚àí 21 0‚àí 11 00 L e a r ni ng r a t e ( l o g a r i t hm i c s c a l e )012345678T r a i ni ng e r r o r Figure11.1:Typicalrelationshipbetweenthelearningrateandthetrainingerror.Notice thesharpriseinerrorwhenthelearningisaboveanoptimalvalue.ThisisforaÔ¨Åxed trainingtime,asasmallerlearningratemaysometimesonlyslowdowntrainingbya factorproportionaltothelearningratereduction. Generalizationerrorcanfollowthis curveorbecomplicatedbyregularizationeÔ¨Äectsarisingoutofhavingatoolargeor toosmalllearningrates,sincepooroptimizationcan,tosomedegree,reduceorprevent overÔ¨Åtting,andevenpointswithequivalenttrainingerrorcanhavediÔ¨Äerentgeneralization error. nowtaketwokindsofactions.Thetesterroristhesumofthetrainingerrorand thegapbetweentrainingandtesterror.Theoptimaltesterrorisfoundbytrading oÔ¨Äthesequantities.Neuralnetworkstypicallyperformbestwhenthetraining errorisverylow(andthus,whencapacityishigh)andthetesterrorisprimarily drivenbythegapbetweentrainandtesterror. Yourgoalistoreducethisgap withoutincreasingtrainingerrorfasterthanthegapdecreases.Toreducethegap, changeregularizationhyperparameters toreduceeÔ¨Äectivemodelcapacity,suchas byaddingdropoutorweightdecay.Usuallythebestperformancecomesfroma largemodelthatisregularizedwell,forexamplebyusingdropout. Mosthyperparameters canbesetbyreasoningaboutwhethertheyincreaseor decreasemodelcapacity.SomeexamplesareincludedinTable.11.1 Whilemanuallytuninghyperparameters,donotlosesightofyourendgoal: goodperformanceonthetestset.Addingregularizationisonlyonewaytoachieve thisgoal.Aslongasyouhavelowtrainingerror,youcanalwaysreducegeneral- izationerrorbycollectingmoretrainingdata.Thebruteforcewaytopractically guaranteesuccessistocontinuallyincreasemodelcapacityandtrainingsetsize untilthetaskissolved.Thisapproachdoesofcourseincreasethecomputational costoftrainingandinference,soitisonlyfeasiblegivenappropriateresources.In 4 3 0 CHAPTER11.PRACTICALMETHODOLOGY HyperparameterIncreases capacity when...Reason Caveats Numberofhid- denunitsincreasedIncreasingthenumberof hiddenunitsincreasesthe representationalcapacity ofthemodel.Increasingthenumber ofhiddenunits increases boththetimeandmemory costofessentiallyeveryop- erationonthemodel. Learningratetunedop- timallyAnimproperlearningrate, whether toohigh ortoo low,resultsinamodel withloweÔ¨Äectivecapacity duetooptimizationfailure Convolutionker- nelwidthincreasedIncreasingthekernelwidth increasesthenumberofpa- rametersinthemodelAwiderkernelresultsin anarroweroutputdimen- sion,reducingmodelca- pacityunlessyouuseim- plicitzeropaddingtore- ducethiseÔ¨Äect.Wider kernelsrequiremoremem- oryforparameterstorage andincreaseruntime,but anarroweroutputreduces memorycost. Implicitzero paddingincreasedAddingimplicitzerosbe- foreconvolutionkeepsthe representationsizelargeIncreasedtimeandmem- orycostofmostopera- tions. Weightdecayco- eÔ¨ÉcientdecreasedDecreasingtheweightde- caycoeÔ¨Écientfreesthe modelparameterstobe- comelarger DropoutratedecreasedDroppingunitslessoften givestheunitsmoreoppor- tunitiesto‚Äúconspire‚Äùwith eachothertoÔ¨Åtthetrain- ingset Table11.1:TheeÔ¨Äectofvarioushyperparametersonmodelcapacity. 4 3 1 CHAPTER11.PRACTICALMETHODOLOGY principle,thisapproachcouldfailduetooptimization diÔ¨Éculties,butformany problemsoptimization doesnotseemtobeasigniÔ¨Åcantbarrier,providedthatthe modelischosenappropriately . 1 1 . 4 . 2 A u t o m a t i c Hyp erp a ra m et er O p t i m i za t i o n A l g o ri t h m s Theideallearningalgorithmjusttakesadatasetandoutputsafunction,without requiringhand-tuning ofhyperparameters .Thepopularityofseverallearning algorithmssuchaslogisticregressionandSVMsstemsinpartfromtheirabilityto performwellwithonlyoneortwotunedhyperparameters .Neuralnetworkscan sometimesperformwellwithonlyasmallnumberoftunedhyperparameters ,but oftenbeneÔ¨ÅtsigniÔ¨Åcantlyfromtuningoffortyormorehyperparameters .Manual hyperparametertuningcanworkverywellwhentheuserhasagoodstartingpoint, suchasonedeterminedbyothershavingworkedonthesametypeofapplication andarchitecture, orwhentheuserhasmonthsoryearsofexperienceinexploring hyperparametervaluesforneuralnetworksappliedtosimilartasks.However, formanyapplications,thesestartingpointsarenotavailable.Inthesecases, automatedalgorithmscanÔ¨Åndusefulvaluesofthehyperparameters . Ifwethinkaboutthewayinwhichtheuserofalearningalgorithmsearchesfor goodvaluesofthehyperparameters ,werealizethatanoptimizationistakingplace: wearetryingtoÔ¨Åndavalueofthehyperparametersthatoptimizesanobjective function,suchasvalidationerror,sometimesunderconstraints(suchasabudget fortrainingtime,memoryorrecognitiontime).Itisthereforepossible,inprinciple, to develop h y p e r par am e t e r o p t i m i z a t i o nalgorithms thatwrap a learnin g algorithmandchooseitshyperparameters ,thushidingthehyperparameters ofthe learningalgorithmfromtheuser.Unfortunately,hyperparameter optimization algorithmsoftenhavetheirownhyperparameters,suchastherangeofvaluesthat shouldbeexploredforeachofthelearningalgorithm‚Äôshyperparameters .However, thesesecondaryhyperparameters areusuallyeasiertochoose,inthesensethat acceptableperformancemaybeachievedonawiderangeoftasksusingthesame secondaryhyperparameters foralltasks. 1 1 . 4 . 3 G</div>
        </div>
    </div>

    <div class="question-card" id="q49">
        <div class="question-header">
            <span class="question-number">Question 49</span>
            <span class="question-type">multiple-choice</span>
        </div>

        <div class="question-text">Long Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU) networks are widely used in deep learning for modeling sequences, particularly because of their specialized gating mechanisms that enhance memory and learning of long-term dependencies. Effective training of these recurrent architectures requires careful optimization techniques to overcome challenges inherent in their structure.

Which of the following statements most accurately describes the primary function of gradient clipping when training deep recurrent neural networks?

1) It accelerates the convergence of second-order optimization algorithms by increasing the Hessian's diagonal dominance.   
2) It enhances the model's ability to remember information by increasing the bias of the forget gate.   
3) It prevents vanishing gradients by amplifying small gradient values during backpropagation.   
4) It restricts excessively large gradient values to stabilize parameter updates and prevent training disruptions.   
5) It automatically tunes learning rates based on the curvature of the loss landscape.   
6) It increases the number of parameters in the gating mechanisms to improve flexibility.   
7) It merges the input and forget gates into a single gating unit for more efficient memory updates.</div>

        <div class="answer-section">
            <div class="answer-label">‚úì Correct Answer:</div>
            <div class="answer-text">The correct answer is 4) It restricts excessively large gradient values to stabilize parameter updates and prevent training disruptions..</div>
        </div>

        <div class="reference-section">
            <div class="reference-label">üìö Reference Text:</div>
            <button class="toggle-button" onclick="toggleReference(49)">
                Show/Hide Reference
            </button>
            <div id="ref49" class="reference-text hidden">t ‚àí i + g( ) t i œÉÔ£´ Ô£≠ b i+ÓÅò jU i , j x( ) t j+ÓÅò jW i , j h( 1 ) t ‚àí jÔ£∂ Ô£∏ ,(10.41) whereb,UandWrespectivelydenotethebiases,inputweightsandrecurrent weightsintotheLSTMcell.Theexternalinputgateunit g( ) t iiscomputed similarlytotheforgetgate(withasigmoidunittoobtainagatingvaluebetween 0and1),butwithitsownparameters: g( ) t i= œÉÔ£´ Ô£≠ bg i+ÓÅò jUg i , j x( ) t j+ÓÅò jWg i , j h( 1 ) t ‚àí jÔ£∂ Ô£∏ .(10.42) Theoutput h( ) t ioftheLSTMcellcanalsobeshutoÔ¨Ä,viatheoutputgate q( ) t i, whichalsousesasigmoidunitforgating: h( ) t i= tanhÓÄê s( ) t iÓÄë q( ) t i (10.43) q( ) t i= œÉÔ£´ Ô£≠ bo i+ÓÅò jUo i , j x( ) t j+ÓÅò jWo i , j h( 1 ) t ‚àí jÔ£∂ Ô£∏ (10.44) whichhasparametersbo,Uo,Woforitsbiases,inputweightsandrecurrent weights,respectively.Amongthevariants,onecanchoosetousethecellstate s( ) t i asanextrainput(withitsweight)intothethreegatesofthe i-thunit,asshown inÔ¨Ågure.Thiswouldrequirethreeadditionalparameters. 10.16 LSTMnetworkshavebeenshowntolearnlong-termdependenciesmoreeasily thanthesimplerecurrentarchitectures,Ô¨ÅrstonartiÔ¨Åcialdatasetsdesignedfor testingtheabilitytolearnlong-termdependencies( ,; Bengio e t a l .1994Hochreiter andSchmidhuber1997Hochreiter 2001 ,; e t a l .,),thenonchallengingsequence processingtaskswherestate-of-the-art performance wasobtained(Graves2012,; Graves2013Sutskever2014 e t a l .,; e t a l .,).VariantsandalternativestotheLSTM havebeenstudiedandusedandarediscussednext. 10.10.2OtherGatedRNNs Whichpieces ofthe LSTMarchitecture are actually necessary?Whatother successfularchitecturescouldbedesignedthatallowthenetworktodynamically controlthetimescaleandforgettingbehaviorofdiÔ¨Äerentunits? 4 1 1 CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS SomeanswerstothesequestionsaregivenwiththerecentworkongatedRNNs, whoseunitsarealsoknownasgatedrecurrentunitsorGRUs(,; Cho e t a l .2014b Chung20142015aJozefowicz2015Chrupala 2015 e t a l .,,; e t a l .,; e t a l .,).Themain diÔ¨ÄerencewiththeLSTMisthatasinglegatingunitsimultaneouslycontrolsthe forgettingfactorandthedecisiontoupdatethestateunit.Theupdateequations arethefollowing: h( ) t i= u( 1 ) t ‚àí i h( 1 ) t ‚àí i+(1‚àí u( 1 ) t ‚àí i) œÉÔ£´ Ô£≠ b i+ÓÅò jU i , j x( 1 ) t ‚àí j +ÓÅò jW i , j r( 1 ) t ‚àí j h( 1 ) t ‚àí jÔ£∂ Ô£∏ , (10.45) whereustandsfor‚Äúupdate‚Äùgateandrfor‚Äúreset‚Äùgate.TheirvalueisdeÔ¨Ånedas usual: u( ) t i= œÉÔ£´ Ô£≠ bu i+ÓÅò jUu i , j x( ) t j+ÓÅò jWu i , j h( ) t jÔ£∂ Ô£∏ (10.46) and r( ) t i= œÉÔ£´ Ô£≠ br i+ÓÅò jUr i , j x( ) t j+ÓÅò jWr i , j h( ) t jÔ£∂ Ô£∏ .(10.47) Theresetandupdatesgatescanindividually‚Äúignore‚Äùpartsofthestatevector. Theupdategatesactlikeconditionalleakyintegratorsthatcanlinearlygateany dimension,thuschoosingtocopyit(atoneextremeofthesigmoid)orcompletely ignoreit(attheotherextreme)byreplacingitbythenew‚Äútargetstate‚Äùvalue (towardswhichtheleakyintegratorwantstoconverge).Theresetgatescontrol whichpartsofthestategetusedtocomputethenexttargetstate,introducingan additionalnonlineareÔ¨Äectintherelationshipbetweenpaststateandfuturestate. Manymorevariantsaroundthisthemecanbedesigned.Forexamplethe resetgate(orforgetgate)outputcouldbesharedacrossmultiplehiddenunits. Alternately,theproductofaglobalgate(coveringawholegroupofunits,suchas anentirelayer)andalocalgate(perunit)couldbeusedtocombineglobalcontrol andlocalcontrol.However,severalinvestigationsoverarchitectural variations oftheLSTMandGRUfoundnovariantthatwouldclearlybeatbothofthese acrossawiderangeoftasks(,; GreÔ¨Ä e t a l .2015Jozefowicz2015GreÔ¨Ä e t a l .,). e t a l .()foundthatacrucialingredientistheforgetgate,while 2015 Jozefowicz e t a l .()foundthataddingabiasof1totheLSTMforgetgate,apractice 2015 advocatedby (),makestheLSTMasstrongasthebestofthe Gers e t a l .2000 exploredarchitecturalvariants. 4 1 2 CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS 10.11OptimizationforLong-TermDependencies Section andsectionhavedescribedthevanishingandexplodinggradient 8.2.5 10.7 problemsthatoccurwhenoptimizingRNNsovermanytimesteps. AninterestingideaproposedbyMartensandSutskever2011()isthatsecond derivativesmayvanishatthesametimethatÔ¨Årstderivativesvanish.Second-order optimization algorithmsmayroughlybeunderstoodasdividingtheÔ¨Årstderivative bythesecondderivative(inhigherdimension,multiplyingthegradientbythe inverseHessian).IfthesecondderivativeshrinksatasimilarratetotheÔ¨Årst derivative,thentheratioofÔ¨Årstandsecondderivativesmayremainrelatively constant.Unfortunately,second-ordermethodshavemanydrawbacks,including highcomputational cost,theneedforalargeminibatch,andatendencytobe attractedtosaddlepoints.MartensandSutskever2011()foundpromisingresults usingsecond-ordermethods.Later,Sutskever2013 e t a l .()foundthatsimpler methodssuchasNesterovmomentumwithcarefulinitialization couldachieve similarresults.SeeSutskever2012()formoredetail. Bothoftheseapproaches havelargelybeenreplacedbysimplyusingSGD(evenwithoutmomentum)applied toLSTMs.Thisispartofacontinuingthemeinmachinelearningthatitisoften mucheasiertodesignamodelthatiseasytooptimizethanitistodesignamore powerfuloptimization algorithm. 10.11.1ClippingGradients Asdiscussedinsection,stronglynonlinearfunctionssuchasthosecomputed 8.2.4 byarecurrentnetovermanytimestepstendtohavederivativesthatcanbe eitherverylargeorverysmallinmagnitude.ThisisillustratedinÔ¨Ågureand8.3 Ô¨Ågure,inwhichweseethattheobjectivefunction(asafunctionofthe 10.17 parameters)hasa‚Äúlandscape‚Äù inwhichoneÔ¨Ånds‚ÄúcliÔ¨Äs‚Äù:wideandratherÔ¨Çat regionsseparatedbytinyregionswheretheobjectivefunctionchangesquickly, formingakindofcliÔ¨Ä. ThediÔ¨Écultythatarisesisthatwhentheparametergradientisverylarge,a gradientdescentparameterupdatecouldthrowtheparametersveryfar,intoa regionwheretheobjectivefunctionislarger,undoingmuchoftheworkthathad beendonetoreachthecurrentsolution.Thegradienttellsusthedirectionthat correspondstothesteepestdescentwithinaninÔ¨Ånitesimalregionsurroundingthe currentparameters.OutsideofthisinÔ¨Ånitesimalregion,thecostfunctionmay begintocurvebackupwards.Theupdatemustbechosentobesmallenoughto avoidtraversingtoomuchupwardcurvature.Wetypicallyuselearningratesthat 4 1 3 CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS decayslowlyenoughthatconsecutivestepshaveapproximatelythesamelearning rate.Astepsizethatisappropriateforarelativelylinearpartofthelandscapeis ofteninappropriate andcausesuphillmotionifweenteramorecurvedpartofthe landscapeonthenextstep. ÓÅ∑ ÓÅ¢ÓÅäÓÅ∑ÓÄª ÓÅ¢ ÓÄ®ÓÄ©ÓÅó ÓÅ© ÓÅ¥ ÓÅ® ÓÅØ ÓÅµ ÓÅ¥ ÓÄ† ÓÅ£ ÓÅ¨ ÓÅ© ÓÅ∞ ÓÅ∞ ÓÅ© ÓÅÆ ÓÅß ÓÅ∑ ÓÅ¢ÓÅäÓÅ∑ÓÄª ÓÅ¢ ÓÄ®ÓÄ©ÓÅó ÓÅ© ÓÅ¥ ÓÅ® ÓÄ† ÓÅ£ ÓÅ¨ ÓÅ© ÓÅ∞ ÓÅ∞ ÓÅ© ÓÅÆ ÓÅß Figure10.17:ExampleoftheeÔ¨Äectofgradientclippinginarecurrentnetworkwith twoparameterswandb.Gradientclippingcanmakegradientdescentperformmore reasonablyinthevicinityofextremelysteepcliÔ¨Äs.ThesesteepcliÔ¨Äscommonlyoccur inrecurrentnetworksnearwherearecurrentnetworkbehavesapproximatelylinearly. ThecliÔ¨Äisexponentiallysteepinthenumberoftimestepsbecausetheweightmatrix ismultipliedbyitselfonceforeachtimestep. ( L e f t )Gradientdescentwithoutgradient clippingovershootsthebottomofthissmallravine,thenreceivesaverylargegradient fromthecliÔ¨Äface.Thelargegradientcatastrophicallypropelstheparametersoutsidethe axesoftheplot.Gradientdescentwithgradientclippinghasamoremoderate ( R i g h t ) reactiontothecliÔ¨Ä.WhileitdoesascendthecliÔ¨Äface,thestepsizeisrestrictedsothat itcannotbepropelledawayfromsteepregionnearthesolution.Figureadaptedwith permissionfromPascanu2013 e t a l .(). Asimpletypeofsolutionhasbeeninusebypractitioners formanyyears: clippingthegradient.TherearediÔ¨Äerentinstancesofthisidea(Mikolov2012,; Pascanu2013 e t a l</div>
        </div>
    </div>

    <div class="question-card" id="q50">
        <div class="question-header">
            <span class="question-number">Question 50</span>
            <span class="question-type">multiple-choice</span>
        </div>

        <div class="question-text">In machine learning, model selection and evaluation are critical for achieving reliable generalization to unseen data. The process often involves careful consideration of dataset partitioning and algorithmic choices to prevent overfitting and ensure robust performance estimates.

Which practice helps prevent biased generalization error estimates when tuning hyperparameters in supervised learning?

1) Using the entire dataset for both training and testing   
2) Selecting hyperparameters based on test set performance   
3) Employing a validation set split from the training data to tune hyperparameters   
4) Setting hyperparameters randomly with each training run   
5) Ignoring regularization when evaluating models   
6) Combining training and test sets for model evaluation   
7) Using model capacity as the sole criterion for hyperparameter selection</div>

        <div class="answer-section">
            <div class="answer-label">‚úì Correct Answer:</div>
            <div class="answer-text">The correct answer is 3) Employing a validation set split from the training data to tune hyperparameters.</div>
        </div>

        <div class="reference-section">
            <div class="reference-label">üìö Reference Text:</div>
            <button class="toggle-button" onclick="toggleReference(50)">
                Show/Hide Reference
            </button>
            <div id="ref50" class="reference-text hidden">ÓÅ° ÓÅ¨ ÓÄ† ÓÅ£ÓÅ° ÓÅ∞ ÓÅ° ÓÅ£ÓÅ©ÓÅ¥ÓÅπ ÓÄ† ÓÄ® ÓÅ∞ ÓÅØ ÓÅ¨ ÓÅπ ÓÅÆ ÓÅØ ÓÅ≠ ÓÅ© ÓÅ° ÓÅ¨ ÓÄ† ÓÅ§ ÓÅ• ÓÅß ÓÅ≤ÓÅ• ÓÅ• ÓÄ© Figure5.4:TheeÔ¨Äectofthetrainingdatasetsizeonthetrainandtesterror,aswellas ontheoptimalmodelcapacity.Weconstructedasyntheticregressionproblembasedon addingamoderateamountofnoisetoadegree-5polynomial,generatedasingletestset, andthengeneratedseveraldiÔ¨Äerentsizesoftrainingset.Foreachsize,wegenerated40 diÔ¨Äerenttrainingsetsinordertoploterrorbarsshowing95percentconÔ¨Ådenceintervals. ( T o p )TheMSEonthetrainingandtestsetfortwodiÔ¨Äerentmodels:aquadraticmodel, andamodelwithdegreechosentominimizethetesterror.BothareÔ¨Åtinclosedform.For thequadraticmodel,thetrainingerrorincreasesasthesizeofthetrainingsetincreases. ThisisbecauselargerdatasetsarehardertoÔ¨Åt.Simultaneously,thetesterrordecreases, becausefewerincorrecthypothesesareconsistentwiththetrainingdata.Thequadratic modeldoesnothaveenoughcapacitytosolvethetask,soitstesterrorasymptotesto ahighvalue.ThetesterroratoptimalcapacityasymptotestotheBayeserror.The trainingerrorcanfallbelowtheBayeserror,duetotheabilityofthetrainingalgorithm tomemorizespeciÔ¨Åcinstancesofthetrainingset.AsthetrainingsizeincreasestoinÔ¨Ånity, thetrainingerrorofanyÔ¨Åxed-capacitymodel(here,thequadraticmodel)mustrisetoat leasttheBayeserror. Asthetrainingsetsizeincreases,theoptimalcapacity ( Bottom ) (shownhereasthedegreeoftheoptimalpolynomialregressor)increases. Theoptimal capacityplateausafterreachingsuÔ¨Écientcomplexitytosolvethetask. 1 1 7 CHAPTER5.MACHINELEARNINGBASICS performance(overallpossibletasks)asmerelypredictingthateverypointbelongs tothesameclass. Fortunately,theseresultsholdonlywhenweaverageoverpossibledata all generatingdistributions.Ifwemakeassumptionsaboutthekindsofprobability distributionsweencounterinreal-worldapplications,thenwecandesignlearning algorithmsthatperformwellonthesedistributions. Thismeansthatthegoalofmachinelearningresearchisnottoseekauniversal learningalgorithmortheabsolutebestlearningalgorithm.Instead,ourgoalisto understandwhatkindsofdistributionsarerelevanttothe‚Äúrealworld‚ÄùthatanAI agentexperiences,andwhatkindsofmachinelearningalgorithmsperformwellon datadrawnfromthekindsofdatageneratingdistributionswecareabout. 5.2.2Regularization Thenofreelunchtheoremimpliesthatwemustdesignourmachinelearning algorithmstoperformwellonaspeciÔ¨Åctask.Wedosobybuildingasetof preferencesintothelearningalgorithm.Whenthesepreferencesarealignedwith thelearningproblemsweaskthealgorithmtosolve,itperformsbetter. Sofar,theonlymethodofmodifyingalearningalgorithmthatwehavediscussed concretelyistoincreaseordecreasethemodel‚Äôsrepresentationalcapacitybyadding orremovingfunctionsfromthehypothesisspaceofsolutionsthelearningalgorithm isabletochoose.WegavethespeciÔ¨Åcexampleofincreasingordecreasingthe degreeofapolynomialforaregressionproblem.Theviewwehavedescribedso farisoversimpliÔ¨Åed. ThebehaviorofouralgorithmisstronglyaÔ¨Äectednotjustbyhowlargewe makethesetoffunctionsallowedinitshypothesisspace,butbythespeciÔ¨Åcidentity ofthosefunctions.Thelearningalgorithmwehavestudiedsofar,linearregression, hasahypothesisspaceconsistingofthesetoflinearfunctionsofitsinput.These linearfunctionscanbeveryusefulforproblemswheretherelationshipbetween inputsandoutputstrulyisclosetolinear.Theyarelessusefulforproblems thatbehaveinaverynonlinearfashion.Forexample,linearregressionwould notperformverywellifwetriedtouseittopredict sin(x)fromx.Wecanthus controltheperformanceofouralgorithmsbychoosingwhatkindoffunctionswe allowthemtodrawsolutionsfrom,aswellasbycontrollingtheamountofthese functions. Wecanalsogivealearningalgorithmapreferenceforonesolutioninits hypothesisspacetoanother.Thismeansthatbothfunctionsareeligible,butone ispreferred.TheunpreferredsolutionwillbechosenonlyifitÔ¨Åtsthetraining 1 1 8 CHAPTER5.MACHINELEARNINGBASICS datasigniÔ¨Åcantlybetterthanthepreferredsolution. Forexample,wecanmodifythetrainingcriterionforlinearregressiontoinclude weightdecay.Toperformlinearregressionwithweightdecay,weminimizeasum comprisingboththemeansquarederroronthetrainingandacriterionJ(w)that expressesapreferencefortheweightstohavesmallersquaredL2norm.SpeciÔ¨Åcally, J() = wMSEtrain+ŒªwÓÄæw, (5.18) whereŒªisavaluechosenaheadoftimethatcontrolsthestrengthofourpreference forsmallerweights.WhenŒª= 0,weimposenopreference,andlargerŒªforcesthe weightstobecomesmaller. MinimizingJ(w)resultsinachoiceofweightsthat makeatradeoÔ¨ÄbetweenÔ¨Åttingthetrainingdataandbeingsmall.Thisgivesus solutionsthathaveasmallerslope,orputweightonfewerofthefeatures.Asan exampleofhowwecancontrolamodel‚ÄôstendencytooverÔ¨ÅtorunderÔ¨Åtviaweight decay,wecantrainahigh-degreepolynomialregressionmodelwithdiÔ¨Äerentvalues of.SeeÔ¨Ågurefortheresults. Œª 5.5 ÓÅ∏ÓÄ∞ÓÅπÓÅï ÓÅÆ ÓÅ§ ÓÅ• ÓÅ≤ ÓÅ¶ ÓÅ© ÓÅ¥ ÓÅ¥ ÓÅ© ÓÅÆ ÓÅß ÓÄ® ÓÅÖ ÓÅ∏ ÓÅ£ ÓÅ• ÓÅ≥ÓÅ≥ÓÅ©ÓÅ∂ ÓÅ• ÓÄ† ÓÇ∏ ÓÄ© ÓÅ∏ÓÄ∞ÓÅπÓÅÅ ÓÅ∞ ÓÅ∞ ÓÅ≤ ÓÅØ ÓÅ∞ ÓÅ≤ ÓÅ© ÓÅ° ÓÅ¥ ÓÅ• ÓÄ† ÓÅ∑ ÓÅ• ÓÅ© ÓÅß ÓÅ® ÓÅ¥ ÓÄ† ÓÅ§ ÓÅ• ÓÅ£ ÓÅ° ÓÅπ ÓÄ® ÓÅç ÓÅ• ÓÅ§ ÓÅ© ÓÅµ ÓÅ≠ ÓÄ† ÓÇ∏ ÓÄ© ÓÅ∏ÓÄ∞ÓÅπÓÅè ÓÅ∂ ÓÅ• ÓÅ≤ ÓÅ¶ ÓÅ© ÓÅ¥ ÓÅ¥ ÓÅ© ÓÅÆ ÓÅß ÓÄ® ÓÄ∞ ÓÄ© ÓÇ∏ ÓÄ° Figure5.5:WeÔ¨Åtahigh-degreepolynomialregressionmodeltoourexampletrainingset fromÔ¨Ågure.Thetruefunctionisquadratic,buthereweuseonlymodelswithdegree9. 5.2 Wevarytheamountofweightdecaytopreventthesehigh-degreemodelsfromoverÔ¨Åtting. ( L e f t )WithverylargeŒª,wecanforcethemodeltolearnafunctionwithnoslopeat all.ThisunderÔ¨Åtsbecauseitcanonlyrepresentaconstantfunction.Witha ( C e n t e r ) mediumvalueof,thelearningalgorithmrecoversacurvewiththerightgeneralshape. Œª Eventhoughthemodeliscapableofrepresentingfunctionswithmuchmorecomplicated shape,weightdecayhasencouragedittouseasimplerfunctiondescribedbysmaller coeÔ¨Écients.Withweightdecayapproachingzero(i.e.,usingtheMoore-Penrose ( R i g h t ) pseudoinversetosolvetheunderdeterminedproblemwithminimalregularization),the degree-9polynomialoverÔ¨ÅtssigniÔ¨Åcantly,aswesawinÔ¨Ågure.5.2 1 1 9 CHAPTER5.MACHINELEARNINGBASICS Moregenerally,wecanregularizeamodelthatlearnsafunctionf(x;Œ∏)by addingapenaltycalledaregularizertothecostfunction.Inthecaseofweight decay,theregularizeris‚Ñ¶(w) =wÓÄæw.Inchapter,wewillseethatmanyother 7 regularizersarepossible. Expressingpreferencesforonefunctionoveranotherisamoregeneralway ofcontrollingamodel‚Äôscapacitythanincludingorexcludingmembersfromthe hypothesisspace.Wecanthinkofexcludingafunctionfromahypothesisspaceas expressinganinÔ¨Ånitelystrongpreferenceagainstthatfunction. Inourweightdecayexample,weexpressedourpreferenceforlinearfunctions deÔ¨Ånedwithsmallerweightsexplicitly, viaanextraterminthecriterionwe minimize.Thereare many otherwaysof expressing preferencesfor diÔ¨Äerent solutions,bothimplicitlyandexplicitly.Together,thesediÔ¨Äerentapproaches areknownasregularization. RegularizationisanymodiÔ¨Åcationwemaketoa learningalgorithmthatisintendedtoreduceitsgeneralizationerrorbutnotits trainingerror.RegularizationisoneofthecentralconcernsoftheÔ¨Åeldofmachine learning,rivaledinitsimportanceonlybyoptimization. Thenofreelunchtheoremhasmadeitclearthatthereisnobestmachine learningalgorithm,and,inparticular,nobestformofregularization. Instead wemustchooseaformofregularizationthatiswell-suitedtotheparticulartask wewanttosolve.Thephilosophyofdeeplearningingeneralandthisbookin particularisthataverywiderangeoftasks(suchasalloftheintellectualtasks thatpeoplecando)mayallbesolvedeÔ¨Äectivelyusingverygeneral-purposeforms ofregularization. 5.3HyperparametersandValidationSets Mostmachinelearningalgorithmshaveseveralsettingsthatwecanusetocontrol thebehaviorofthelearningalgorithm.Thesesettingsarecalledhyperparame- ters.Thevaluesofhyperparameters arenotadaptedbythelearningalgorithm itself(thoughwecan designa nestedlearning procedure where one learning algorithmlearnsthebesthyperparametersforanotherlearningalgorithm). InthepolynomialregressionexamplewesawinÔ¨Ågure,thereisasingle 5.2 hyperparameter:thedegreeofthepolynomial,whichactsasacapacityhyper- parameter.TheŒªvalueusedtocontrolthestrengthofweightdecayisanother exampleofahyperparameter. Sometimesasettingischosentobeahyperparameter thatthelearningal- gorithmdoesnotlearnbecauseitisdiÔ¨Éculttooptimize.Morefrequently,the 1 2 0 CHAPTER5.MACHINELEARNINGBASICS settingmustbeahyperparameter becauseitisnotappropriatetolearnthat hyperparameteronthetrainingset.Thisappliestoallhyperparameters that controlmodelcapacity.Iflearnedonthetrainingset,suchhyperparameters would alwayschoosethemaximumpossiblemodelcapacity,resultinginoverÔ¨Åtting(refer toÔ¨Ågure).Forexample,wecanalwaysÔ¨Åtthetrainingsetbetterwithahigher 5.3 degreepolynomialandaweightdecaysettingofŒª= 0thanwecouldwithalower degreepolynomialandapositiveweightdecaysetting. Tosolvethisproblem,weneedavalidationsetofexamplesthatthetraining algorithmdoesnotobserve. Earlierwediscussedhowaheld-outtestset,composedofexamplescomingfrom thesamedistributionasthetrainingset,canbeusedtoestimatethegeneralization errorofalearner,afterthelearningprocesshascompleted.Itisimportantthatthe testexamplesarenotusedinanywaytomakechoicesaboutthemodel,including itshyperparameters . Forthisreason,noexamplefromthetestsetcanbeused inthevalidationset.Therefore,wealwaysconstructthevalidationsetfromthe trainingdata.SpeciÔ¨Åcally,wesplitthetrainingdataintotwodisjointsubsets.One ofthesesubsetsisusedtolearntheparameters.Theothersubsetisourvalidation set,usedtoestimatethegeneralization errorduringoraftertraining,allowing forthehyperparameterstobeupdatedaccordingly.Thesubsetofdatausedto learntheparametersisstilltypicallycalledthetrainingset,eventhoughthis maybeconfusedwiththelargerpoolofdatausedfortheentiretrainingprocess. Thesubsetofdatausedtoguidetheselectionofhyperparameters iscalledthe validationset.Typically,oneusesabout80%ofthetrainingdatafortrainingand 20%forvalidation.Sincethevalidationsetisusedto‚Äútrain‚Äùthehyperparameters , thevalidationseterrorwillunderestimatethegeneralization error,thoughtypically byasmalleramountthanthetrainingerror.Afterallhyperparameter optimization iscomplete,thegeneralization errormaybeestimatedusingthetestset. Inpractice, when thesametestsethasbeenusedrepeatedlytoevaluate performanceofdiÔ¨Äerentalgorithmsovermanyyears,andespeciallyifweconsider alltheattemptsfromthescientiÔ¨Åccommunityatbeatingthereportedstate-of- the-artperformanceonthattestset,weenduphavingoptimisticevaluationswith thetestsetaswell.BenchmarkscanthusbecomestaleandthendonotreÔ¨Çectthe trueÔ¨Åeldperformance ofatrainedsystem.Thankfully,thecommunitytendsto moveontonew(andusuallymoreambitiousandlarger)benchmarkdatasets. 1 2 1 CHAPTER5.MACHINELEARNINGBASICS 5.3.1Cross-Validation DividingthedatasetintoaÔ¨ÅxedtrainingsetandaÔ¨Åxedtestsetcanbeproblematic ifitresultsinthetestsetbeingsmall.Asmalltestsetimpliesstatisticaluncertainty aroundtheestimatedaveragetesterror,makingitdiÔ¨Éculttoclaimthatalgorithm Aworksbetterthanalgorithmonthegiventask. B Whenthedatasethashundredsofthousandsofexamplesormore,thisisnota seriousissue.Whenthedatasetistoosmall,arealternativeproceduresenableone tousealloftheexamplesintheestimationofthemeantesterror,atthepriceof increasedcomputational cost.Theseproceduresarebasedontheideaofrepeating thetrainingandtestingcomputationondiÔ¨Äerentrandomlychosensubsetsorsplits oftheoriginaldataset.Themostcommonoftheseisthek-foldcross-validation procedure,showninalgorithm ,inwhichapartitionofthedatasetisformedby 5.1 splittingitintoknon-overlappingsubsets.Thetesterrormaythenbeestimated bytakingtheaveragetesterroracrossktrials.Ontriali,thei-thsubsetofthe dataisusedasthetestsetandtherestofthedataisusedasthetrainingset.One problemisthatthereexistnounbiasedestimatorsofthevarianceofsuchaverage errorestimators(BengioandGrandvalet2004,),butapproximationsaretypically used. 5.4Estimators,BiasandVariance TheÔ¨Åeldofstatisticsgivesusmanytoolsthatcanbeusedtoachievethemachine learninggoalofsolvingatasknotonlyonthetrainingsetbutalsotogeneralize. Foundationalconceptssuchasparameterestimation,biasandvarianceareuseful toformallycharacterizenotionsofgeneralization, underÔ¨ÅttingandoverÔ¨Åtting. 5.4.1PointEstimation Pointestimationistheattempttoprovidethesingle‚Äúbest‚Äùpredictionofsome quantityofinterest.Ingeneralthequantityofinterestcanbeasingleparameter oravectorofparametersinsomeparametricmodel,suchastheweightsinour linearregressionexampleinsection,butitcanalsobeawholefunction. 5.1.4 Inordertodistinguishestimatesofparametersfromtheirtruevalue,our conventionwillbetodenoteapointestimateofaparameterbyŒ∏ ÀÜŒ∏. Let{x(1),...,x() m}beasetofmindependentandidenticallydistributed 1 2 2 CHAPTER5.MACHINELEARNINGBASICS Algorithm5.1Thek-foldcross-validationalgorithm.Itcanbeusedtoestimate generalization errorofalearningalgorithmAwhenthegivendataset Distoo smallforasimpletrain/testortrain/validsplittoyieldaccurateestimationof generalization error,becausethemeanofalossLonasmalltestsetmayhavetoo highvariance.Thedataset Dcontainsaselementstheabstractexamplesz() i(for thei-thexample),whichcouldstandforan(input,target) pairz() i= (x() i,y() i) inthecaseofsupervisedlearning,orforjustaninputz() i=x() iinthecase ofunsupervisedlearning. The algorithmreturnsthevectoroferrorseforeach examplein D,whosemeanistheestimatedgeneralization error. Theerrorson individualexamplescanbeusedtocomputeaconÔ¨Ådenceintervalaroundthemean (equation). WhiletheseconÔ¨Ådenceintervalsarenotwell-justiÔ¨Åedafterthe 5.47 useofcross-validation,itisstillcommonpracticetousethemtodeclarethat algorithmAisbetterthanalgorithmBonlyiftheconÔ¨Ådenceintervaloftheerror ofalgorithmAliesbelowanddoesnotintersecttheconÔ¨Ådenceintervalofalgorithm B. DeÔ¨ÅneKFoldXV(): D,A,L,k Require: D,thegivendataset,withelementsz() i Require:A,thelearningalgorithm,seenasafunctionthattakesadatasetas inputandoutputsalearnedfunction Require:L,thelossfunction,seenasafunctionfromalearnedfunctionfand anexamplez() i‚àà ‚àà Dtoascalar R Require:k,thenumberoffolds Splitintomutuallyexclusivesubsets Dk D i,whoseunionis. D fordoikfromto1 f i= (A D D\ i) forz() jin D ido e j= (Lf i,z() j) endfor endfor Returne 1 2 3 CHAPTER5.MACHINELEARNINGBASICS (i.i.d.)datapoints.A orisanyfunctionofthedata: pointestimatorstatistic ÀÜŒ∏ m= (gx(1),...,x() m). (5.19) ThedeÔ¨Ånitiondoesnotrequirethatgreturnavaluethatisclosetothetrue Œ∏oreventhattherangeofgisthesameasthesetofallowablevaluesofŒ∏. ThisdeÔ¨Ånitionofapointestimatorisverygeneralandallowsthedesignerofan estimatorgreatÔ¨Çexibility.WhilealmostanyfunctionthusqualiÔ¨Åesasanestimator, agoodestimatorisafunctionwhoseoutputisclosetothetrueunderlyingŒ∏that generatedthetrainingdata. Fornow,wetakethefrequentistperspectiveonstatistics.Thatis,weassume thatthetrueparametervalueŒ∏isÔ¨Åxedbutunknown,whilethepointestimate ÀÜŒ∏isafunctionofthedata.Sincethedataisdrawnfromarandomprocess,any functionofthedataisrandom.Therefore ÀÜŒ∏isarandomvariable. Pointestimationcanalsorefertotheestimationoftherelationshipbetween inputandtargetvariables.Werefertothesetypesofpointestimatesasfunction estimators. FunctionEstimationAswementionedabove,sometimesweareinterestedin performingfunctionestimation(orfunctionapproximation).Herewearetryingto predictavariableygivenaninputvectorx.Weassumethatthereisafunction f(x)thatdescribestheapproximate relationshipbetweenyandx.Forexample, wemayassumethaty=f(x)+ÓÄè,whereÓÄèstandsforthepartofythatisnot predictablefromx. Infunctionestimation,weareinterestedinapproximating fwithamodelorestimate ÀÜf.Functionestimationisreallyjustthesameas estimatingaparameterŒ∏;thefunctionestimator ÀÜfissimplyapointestimatorin functionspace.Thelinearregressionexample(discussedaboveinsection)and5.1.4 thepolynomialregressionexample(discussedinsection)arebothexamplesof 5.2 scenariosthatmaybeinterpretedeitherasestimatingaparameterworestimating afunction ÀÜf</div>
        </div>
    </div>

    <div class="question-card" id="q51">
        <div class="question-header">
            <span class="question-number">Question 51</span>
            <span class="question-type">multiple-choice</span>
        </div>

        <div class="question-text">In statistics, point estimators are used to infer unknown parameters based on observed data. The concept of bias is crucial for evaluating how accurately an estimator reflects the true parameter value.

Which of the following statements best characterizes an unbiased estimator for a parameter Œ∏ in a statistical distribution?

1) Its expected value exactly equals Œ∏ for any sample size.   
2) Its bias approaches zero as the sample size becomes infinitely large.   
3) Its variance is minimized for all possible sample sizes.   
4) It always produces sample estimates equal to Œ∏ in every trial.   
5) It is unaffected by changes in the underlying data distribution.   
6) Its expected value is always greater than Œ∏.   
7) It never converges to Œ∏ as more data is collected.</div>

        <div class="answer-section">
            <div class="answer-label">‚úì Correct Answer:</div>
            <div class="answer-text">The correct answer is 1) Its expected value exactly equals Œ∏ for any sample size..</div>
        </div>

        <div class="reference-section">
            <div class="reference-label">üìö Reference Text:</div>
            <button class="toggle-button" onclick="toggleReference(51)">
                Show/Hide Reference
            </button>
            <div id="ref51" class="reference-text hidden">y mappingfromtox. Wenowreviewthemostcommonlystudiedpropertiesofpointestimatorsand discusswhattheytellusabouttheseestimators. 5.4.2Bias ThebiasofanestimatorisdeÔ¨Ånedas: bias(ÀÜŒ∏ m) = ( EÀÜŒ∏ m)‚àíŒ∏ (5.20) 1 2 4 CHAPTER5.MACHINELEARNINGBASICS wheretheexpectationisoverthedata(seenassamplesfromarandomvariable) andŒ∏isthetrueunderlyingvalueofŒ∏usedtodeÔ¨Ånethedatageneratingdistri- bution.Anestimator ÀÜŒ∏ missaidtobeunbiasedifbias(ÀÜŒ∏ m) = 0,whichimplies that E(ÀÜŒ∏ m)=Œ∏.Anestimator ÀÜŒ∏ missaidtobeasymptoticallyunbiasedif lim m ‚Üí ‚àûbias(ÀÜŒ∏ m) = 0,whichimpliesthatlim m ‚Üí ‚àû E(ÀÜŒ∏ m) = Œ∏. Example:BernoulliDistributionConsiderasetofsamples {x(1),...,x() m} thatareindependentlyandidenticallydistributedaccordingtoaBernoullidistri- butionwithmean:Œ∏ Px(() i;) = Œ∏Œ∏x() i(1 )‚àíŒ∏(1 ‚àí x() i). (5.21) AcommonestimatorfortheŒ∏parameterofthisdistributionisthemeanofthe trainingsamples: ÀÜŒ∏ m=1 mmÓÅò i=1x() i. (5.22) Todeterminewhetherthisestimatorisbiased,wecansubstituteequation5.22 intoequation:5.20 bias(ÀÜŒ∏ m) = [ EÀÜŒ∏ m]‚àíŒ∏ (5.23) = EÓÄ¢ 1 mmÓÅò i=1x() iÓÄ£ ‚àíŒ∏ (5.24) =1 mmÓÅò i=1EÓÅ® x() iÓÅ© ‚àíŒ∏ (5.25) =1 mmÓÅò i=11ÓÅò x() i=0ÓÄê x() iŒ∏x() i(1 )‚àíŒ∏(1 ‚àí x() i)ÓÄë ‚àíŒ∏(5.26) =1 mmÓÅò i=1()Œ∏‚àíŒ∏ (5.27) = = 0Œ∏Œ∏‚àí (5.28) Since bias(ÀÜŒ∏) = 0,wesaythatourestimator ÀÜŒ∏isunbiased. Example:GaussianDistributionEstimatoroftheMeanNow,consider asetofsamples {x(1),...,x() m}thatareindependentlyandidenticallydistributed accordingtoaGaussiandistributionp(x() i) =N(x() i;¬µ,œÉ2),wherei‚àà{1,...,m}. 1 2 5</div>
        </div>
    </div>

    <div class="question-card" id="q52">
        <div class="question-header">
            <span class="question-number">Question 52</span>
            <span class="question-type">multiple-choice</span>
        </div>

        <div class="question-text">Statistical and neural language models are essential for natural language processing tasks, with each approach offering unique ways to generalize and handle data sparsity. Advances in word representation have profoundly affected both computational efficiency and semantic understanding.

Which of the following is a key advantage of neural language models over classical n-gram models in overcoming the curse of dimensionality?

1) They use hierarchical clustering to group similar words.   
2) They rely exclusively on one-hot encoding for word representation.   
3) They estimate probabilities using only observed sequences.   
4) They employ distributed representations (embeddings) that map words to dense, lower-dimensional vectors capturing semantic similarity.   
5) They eliminate the need for smoothing techniques entirely.   
6) They restrict the output vocabulary to only frequently observed words.   
7) They utilize rule-based systems to predict word sequences.</div>

        <div class="answer-section">
            <div class="answer-label">‚úì Correct Answer:</div>
            <div class="answer-text">The correct answer is 4) They employ distributed representations (embeddings) that map words to dense, lower-dimensional vectors capturing semantic similarity..</div>
        </div>

        <div class="reference-section">
            <div class="reference-label">üìö Reference Text:</div>
            <button class="toggle-button" onclick="toggleReference(52)">
                Show/Hide Reference
            </button>
            <div id="ref52" class="reference-text hidden">Usuallywetrainbothan n-grammodelandan n‚àí1 grammodelsimultaneously. Thismakesiteasytocompute P x( t| x t n‚àí+1 , . . . , x t‚àí1) =P n( x t n‚àí+1 , . . . , x t) P n‚àí1( x t n‚àí+1 , . . . , x t‚àí1)(12.6) simplybylookinguptwostoredprobabilities. Forthistoexactlyreproduce inferencein P n,wemustomittheÔ¨Ånalcharacterfromeachsequencewhenwe train P n‚àí1. Asanexample,wedemonstratehowatrigrammodelcomputestheprobability ofthesentence‚ÄúTHEDOGRANAWAY.‚ÄùTheÔ¨Årstwordsofthesentencecannotbe handledbythedefaultformulabasedonconditionalprobabilitybecausethereisno contextatthebeginningofthesentence.Instead,wemustusethemarginalprob- abilityoverwordsatthestartofthesentence.Wethusevaluate P3( T H E D O G R A N). Finally,thelastwordmaybepredictedusingthetypicalcase,ofusingthecondi- tionaldistribution P( A W A Y D O G R A N | ).Puttingthistogetherwithequation,12.6 weobtain: P P ( ) = T H E D O G R A N A W A Y3( ) T H E D O G R A N P3( ) D O G R A N A W A Y /P2( ) D O G R A N . (12.7) Afundamentallimitationofmaximumlikelihoodfor n-grammodelsisthat P n asestimatedfromtrainingsetcountsisverylikelytobezeroinmanycases,even thoughthetuple ( x t n‚àí+1 , . . . , x t)mayappearinthetestset.Thiscancausetwo diÔ¨Äerentkindsofcatastrophicoutcomes.When P n‚àí1iszero,theratioisundeÔ¨Åned, sothemodeldoesnotevenproduceasensibleoutput.When P n‚àí1isnon-zerobut P niszero,thetestlog-likelihoodis‚àí‚àû. Toavoidsuchcatastrophicoutcomes, most n-grammodelsemploysomeformofsmoothing.Smoothingtechniques 4 6 2 CHAPTER12.APPLICATIONS shiftprobabilitymassfromtheobservedtuplestounobservedonesthataresimilar. See ()forareviewandempiricalcomparisons.Onebasic ChenandGoodman1999 techniqueconsistsofaddingnon-zeroprobabilitymasstoallofthepossiblenext symbolvalues.ThismethodcanbejustiÔ¨ÅedasBayesianinferencewithauniform orDirichletprioroverthecountparameters.Anotherverypopularideaistoform amixturemodelcontaininghigher-orderandlower-order n-grammodels,withthe higher-order modelsprovidingmorecapacityandthelower-ordermodelsbeing morelikelytoavoidcountsofzero.Back-oÔ¨Ämethodslook-upthelower-order n-gramsifthefrequencyofthecontext x t‚àí1 , . . . , x t n‚àí+1istoosmalltousethe higher-ordermodel.Moreformally,theyestimatethedistributionover x tbyusing contexts x t n k ‚àí+ , . . . , x t‚àí1,forincreasing k,untilasuÔ¨Écientlyreliableestimateis found. Classical n-grammodelsareparticularlyvulnerabletothecurseofdimension- ality.Thereare|| Vnpossible n-gramsand|| Visoftenverylarge.Evenwitha massivetrainingsetandmodest n,most n-gramswillnotoccurinthetrainingset. Onewaytoviewaclassical n-grammodelisthatitisperformingnearest-neighbor lookup.Inotherwords,itcanbeviewedasalocalnon-parametric predictor, similarto k-nearestneighbors.Thestatisticalproblemsfacingtheseextremely localpredictorsaredescribedinsection.Theproblemforalanguagemodel 5.11.2 isevenmoreseverethanusual,becauseanytwodiÔ¨Äerentwordshavethesamedis- tancefromeachotherinone-hotvectorspace.ItisthusdiÔ¨Éculttoleveragemuch informationfromany‚Äúneighbors‚Äù‚Äîonlytrainingexamplesthatrepeatliterallythe samecontextareusefulforlocalgeneralization. T oovercometheseproblems,a languagemodelmustbeabletoshareknowledgebetweenonewordandother semanticallysimilarwords. ToimprovethestatisticaleÔ¨Éciencyof n-grammodels,class-basedlanguage models(Brown1992NeyandKneser1993Niesler1998 e t a l .,; ,; e t a l .,)introduce thenotionofwordcategoriesandthensharestatisticalstrengthbetweenwordsthat areinthesamecategory.Theideaistouseaclusteringalgorithmtopartitionthe setofwordsintoclustersorclasses,basedontheirco-occurrencefrequencieswith otherwords.ThemodelcanthenusewordclassIDsratherthanindividualword IDstorepresentthecontextontherightsideoftheconditioningbar.Composite modelscombiningword-basedandclass-basedmodelsviamixingorback-oÔ¨Äare alsopossible.Althoughwordclassesprovideawaytogeneralizebetweensequences inwhichsomewordisreplacedbyanotherofthesameclass,muchinformationis lostinthisrepresentation. 4 6 3 CHAPTER12.APPLICATIONS 12.4.2NeuralLanguageModels NeurallanguagemodelsorNLMsare aclassoflanguagemodeldesigned toovercomethecurseofdimensionalityproblemformodelingnaturallanguage sequencesbyusingadistributedrepresentationofwords( ,). Bengio e t a l .2001 Unlikeclass-based n-grammodels,neurallanguagemodelsareabletorecognize thattwowordsaresimilarwithoutlosingtheabilitytoencodeeachwordasdistinct fromtheother.Neurallanguagemodelssharestatisticalstrengthbetweenone word(anditscontext)andothersimilarwordsandcontexts.Thedistributed representationthemodellearnsforeachwordenablesthissharingbyallowingthe modeltotreatwordsthathavefeaturesincommonsimilarly.Forexample,ifthe worddogandthewordcatmaptorepresentationsthatsharemanyattributes,then sentencesthatcontainthewordcatcaninformthepredictionsthatwillbemadeby themodelforsentencesthatcontaintheworddog,andvice-versa.Becausethere aremanysuchattributes,therearemanywaysinwhichgeneralization canhappen, transferringinformationfromeachtrainingsentencetoanexponentiallylarge numberofsemanticallyrelatedsentences.Thecurseofdimensionalityrequiresthe modeltogeneralizetoanumberofsentencesthatisexponentialinthesentence length.Themodelcountersthiscursebyrelatingeachtrainingsentencetoan exponentialnumberofsimilarsentences. Wesometimescallthesewordrepresentationswordembeddings.Inthis interpretation,weviewtherawsymbolsaspointsinaspaceofdimensionequal tothevocabularysize.Thewordrepresentationsembedthosepointsinafeature spaceoflowerdimension.Intheoriginalspace,everywordisrepresentedby aone-hotvector,soeverypairofwordsisatEuclideandistance‚àö 2fromeach other.Intheembeddingspace,wordsthatfrequentlyappearinsimilarcontexts (oranypairofwordssharingsome‚Äúfeatures‚Äùlearnedbythemodel)arecloseto eachother.Thisoftenresultsinwordswithsimilarmeaningsbeingneighbors. FigurezoomsinonspeciÔ¨Åcareasofalearnedwordembeddingspacetoshow 12.3 howsemanticallysimilarwordsmaptorepresentationsthatareclosetoeachother. NeuralnetworksinotherdomainsalsodeÔ¨Åneembeddings.Forexample,a hiddenlayerofaconvolutionalnetworkprovidesan‚Äúimageembedding.‚ÄùUsually NLPpractitioners aremuchmoreinterestedinthisideaofembeddingsbecause naturallanguagedoesnotoriginallylieinareal-valuedvectorspace.Thehidden layerhasprovidedamorequalitativelydramaticchangeinthewaythedatais represented. Thebasicideaofusingdistributedrepresentationstoimprovemodelsfor naturallanguageprocessingisnotrestrictedtoneuralnetworks.Itmayalsobe usedwithgraphicalmodelsthathavedistributedrepresentationsintheformof 4 6 4 CHAPTER12.APPLICATIONS multiplelatentvariables(MnihandHinton2007,). ‚àí ‚àí ‚àí ‚àí ‚àí 3432302826‚àí14‚àí13‚àí12‚àí11‚àí10‚àí9‚àí8‚àí7‚àí6 CanadaEuropeOntario NorthEnglish CanadianUnionAfricanAfrica BritishFrance RussianChina GermanyFrench AssemblyEU JapanIraq SouthEuropean 350355360365370375380 . . . . . . .171819202122 1995199619971998199920002001 200220032004 20052006200720082009 Figure12.3:Two-dimensionalvisualizationsofwordembeddingsobtainedfromaneural machinetranslationmodel( ,),zoominginonspeciÔ¨Åcareaswhere Bahdanau e t a l .2015 semanticallyrelatedwordshaveembeddingvectorsthatareclosetoeachother.Countries appearontheleftandnumbersontheright.Keepinmindthattheseembeddingsare2-D forthepurposeofvisualization.Inrealapplications,embeddingstypicallyhavehigher dimensionalityandcansimultaneouslycapturemanykindsofsimilaritybetweenwords. 12.4.3High-DimensionalOutputs Inmanynaturallanguageapplications,weoftenwantourmodelstoproduce words(ratherthancharacters)asthefundamentalunitoftheoutput.Forlarge vocabularies,itcanbeverycomputationally expensivetorepresentanoutput distributionoverthechoiceofaword,becausethevocabularysizeislarge.Inmany applications, Vcontainshundredsofthousandsofwords.Thenaiveapproachto representingsuchadistributionistoapplyanaÔ¨Énetransformationfromahidden representationtotheoutputspace,thenapplythesoftmaxfunction.Suppose wehaveavocabulary Vwithsize|| V.Theweightmatrixdescribingthelinear componentofthisaÔ¨Énetransformationisverylarge,becauseitsoutputdimension is|| V.Thisimposesahighmemorycosttorepresentthematrix,andahigh computational costtomultiplybyit.Becausethesoftmaxisnormalizedacrossall || Voutputs,itisnecessarytoperformthefullmatrixmultiplicationattraining timeaswellastesttime‚Äîwecannotcalculateonlythedotproductwiththeweight vectorforthecorrectoutput.Thehighcomputational costsoftheoutputlayer thusarisebothattrainingtime(tocomputethelikelihoodanditsgradient)and attesttime(tocomputeprobabilities forallorselectedwords).Forspecialized 4 6 5</div>
        </div>
    </div>

    <div class="question-card" id="q53">
        <div class="question-header">
            <span class="question-number">Question 53</span>
            <span class="question-type">multiple-choice</span>
        </div>

        <div class="question-text">Deep learning relies on solid foundations in applied mathematics, particularly linear algebra and probability theory. Techniques such as principal component analysis (PCA) and probabilistic modeling are essential for data representation and reasoning under uncertainty.

Which concept is most directly used to compress and represent high-dimensional data by identifying its directions of maximum variance?

1) Trace of a matrix   
2) Chain rule for probability   
3) Marginal probability   
4) Matrix determinant   
5) Independence of random variables   
6) Principal component analysis   
7) Matrix pseudoinverse</div>

        <div class="answer-section">
            <div class="answer-label">‚úì Correct Answer:</div>
            <div class="answer-text">The correct answer is 6) Principal component analysis.</div>
        </div>

        <div class="reference-section">
            <div class="reference-label">üìö Reference Text:</div>
            <button class="toggle-button" onclick="toggleReference(53)">
                Show/Hide Reference
            </button>
            <div id="ref53" class="reference-text hidden">Deep L ea r ni n g I a n G o o d f e l l o w Y o s h u a B e n g i o A a r o n C o u r v i l l e C on t e n t s Website vii Acknowledgments viii Notation xi 1Introduction 1 1.1WhoShouldReadThisBook?. . . . . .. . . . . . . . . . . . . .8 1.2HistoricalTrendsinDeepLearning. . . . . . . . . . . . . . . . . 11 IAppliedMathandMachineLearningBasics 29 2LinearAlgebra 31 2.1Scalars,Vectors,MatricesandTensors. . . . . . . . . . . . . . .31 2.2MultiplyingMatricesandVectors. . . . . . . . . . . . . . . . . .34 2.3IdentityandInverseMatrices. . . . . . . . . . . . . . . . . . . .36 2.4LinearDependenceandSpan. . . . . . . . . . . . . . . . . . . .37 2.5Norms. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39 2.6SpecialKindsofMatricesandVectors. . . . . . . . . . . . . . . 40 2.7Eigendecomposition. . . . . . . . . . . . . . . . . . . . . . . . . . 42 2.8SingularValueDecomposition. . . . . . . . . . . . . . . . . . . .44 2.9TheMoore-PenrosePseudoinverse. . . . . . . . . . . . . . . . . .45 2.10TheTraceOperator. . . . . . . . . . . . . . . . . . . . . . . . . 46 2.11TheDeterminant. .. . . . . . . . . . . . . . . . . . . . . . . . . 47 2.12Example:PrincipalComponentsAnalysis. . . . . . . . . . . . .48 3ProbabilityandInformationTheory 53 3.1WhyProbability?. . . . .. . . . . . . . . . . . . . . . . . . . . .54 i CO NTE NT S 3.2RandomVariables. . . . .. . . . . . . . . . . . . . . . . . . . .56 3.3ProbabilityDistributions. . . . . . . . . . . . . . . . . . . . . . .56 3.4MarginalProbability. . . . . . . . . . . . . . . . . . . . . . . . . 58 3.5ConditionalProbability. .. . . . . . . . . . . . . . . . . . . . .59 3.6TheChainRuleofConditionalProbabilities. . . . . . . . . . . .59 3.7IndependenceandConditionalIndependence. . .</div>
        </div>
    </div>

    <div class="question-card" id="q54">
        <div class="question-header">
            <span class="question-number">Question 54</span>
            <span class="question-type">multiple-choice</span>
        </div>

        <div class="question-text">In neural network design for computer vision, different convolutional layer types achieve varying degrees of parameter sharing and invariance, affecting how models generalize and reconstruct input data. Understanding these distinctions is vital for choosing appropriate architectures for specific learning tasks.

Which statement accurately describes the key difference between tiled convolution and standard convolution in neural networks?

1) Tiled convolution uses shared weights everywhere, while standard convolution cycles through several unique kernels per output location.   
2) Standard convolution assigns unique weights to each connection, making it parameter-inefficient compared to tiled convolution.   
3) Tiled convolution enforces strict translation invariance by hard-coding the same kernel for all positions.   
4) Standard convolution applies multiple kernels in a periodic fashion, determined by modulo operations over the output map.   
5) Tiled convolution cycles through a set of kernels across the output map for moderate parameter sharing, while standard convolution uses one kernel everywhere for maximal parameter sharing.   
6) Tiled convolution reconstructs inputs by multiplying with the transpose of the kernel tensor, whereas standard convolution does not support this operation.   
7) Standard convolution is typically used only for unsupervised learning tasks, while tiled convolution is reserved for generative models.</div>

        <div class="answer-section">
            <div class="answer-label">‚úì Correct Answer:</div>
            <div class="answer-text">The correct answer is 5) Tiled convolution cycles through a set of kernels across the output map for moderate parameter sharing, while standard convolution uses one kernel everywhere for maximal parameter sharing..</div>
        </div>

        <div class="reference-section">
            <div class="reference-label">üìö Reference Text:</div>
            <button class="toggle-button" onclick="toggleReference(54)">
                Show/Hide Reference
            </button>
            <div id="ref54" class="reference-text hidden">x 2 x 2 x 3 x 3s 2 s 2 s 1 s 1 s 3 s 3 x 4 x 4s 4 s 4 x 5 x 5s 5 s 5x 1 x 1 x 2 x 2 x 3 x 3s 2 s 2 s 1 s 1 s 3 s 3 x 4 x 4s 4 s 4 x 5 x 5s 5 s 5 a b a b a b a b a a b c d e f g h i x 1 x 1 x 2 x 2 x 3 x 3s 2 s 2 s 1 s 1 s 3 s 3 x 4 x 4s 4 s 4 x 5 x 5s 5 s 5 a b c d a b c d a Figure9.16:Acomparisonoflocallyconnectedlayers,tiledconvolution,andstandard convolution.Allthreehavethesamesetsofconnectionsbetweenunits,whenthesame sizeofkernelisused.Thisdiagramillustratestheuseofakernelthatistwopixelswide. ThediÔ¨Äerencesbetweenthemethodsliesinhowtheyshareparameters. ( T o p )Alocally connectedlayerhasnosharingatall.Weindicatethateachconnectionhasitsownweight bylabelingeachconnectionwithauniqueletter.Tiledconvolutionhasasetof ( C e n t e r ) tdiÔ¨Äerentkernels.Hereweillustratethecaseof t= 2. Oneofthesekernelshasedges labeled‚Äúa‚Äùand‚Äúb,‚Äùwhiletheotherhasedgeslabeled‚Äúc‚Äùand‚Äúd.‚Äù Eachtimewemoveone pixeltotherightintheoutput,wemoveontousingadiÔ¨Äerentkernel.Thismeansthat, likethelocallyconnectedlayer,neighboringunitsintheoutputhavediÔ¨Äerentparameters. Unlikethelocallyconnectedlayer,afterwehavegonethroughall tavailablekernels, wecyclebacktotheÔ¨Årstkernel.Iftwooutputunitsareseparatedbyamultipleof t steps,thentheyshareparameters.Traditionalconvolutionisequivalenttotiled ( Bottom ) convolutionwith t= 1.Thereisonlyonekernelanditisappliedeverywhere,asindicated inthediagrambyusingthekernelwithweightslabeled‚Äúa‚Äùand‚Äúb‚Äùeverywhere. 3 5 5 CHAPTER9.CONVOLUTIONALNETWORKS TodeÔ¨Ånetiledconvolutionalgebraically,let kbea6-Dtensor,wheretwoof thedimensionscorrespondtodiÔ¨Äerentlocationsintheoutputmap.Ratherthan havingaseparateindexforeachlocationintheoutputmap,outputlocationscycle throughasetof tdiÔ¨Äerentchoicesofkernelstackineachdirection.If tisequalto theoutputwidth,thisisthesameasalocallyconnectedlayer. Z i , j , k=ÓÅò l , m , nV l , j m , k n + ‚àí 1 + ‚àí 1 K i , l , m , n , j t , k t % + 1 % + 1 ,(9.10) whereis themodulooperation,with % t% t=0(, t+1)% t=1,etc.It is straightforwardtogeneralizethisequationtouseadiÔ¨Äerenttilingrangeforeach dimension. Bothlocallyconnectedlayersandtiledconvolutionallayershaveaninteresting interactionwithmax-pooling:thedetectorunitsoftheselayersaredrivenby diÔ¨ÄerentÔ¨Ålters.IftheseÔ¨ÅlterslearntodetectdiÔ¨Äerenttransformedversionsof thesameunderlyingfeatures,thenthemax-pooledunitsbecomeinvarianttothe learnedtransformation(seeÔ¨Ågure).Convolutionallayersarehard-codedtobe 9.9 invariantspeciÔ¨Åcallytotranslation. Otheroperationsbesidesconvolutionareusuallynecessarytoimplementa convolutionalnetwork.Toperformlearning,onemustbeabletocomputethe gradientwithrespecttothekernel,giventhegradientwithrespecttotheoutputs. Insomesimplecases, thisoperationcanbeperformedusingtheconvolution operation,butmanycasesofinterest,includingthecaseofstridegreaterthan1, donothavethisproperty. Recallthatconvolutionisalinearoperationandcanthusbedescribedasa matrixmultiplication (ifweÔ¨ÅrstreshapetheinputtensorintoaÔ¨Çatvector).The matrixinvolvedisafunctionoftheconvolutionkernel.Thematrixissparseand eachelementofthekerneliscopiedtoseveralelementsofthematrix.Thisview helpsustoderivesomeoftheotheroperationsneededtoimplementaconvolutional network. Multiplication bythetransposeofthematrixdeÔ¨Ånedbyconvolutionisone suchoperation.Thisistheoperationneededtoback-propagate errorderivatives throughaconvolutionallayer,soitisneededtotrainconvolutionalnetworks thathavemorethanonehiddenlayer.Thissameoperationisalsoneededifwe wishtoreconstructthevisibleunitsfromthehiddenunits( ,). Simard etal.1992 Reconstructingthevisibleunitsisanoperationcommonlyusedinthemodels describedinpartofthisbook,suchasautoencoders,RBMs,andsparsecoding. III Transposeconvolutionisnecessarytoconstructconvolutionalversionsofthose models.Likethekernelgradientoperation,thisinputgradientoperationcanbe 3 5 6 CHAPTER9.CONVOLUTIONALNETWORKS implementedusingaconvolutioninsomecases,butinthegeneralcaserequires athirdoperationtobeimplemented.Caremustbetakentocoordinatethis transposeoperationwiththeforwardpropagation. Thesizeoftheoutputthatthe transposeoperationshouldreturndependsonthezeropaddingpolicyandstrideof theforwardpropagationoperation,aswellasthesizeoftheforwardpropagation‚Äôs outputmap.Insomecases,multiplesizesofinputtoforwardpropagationcan resultinthesamesizeofoutputmap,sothetransposeoperationmustbeexplicitly toldwhatthesizeoftheoriginalinputwas. Thesethreeoperations‚Äîconvolution,backpropfromoutputtoweights,and backpropfromoutputtoinputs‚ÄîaresuÔ¨Écienttocomputeallofthegradients neededtotrainanydepthoffeedforwardconvolutionalnetwork,aswellastotrain convolutionalnetworkswithreconstructionfunctionsbasedonthetransposeof convolution. See ()forafullderivationoftheequationsinthe Goodfellow2010 fullygeneralmulti-dimensional,multi-example case.Togiveasenseofhowthese equationswork,wepresentthetwodimensional,singleexampleversionhere. Supposewewanttotrainaconvolutionalnetworkthatincorporatesstrided convolutionofkernelstack Kappliedtomulti-channelimage Vwithstride sas deÔ¨Ånedby c( K V , , s)asinequation.Supposewewanttominimizesomeloss 9.8 function J( V K ,).Duringforwardpropagation, wewillneedtouse citselfto output Z,whichisthenpropagatedthroughtherestofthenetworkandusedto computethecostfunction J.Duringback-propagation, wewillreceiveatensor G suchthat G i , j , k=‚àÇ ‚àÇ Z i , j , kJ , . ( V K) Totrainthenetwork,weneedtocomputethederivativeswithrespecttothe weightsinthekernel.Todoso,wecanuseafunction g , , s ( G V) i , j , k, l=‚àÇ ‚àÇ K i , j , k, lJ ,( V K) =ÓÅò m , nG i , m , n V j , m s k, n s l ( ‚àí √ó 1 ) + ( ‚àí √ó 1 ) + .(9.11) Ifthislayerisnotthebottomlayerofthenetwork,wewillneedtocompute thegradientwithrespectto Vinordertoback-propagate theerrorfartherdown. Todoso,wecanuseafunction h , , s ( K G) i , j , k=‚àÇ ‚àÇ V i , j , kJ ,( V K) (9.12) =ÓÅò l , m s . t . ( 1 ) + = l ‚àí √ó s m jÓÅò n , p s . t . ( 1 ) + = n ‚àí √ó s p kÓÅò qK q , i , m , p G q , l , n .(9.13) Autoencodernetworks, describedinchapter, arefeedforwardnetworks 14 trainedtocopytheirinputtotheiroutput.AsimpleexampleisthePCAalgorithm, 3 5 7 CHAPTER9.CONVOLUTIONALNETWORKS thatcopiesitsinput xtoanapproximatereconstruction rusingthefunction WÓÄæW x.Itiscommonformore general autoencoders tousemultiplication bythetransposeoftheweightmatrixjustasPCAdoes. Tomakesuchmodels</div>
        </div>
    </div>

    <div class="question-card" id="q55">
        <div class="question-header">
            <span class="question-number">Question 55</span>
            <span class="question-type">multiple-choice</span>
        </div>

        <div class="question-text">Manifold learning is a set of machine learning techniques used to identify low-dimensional structures underlying high-dimensional data, such as images of faces that vary due to changes in pose or rotation. These approaches are central to unsupervised representation learning, generative modeling, and deep learning applications.

When applying manifold learning to a collection of facial images where subjects rotate their heads, which benefit is directly achieved by mapping the data onto a lower-dimensional manifold representing rotation angles?

1) Improving supervised classification accuracy of emotion expressions   
2) Reducing memory usage for storing image datasets   
3) Enhancing image resolution through upsampling   
4) Disentangling the underlying factors of variation responsible for different facial appearances   
5) Automatically labeling poses with semantic tags   
6) Increasing robustness to occlusions in images   
7) Compressing pixel values into binary codes</div>

        <div class="answer-section">
            <div class="answer-label">‚úì Correct Answer:</div>
            <div class="answer-text">The correct answer is 4) Disentangling the underlying factors of variation responsible for different facial appearances.</div>
        </div>

        <div class="reference-section">
            <div class="reference-label">üìö Reference Text:</div>
            <button class="toggle-button" onclick="toggleReference(55)">
                Show/Hide Reference
            </button>
            <div id="ref55" class="reference-text hidden">principleisappliedinmanycontexts.Figureshowsthemanifoldstructureof 5.13 adatasetconsistingoffaces.Bytheendofthisbook,wewillhavedevelopedthe methodsnecessarytolearnsuchamanifoldstructure.InÔ¨Ågure,wewillsee 20.6 howamachinelearningalgorithmcansuccessfullyaccomplishthisgoal. Thisconcludespart,whichhasprovidedthebasicconceptsinmathematics I andmachinelearningwhichareemployedthroughouttheremainingpartsofthe book.Youarenowpreparedtoembarkuponyourstudyofdeeplearning. 1 6 4 CHAPTER5.MACHINELEARNINGBASICS Figure5.13:TrainingexamplesfromtheQMULMultiviewFaceDataset( ,) Gong e t a l .2000 forwhichthesubjectswereaskedtomoveinsuchawayastocoverthetwo-dimensional manifoldcorrespondingtotwoanglesofrotation.Wewouldlikelearningalgorithmstobe abletodiscoveranddisentanglesuchmanifoldcoordinates.Figureillustratessucha 20.6 feat. 1 6 5</div>
        </div>
    </div>

    <div class="question-card" id="q56">
        <div class="question-header">
            <span class="question-number">Question 56</span>
            <span class="question-type">multiple-choice</span>
        </div>

        <div class="question-text">Probabilistic models with latent variables are foundational in unsupervised learning and generative modeling, especially for understanding the structure of data through linear and nonlinear transformations. Techniques like PCA, factor analysis, and ICA are widely used for dimensionality reduction and signal separation.

Which modeling approach uniquely seeks to decompose observed data into statistically independent latent components, thereby enabling applications such as blind source separation?

1) Probabilistic PCA with zero noise variance   
2) Classical PCA using orthogonal projections   
3) Factor analysis with variable-specific noise   
4) Expectation-Maximization in mixture models   
5) Deep generative models with nonlinear mappings   
6) Linear factor models with shared Gaussian noise   
7) Independent Component Analysis (ICA) </div>

        <div class="answer-section">
            <div class="answer-label">‚úì Correct Answer:</div>
            <div class="answer-text">The correct answer is 7) Independent Component Analysis (ICA).</div>
        </div>

        <div class="reference-section">
            <div class="reference-label">üìö Reference Text:</div>
            <button class="toggle-button" onclick="toggleReference(56)">
                Show/Hide Reference
            </button>
            <div id="ref56" class="reference-text hidden">who wan t s t o un- I I I ders t and t he breadth of p e r s p e c t iv e s t hat hav e b e e n brought t o t he Ô¨Å e ld of deep learning, and pus h t he Ô¨Å e ld f orward t ow ards t r ue artiÔ¨Åcial intelligence. 4 8 8 C h a p t e r 1 3 L i n e ar F act or Mo d e l s Manyoftheresearchfrontiersindeeplearninginvolvebuildingaprobabilisticmodel oftheinput, p m o de l( x).Suchamodelcan,inprinciple,useprobabilisticinferenceto predictanyofthevariablesinitsenvironmentgivenanyoftheothervariables.Many ofthesemodelsalsohavelatentvariables h,with p m o de l() = x E h p m o de l( ) x h|. Theselatentvariablesprovideanothermeansofrepresentingthedata.Distributed representationsbased onlatent variablescanobtain alloftheadvantagesof representationlearningthatwehaveseenwithdeepfeedforwardandrecurrent networks. Inthischapter,wedescribesomeofthesimplestprobabilisticmodelswith latentvariables:linearfactormodels.Thesemodelsaresometimesusedasbuilding blocksofmixturemodels(Hinton1995aGhahramaniandHinton1996 e t a l .,; ,; Roweis2002 Tang2012 e t a l .,)orlarger,deepprobabilisticmodels( e t a l .,).They alsoshowmanyofthebasicapproachesnecessarytobuildgenerativemodelsthat themoreadvanceddeepmodelswillextendfurther. AlinearfactormodelisdeÔ¨Ånedbytheuseofastochastic,lineardecoder functionthatgeneratesbyaddingnoisetoalineartransformationof. x h Thesemodelsareinterestingbecausetheyallowustodiscoverexplanatory factorsthathaveasimplejointdistribution.Thesimplicityofusingalineardecoder madethesemodelssomeoftheÔ¨Årstlatentvariablemodelstobeextensivelystudied. Alinearfactormodeldescribesthedatagenerationprocessasfollows.First, wesampletheexplanatoryfactorsfromadistribution h h‚àº p ,() h (13.1) where p( h)isafactorialdistribution,with p( h) =ÓÅë i p( h i),sothatitiseasyto 489 CHAPTER13.LINEARFACTORMODELS samplefrom.Nextwesamplethereal-valuedobservablevariablesgiventhefactors: x W h b = ++noise (13.2) wherethenoiseistypicallyGaussiananddiagonal(independentacrossdimensions). ThisisillustratedinÔ¨Ågure.13.1 h 1 h 1 h 2 h 2 h 3 h 3 x 1 x 1 x 2 x 2 x 3 x 3 x h n o i s e x h n o i s e = W + + b = W + + b Figure13.1:Thedirectedgraphicalmodeldescribingthelinearfactormodelfamily,in whichweassumethatanobserveddatavector xisobtainedbyalinearcombinationof independentlatentfactors h,plussomenoise.DiÔ¨Äerentmodels,suchasprobabilistic PCA,factoranalysisorICA,makediÔ¨Äerentchoicesabouttheformofthenoiseandof theprior. p() h 13.1ProbabilisticPCAandFactorAnalysis ProbabilisticPCA(principalcomponentsanalysis),factoranalysisandotherlinear factormodelsarespecialcasesoftheaboveequations(and)andonly 13.113.2 diÔ¨Äerinthechoicesmadeforthenoisedistributionandthemodel‚Äôspriorover latentvariablesbeforeobserving. h x In f ac t o r analysis( ,;,),thelatentvariable Bartholomew1987Basilevsky1994 priorisjusttheunitvarianceGaussian h 0 ‚àºN(; h , I) (13.3) whiletheobservedvariables x iareassumedtobe c o ndi t i o n a l l y i ndep e ndent, given h.SpeciÔ¨Åcally, the noiseisassumed tobedrawnfroma diagonalco- variance Gaussian distribution,with covariancematrix œà=diag( œÉ2),with œÉ2= [ œÉ2 1 , œÉ2 2 , . . . , œÉ2 n]ÓÄæavectorofper-variablevariances. Theroleofthelatentvariablesisthusto c a p t u r e t h e d e p e nde nc i e sbetween thediÔ¨Äerentobservedvariables x i.Indeed,itcaneasilybeshownthat xisjusta multivariatenormalrandomvariable,with x‚àºN(; x b W W ,ÓÄæ+) œà . (13.4) 490 CHAPTER13.LINEARFACTORMODELS InordertocastPCAinaprobabilisticframework, wecanmakeaslight modiÔ¨Åcationtothefactoranalysismodel,makingtheconditionalvariances œÉ2 i equaltoeachother.Inthatcasethecovarianceof xisjust W WÓÄæ+ œÉ2I,where œÉ2isnowascalar.Thisyieldstheconditionaldistribution x‚àºN(; x b W W ,ÓÄæ+ œÉ2I) (13.5) orequivalently x h z = W ++ b œÉ (13.6) where z‚àºN( z; 0 , I)isGaussiannoise. ()thenshowan TippingandBishop1999 iterativeEMalgorithmforestimatingtheparameters and W œÉ2. This pr o babili s t i c P CAmodeltakesadvantageoftheobservationthatmost variationsinthedatacanbecapturedbythelatentvariables h,uptosomesmall residual r e c o nst r u c t i o n e r r o r œÉ2.Asshownby (), TippingandBishop1999 probabilisticPCAbecomesPCAas œÉ‚Üí0.Inthatcase,theconditionalexpected valueof hgiven xbecomesanorthogonalprojectionof x b‚àíontothespace spannedbythecolumnsof,likeinPCA. d W As œÉ‚Üí0,thedensitymodeldeÔ¨ÅnedbyprobabilisticPCAbecomesverysharp aroundthese ddimensionsspannedbythecolumnsof W.Thiscanmakethe modelassignverylowlikelihoodtothedataifthedatadoesnotactuallycluster nearahyperplane. 13.2IndependentComponentAnalysis(ICA) Independentcomponentanalysis(ICA)isamongtheoldestrepresentationlearning algorithms( ,; ,; ,; Herault andAns1984Jutten andHerault1991Comon1994 Hyv√§rinen1999Hyv√§rinen 2001aHinton2001Teh2003 ,; e t a l .,; e t a l .,; e t a l .,). Itisanapproachtomodelinglinearfactorsthatseekstoseparateanobserved signalintomanyunderlyingsignalsthatarescaledandaddedtogethertoform theobserveddata.Thesesignalsareintendedtobefullyindependent,ratherthan merelydecorrelatedfromeachother.1 ManydiÔ¨ÄerentspeciÔ¨ÅcmethodologiesarereferredtoasICA.Thevariant thatismostsimilartotheothergenerativemodelswehavedescribedhereisa variant(,)thattrainsafullyparametricgenerativemodel.The Pham e t a l .1992 priordistributionovertheunderlyingfactors, p( h),mustbeÔ¨Åxedaheadoftimeby theuser.Themodelthendeterministicallygenerates x= W h.Wecanperforma 1SeesectionforadiscussionofthediÔ¨Äerencebetweenuncorrelatedvariablesandindepen- 3.8 dentvariables. 491 CHAPTER13.LINEARFACTORMODELS nonlinearchangeofvariables(usingequation)todetermine3.47 p( x) .Learning themodelthenproceedsasusual,usingmaximumlikelihood. Themotivationforthisapproachisthatbychoosing p( h)tobeindependent, wecanrecoverunderlyingfactorsthatareascloseaspossibletoindependent. Thisiscommonlyused,nottocapturehigh-levelabstractcausalfactors,butto</div>
        </div>
    </div>

    <div class="question-card" id="q57">
        <div class="question-header">
            <span class="question-number">Question 57</span>
            <span class="question-type">multiple-choice</span>
        </div>

        <div class="question-text">In deep learning, determining which features are most important for a model to represent is a fundamental challenge, especially in complex environments and tasks. Approaches to feature selection and salience often involve designing loss functions and architectures that prioritize relevant patterns and causal factors.

Which method is most effective for ensuring that a neural network model learns to encode subtle but structured features, such as human ears, even when those features have minimal impact on pixel intensity?

1) Increasing the size of the model's latent representation in an unsupervised training setup   
2) Relying solely on pixel-wise mean squared error as the training objective   
3) Training the model with adversarial loss using a feedforward discriminator   
4) Applying dropout regularization to all network layers   
5) Using only supervised labels without any unsupervised learning components   
6) Ignoring features that do not significantly change pixel brightness   
7) Restricting model capacity to prevent overfitting on small features</div>

        <div class="answer-section">
            <div class="answer-label">‚úì Correct Answer:</div>
            <div class="answer-text">The correct answer is 3) Training the model with adversarial loss using a feedforward discriminator.</div>
        </div>

        <div class="reference-section">
            <div class="reference-label">üìö Reference Text:</div>
            <button class="toggle-button" onclick="toggleReference(57)">
                Show/Hide Reference
            </button>
            <div id="ref57" class="reference-text hidden">psychological phenomenon thathumanbeingsfailtoperceivechangesintheirenvironmentthat arenotimmediately relevanttothetasktheyareperforming‚Äîsee,e.g.,Simons andLevin1998().Animportantresearchfrontierinsemi-supervisedlearningis determining toencodeineachsituation.Currently,twoofthemainstrategies what fordealingwithalargenumberofunderlyingcausesaretouseasupervised learningsignalatthesametimeastheunsupervisedlearningsignalsothatthe modelwillchoosetocapturethemostrelevantfactorsofvariation,ortousemuch largerrepresentationsifusingpurelyunsupervisedlearning. AnemergingstrategyforunsupervisedlearningistomodifythedeÔ¨Ånitionof whichunderlyingcausesaremostsalient.Historically,autoencodersandgenerative modelshavebeentrainedtooptimizeaÔ¨Åxedcriterion,oftensimilartomean squarederror.TheseÔ¨Åxedcriteriadeterminewhichcausesareconsideredsalient. Forexample,meansquarederrorappliedtothepixelsofanimageimplicitly speciÔ¨ÅesthatanunderlyingcauseisonlysalientifitsigniÔ¨Åcantlychangesthe brightnessofalargenumberofpixels.Thiscanbeproblematicifthetaskwewish tosolveinvolvesinteractingwithsmallobjects.SeeÔ¨Ågureforanexample 15.5 5 4 3 CHAPTER15.REPRESENTATIONLEARNING Input Reconstruction Figure15.5:Anautoencodertrainedwithmeansquarederrorforaroboticstaskhas failedtoreconstructapingpongball.Theexistenceofthepingpongballandallofits spatialcoordinatesareimportantunderlyingcausalfactorsthatgeneratetheimageand arerelevanttotheroboticstask. Unfortunately,theautoencoderhaslimitedcapacity, andthetrainingwithmeansquarederrordidnotidentifythepingpongballasbeing salientenoughtoencode.ImagesgraciouslyprovidedbyChelseaFinn. ofaroboticstaskinwhichanautoencoderhasfailedtolearntoencodeasmall pingpongball.Thissamerobotiscapableofsuccessfullyinteractingwithlarger objects,suchasbaseballs,whicharemoresalientaccordingtomeansquarederror. OtherdeÔ¨Ånitionsofsaliencearepossible.Forexample,ifagroupofpixels followahighlyrecognizablepattern,evenifthatpatterndoesnotinvolveextreme brightnessordarkness,thenthatpatterncouldbeconsideredextremelysalient. OnewaytoimplementsuchadeÔ¨Ånitionofsalienceistousearecentlydeveloped approachcalled g e ner at i v e adv e r sar i al net w o r k s( ,). Goodfellow etal.2014c Inthisapproach,agenerativemodelistrainedtofoolafeedforwardclassiÔ¨Åer. ThefeedforwardclassiÔ¨Åerattemptstorecognizeallsamplesfromthegenerative modelasbeingfake,andallsamplesfromthetrainingsetasbeingreal.Inthis framework,anystructuredpatternthatthefeedforwardnetworkcanrecognizeis highlysalient.Thegenerativeadversarialnetworkwillbedescribedinmoredetail insection.Forthepurposesofthepresentdiscussion,itissuÔ¨Écientto 20.10.4 understandthattheylearnhowtodeterminewhatissalient. () Lotteretal.2015 showedthatmodelstrainedtogenerateimagesofhumanheadswilloftenneglect togeneratetheearswhentrainedwithmeansquarederror,butwillsuccessfully generatetheearswhentrainedwiththeadversarialframework.Becausethe earsarenotextremelybrightordarkcomparedtothesurroundingskin,they arenotespeciallysalientaccordingtomeansquarederrorloss,buttheirhighly 5 4 4 CHAPTER15.REPRESENTATIONLEARNING GroundTruth MSE Adversarial Figure15.6:Predictivegenerativenetworksprovideanexampleoftheimportanceof learningwhichfeaturesaresalient. Inthisexample,thepredictivegenerativenetwork hasbeentrainedtopredicttheappearanceofa3-DmodelofahumanheadataspeciÔ¨Åc viewingangle. ( L e f t )Groundtruth.Thisisthecorrectimage,thatthenetworkshould emit.Imageproducedbyapredictivegenerativenetworktrainedwithmean ( C e n t e r ) squarederroralone.BecausetheearsdonotcauseanextremediÔ¨Äerenceinbrightness comparedtotheneighboringskin,theywerenotsuÔ¨Écientlysalientforthemodeltolearn torepresentthem. ( R i g h t )Imageproducedbyamodeltrainedwithacombinationof meansquarederrorandadversarialloss. Usingthislearnedcostfunction,theearsare salientbecausetheyfollowapredictablepattern.Learningwhichunderlyingcausesare importantandrelevantenoughtomodelisanimportantactiveareaofresearch.Figures graciouslyprovidedby (). Lotter e t a l .2015 recognizableshapeandconsistentpositionmeansthatafeedforwardnetwork caneasilylearntodetectthem,makingthemhighlysalientunderthegenerative adversarialframework.SeeÔ¨Ågureforexampleimages.Generativeadversarial 15.6 networksareonlyonesteptowarddeterminingwhichfactorsshouldberepresented. Weexpectthatfutureresearchwilldiscoverbetterwaysofdeterminingwhich factorstorepresent,anddevelopmechanismsforrepresentingdiÔ¨Äerentfactors dependingonthetask. AbeneÔ¨Åtoflearningtheunderlyingcausalfactors,aspointedoutbySch√∂lkopf etal.(),isthatifthetruegenerativeprocesshas 2012 xasaneÔ¨Äectandyas acause,thenmodelingp(x y|)isrobusttochangesinp(y). Ifthecause-eÔ¨Äect relationshipwasreversed,thiswouldnotbetrue,sincebyBayes‚Äôrule,p(x y|) wouldbesensitivetochangesinp(y).Veryoften,whenweconsiderchangesin distributionduetodiÔ¨Äerentdomains,temporalnon-stationarity,orchangesin thenatureofthetask,thecausalmechanismsremaininvariant(thelawsofthe universeareconstant)whilethemarginaldistributionovertheunderlyingcauses canchange.Hence,bettergeneralization androbustnesstoallkindsofchangescan 5 4 5</div>
        </div>
    </div>

    <div class="question-card" id="q58">
        <div class="question-header">
            <span class="question-number">Question 58</span>
            <span class="question-type">multiple-choice</span>
        </div>

        <div class="question-text">Binary sparse coding models are probabilistic frameworks used for unsupervised feature learning, where hidden units are binary and inference over their states is often computationally demanding. Variational inference, especially the mean field approximation, is a common approach to efficiently approximate posteriors in such models.

In the context of training binary sparse coding models using a mean field variational inference approach, what is the primary reason for constraining the mean field parameters (\(\hat{h}_i\)) to avoid values exactly at 0 or 1?

1) To enforce sparsity by preventing hidden units from ever being fully activated or deactivated   
2) To ensure the weights matrix is properly normalized during optimization   
3) To avoid overfitting by limiting the range of possible feature values   
4) To maintain independence between visible units in the model   
5) To guarantee the diagonal precision matrix remains invertible   
6) To prevent numerical errors such as undefined logarithms in entropy and expectation calculations   
7) To increase the entropy term in the evidence lower bound for improved regularization</div>

        <div class="answer-section">
            <div class="answer-label">‚úì Correct Answer:</div>
            <div class="answer-text">The correct answer is 6) To prevent numerical errors such as undefined logarithms in entropy and expectation calculations.</div>
        </div>

        <div class="reference-section">
            <div class="reference-label">üìö Reference Text:</div>
            <button class="toggle-button" onclick="toggleReference(58)">
                Show/Hide Reference
            </button>
            <div id="ref58" class="reference-text hidden">v W h Œ≤‚àí 1) (19.20) where bisalearnablesetofbiases, Wisalearnableweightmatrix,and Œ≤isa learnable,diagonalprecisionmatrix. Trainingthismodelwithmaximumlikelihoodrequirestakingthederivative withrespecttotheparameters.Considerthederivativewithrespecttooneofthe biases: ‚àÇ ‚àÇ b ilog() p v (19.21) =‚àÇ ‚àÇ b ip() v p() v(19.22) =‚àÇ ‚àÇ b iÓÅê hp ,( h v) p() v(19.23) =‚àÇ ‚àÇ b iÓÅê hp p() h( ) v h| p() v(19.24) 6 4 0 CHAPTER19.APPROXIMATEINFERENCE h 1 h 1 h 2 h 2 h 3 h 3 v 1 v 1 v 2 v 2 v 3 v 3h 4 h 4 h 1 h 1 h 2 h 2h 3 h 3 h 4 h 4 Figure19.2:Thegraphstructureofabinarysparsecodingmodelwithfourhiddenunits. ( L e f t )Thegraphstructureof p( h v ,).Notethattheedgesaredirected,andthateverytwo hiddenunitsareco-parentsofeveryvisibleunit.Thegraphstructureof ( R i g h t ) p( h v|). Inordertoaccountfortheactivepathsbetweenco-parents,theposteriordistribution needsanedgebetweenallofthehiddenunits. =ÓÅê h p( ) v h|‚àÇ ‚àÇ b ip() h p() v(19.25) =ÓÅò hp( ) h v|‚àÇ ‚àÇ b ip() h p() h(19.26) = E h‚àº | p ( h v )‚àÇ ‚àÇ b ilog() p h . (19.27) Thisrequirescomputingexpectationswithrespectto p( h v|).Unfortunately, p( h v|)isacomplicateddistribution.SeeÔ¨Ågureforthegraphstructureof 19.2 p( h v ,)and p( h v|).Theposteriordistributioncorrespondstothecompletegraph overthehiddenunits,sovariableeliminationalgorithmsdonothelpustocompute therequiredexpectationsanyfasterthanbruteforce. WecanresolvethisdiÔ¨Écultybyusingvariationalinferenceandvariational learninginstead. WecanmakeameanÔ¨Åeldapproximation: q( ) = h v|ÓÅô iq h( i| v) . (19.28) Thelatentvariablesofthebinarysparsecodingmodelarebinary,sotorepresent afactorial qwesimplyneedtomodel mBernoullidistributions q( h i| v).Anatural waytorepresentthemeansoftheBernoullidistributionsiswithavectorÀÜhof probabilities, with q( h i=1| v)=ÀÜh i.WeimposearestrictionthatÀÜh iisnever equalto0orto1,inordertoavoiderrorswhencomputing,forexample, logÀÜh i. Wewillseethatthevariationalinferenceequationsneverassignorto 01 ÀÜh i 6 4 1 CHAPTER19.APPROXIMATEINFERENCE analytically.However,inasoftwareimplementation,machineroundingerrorcould resultinorvalues.Insoftware,wemaywishtoimplementbinarysparse 0 1 codingusinganunrestrictedvectorofvariationalparameters zandobtain ÀÜ hvia therelation ÀÜh= œÉ( z).Wecanthussafelycompute logÀÜh ionacomputerbyusing theidentitylog( œÉ z i) = (‚àí Œ∂‚àí z i)relatingthesigmoidandthesoftplus. Tobeginourderivationofvariationallearninginthebinarysparsecoding model,weshowthattheuseofthismeanÔ¨Åeldapproximationmakeslearning tractable. Theevidencelowerboundisgivenby L( ) v Œ∏ , , q (19.29) = E h‚àº q[log( )]+() p h v , H q (19.30) = E h‚àº q[log()+log( )log( )] p h p v h|‚àí q h v| (19.31) = E h‚àº qÓÄ¢mÓÅò i = 1log( p h i)+nÓÅò i = 1log( p v i|‚àí h)mÓÅò i = 1log( q h i| v)ÓÄ£ (19.32) =mÓÅò i = 1ÓÅ® ÀÜ h i(log( œÉ b i)log‚àí ÀÜ h i)+(1‚àíÀÜ h i)(log( œÉ‚àí b i)log(1 ‚àí ‚àíÀÜ h i))ÓÅ© (19.33) + E h‚àº qÓÄ¢nÓÅò i = 1logÓÅ≤ Œ≤ i 2 œÄexpÓÄí ‚àíŒ≤ i 2( v i‚àí W i , : h)2ÓÄìÓÄ£ (19.34) =mÓÅò i = 1ÓÅ® ÀÜ h i(log( œÉ b i)log‚àí ÀÜ h i)+(1‚àíÀÜ h i)(log( œÉ‚àí b i)log(1 ‚àí ‚àíÀÜ h i))ÓÅ© (19.35) +1 2nÓÅò i = 1Ô£Æ Ô£∞logŒ≤ i 2 œÄ‚àí Œ≤ iÔ£´ Ô£≠ v2 i‚àí2 v i W i , :ÀÜh+ÓÅò jÔ£Æ Ô£∞ W2 i , jÀÜ h j+ÓÅò k jÓÄ∂=W i , j W i , kÀÜh jÀÜh kÔ£π Ô£ªÔ£∂ Ô£∏Ô£π Ô£ª . (19.36) Whiletheseequationsaresomewhatunappealingaesthetically,theyshowthatL canbeexpressedinasmallnumberofsimplearithmeticoperations.Theevidence lowerbound Listhereforetractable.WecanuseLasareplacementforthe intractablelog-likelihood. Inprinciple,wecouldsimplyrungradientascentonboth vand handthis wouldmakeaperfectlyacceptablecombinedinferenceandtrainingalgorithm. Usually,however,wedonotdothis,fortworeasons.First,thiswouldrequire storing ÀÜ hforeach v.Wetypicallypreferalgorithmsthatdonotrequireper- examplememory.ItisdiÔ¨Éculttoscalelearningalgorithmstobillionsofexamples ifwemustrememberadynamicallyupdatedvectorassociatedwitheachexample. 6 4 2 CHAPTER19.APPROXIMATEINFERENCE Second,wewouldliketobeabletoextractthefeatures ÀÜhveryquickly,inorderto recognizethecontentof v. Inarealisticdeployedsetting,wewouldneedtobe abletocompute ÀÜhinrealtime. Forboththesereasons,wetypicallydonotusegradientdescenttocompute themeanÔ¨Åeldparameters ÀÜ h.Instead,werapidlyestimatethemwithÔ¨Åxedpoint equations. TheideabehindÔ¨Åxedpointequationsisthatweareseekingalocalmaximum withrespecttoÀÜh, where ‚àá hL( v Œ∏ , ,ÀÜh)= 0.WecannoteÔ¨Écientlysolvethis equationwithrespecttoallofÀÜhsimultaneously.However,wecansolveforasingle variable:‚àÇ ‚àÇÀÜh iL( v Œ∏ , ,ÀÜh) = 0 . (19.37) Wecantheniterativelyapplythesolutiontotheequationfor i=1 , . . . , m, andrepeatthecycleuntilwesatisfyaconvergecriterion.Commonconvergence criteriaincludestoppingwhenafullcycleofupdatesdoesnotimproveLbymore thansometoleranceamount,orwhenthecycledoesnotchange ÀÜhbymorethan someamount. IteratingmeanÔ¨ÅeldÔ¨Åxedpointequationsisageneraltechniquethatcan providefastvariationalinferenceinabroadvarietyofmodels.Tomakethismore concrete,weshowhowtoderivetheupdatesforthebinarysparsecodingmodelin particular. First,wemustwriteanexpressionforthederivativeswithrespecttoÀÜh i.To doso,wesubstituteequation intotheleftsideofequation: 19.36 19.37 ‚àÇ ‚àÇÀÜh iL( v Œ∏ ,</div>
        </div>
    </div>

    <div class="question-card" id="q59">
        <div class="question-header">
            <span class="question-number">Question 59</span>
            <span class="question-type">multiple-choice</span>
        </div>

        <div class="question-text">Importance sampling and Markov Chain Monte Carlo (MCMC) are fundamental techniques in probabilistic inference, widely used to estimate expectations and sample from complex distributions in machine learning and statistics. Their effectiveness depends on careful algorithmic choices, particularly in high-dimensional settings.

Which of the following is a key reason why a poorly chosen proposal distribution q(x) in importance sampling leads to unreliable estimates, especially in high-dimensional spaces?

1) The proposal distribution q(x) always produces uncorrelated samples, regardless of dimensionality.   
2) Importance sampling estimators become biased and never converge with poor choices of q(x).   
3) Markov Chain Monte Carlo methods cannot compensate for a bad proposal distribution in importance sampling.   
4) The equilibrium distribution of the Markov chain fails to exist when q(x) is poorly chosen.   
5) Most samples from a simple q(x) contribute negligibly, while rare samples have extremely large weights, resulting in high estimator variance.   
6) Thinning the chain is unnecessary when q(x) is not well matched to p(x).   
7) The transition operator T(x'|x) always ensures low variance in importance sampling, independent of q(x).</div>

        <div class="answer-section">
            <div class="answer-label">‚úì Correct Answer:</div>
            <div class="answer-text">The correct answer is 5) Most samples from a simple q(x) contribute negligibly, while rare samples have extremely large weights, resulting in high estimator variance..</div>
        </div>

        <div class="reference-section">
            <div class="reference-label">üìö Reference Text:</div>
            <button class="toggle-button" onclick="toggleReference(59)">
                Show/Hide Reference
            </button>
            <div id="ref59" class="reference-text hidden">=ÓÅên i = 1Àú p ( x( ) i) Àú q ( x( ) i )f( x( ) i) ÓÅên i = 1Àú p ( x( ) i ) Àú q ( x( ) i ), (17.16) where Àú pandÀú qaretheunnormalized formsof pand qandthe x( ) iarethesamples from q.Thisestimatorisbiasedbecause E[ÀÜ s B I S]ÓÄ∂= s,exceptasymptoticallywhen n‚Üí‚àûandthedenominator ofequationconvergesto1.Hencethisestimator 17.14 iscalledasymptoticallyunbiased. Althoughagoodchoiceof qcangreatlyimprovetheeÔ¨ÉciencyofMonteCarlo estimation,apoorchoiceof qcanmaketheeÔ¨Éciencymuchworse.Goingbackto equation,weseethatiftherearesamplesof 17.12 qforwhichp f ( ) x| ( ) x| q ( ) xislarge, thenthevarianceoftheestimatorcangetverylarge.Thismayhappenwhen q( x)istinywhileneither p( x)nor f( x)aresmallenoughtocancelit.The q distributionisusuallychosentobeaverysimpledistributionsothatitiseasy tosamplefrom.When xishigh-dimensional,thissimplicityin qcausesitto match por p f||poorly.When q( x( ) i)ÓÄù p( x( ) i)| f( x( ) i)|,importancesampling collectsuselesssamples(summingtinynumbersorzeros).Ontheotherhand,when q( x( ) i)ÓÄú p( x( ) i)| f( x( ) i)|,whichwillhappenmorerarely,theratiocanbehuge. Becausetheselattereventsarerare,theymaynotshowupinatypicalsample, yieldingtypicalunderestimationof s,compensatedrarelybygrossoverestimation. Suchverylargeorverysmallnumbersaretypicalwhen xishighdimensional, becauseinhighdimensionthedynamicrangeofjointprobabilities canbevery large. Inspiteofthisdanger,importancesamplinganditsvariantshavebeenfound veryusefulinmanymachinelearningalgorithms,includingdeeplearningalgorithms. Forexample,seetheuseofimportancesamplingtoacceleratetraininginneural languagemodelswithalargevocabulary(section)orotherneuralnets 12.4.3.3 withalargenumberofoutputs.Seealsohowimportancesamplinghasbeen usedtoestimateapartitionfunction(thenormalization constantofaprobability 5 9 4 CHAPTER17.MONTECARLOMETHODS distribution)insection,andtoestimatethelog-likelihoodindeepdirected 18.7 modelssuchasthevariationalautoencoder,insection.Importancesampling 20.10.3 mayalsobeusedtoimprovetheestimateofthegradientofthecostfunctionused totrainmodelparameterswithstochasticgradientdescent,particularlyformodels suchasclassiÔ¨Åerswheremostofthetotalvalueofthecostfunctioncomesfroma smallnumberofmisclassiÔ¨Åedexamples.SamplingmorediÔ¨Écultexamplesmore frequentlycanreducethevarianceofthegradientinsuchcases(,). Hinton2006 17.3MarkovChainMonteCarloMethods Inmanycases,wewishtouseaMonteCarlotechniquebutthereisnotractable methodfordrawingexactsamplesfromthedistribution p m o de l( x)orfromagood (lowvariance)importancesamplingdistribution q( x).Inthecontextofdeep learning,thismostoftenhappenswhen p m o de l( x)isrepresentedbyanundirected model.Inthesecases,weintroduceamathematical toolcalledaMarkovchain toapproximately samplefrom p m o de l( x).ThefamilyofalgorithmsthatuseMarkov chainstoperformMonteCarloestimatesiscalledMarkovchainMonteCarlo methods(MCMC).MarkovchainMonteCarlomethodsformachinelearningare describedatgreaterlengthinKollerandFriedman2009(). Themoststandard, genericguaranteesforMCMCtechniquesareonlyapplicablewhenthemodel doesnotassignzeroprobabilitytoanystate.Therefore,itismostconvenient to present these techniques assampling froman energy-basedmodel (EBM) p( x)‚àù ‚àíexp( E()) xasdescribedinsection.IntheEBMformulation,every 16.2.4 stateisguaranteedtohavenon-zeroprobability.MCMCmethodsareinfact morebroadlyapplicableandcanbeusedwithmanyprobabilitydistributionsthat containzeroprobabilitystates.However,thetheoreticalguaranteesconcerningthe behaviorofMCMCmethodsmustbeprovenonacase-by-casebasisfordiÔ¨Äerent familiesofsuchdistributions.Inthecontextofdeeplearning,itismostcommon torelyonthemostgeneraltheoreticalguaranteesthatnaturallyapplytoall energy-basedmodels. Tounderstandwhydrawingsamplesfromanenergy-basedmodelisdiÔ¨Écult, consideranEBMoverjusttwovariables,deÔ¨Åningadistributionab.Inorder p( ,) tosamplea,wemustdrawafrom p(ab|),andinordertosampleb,wemust drawitfrom p(ba|).Itseemstobeanintractablechicken-and-eggproblem. Directedmodelsavoidthisbecausetheirgraphisdirectedandacyclic.Toperform ancestralsamplingonesimplysampleseachofthevariablesintopologicalorder, conditioningoneachvariable‚Äôsparents,whichareguaranteedtohavealreadybeen sampled(section).AncestralsamplingdeÔ¨ÅnesaneÔ¨Écient,single-passmethod 16.3 5 9 5 CHAPTER17.MONTECARLOMETHODS ofobtainingasample. InanEBM,wecanavoidthischickenandeggproblembysamplingusinga Markovchain.ThecoreideaofaMarkovchainistohaveastate xthatbegins asanarbitraryvalue.Overtime,werandomlyupdate xrepeatedly.Eventually xbecomes(verynearly)afairsamplefrom p( x).Formally,aMarkovchainis deÔ¨Ånedbyarandomstate xandatransitiondistribution T( xÓÄ∞| x)specifying theprobabilitythatarandomupdatewillgotostate xÓÄ∞ifitstartsinstate x. RunningtheMarkovchainmeansrepeatedlyupdatingthestate xtoavalue xÓÄ∞ sampledfrom T( xÓÄ∞| x). TogainsometheoreticalunderstandingofhowMCMCmethodswork,itis usefultoreparametrizetheproblem.First,werestrictourattentiontothecase wheretherandomvariable xhascountablymanystates.Wecanthenrepresent thestateasjustapositiveinteger x. DiÔ¨Äerentintegervaluesof xmapbackto diÔ¨Äerentstatesintheoriginalproblem. x ConsiderwhathappenswhenweruninÔ¨ÅnitelymanyMarkovchainsinparallel. AllofthestatesofthediÔ¨ÄerentMarkovchainsaredrawnfromsomedistribution q( ) t( x),where tindicatesthenumberoftimestepsthathaveelapsed.Atthe beginning, q( 0 )issomedistributionthatweusedtoarbitrarilyinitialize xforeach Markovchain.Later, q( ) tisinÔ¨ÇuencedbyalloftheMarkovchainstepsthathave runsofar.Ourgoalisfor q( ) t() xtoconvergeto. p x() Becausewehavereparametrized theproblemintermsofpositiveinteger x,we candescribetheprobabilitydistributionusingavector,with q v q i v (= x ) = i . (17.17) ConsiderwhathappenswhenweupdateasingleMarkovchain‚Äôsstate xtoa newstate xÓÄ∞.Theprobabilityofasinglestatelandinginstate xÓÄ∞isgivenby q( + 1 ) t( xÓÄ∞) =ÓÅò xq( ) t()( x T xÓÄ∞| x .) (17.18) Usingourintegerparametrization, wecanrepresenttheeÔ¨Äectofthetransition operatorusingamatrix.WedeÔ¨Ånesothat T A A A i , j= ( T xÓÄ∞= = ) i| x j . (17.19) UsingthisdeÔ¨Ånition,wecannowrewriteequation.Ratherthanwritingitin 17.18 termsof qand Ttounderstandhowasinglestateisupdated,wemaynowuse v and AtodescribehowtheentiredistributionoverallthediÔ¨ÄerentMarkovchains (runninginparallel)shiftsasweapplyanupdate: v( ) t= A v( 1 ) t‚àí. (17.20) 5 9 6 CHAPTER17.MONTECARLOMETHODS ApplyingtheMarkovchainupdaterepeatedlycorrespondstomultiplyingbythe matrix Arepeatedly.Inotherwords,wecanthinkoftheprocessasexponentiating thematrix: A v( ) t= Atv( 0 ). (17.21) Thematrix Ahasspecialstructurebecauseeachofitscolumnsrepresentsa probabilitydistribution.Suchmatricesarecalledstochasticmatrices.Ifthere isanon-zeroprobabilityoftransitioningfromanystate xtoanyotherstate xÓÄ∞for somepower t,thenthePerron-Frobeniustheorem(,;Perron1907Frobenius1908,) guaranteesthatthelargesteigenvalueisrealandequalto.Overtime,wecan 1 seethatalloftheeigenvaluesareexponentiated: v( ) t=ÓÄÄ V Œª Vdiag()‚àí 1ÓÄÅtv( 0 )= () Vdiag ŒªtV‚àí 1v( 0 ).(17.22) Thisprocesscausesalloftheeigenvaluesthatarenotequaltotodecaytozero. 1 Undersomeadditionalmildconditions, Aisguaranteedtohaveonlyoneeigenvector witheigenvalue.Theprocessthusconvergestoa 1 stationarydistribution, sometimesalsocalledthe .Atconvergence, equilibriumdistribution vÓÄ∞= = A v v , (17.23) andthissameconditionholdsforeveryadditionalstep.Thisisaneigenvector equation.Tobeastationarypoint, vmustbeaneigenvectorwithcorresponding eigenvalue.Thisconditionguaranteesthatoncewehavereachedthestationary 1 distribution,repeatedapplicationsofthetransitionsamplingproceduredonot changethe overthestatesofallthevariousMarkovchains(although d i s t r i b u t i o n transitionoperatordoeschangeeachindividualstate,ofcourse). Ifwehavechosen Tcorrectly,thenthestationarydistribution qwillbeequal tothedistribution pwewishtosamplefrom.Wewilldescribehowtochoose T shortly,insection.17.4 MostpropertiesofMarkovChainswithcountablestatescanbegeneralized tocontinuousvariables.Inthissituation,someauthorscalltheMarkovChain aHarrischainbutweusethetermMarkovChaintodescribebothconditions. Ingeneral,aMarkovchainwithtransitionoperator Twillconverge,undermild conditions,toaÔ¨Åxedpointdescribedbytheequation qÓÄ∞( xÓÄ∞) = E x‚àº q T( xÓÄ∞| x) , (17.24) whichinthediscretecaseisjustrewritingequation.When17.23 xisdiscrete, theexpectationcorrespondstoasum,andwhen xiscontinuous,theexpectation correspondstoanintegral. 5 9 7 CHAPTER17.MONTECARLOMETHODS Regardlessofwhetherthestateiscontinuousordiscrete,allMarkovchain methodsconsistofrepeatedlyapplyingstochasticupdatesuntileventuallythe statebeginstoyieldsamplesfromtheequilibriumdistribution.Runningthe Markovchainuntilitreachesitsequilibriumdistributioniscalled‚Äúburningin‚Äù theMarkovchain.Afterthechainhasreachedequilibrium,asequenceofinÔ¨Ånitely manysamplesmaybedrawnfromfromtheequilibriumdistribution. Theyare identicallydistributedbutanytwosuccessivesampleswillbehighlycorrelated witheachother.AÔ¨Ånitesequenceofsamplesmaythusnotbeveryrepresentative oftheequilibriumdistribution.Onewaytomitigatethisproblemistoreturn onlyevery nsuccessivesamples, sothatourestimateofthestatisticsofthe equilibriumdistributionisnotasbiasedbythecorrelationbetweenanMCMC sampleandthenextseveralsamples.Markovchainsarethusexpensivetouse becauseofthetimerequiredtoburnintotheequilibriumdistributionandthetime requiredtotransitionfromonesampletoanotherreasonablydecorrelatedsample afterreachingequilibrium.Ifonedesirestrulyindependentsamples,onecanrun multipleMarkovchainsinparallel.Thisapproachusesextraparallelcomputation toeliminatelatency.ThestrategyofusingonlyasingleMarkovchaintogenerate allsamplesandthestrategyofusingoneMarkovchainforeachdesiredsampleare twoextremes;deeplearningpractitioners usuallyuseanumberofchainsthatis similartothenumberofexamplesinaminibatchandthendrawasmanysamples asareneededfromthisÔ¨ÅxedsetofMarkovchains.Acommonlyusednumberof Markovchainsis100. AnotherdiÔ¨Écultyisthatwedonotknowinadvancehowmanystepsthe Markovchainmustrunbeforereachingitsequilibriumdistribution.Thislengthof timeiscalledthemixingtime.ItisalsoverydiÔ¨ÉculttotestwhetheraMarkov chainhasreachedequilibrium.Wedonothaveapreciseenoughtheoryforguiding usinansweringthisquestion.Theorytellsusthatthechainwillconverge,butnot muchmore.IfweanalyzetheMarkovchainfromthepointofviewofamatrix A actingonavectorofprobabilities</div>
        </div>
    </div>

    <div class="question-card" id="q60">
        <div class="question-header">
            <span class="question-number">Question 60</span>
            <span class="question-type">multiple-choice</span>
        </div>

        <div class="question-text">Neural coding in the primate visual system and artificial intelligence models such as convolutional neural networks both play crucial roles in object and concept recognition. However, significant differences exist in the mechanisms and integration of sensory information between biological and artificial systems.

Which of the following best describes the function of "grandmother cells" found in the human medial temporal lobe?

1) Neurons that respond selectively to highly specific abstract concepts, activating for varied representations of a particular person or idea   
2) Cells responsible for pooling responses from multiple sensory modalities and integrating emotional context   
3) Neurons that fire for any visual stimulus within the foveal region, regardless of identity or context   
4) Units in artificial neural networks that encode low-level features like edges and textures   
5) Neurons that only respond to exact visual matches of a stimulus, failing to generalize across different representations   
6) Cells that provide strong feedback to earlier visual areas to enhance local contrast sensitivity   
7) Sensory neurons in the retina that initiate feedforward processing for all incoming light signals</div>

        <div class="answer-section">
            <div class="answer-label">‚úì Correct Answer:</div>
            <div class="answer-text">The correct answer is 1) Neurons that respond selectively to highly specific abstract concepts, activating for varied representations of a particular person or idea.</div>
        </div>

        <div class="reference-section">
            <div class="reference-label">üìö Reference Text:</div>
            <button class="toggle-button" onclick="toggleReference(60)">
                Show/Hide Reference
            </button>
            <div id="ref60" class="reference-text hidden">CHAPTER9.CONVOLUTIONALNETWORKS nicknamed‚Äúgrandmother cells‚Äù‚Äîtheideaisthatapersoncouldhaveaneuronthat activateswhenseeinganimageoftheirgrandmother, regardlessofwhethershe appearsintheleftorrightsideoftheimage,whethertheimageisaclose-upof herfaceorzoomedoutshotofherentirebody,whethersheisbrightlylit,orin shadow,etc. Thesegrandmother cellshavebeenshowntoactuallyexistinthehumanbrain, inaregioncalledthemedialtemporallobe( ,).Researchers Quiroga etal.2005 testedwhetherindividualneuronswouldrespondtophotosoffamousindividuals. Theyfoundwhathascometobecalledthe‚ÄúHalleBerryneuron‚Äù:anindividual neuronthatisactivatedbytheconceptofHalleBerry.ThisneuronÔ¨Åreswhena personseesaphotoofHalleBerry,adrawingofHalleBerry,oreventextcontaining thewords‚ÄúHalleBerry.‚ÄùOfcourse,thishasnothingtodowithHalleBerryherself; otherneuronsrespondedtothepresenceofBillClinton,JenniferAniston,etc. Thesemedialtemporallobeneuronsaresomewhatmoregeneralthanmodern convolutionalnetworks,whichwouldnotautomatically generalizetoidentifying apersonorobjectwhenreadingitsname.Theclosestanalogtoaconvolutional network‚Äôslastlayeroffeaturesisabrainareacalledtheinferotemporal cortex (IT).Whenviewinganobject,informationÔ¨Çowsfromtheretina,throughthe LGN,toV1,thenonwardtoV2,thenV4,thenIT.ThishappenswithintheÔ¨Årst 100msofglimpsinganobject. Ifapersonisallowedtocontinuelookingatthe objectformoretime,theninformationwillbegintoÔ¨Çowbackwardsasthebrain usestop-downfeedbacktoupdatetheactivationsinthelowerlevelbrainareas. However,ifweinterrupttheperson‚Äôsgaze,andobserveonlytheÔ¨Åringratesthat resultfromtheÔ¨Årst100msofmostlyfeedforwardactivation,thenITprovestobe verysimilartoaconvolutionalnetwork.ConvolutionalnetworkscanpredictIT Ô¨Åringrates,andalsoperformverysimilarlyto(timelimited)humansonobject recognitiontasks(,). DiCarlo2013 Thatbeingsaid,therearemanydiÔ¨Äerencesbetweenconvolutionalnetworks andthemammalianvisionsystem.SomeofthesediÔ¨Äerencesarewellknown tocomputational neuroscientists,butoutsidethescopeofthisbook.Someof thesediÔ¨Äerencesarenotyetknown,becausemanybasicquestionsabouthowthe mammalianvisionsystemworksremainunanswered.Asabrieflist: ‚Ä¢Thehumaneyeismostlyverylowresolution,exceptforatinypatchcalledthe f o v e a.Thefoveaonlyobservesanareaaboutthesizeofathumbnailheldat armslength.Thoughwefeelasifwecanseeanentiresceneinhighresolution, thisisanillusioncreatedbythesubconsciouspartofourbrain,asitstitches togetherseveralglimpsesofsmallareas.Mostconvolutionalnetworksactually receivelargefullresolutionphotographsasinput.Thehumanbrainmakes 3 6 6 CHAPTER9.CONVOLUTIONALNETWORKS severaleyemovementscalled sac c adestoglimpsethemostvisuallysalient ortask-relevantpartsofascene.Incorporatingsimilarattentionmechanisms intodeeplearningmodelsisanactiveresearchdirection.Inthecontextof deeplearning,attentionmechanismshavebeenmostsuccessfulfornatural languageprocessing,asdescribedinsection.Severalvisualmodels 12.4.5.1 withfoveationmechanismshavebeendevelopedbutsofarhavenotbecome thedominantapproach(LarochelleandHinton2010Denil2012 ,;etal.,). ‚Ä¢Thehumanvisualsystemisintegratedwithmanyothersenses,suchas hearing,andfactorslikeourmoodsandthoughts.Convolutionalnetworks sofararepurelyvisual. ‚Ä¢Thehumanvisualsystemdoesmuchmorethanjustrecognizeobjects.Itis abletounderstandentirescenesincludingmanyobjectsandrelationships betweenobjects,andprocessesrich3-Dgeometricinformationneededfor ourbodiestointerfacewiththeworld.Convolutionalnetworkshavebeen appliedtosomeoftheseproblemsbuttheseapplicationsareintheirinfancy. ‚Ä¢EvensimplebrainareaslikeV1areheavilyimpactedbyfeedbackfromhigher levels.Feedbackhasbeenexploredextensivelyinneuralnetworkmodelsbut hasnotyetbeenshowntooÔ¨Äeracompellingimprovement. ‚Ä¢WhilefeedforwardITÔ¨Åringratescapturemuchofthesameinformationas convolutionalnetworkfeatures,itisnotclearhowsimilartheintermediate computations are.ThebrainprobablyusesverydiÔ¨Äerentactivationand poolingfunctions.Anindividualneuron‚Äôsactivationprobablyisnotwell- characterizedbyasinglelinearÔ¨Ålterresponse.ArecentmodelofV1involves multiplequadraticÔ¨Åltersforeachneuron(,).Indeedour Rustetal.2005 cartoonpictureof‚Äúsimplecells‚Äù and‚Äúcomplexcells‚Äù mightcreateanon- existentdistinction;simplecellsandcomplexcellsmightbothbethesame kindofcellbutwiththeir‚Äúparameters‚Äùenablingacontinuumofbehaviors rangingfromwhatwecall‚Äúsimple‚Äùtowhatwecall‚Äúcomplex.‚Äù Itis alsoworthmentioningthatneuroscience hastold usrelativelylittle abouthowtotrainconvolutionalnetworks.Modelstructureswithparameter sharingacrossmultiplespatiallocationsdatebacktoearlyconnectionistmodels ofvision( ,),butthesemodelsdidnotusethemodern MarrandPoggio1976 back-propagationalgorithmandgradientdescent.Forexample,theNeocognitron (Fukushima1980,)incorporatedmostofthemodelarchitecturedesignelementsof themodernconvolutionalnetworkbutreliedonalayer-wiseunsupervisedclustering algorithm. 3 6 7 CHAPTER9.CONVOLUTIONALNETWORKS Lang andHinton 1988()introducedthe use ofback-propagation totrain t i m e - del a y neur al net w o r k s(TDNNs).Tousecontemporary terminology, TDNNsareone-dimensional convolutionalnetworksappliedtotimeseries.Back- propagationappliedtothesemodelswasnotinspiredbyanyneuroscientiÔ¨Åcobserva- tionandisconsideredbysometobebiologicallyimplausible.Followingthesuccess ofback-propagation-based trainingofTDNNs,( ,)developed LeCunetal.1989 themodernconvolutionalnetworkbyapplyingthesametrainingalgorithmto2-D convolutionappliedtoimages. Sofarwehavedescribedhowsimplecellsareroughlylinearandselectivefor certainfeatures,complexcellsaremorenonlinearandbecomeinvarianttosome transformationsofthesesimplecellfeatures,andstacksoflayersthatalternate betweenselectivityandinvariancecanyieldgrandmother cellsforveryspeciÔ¨Åc phenomena.Wehavenotyetdescribedpreciselywhattheseindividualcellsdetect. Inadeep,nonlinearnetwork,itcanbediÔ¨Éculttounderstandthefunctionof individualcells.SimplecellsintheÔ¨Årstlayerareeasiertoanalyze,becausetheir responsesaredrivenbyalinearfunction.InanartiÔ¨Åcialneuralnetwork,wecan justdisplayanimageoftheconvolutionkerneltoseewhatthecorresponding channelofaconvolutionallayerrespondsto.Inabiologicalneuralnetwork,we donothaveaccesstotheweightsthemselves.Instead,weputanelectrodeinthe neuronitself,displayseveralsamplesofwhitenoiseimagesinfrontoftheanimal‚Äôs retina,andrecordhoweachofthesesamplescausestheneurontoactivate.Wecan thenÔ¨Åtalinearmodeltotheseresponsesinordertoobtainanapproximation of theneuron‚Äôsweights.Thisapproachisknownas r e v e r se c o r r e l at i o n(Ringach andShapley2004,). ReversecorrelationshowsusthatmostV1cellshaveweightsthataredescribed by G ab o r f unc t i o ns. TheGaborfunctiondescribestheweightata2-Dpoint intheimage.Wecanthinkofanimageasbeingafunctionof2-Dcoordinates, I( x , y).Likewise,wecanthinkofasimplecellassamplingtheimageatasetof locations,deÔ¨Ånedbyasetof xcoordinates Xandasetof ycoordinates, Y,and applyingweightsthatarealsoafunctionofthelocation, w( x , y).Fromthispoint ofview,theresponseofasimplecelltoanimageisgivenby s I() =ÓÅò x ‚àà XÓÅò y ‚àà Yw x , y I x , y . ()() (9.15) SpeciÔ¨Åcally,takestheformofaGaborfunction: w x , y() w x , y Œ± , Œ≤ (; x , Œ≤ y , f , œÜ , x 0 , y 0 , œÑ Œ±) = expÓÄÄ‚àí Œ≤ x xÓÄ∞ 2‚àí Œ≤ y yÓÄ∞ 2ÓÄÅ cos( f xÓÄ∞+) œÜ ,(9.16) where xÓÄ∞= ( x x ‚àí 0)cos()+( œÑ y y ‚àí 0)sin() œÑ (9.17) 3 6 8 CHAPTER9.CONVOLUTIONALNETWORKS and yÓÄ∞= ( ‚àí x x ‚àí 0)sin()+( œÑ y y ‚àí 0)cos() œÑ . (9.18) Here, Œ±, Œ≤ x, Œ≤ y, f, œÜ, x 0, y 0,and œÑareparametersthatcontroltheproperties oftheGaborfunction.FigureshowssomeexamplesofGaborfunctionswith 9.18 diÔ¨Äerentsettingsoftheseparameters. Theparameters x 0, y 0,and œÑdeÔ¨Åneacoordinatesystem. Wetranslateand rotate xand ytoform xÓÄ∞and yÓÄ∞.SpeciÔ¨Åcally,thesimplecellwillrespondtoimage featurescenteredatthepoint( x 0, y 0),anditwillrespondtochangesinbrightness aswemovealongalinerotatedradiansfromthehorizontal. œÑ Viewedasafunctionof xÓÄ∞and yÓÄ∞,thefunction wthenrespondstochangesin brightnessaswemovealongthe xÓÄ∞axis. Ithastwoimportantfactors:oneisa Gaussianfunctionandtheotherisacosinefunction. TheGaussianfactor Œ±expÓÄÄ ‚àí Œ≤ x xÓÄ∞ 2‚àí Œ≤ y yÓÄ∞ 2ÓÄÅ canbeseenasagatingtermthat ensuresthesimplecellwillonlyrespondtovaluesnearwhere xÓÄ∞and yÓÄ∞areboth zero,inotherwords,nearthecenterofthecell‚ÄôsreceptiveÔ¨Åeld.Thescalingfactor Œ±adjuststhetotalmagnitudeofthesimplecell‚Äôsresponse,while Œ≤ xand Œ≤ ycontrol howquicklyitsreceptiveÔ¨ÅeldfallsoÔ¨Ä. Thecosinefactor cos( f xÓÄ∞+ œÜ) controlshowthesimplecellrespondstochanging brightnessalongthe xÓÄ∞axis.Theparameter fcontrolsthefrequencyofthecosine andcontrolsitsphaseoÔ¨Äset. œÜ Altogether,thiscartoonviewofsimplecellsmeansthatasimplecellresponds toaspeciÔ¨ÅcspatialfrequencyofbrightnessinaspeciÔ¨ÅcdirectionataspeciÔ¨Åc location.Simplecellsaremostexcitedwhenthewaveofbrightnessintheimage hasthesamephaseastheweights.Thisoccurswhentheimageisbrightwherethe weightsarepositiveanddarkwheretheweightsarenegative.Simplecellsaremost inhibitedwhenthewaveofbrightnessisfullyoutofphasewiththeweights‚Äîwhen theimageisdarkwheretheweightsarepositiveandbrightwheretheweightsare negative. Thecartoonviewofacomplexcellisthatitcomputesthe L2normofthe 2-Dvectorcontainingtwosimplecells‚Äôresponses: c( I)=ÓÅ∞ s 0() I2+ s 1() I2. An importantspecialcaseoccurswhen s 1hasallofthesameparametersas s 0except for œÜ,and œÜissetsuchthat s 1isonequartercycleoutofphasewith s 0.Inthis case, s 0and s 1forma q uadr at u r e pai r.AcomplexcelldeÔ¨Ånedinthisway respondswhentheGaussianreweightedimage I( x , y)exp( ‚àí Œ≤ x xÓÄ∞ 2‚àí Œ≤ y yÓÄ∞ 2) contains ahighamplitudesinusoidalwavewithfrequency findirection œÑnear ( x 0 , y 0), regardlessofthephaseoÔ¨Äsetofthiswave.Inotherwords,thecomplexcellis invarianttosmalltranslationsoftheimageindirection œÑ,ortonegatingtheimage 3 6 9 CHAPTER9.CONVOLUTIONALNETWORKS Figure9.18:Gaborfunctionswithavarietyofparametersettings.Whiteindicates largepositiveweight,blackindicateslargenegativeweight,andthebackgroundgray correspondstozeroweight. ( L e f t )GaborfunctionswithdiÔ¨Äerentvaluesoftheparameters thatcontrolthecoordinatesystem: x 0, y 0,and œÑ. EachGaborfunctioninthisgridis assignedavalueof x 0and y 0proportionaltoitspositioninitsgrid,and œÑischosenso thateachGaborÔ¨Ålterissensitivetothedirectionradiatingoutfromthecenterofthegrid. Fortheothertwoplots, x 0, y 0,and œÑareÔ¨Åxedtozero. Gaborfunctionswith ( C e n t e r ) diÔ¨ÄerentGaussianscaleparameters Œ≤ xand Œ≤</div>
        </div>
    </div>

    <div class="question-card" id="q61">
        <div class="question-header">
            <span class="question-number">Question 61</span>
            <span class="question-type">multiple-choice</span>
        </div>

        <div class="question-text">Denoising autoencoders (DAEs) are neural network architectures used in unsupervised learning to reconstruct clean data from corrupted inputs. Their training objectives and theoretical properties link them to statistical concepts such as score matching and generative modeling.

Which of the following statements most accurately describes the relationship between denoising autoencoders and Restricted Boltzmann Machines under specific training conditions?

1) DAEs trained with binary corruption and cross-entropy loss are equivalent to RBMs with binary visible units under maximum likelihood.   
2) DAEs trained without corruption implement the same learning algorithm as contrastive divergence in RBMs.   
3) DAEs trained with masking noise and L1 loss perform exact score matching for RBMs with discrete visible units.   
4) DAEs trained with Gaussian noise and cross-entropy loss directly estimate the partition function of RBMs.   
5) DAEs trained with salt-and-pepper noise and squared error minimize the KL divergence between data and RBM distributions.   
6) DAEs trained with zero noise and mean squared error always correspond to RBMs with deterministic hidden units.   
7) DAEs trained with Gaussian noise and squared error are mathematically equivalent to training RBMs with Gaussian visible units under denoising score matching. </div>

        <div class="answer-section">
            <div class="answer-label">‚úì Correct Answer:</div>
            <div class="answer-text">The correct answer is 7) DAEs trained with Gaussian noise and squared error are mathematically equivalent to training RBMs with Gaussian visible units under denoising score matching..</div>
        </div>

        <div class="reference-section">
            <div class="reference-label">üìö Reference Text:</div>
            <button class="toggle-button" onclick="toggleReference(61)">
                Show/Hide Reference
            </button>
            <div id="ref61" class="reference-text hidden">ng f unc t i o n f( x)to an e nc o di ng di st r i but i o n pencoder( ) h x|,asillustratedinÔ¨Ågure.14.2 Anylatentvariablemodel pmodel( ) h x ,deÔ¨Ånesastochasticencoder pencoder( ) = h x| pmodel( ) h x| (14.12) andastochasticdecoder pdecoder( ) = x h| pmodel( ) x h| . (14.13) Ingeneral,theencoderanddecoderdistributionsarenotnecessarilyconditional distributionscompatiblewithauniquejointdistribution pmodel( x h ,).Alain e t a l . ()showedthattrainingtheencoderanddecoderasadenoisingautoencoder 2015 willtendtomakethemcompatibleasymptotically(withenoughcapacityand examples). 14.5DenoisingAutoencoders The denoising aut o e nc o der(DAE)isanautoencoderthatreceivesacorrupted datapointasinputandistrainedtopredicttheoriginal,uncorrupteddatapoint asitsoutput. TheDAEtrainingprocedureisillustratedinÔ¨Ågure.Weintroducea 14.3 corruptionprocess C(Àúx x|)whichrepresentsaconditional distrib utionover 5 1 0 CHAPTER14.AUTOENCODERS Àú x Àú x L Lh h fg xxC ( Àú x x| ) Figure14.3:Thecomputationalgraphofthecostfunctionforadenoisingautoencoder, whichistrainedtoreconstructthecleandatapoint xfromitscorruptedversionÀú x. Thisisaccomplishedbyminimizingtheloss L=‚àílog pdecoder( x h|= f(Àú x)),where Àú xisacorruptedversionofthedataexample x,obtainedthroughagivencorruption process C(Àú x x|).Typicallythedistribution pdecoderisafactorialdistributionwhosemean parametersareemittedbyafeedforwardnetwork. g corruptedsamples Àú x,givenadatasample x.Theautoencoderthenlearnsa r e c o nst r u c t i o n di st r i but i o n preconstruct( x|Àú x)estimatedfromtrainingpairs ( x ,Àú x),asfollows: 1. Sampleatrainingexamplefromthetrainingdata. x 2. SampleacorruptedversionÀú xfrom C(Àú x x|= ) x. 3.Use( x ,Àú x)asatrainingexampleforestimatingtheautoencoderreconstruction distribution preconstruct( x|Àúx) = pdecoder( x h|)with htheoutputofencoder f(Àú x)and pdecodertypicallydeÔ¨Ånedbyadecoder. g() h Typicallywecansimplyperformgradient-basedapproximate minimization (such asminibatchgradientdescent)onthenegativelog-likelihood‚àílog pdecoder( x h|). Solongastheencoderisdeterministic,thedenoisingautoencoderisafeedforward network andmay be trainedwith exactlythesame techniques as anyother feedforwardnetwork. WecanthereforeviewtheDAEasperformingstochasticgradientdescenton thefollowingexpectation: ‚àí E x‚àºÀÜ p d a t a() x EÀú x‚àº C(Àúx| x)log pdecoder( = ( x h| fÀú x))(14.14) where ÀÜ pdata() xisthetrainingdistribution. 5 1 1 CHAPTER14.AUTOENCODERS xÀú x g f‚ó¶ Àú x C ( Àú x x| ) x Figure14.4:AdenoisingautoencoderistrainedtomapacorrupteddatapointÀúxbackto theoriginaldatapoint x.Weillustratetrainingexamples xasredcrosseslyingneara low-dimensionalmanifoldillustratedwiththeboldblackline.Weillustratethecorruption process C(Àúx x|) withagraycircleofequiprobablecorruptions.Agrayarrowdemonstrates howonetrainingexampleistransformedintoonesamplefromthiscorruptionprocess. Whenthedenoisingautoencoderistrainedtominimizetheaverageofsquarederrors || g( f(Àú x))‚àí|| x2,thereconstruction g( f(Àú x)) estimates E x ,Àú x‚àº p dat a()( x CÀúx x|)[ x|Àú x].Thevector g( f(Àúx))‚àíÀú xpointsapproximatelytowardsthenearestpointonthemanifold,since g( f(Àúx)) estimatesthecenterofmassofthecleanpoints xwhichcouldhavegivenrisetoÀú x.The autoencoderthuslearnsavectorÔ¨Åeld g( f( x))‚àí xindicatedbythegreenarrows.This vectorÔ¨Åeldestimatesthescore‚àá xlog pdata( x)uptoamultiplicativefactorthatisthe averagerootmeansquarereconstructionerror. 5 1 2 CHAPTER14.AUTOENCODERS 1 4 . 5 . 1 E s t i m a t i n g t h e S co re Scorematching(,)isanalternativetomaximumlikelihood.It Hyv√§rinen2005 providesaconsistentestimatorofprobabilitydistributionsbasedonencouraging themodeltohavethesame sc o r easthedatadistributionateverytrainingpoint x.Inthiscontext,thescoreisaparticulargradientÔ¨Åeld: ‚àá xlog() p x . (14.15) Scorematchingisdiscussedfurtherinsection.Forthepresentdiscussion 18.4 regardingautoencoders,itissuÔ¨Écienttounderstandthatlearningthegradient Ô¨Åeldoflog pdataisonewaytolearnthestructureof pdataitself. AveryimportantpropertyofDAEsisthat theirtrainingcriterion(with conditionallyGaussian p( x h|))makes theautoencoder learnavectorÔ¨Åeld ( g( f( x))‚àí x)thatestimatesthescoreofthedatadistribution.Thisisillustrated inÔ¨Ågure.14.4 DenoisingtrainingofaspeciÔ¨Åckindofautoencoder(sigmoidalhiddenunits, linear reconstr uction units) usingGaussiannoiseand meansquared erroras thereconstructioncostisequivalent(,)totrainingaspeciÔ¨Åckind Vincent2011 ofundirectedprobabilisticmodelcalledanRBMwithGaussianvisibleunits. Thiskindofmodelwillbedescribedindetailinsection;forthepresent 20.5.1 discussionitsuÔ¨Écestoknowthatitisamodelthatprovidesanexplicit pmodel( x; Œ∏). WhentheRBMistrainedusing denoising sc o r e m at c hi n g( , KingmaandLeCun 2010),itslearningalgorithmisequivalenttodenoisingtraininginthecorresponding autoencoder.WithaÔ¨Åxednoiselevel,regularizedscorematchingisnotaconsistent estimator;itinsteadrecoversablurredversionofthedistribution.However,if thenoiselevelischosentoapproach0whenthenumberofexamplesapproaches inÔ¨Ånity,thenconsistencyisrecovered.Denoisingscorematchingisdiscussedin moredetailinsection.18.5 OtherconnectionsbetweenautoencodersandRBMsexist.Scorematching appliedtoRBMsyieldsacostfunctionthatisidenticaltoreconstructionerror combinedwitharegularizationtermsimilartothecontractivepenaltyofthe CAE(Swersky2011BengioandDelalleau2009 e t a l .,). ()showedthatanautoen- codergradientprovidesanapproximationtocontrastivedivergencetrainingof RBMs. Forcontinuous-valued x,thedenoisingcriterionwithGaussiancorruptionand reconstructiondistributionyieldsanestimatorofthescorethatisapplicableto generalencoderanddecoderparametrizations ( ,).This AlainandBengio2013 meansagenericencoder-decoderarchitecturemaybemadetoestimatethescore 5 1 3 CHAPTER14.AUTOENCODERS bytrainingwiththesquarederrorcriterion || g f((Àú x x))‚àí||2(14.16) andcorruption C(Àú x=Àúx x|) = (NÀú x x ;= ¬µ , œÉŒ£ = 2I) (14.17) withnoisevariance œÉ2.SeeÔ¨Ågureforanillustrationofhowthisworks. 14.5 Figure14.5:VectorÔ¨Åeldlearnedbyadenoisingautoencoderarounda1-Dcurvedmanifold nearwhichthedataconcentratesina2-Dspace.Eacharrowisproportionaltothe reconstructionminusinputvectoroftheautoencoderandpointstowardshigherprobability accordingtotheimplicitlyestimatedprobabilitydistribution.ThevectorÔ¨Åeldhaszeros atbothmaximaoftheestimateddensityfunction(onthedatamanifolds)andatminima ofthatdensityfunction.Forexample,thespiralarmformsaone-dimensionalmanifoldof localmaximathatareconnectedtoeachother.Localminimaappearnearthemiddleof thegapbetweentwoarms.Whenthenormofreconstructionerror(shownbythelength ofthearrows)islarge,itmeansthatprobabilitycanbesigniÔ¨Åcantlyincreasedbymoving inthedirectionofthearrow,andthatismostlythecaseinplacesoflowprobability. Theautoencodermapstheselowprobabilitypointstohigherprobabilityreconstructions. Whereprobabilityismaximal,thearrowsshrinkbecausethereconstructionbecomesmore accurate.Figurereproducedwithpermissionfrom (). AlainandBengio2013 Ingeneral,thereisnoguaranteethatthereconstruction g( f( x))minusthe input xcorrespondstothegradientofanyfunction,letalonetothescore.Thatis 5 1 4 CHAPTER14.AUTOENCODERS whytheearlyresults(,)arespecializedtoparticularparametrizations Vincent2011 where g( f( x))‚àí xmaybeobtainedbytakingthederivativeofanotherfunction. KamyshanskaandMemisevic2015 Vincent2011 ()generalizedtheresultsof()by identifyingafamilyofshallowautoencoderssuchthat g( f( x))‚àí xcorrespondsto ascoreforallmembersofthefamily. Sofarwehavedescribedonlyhowthedenoisingautoencoderlearnstorepresent aprobabilitydistribution.Moregenerally,onemaywanttousetheautoencoderas agenerativemodelanddrawsamplesfromthisdistribution.Thiswillbedescribed later,insection.20.11 1 4 . 5 . 1 . 1 Hi st o r i</div>
        </div>
    </div>

    <div class="question-card" id="q62">
        <div class="question-header">
            <span class="question-number">Question 62</span>
            <span class="question-type">multiple-choice</span>
        </div>

        <div class="question-text">Sparse coding and autoencoders are two influential methods in unsupervised representation learning, each employing distinct strategies for feature extraction and dimensionality reduction. Their differences in encoder architecture can strongly affect performance and generalization, especially when labeled data is scarce.

Which statement best characterizes the advantage of non-parametric encoders over parametric encoders in sparse coding when applied to tasks with limited labeled data?

1) Non-parametric encoders always achieve faster inference times due to direct feedforward computations.   
2) Parametric encoders outperform non-parametric encoders in generalization because they minimize reconstruction error.   
3) Non-parametric encoders are less robust to out-of-distribution inputs due to fixed mappings.   
4) Parametric encoders ensure differentiability for backpropagation, making them superior for unsupervised learning.   
5) Non-parametric encoders cannot be used for feature extraction because they lack a learned mapping.   
6) Parametric encoders are preferred in low-data regimes due to their higher generalization power.   
7) Non-parametric encoders generalize better than parametric encoders because their iterative optimization is less prone to encoder mapping errors in settings with limited labeled data. </div>

        <div class="answer-section">
            <div class="answer-label">‚úì Correct Answer:</div>
            <div class="answer-text">The correct answer is 7) Non-parametric encoders generalize better than parametric encoders because their iterative optimization is less prone to encoder mapping errors in settings with limited labeled data..</div>
        </div>

        <div class="reference-section">
            <div class="reference-label">üìö Reference Text:</div>
            <button class="toggle-button" onclick="toggleReference(62)">
                Show/Hide Reference
            </button>
            <div id="ref62" class="reference-text hidden">isanoptimization algorithm,thatsolvesanoptimization probleminwhichweseek thesinglemostlikelycodevalue: h‚àó= () = argmax f x hp . ( ) h x| (13.15) Whencombinedwithequationandequation,thisyieldsthefollowing 13.13 13.12 optimization problem: argmax hp( ) h x| (13.16) =argmax hlog( ) p h x| (13.17) =argmin hŒª|||| h 1+ Œ≤||‚àí || x W h2 2 , (13.18) wherewehavedroppedtermsnotdependingon handdividedbypositivescaling factorstosimplifytheequation. Duetotheimpositionofan L1normon h,thisprocedurewillyieldasparse h‚àó(Seesection).7.1.2 Totrainthemodelratherthanjustperforminference,wealternatebetween minimization withrespectto handminimization withrespectto W.Inthis presentation,wetreat Œ≤asahyperparameter.Typicallyitissetto1becauseits roleinthisoptimization problemissharedwith Œªandthereisnoneedforboth hyperparameters.Inprinciple,wecouldalsotreat Œ≤asaparameterofthemodel andlearnit.Ourpresentationherehasdiscardedsometermsthatdonotdepend on hbutdodependon Œ≤.Tolearn Œ≤,thesetermsmustbeincluded,or Œ≤will collapseto.0 Notallapproachestosparsecodingexplicitlybuilda p( h)anda p( x h|). Oftenwearejustinterestedinlearningadictionaryoffeatureswithactivation valuesthatwilloftenbezerowhenextractedusingthisinferenceprocedure. Ifwesample hfromaLaplaceprior,itisinfactazeroprobabilityeventfor anelementof htoactuallybezero.Thegenerativemodelitselfisnotespecially sparse,onlythefeatureextractoris. ()describeapproximate Goodfellow e t a l .2013d 497 CHAPTER13.LINEARFACTORMODELS inferenceinadiÔ¨Äerentmodelfamily,thespikeandslabsparsecodingmodel,for whichsamplesfromthepriorusuallycontaintruezeros. Thesparsecodingapproachcombinedwiththeuseofthenon-parametric encodercaninprincipleminimizethecombinationofreconstructionerrorand log-priorbetterthananyspeciÔ¨Åcparametricencoder.Anotheradvantageisthat thereisnogeneralization errortotheencoder.Aparametricencodermustlearn howtomap xto hinawaythatgeneralizes.Forunusual xthatdonotresemble thetrainingdata,alearned,parametricencodermayfailtoÔ¨Åndan hthatresults inaccuratereconstructionorasparsecode.Forthevastmajorityofformulations ofsparsecodingmodels,wheretheinferenceproblemisconvex,theoptimization procedurewillalwaysÔ¨Åndtheoptimalcode(unlessdegeneratecasessuchas replicatedweightvectorsoccur).Obviously,thesparsityandreconstructioncosts canstillriseonunfamiliarpoints,butthisisduetogeneralization errorinthe decoderweights,ratherthangeneralization errorintheencoder.Thelackof generalization errorinsparsecoding‚Äôsoptimization-based encodingprocessmay resultinbettergeneralization whensparsecodingisusedasafeatureextractorfor aclassiÔ¨Åerthanwhenaparametricfunctionisusedtopredictthecode.Coates andNg2011()demonstratedthatsparsecodingfeaturesgeneralizebetterfor objectrecognitiontasksthanthefeaturesofarelatedmodelbasedonaparametric encoder,thelinear-sigmoidautoencoder.Inspiredbytheirwork,Goodfellow e t a l . ()showedthatavariantofsparsecodinggeneralizesbetterthanotherfeature 2013d extractorsintheregimewhereextremelyfewlabelsareavailable(twentyorfewer labelsperclass). Theprimarydisadvantageofthenon-parametric encoderisthatitrequires greatertimetocompute hgiven xbecausethenon-parametric approachrequires runninganiterativealgorithm.Theparametricautoencoderapproach,developed in chapter ,usesonly a Ô¨Åxed n umber of layers, often only one.Another 14 disadvantageisthatitisnotstraight-forwardtoback-propagatethroughthe non-parametric encoder,whichmakesitdiÔ¨Éculttopretrainasparsecodingmodel withanunsupervisedcriterionandthenÔ¨Åne-tuneitusingasupervisedcriterion. ModiÔ¨Åedversionsofsparsecodingthatpermitapproximate derivativesdoexist butarenotwidelyused( ,). BagnellandBradley2009 Sparsecoding,likeotherlinearfactormodels,oftenproducespoorsamples,as showninÔ¨Ågure.Thishappensevenwhenthemodelisabletoreconstruct 13.2 thedatawellandprovideusefulfeaturesforaclassiÔ¨Åer.Thereasonisthateach individualfeaturemaybelearnedwell,butthefactorialprioronthehiddencode resultsinthemodelincludingrandomsubsetsofallofthefeaturesineachgenerated sample.Thismotivatesthedevelopmentofdeepermodelsthatcanimposeanon- 498 CHAPTER13.LINEARFACTORMODELS Figure13.2: Example samplesandweightsfromaspikeandslabsparsecodingmodel trainedontheMNISTdataset. ( L e f t )Thesamplesfromthemodeldonotresemblethe trainingexamples.AtÔ¨Årstglance,onemightassumethemodelispoorlyÔ¨Åt.The ( R i g h t ) weightvectorsofthemodelhavelearnedtorepresentpenstrokesandsometimescomplete digits.Themodelhasthuslearnedusefulfeatures.Theproblemisthatthefactorialprior overfeaturesresultsinrandomsubsetsoffeaturesbeingcombined.Fewsuchsubsets areappropriatetoformarecognizableMNISTdigit.Thismotivatesthedevelopmentof generativemodelsthathavemorepowerfuldistributionsovertheirlatentcodes.Figure reproducedwithpermissionfromGoodfellow2013d e t a l .(). factorialdistributiononthedeepestcodelayer,aswellasthedevelopmentofmore sophisticatedshallowmodels. 13.5ManifoldInterpretationofPCA LinearfactormodelsincludingPCAandfactoranalysiscanbeinterpretedas learningamanifold( ,).WecanviewprobabilisticPCAas Hinton e t a l .1997 deÔ¨Åningathinpancake-shapedregionofhighprobability‚ÄîaGaussiandistribution thatisverynarrowalongsomeaxes,justasapancakeisveryÔ¨Çatalongitsvertical axis,butiselongatedalongotheraxes,justasapancakeiswidealongitshorizontal axes. ThisisillustratedinÔ¨Ågure. PCAcanbeinterpretedasaligningthis 13.3 pancakewithalinearmanifoldinahigher-dimens ionalspace.Thisinterpretation appliesnotjusttotraditionalPCAbutalsotoanylinearautoencoderthatlearns matrices Wand Vwiththegoalofmakingthereconstructionof xlieascloseto xaspossible, Lettheencoderbe h x W = ( f) = ÓÄæ( ) x ¬µ‚àí . (13.19) 499 CHAPTER13.LINEARFACTORMODELS Theencodercomputesalow-dimensional representationof h.Withtheautoencoder view,wehaveadecodercomputingthereconstruction ÀÜ x h b V h = ( g) = + . (13.20) Figure13.3:FlatGaussiancapturingprobabilityconcentrationnearalow-dimensional manifold.TheÔ¨Ågureshowstheupperhalfofthe‚Äúpancake‚Äùabovethe‚Äúmanifoldplane‚Äù whichgoesthroughitsmiddle.Thevarianceinthedirectionorthogonaltothemanifoldis verysmall(arrowpointingoutofplane)andcanbeconsideredlike‚Äúnoise,‚Äùwhiletheother variancesarelarge(arrowsintheplane)andcorrespondto‚Äúsignal,‚Äùandacoordinate systemforthereduced-dimensiondata. Thechoicesoflinearencoderanddecoderthatminimizereconstructionerror E[||‚àí xÀÜ x||2] (13.21) correspondto V= W, ¬µ= b= E[ x]andthecolumnsof Wformanorthonormal basiswhichspansthesamesubspaceastheprincipaleigenvectorsofthecovariance matrix C x ¬µ x ¬µ = [( E ‚àí)(‚àí)ÓÄæ] . (13.22) InthecaseofPCA,thecolumnsof Waretheseeigenvectors,orderedbythe magnitudeofthecorrespondingeigenvalues(whichareallrealandnon-negative). Onecanalsoshowthateigenvalue Œª iof Ccorrespondstothevarianceof x inthedirectionofeigenvector v( ) i.If x‚àà RDand h‚àà Rdwith d < D,thenthe 500 CHAPTER13.LINEARFACTORMODELS optimalreconstructionerror(choosing,,andasabove)is ¬µ b V W min[ E||‚àí xÀÜ x||2] =D ÓÅò i d = + 1Œª i . (13.23) Hence,ifthecovariancehasrank d,theeigenvalues Œª d + 1to Œª Dare0andrecon- structionerroris0. Furthermore,onecanalsoshowthattheabovesolutioncanbeobtainedby maximizingthevariancesoftheelementsof h,underorthogonal W,insteadof minimizingreconstructionerror. Linearfactormodelsaresomeofthesimplestgenerativemodelsandsomeofthe simplestmodelsthatlearnarepresentationofdata.MuchaslinearclassiÔ¨Åersand linearregressionmodelsmaybeextendedtodeepfeedforwardnetworks,theselinear factormodelsmaybeextendedtoautoencodernetworksanddeepprobabilistic modelsthatperformthesametasksbutwithamuchmorepowerfulandÔ¨Çexible modelfamily. 501 C h a p t e r 1 4 A u t o e n co d e rs An aut o e nc o derisaneuralnetworkthatistrainedtoattempttocopyitsinput toitsoutput. Internally ,ithasahiddenlayer hthatdescribesa c o deusedto representtheinput.Thenetworkmaybeviewedasconsistingoftwoparts:an encoderfunction h= f( x)andadecoderthatproducesareconstruction r= g( h). ThisarchitectureispresentedinÔ¨Ågure.Ifanautoencodersucceedsinsimply 14.1 learningtoset g( f( x)) = xeverywhere,thenitisnotespeciallyuseful.Instead, autoencodersaredesignedtobeunabletolearntocopyperfectly.Usuallytheyare restrictedinwaysthatallowthemtocopyonlyapproximately ,andtocopyonly inputthatresemblesthetrainingdata.Becausethemodelisforcedtoprioritize whichaspectsoftheinputshouldbecopied,itoftenlearnsusefulpropertiesofthe data. Modern autoencoders havegeneralized the idea of anencoder and ade- coderbeyonddeterministicfunctionstostochasticmappings pencoder( h x|)and pdecoder( ) x h|. Theideaofautoencodershasbeenpartofthehistoricallandscapeofneural networksfordecades(,; ,; , LeCun1987BourlardandKamp1988HintonandZemel 1994).Traditionally, autoencoderswereused fordimensionalityreductionor featurelearning.Recently,theoreticalconnectionsbetweenautoencodersand latentvariablemodelshavebroughtautoencoderstotheforefrontofgenerative modeling,aswewillseeinchapter.Autoencodersmaybethoughtofasbeing 20 aspecialcaseoffeedforwardnetworks,andmaybetrainedwithallofthesame techniques,typicallyminibatchgradientdescentfollowinggradientscomputed byback-propagation. Unlikegeneralfeedforwardnetworks,autoencodersmay alsobetrainedusing r e c i r c ul at i o n(HintonandMcClelland1988,),alearning algorithmbasedoncomparingtheactivationsofthenetworkontheoriginalinput 502 CHAPTER14.AUTOENCODERS totheactivationsonthereconstructedinput.Recirculationisregardedasmore biologicallyplausiblethanback-propagation, butisrarelyusedformachinelearning applications. xx rrh h f g Figure14.1:Thegeneralstructureofanautoencoder,mappinganinputtoanoutput x (calledreconstruction) rthroughaninternalrepresentationorcode h.Theautoencoder hastwocomponents:theencoder f(mapping xto h)andthedecoder g(mapping hto r). 14.1UndercompleteAutoencoders Copyingtheinputtotheoutputmaysounduseless,butwearetypicallynot interestedintheoutputofthe decoder. Instead, wehope thattrainingthe autoencodertoperformtheinputcopyingtaskwillresultin htakingonuseful properties. Onewaytoobtainusefulfeaturesfromtheautoencoderistoconstrain hto havesmallerdimensionthan x.Anautoencoderwhosecodedimensionisless thantheinputdimensioniscalled under c o m p l</div>
        </div>
    </div>

    <div class="question-card" id="q63">
        <div class="question-header">
            <span class="question-number">Question 63</span>
            <span class="question-type">multiple-choice</span>
        </div>

        <div class="question-text">Probabilistic modeling and deep learning rely on numerical methods, structured representations of distributions, and divergence measures to optimize models and ensure computational efficiency. Understanding how choices in these areas affect both theoretical properties and practical outcomes is critical for designing robust machine learning systems.

Which of the following most accurately describes the consequence of minimizing D_KL(q||p) (the KL divergence from the approximating distribution q to the target distribution p) when approximating a multimodal probability distribution in variational inference?

1) The approximation covers all high-probability regions, even if it introduces extra modes.   
2) The approximation avoids assigning probability mass to any mode, focusing on low-probability regions.   
3) The approximation results in equal coverage of all modes, regardless of their probabilities.   
4) The approximation often concentrates on a single mode, ignoring low-probability areas between modes.   
5) The approximation matches the target distribution exactly, regardless of its complexity.   
6) The approximation always underestimates the true distribution's variance.   
7) The approximation leads to deterministic outputs by collapsing all modes into a single value.</div>

        <div class="answer-section">
            <div class="answer-label">‚úì Correct Answer:</div>
            <div class="answer-text">The correct answer is 4) The approximation often concentrates on a single mode, ignoring low-probability areas between modes..</div>
        </div>

        <div class="reference-section">
            <div class="reference-label">üìö Reference Text:</div>
            <button class="toggle-button" onclick="toggleReference(63)">
                Show/Hide Reference
            </button>
            <div id="ref63" class="reference-text hidden">Thechoiceofwhichdirectionofthe KLdivergencetouseisproblem-dependent.Someapplicationsrequireanapproximation thatusuallyplaceshighprobabilityanywherethatthetruedistributionplaceshigh probability,whileotherapplicationsrequireanapproximationthatrarelyplaceshigh probabilityanywherethatthetruedistributionplaceslowprobability.Thechoiceofthe directionoftheKLdivergencereÔ¨Çectswhichoftheseconsiderationstakespriorityforeach application. ( L e f t )TheeÔ¨ÄectofminimizingD KL(pqÓÅ´).Inthiscase,weselectaqthathas highprobabilitywherephashighprobability.Whenphasmultiplemodes,qchoosesto blurthemodestogether,inordertoputhighprobabilitymassonallofthem. ( R i g h t )The eÔ¨ÄectofminimizingD KL(qpÓÅ´).Inthiscase,weselectaqthathaslowprobabilitywhere phaslowprobability.WhenphasmultiplemodesthataresuÔ¨Écientlywidelyseparated, asinthisÔ¨Ågure,theKLdivergenceisminimizedbychoosingasinglemode,inorderto avoidputtingprobabilitymassinthelow-probabilityareasbetweenmodesofp.Here,we illustratetheoutcomewhenqischosentoemphasizetheleftmode.Wecouldalsohave achievedanequalvalueoftheKLdivergencebychoosingtherightmode.Ifthemodes arenotseparatedbyasuÔ¨Écientlystronglowprobabilityregion,thenthisdirectionofthe KLdivergencecanstillchoosetoblurthemodes. 76 CHAPTER3.PROBABILITYANDINFORMATIONTHEORY describetheentirejointprobabilitydistributioncanbeveryineÔ¨Écient(both computationally andstatistically). Insteadofusingasinglefunctiontorepresentaprobabilitydistribution,we cansplitaprobabilitydistributionintomanyfactorsthatwemultiplytogether. Forexample,supposewehavethreerandomvariables:a,bandc.Supposethat ainÔ¨ÇuencesthevalueofbandbinÔ¨Çuencesthevalueofc,butthataandcare independentgivenb.Wecanrepresenttheprobabilitydistributionoverallthree variablesasaproductofprobabilitydistributionsovertwovariables: p,,ppp. (abc) = ()a( )ba|( )cb| (3.52) Thesefactorizationscangreatlyreducethenumberofparametersneeded todescribethedistribution.Eachfactorusesanumberofparametersthatis exponentialinthenumberofvariablesinthefactor.Thismeansthatwecangreatly reducethecostofrepresentingadistributionifweareabletoÔ¨Åndafactorization intodistributionsoverfewervariables. Wecandescribethesekindsoffactorizationsusinggraphs.Hereweusetheword ‚Äúgraph‚Äùinthesenseofgraphtheory:asetofverticesthatmaybeconnectedtoeach otherwithedges.Whenwerepresentthefactorizationofaprobabilitydistribution withagraph,wecallita st r uc t ur e d pr o babili s t i c m o delor g r aphic al m o del. Therearetwomainkindsofstructuredprobabilisticmodels:directedand undirected.Bothkindsofgraphicalmodelsuseagraph Ginwhicheachnode inthegraphcorrespondstoarandomvariable, and anedgeconnectingtwo randomvariablesmeansthattheprobabilitydistributionisabletorepresentdirect interactionsbetweenthosetworandomvariables. D i r e c t e dmodelsuse graphswithdirectededges, andtheyrepresentfac- torizationsintoconditionalprobabilitydistributions,asintheexampleabove. SpeciÔ¨Åcally,adirectedmodelcontainsonefactorforeveryrandomvariablex iin thedistribution,andthatfactorconsistsoftheconditionaldistributionoverx i giventheparentsofx i,denotedPa G(x i): p() = xÓÅô ip(x i|Pa G(x i)). (3.53) SeeÔ¨Ågureforanexampleofadirectedgraphandthefactorizationofprobability 3.7 distributionsitrepresents. U ndi r e c t e dmodelsusegraphswithundirectededges,andtheyrepresent factorizationsintoasetoffunctions;unlikeinthedirectedcase,thesefunctions 77 CHAPTER3.PROBABILITYANDINFORMATIONTHEORY aa ccbb eedd Figure3.7:Adirectedgraphicalmodeloverrandomvariablesa,b,c,dande.Thisgraph correspondstoprobabilitydistributionsthatcanbefactoredas p,,,,ppp,pp. (abcde) = ()a( )ba|(ca|b)( )db|( )ec| (3.54) Thisgraphallowsustoquicklyseesomepropertiesofthedistribution.Forexample,a andcinteractdirectly,butaandeinteractonlyindirectlyviac. areusuallynotprobabilitydistributionsofanykind.Anysetofnodesthatareall connectedtoeachotherinGiscalledaclique.Eachclique C( ) iinanundirected modelisassociatedwithafactorœÜ( ) i(C( ) i).Thesefactorsarejustfunctions,not probabilitydistributions.Theoutputofeachfactormustbenon-negative, but thereisnoconstraintthatthefactormustsumorintegrateto1likeaprobability distribution. TheprobabilityofaconÔ¨Ågurationofrandomvariablesis pr o p o r t i o naltothe productofallofthesefactors‚Äîassignmentsthatresultinlargerfactorvaluesare morelikely.Ofcourse,thereisnoguaranteethatthisproductwillsumto1.We thereforedividebyanormalizingconstantZ,deÔ¨Ånedtobethesumorintegral overallstatesoftheproductoftheœÜfunctions,inordertoobtainanormalized probabilitydistribution: p() = x1 ZÓÅô iœÜ( ) iÓÄê C( ) iÓÄë . (3.55) SeeÔ¨Ågureforanexampleofanundirectedgraphandthefactorizationof 3.8 probabilitydistributionsitrepresents. Keep inmind thatthese graphicalrepresentationsof factorizations are a languagefordescribingprobabilitydistributions.Theyarenotmutuallyexclusive familiesofprobabilitydistributions.Beingdirectedorundirectedisnotaproperty ofaprobabilitydistribution;itisapropertyofaparticular desc r i pti o nofa 78 CHAPTER3.PROBABILITYANDINFORMATIONTHEORY aa ccbb eedd Figure3.8:Anundirectedgraphicalmodeloverrandomvariablesa,b,c,dande.This graphcorrespondstoprobabilitydistributionsthatcanbefactoredas p,,,, (abcde) =1 ZœÜ( 1 )( )abc,,œÜ( 2 )()bd,œÜ( 3 )()ce,. (3.56) Thisgraphallowsustoquicklyseesomepropertiesofthedistribution.Forexample,a andcinteractdirectly,butaandeinteractonlyindirectlyviac. probabilitydistribution,butanyprobabilitydistributionmaybedescribedinboth ways. Throughoutpartsandofthisbook,wewillusestructuredprobabilistic III modelsmerelyasalanguagetodescribewhichdirectprobabilisticrelationships diÔ¨Äerentmachinelearningalgorithmschoosetorepresent.Nofurtherunderstanding ofstructuredprobabilisticmodelsisneededuntilthediscussionofresearchtopics, inpart,wherewewillexplorestructuredprobabilisticmodelsinmuchgreater III detail. Thischapterhasreviewedthebasicconceptsofprobabilitytheorythatare mostrelevanttodeeplearning.Onemoresetoffundamentalmathematical tools remains:numericalmethods. 79 C h a p t e r 4 NumericalComputation Machinelearningalgorithmsusuallyrequireahighamountofnumericalcompu- tation.Thistypicallyreferstoalgorithmsthatsolvemathematical problemsby methodsthatupdateestimatesofthesolutionviaaniterativeprocess,ratherthan analyticallyderivingaformulaprovidingasymbolicexpressionforthecorrectso- lution.Commonoperationsincludeoptimization (Ô¨Åndingthevalueofanargument thatminimizesormaximizesafunction)andsolvingsystemsoflinearequations. Evenjustevaluatingamathematical functiononadigitalcomputercanbediÔ¨Écult whenthefunctioninvolvesrealnumbers,whichcannotberepresentedprecisely usingaÔ¨Åniteamountofmemory. 4. 1 O v erÔ¨Ç o w an d Un d erÔ¨Ç o w ThefundamentaldiÔ¨Écultyinperformingcontinuousmathonadigitalcomputer isthatweneedtorepresentinÔ¨ÅnitelymanyrealnumberswithaÔ¨Ånitenumber ofbitpatterns.Thismeansthatforalmostallrealnumbers, weincursome approximationerrorwhenwerepresentthenumberinthecomputer.Inmany cases,thisisjustroundingerror.Roundingerrorisproblematic, especiallywhen itcompoundsacrossmanyoperations,andcancausealgorithmsthatworkin theorytofailinpracticeiftheyarenotdesignedtominimizetheaccumulationof roundingerror. Oneformofroundingerrorthatisparticularlydevastatingis under Ô¨Ço w. UnderÔ¨Çowoccurswhennumbersnearzeroareroundedtozero.Manyfunctions behavequalitativelydiÔ¨Äerentlywhentheirargumentiszeroratherthanasmall positivenumber.Forexample,weusuallywanttoavoiddivisionbyzero(some 80 CHAPTER4.NUMERICALCOMPUTATION softwareenvironmentswillraiseexceptionswhenthisoccurs,otherswillreturna resultwithaplaceholdernot-a-numbervalue)ortakingthelogarithmofzero(this isusuallytreatedas‚àí‚àû,whichthenbecomesnot-a-numberifitisusedformany furtherarithmeticoperations). Anotherhighlydamagingformofnumericalerroris o v e r Ô¨Ço w.OverÔ¨Çowoccurs whennumberswithlargemagnitudeareapproximatedas‚àûor‚àí‚àû.Further arithmeticwillusuallychangetheseinÔ¨Ånitevaluesintonot-a-numbervalues. OneexampleofafunctionthatmustbestabilizedagainstunderÔ¨Çowand overÔ¨Çowisthesoftmaxfunction.Thesoftmaxfunctionisoftenusedtopredictthe probabilities associatedwithamultinoullidistribution.Thesoftmaxfunctionis deÔ¨Ånedtobe softmax() x i=exp( x i)ÓÅên j = 1exp( x j). (4.1) Considerwhathappenswhenallofthe x iareequaltosomeconstant c.Analytically, wecanseethatalloftheoutputsshouldbeequalto1 n.Numerically,thismay notoccurwhen chaslargemagnitude.If cisverynegative,thenexp( c)will underÔ¨Çow.Thismeansthedenominator ofthesoftmaxwillbecome0,sotheÔ¨Ånal resultisundeÔ¨Åned.When cisverylargeandpositive,exp( c)willoverÔ¨Çow,again resultingintheexpressionasawholebeingundeÔ¨Åned.BothofthesediÔ¨Éculties canberesolvedbyinsteadevaluating softmax( z)where z= x‚àímax i x i.Simple algebrashowsthatthevalueofthesoftmaxfunctionisnotchangedanalyticallyby addingorsubtractingascalarfromtheinputvector.Subtracting max i x iresults inthelargestargumenttoexpbeing0,whichrulesoutthepossibilityofoverÔ¨Çow. Likewise,atleastoneterminthedenominator hasavalueof1,whichrulesout thepossibilityofunderÔ¨Çowinthedenominator leadingtoadivisionbyzero. Thereisstillonesmallproblem.UnderÔ¨Çowinthenumeratorcanstillcause theexpressionasawholetoevaluatetozero.Thismeansthatifweimplement logsoftmax( x)byÔ¨Årstrunningthesoftmaxsubroutinethenpassingtheresultto thelogfunction,wecoulderroneouslyobtain ‚àí‚àû.Instead,wemustimplement aseparatefunctionthatcalculates logsoftmaxinanumericallystableway.The logsoftmaxfunctioncanbestabilizedusingthesametrickasweusedtostabilize thefunction. softmax Forthemostpart,wedonotexplicitlydetailallofthenumericalconsiderations involvedinimplementing thevariousalgorithmsdescribedinthisbook.Developers oflow-levellibrariesshouldkeepnumericalissuesinmindwhenimplementing deeplearningalgorithms.Mostreadersofthisbookcansimplyrelyonlow- levellibrariesthatprovidestableimplementations .Insomecases,itispossible toimplementanewalgorithmandhavethenewimplementation automatically 8 1 CHAPTER4.NUMERICALCOMPUTATION stabilized.Theano( ,; ,)isanexample Bergstra e t a l .2010Bastien e t a l .2012 ofasoftwarepackagethatautomatically detectsandstabilizesmanycommon numericallyunstableexpressionsthatariseinthecontextofdeeplearning. 4. 2 P o or C on d i t i o n i n g Conditioning referstohowrapidlyafunctionchangeswithrespecttosmallchanges initsinputs.Functionsthatchangerapidlywhentheirinputsareperturbedslightly canbeproblematicforscientiÔ¨Åccomputationbecauseroundingerrorsintheinputs canresultinlargechangesintheoutput. Considerthefunction f( x)= A‚àí 1x.When A‚àà Rn n √óhasaneigenvalue decomposition,its c o ndi t i o n num beris max i , jÓÄåÓÄåÓÄåÓÄåŒª i Œª jÓÄåÓÄåÓÄåÓÄå. (4.2) Thisistheratioofthemagnitudeofthelargestandsmallesteigenvalue.When thisnumberislarge,matrixinversionisparticularlysensitivetoerrorintheinput. Thissensitivityisanintrinsicpropertyofthematrixitself,nottheresult ofroundingerrorduringmatrixinversion.Poorlyconditionedmatricesamplify pre-existingerrorswhenwemultiplybythetruematrixinverse.Inpractice,the errorwillbecompoundedfurtherbynumericalerrorsintheinversionprocessitself. 4. 3 Gradi en t - Bas e d O p t i m i z a t i o n Mostdeeplearningalgorithmsinvolveoptimization ofsomesort. Optimization referstothetaskofeitherminimizingormaximizingsomefunction f( x) byaltering x. Weusuallyphrasemostoptimization problemsintermsofminimizing f( x). Maximization maybeaccomplishedviaaminimization algorithmbyminimizing ‚àí f() x. Thefunctionwewanttominimizeormaximizeiscalledthe o b j e c</div>
        </div>
    </div>

    <div class="question-card" id="q64">
        <div class="question-header">
            <span class="question-number">Question 64</span>
            <span class="question-type">multiple-choice</span>
        </div>

        <div class="question-text">Structured probabilistic models are fundamental tools in machine learning for representing complex relationships among random variables, often using graphs to encode dependencies efficiently. Their design is critical for tasks such as density estimation, missing value imputation, and denoising, especially in high-dimensional settings.

Which of the following is a key advantage of using structured probabilistic models with graphical representations when modeling high-dimensional data?

1) They eliminate the need for probabilistic inference during model training.   
2) They guarantee perfect reconstruction of missing values in all cases.   
3) They require all variables to interact directly, simplifying model structure.   
4) They allow efficient estimation and inference by reducing the number of parameters through modeling only direct dependencies.   
5) They store joint probability distributions using large lookup tables for memory efficiency.   
6) They only support classification tasks and are unsuitable for density estimation.   
7) They enforce undirected interactions exclusively, limiting their application to causal modeling.</div>

        <div class="answer-section">
            <div class="answer-label">‚úì Correct Answer:</div>
            <div class="answer-text">The correct answer is 4) They allow efficient estimation and inference by reducing the number of parameters through modeling only direct dependencies..</div>
        </div>

        <div class="reference-section">
            <div class="reference-label">üìö Reference Text:</div>
            <button class="toggle-button" onclick="toggleReference(64)">
                Show/Hide Reference
            </button>
            <div id="ref64" class="reference-text hidden">learning.Inordertopreparetodiscusstheseresearchideas,thischapterdescribes structuredprobabilisticmodelsinmuchgreaterdetail.Thischapterisintended tobeself-contained;thereaderdoesnotneedtoreviewtheearlierintroduction beforecontinuingwiththischapter. Astructuredprobabilisticmodelisawayofdescribingaprobabilitydistribution, usingagraphtodescribewhichrandomvariablesintheprobabilitydistribution interactwitheachotherdirectly.Hereweuse‚Äúgraph‚Äùinthegraphtheorysense‚Äîa setofverticesconnectedtooneanotherbyasetofedges.Becausethestructure ofthemodelisdeÔ¨Ånedbyagraph,thesemodelsareoftenalsoreferredtoas graphicalmodels. Thegraphicalmodelsresearchcommunityislargeandhasdevelopedmany diÔ¨Äerentmodels,trainingalgorithms,andinferencealgorithms.Inthischapter,we providebasicbackgroundonsomeofthemostcentralideasofgraphicalmodels, withanemphasisontheconceptsthathaveprovenmostusefultothedeeplearning researchcommunity.Ifyoualreadyhaveastrongbackgroundingraphicalmodels, youmaywishtoskipmostofthischapter.However,evenagraphicalmodelexpert 558 CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING maybeneÔ¨ÅtfromreadingtheÔ¨Ånalsectionofthischapter,section,inwhichwe 16.7 highlightsomeoftheuniquewaysthatgraphicalmodelsareusedfordeeplearning algorithms.Deeplearningpractitioners tendtouseverydiÔ¨Äerentmodelstructures, learningalgorithmsandinferenceproceduresthanarecommonlyusedbytherest ofthegraphicalmodelsresearchcommunity.Inthischapter,weidentifythese diÔ¨Äerencesinpreferencesandexplainthereasonsforthem. InthischapterweÔ¨Årstdescribethechallengesofbuildinglarge-scaleproba- bilisticmodels. Next,wedescribehowtouseagraphtodescribethestructure ofaprobabilitydistribution.Whilethisapproachallowsustoovercomemany challenges,itisnotwithoutitsowncomplications. OneofthemajordiÔ¨Écultiesin graphicalmodelingisunderstandingwhichvariablesneedtobeabletointeract directly,i.e.,whichgraphstructuresaremostsuitableforagivenproblem. We outlinetwoapproachestoresolvingthisdiÔ¨Écultybylearningaboutthedependen- ciesinsection.Finally,weclosewithadiscussionoftheuniqueemphasisthat 16.5 deeplearningpractitioners placeonspeciÔ¨Åcapproachestographicalmodelingin section.16.7 16.1TheChallengeofUnstructuredModeling Thegoalofdeeplearningistoscalemachinelearningtothekindsofchallenges neededtosolveartiÔ¨Åcialintelligence.Thismeansbeingabletounderstandhigh- dimensionaldatawithrichstructure.Forexample,wewouldlikeAIalgorithmsto beabletounderstandnaturalimages,1audiowaveformsrepresentingspeech,and documentscontainingmultiplewordsandpunctuationcharacters. ClassiÔ¨Åcationalgorithmscantakeaninputfromsucharichhigh-dimensional distributionandsummarizeitwithacategoricallabel‚Äîwhatobjectisinaphoto, whatwordisspokeninarecording,whattopicadocumentisabout.Theprocess ofclassiÔ¨Åcationdiscardsmostoftheinformationintheinputandproducesa singleoutput(oraprobabilitydistributionovervaluesofthatsingleoutput).The classiÔ¨Åerisalsooftenabletoignoremanypartsoftheinput.Forexample,when recognizinganobjectinaphoto,itisusuallypossibletoignorethebackgroundof thephoto. Itispossibletoaskprobabilisticmodelstodomanyothertasks.Thesetasksare oftenmoreexpensivethanclassiÔ¨Åcation.Someofthemrequireproducingmultiple outputvalues.Mostrequireacompleteunderstandingoftheentirestructureof 1A n a t u ra l im a ge i s a n i m a g e t h a t m i g h t b e c a p t u re d b y a c a m e ra i n a re a s o n a b l y o rd i n a ry e n v i ro n m e n t , a s o p p o s e d t o a s y n t h e t i c a l l y re n d e re d i m a g e , a s c re e n s h o t o f a we b p a g e , e t c . 5 5 9 CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING theinput,withnooptiontoignoresectionsofit.Thesetasksincludethefollowing: ‚Ä¢Densityestimation:givenaninput x,themachinelearningsystemreturns anestimateofthetruedensity p( x)underthedatageneratingdistribution. Thisrequiresonlyasingleoutput,butitdoesrequireacompleteunderstand- ingoftheentireinput.Ifevenoneelementofthevectorisunusual,the systemmustassignitalowprobability. ‚Ä¢Denoising:givenadamagedorincorrectlyobservedinput Àú x,themachine learningsystemreturnsanestimateoftheoriginalorcorrect x.Forexample, themachinelearningsystemmightbeaskedtoremovedustorscratches fromanoldphotograph.Thisrequiresmultipleoutputs(everyelementofthe estimatedcleanexample x)andanunderstandingoftheentireinput(since evenonedamagedareawillstillrevealtheÔ¨Ånalestimateasbeingdamaged). ‚Ä¢Missingvalueimputation:giventheobservationsofsomeelementsof x, themodelisaskedtoreturnestimatesoforaprobabilitydistributionover someoralloftheunobservedelementsof x.Thisrequiresmultipleoutputs. Becausethemodelcouldbeaskedtorestoreanyoftheelementsof x,it mustunderstandtheentireinput. ‚Ä¢Sampling:themodelgeneratesnewsamplesfromthedistribution p( x). Applicationsincludespeechsynthesis,i.e.producingnewwaveformsthat soundlikenaturalhumanspeech.Thisrequiresmultipleoutputvaluesanda goodmodeloftheentireinput.Ifthesampleshaveevenoneelementdrawn fromthewrongdistribution,thenthesamplingprocessiswrong. Foranexampleofasamplingtaskusingsmallnaturalimages,seeÔ¨Ågure.16.1 Modelingarichdistributionoverthousandsormillionsofrandomvariablesisa challengingtask,bothcomputationally andstatistically.Supposeweonlywanted tomodelbinaryvariables.Thisisthesimplestpossiblecase,andyetalreadyit seemsoverwhelming.Forasmall, 32√ó32 2 pixelcolor(RGB)image,thereare3 0 7 2 possiblebinaryimagesofthisform.Thisnumberisover108 0 0timeslargerthan theestimatednumberofatomsintheuniverse. Ingeneral,ifwewishtomodeladistributionoverarandomvectorxcontaining ndiscretevariablescapableoftakingon kvalueseach,thenthenaiveapproachof representing P(x)bystoringalookuptablewithoneprobabilityvalueperpossible outcomerequires knparameters! Thisisnotfeasibleforseveralreasons: 5 6 0 CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING Figure16.1:Probabilisticmodelingofnaturalimages. ( T o p )Example32√ó32pixelcolor imagesfromtheCIFAR-10dataset( ,).Samples KrizhevskyandHinton2009 ( Bottom ) drawnfromastructuredprobabilisticmodeltrainedonthisdataset.Eachsampleappears atthesamepositioninthegridasthetrainingexamplethatisclosesttoitinEuclidean space.Thiscomparisonallowsustoseethatthemodelistrulysynthesizingnewimages, ratherthanmemorizingthetrainingdata.Contrastofbothsetsofimageshasbeen adjustedfordisplay.Figurereproducedwithpermissionfrom (). Courville e t a l .2011 5 6 1 CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING ‚Ä¢ M e m o r y : t h e c o s t o f s t o r i ng t h e r e p r e s e nt a t i o n:Forallbutverysmallvalues of nand k,representingthedistributionasatablewillrequiretoomany valuestostore. ‚Ä¢ St a t i s t i c a l e Ô¨É c i e nc y:Asthenumberofparametersinamodelincreases, sodoestheamountoftrainingdataneededtochoosethevaluesofthose parametersusingastatisticalestimator.Becausethetable-basedmodel hasanastronomicalnumberofparameters,itwillrequireanastronomically largetrainingsettoÔ¨Åtaccurately.AnysuchmodelwilloverÔ¨Åtthetraining setverybadlyunlessadditionalassumptionsaremadelinkingthediÔ¨Äerent entriesinthetable(forexample,likeinback-oÔ¨Äorsmoothed n-grammodels, section).12.4.1 ‚Ä¢ R u nt i m e : t h e c o s t o f i nfe r e nc e: Supposewewanttoperformaninference taskwhereweuseourmodelofthejointdistribution P(x)tocomputesome otherdistribution,suchasthemarginaldistribution P(x 1)ortheconditional distribution P(x 2|x 1).Computingthesedistributionswillrequiresumming acrosstheentiretable,sotheruntimeoftheseoperationsisashighasthe intractablememorycostofstoringthemodel. ‚Ä¢ R u nt i m e : t h e c o s t o f s a m p l i ng:Likewise,supposewewanttodrawasample fromthemodel.Thenaivewaytodothisistosamplesomevalueu‚àº U(0 ,1), theniteratethroughthetable,addinguptheprobabilityvaluesuntilthey exceed uandreturntheoutcomecorrespondingtothatpositioninthetable. Thisrequiresreadingthroughthewholetableintheworstcase,soithas thesameexponentialcostastheotheroperations. Theproblemwiththetable-basedapproachisthatweareexplicitlymodeling everypossiblekindofinteractionbetweeneverypossiblesubsetofvariables.The probabilitydistributionsweencounterinrealtasksaremuchsimplerthanthis. Usually,mostvariablesinÔ¨Çuenceeachotheronlyindirectly. Forexample,considermodelingtheÔ¨Ånishingtimesofateaminarelayrace. Supposetheteamconsistsofthreerunners:Alice,BobandCarol.Atthestartof therace,Alicecarriesabatonandbeginsrunningaroundatrack.Aftercompleting herlaparoundthetrack,shehandsthebatontoBob.Bobthenrunshisown lapandhandsthebatontoCarol,whorunstheÔ¨Ånallap.Wecanmodeleachof theirÔ¨Ånishingtimesasacontinuousrandomvariable.Alice‚ÄôsÔ¨Ånishingtimedoes notdependonanyoneelse‚Äôs,sinceshegoesÔ¨Årst.Bob‚ÄôsÔ¨Ånishingtimedepends onAlice‚Äôs,becauseBobdoesnothavetheopportunitytostarthislapuntilAlice hascompletedhers. IfAliceÔ¨Ånishesfaster,BobwillÔ¨Ånishfaster,allelsebeing 5 6 2 CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING equal.Finally,Carol‚ÄôsÔ¨Ånishingtimedependsonbothherteammates.IfAliceis slow,BobwillprobablyÔ¨Ånishlatetoo.Asaconsequence,Carolwillhavequitea latestartingtimeandthusislikelytohavealateÔ¨Ånishingtimeaswell.However, Carol‚ÄôsÔ¨Ånishingtimedependsonly i ndir e c t l yonAlice‚ÄôsÔ¨ÅnishingtimeviaBob‚Äôs. IfwealreadyknowBob‚ÄôsÔ¨Ånishingtime,wewillnotbeabletoestimateCarol‚Äôs Ô¨ÅnishingtimebetterbyÔ¨ÅndingoutwhatAlice‚ÄôsÔ¨Ånishingtimewas.Thismeans wecanmodeltherelayraceusingonlytwointeractions: Alice‚ÄôseÔ¨ÄectonBoband Bob‚ÄôseÔ¨ÄectonCarol.Wecanomitthethird,indirectinteractionbetweenAlice andCarolfromourmodel. Structuredprobabilisticmodelsprovideaformalframeworkformodelingonly directinteractionsbetweenrandomvariables.Thisallowsthemodelstohave signiÔ¨Åcantlyfewerparametersandthereforebeestimatedreliablyfromlessdata. Thesesmallermodelsalsohavedramatically reducedcomputational costinterms ofstoringthemodel,performinginferenceinthemodel,anddrawingsamplesfrom themodel. 16.2UsingGraphstoDescribeModelStructure Structuredprobabilisticmodelsusegraphs(inthegraphtheorysenseof‚Äúnodes‚Äùor ‚Äúvertices‚Äùconnectedbyedges)torepresentinteractionsbetweenrandomvariables. Eachnoderepresentsarandomvariable.Eachedgerepresentsadirectinteraction. Thesedirectinteractionsimplyother,indirectinteractions,butonlythedirect interactionsneedtobeexplicitlymodeled. Thereismore thanone wayto describe theinteractionsin aprobability distributionusingagraph.Inthefollowingsectionswedescribesomeofthemost popularandusefulapproaches.Graphicalmodelscanbelargelydividedinto twocategories:modelsbasedondirectedacyclicgraphs,andmodelsbasedon undirectedgraphs. 1 6 . 2 . 1 D i rect ed Mo d el s Onekindofstructuredprobabilisticmodelisthedirectedgraphicalmodel, otherwiseknownasthebeliefnetworkBayesiannetwork or2(Pearl1985,). Directedgraphicalmodelsarecalled‚Äúdirected‚Äùbecausetheiredgesaredirected,</div>
        </div>
    </div>

    <div class="question-card" id="q65">
        <div class="question-header">
            <span class="question-number">Question 65</span>
            <span class="question-type">multiple-choice</span>
        </div>

        <div class="question-text">Optimization algorithms are fundamental in machine learning and statistics, where the geometry of the objective function and the presence of constraints shape algorithmic choices. Understanding how second-order information and convexity affect guarantees and efficiency is critical for selecting appropriate methods.

Which property of an objective function ensures that any local minimum is also a global minimum, allowing optimization algorithms to avoid saddle points and guaranteeing strong theoretical results?

1) Positive condition number of the Hessian   
2) Existence of stationary points   
3) Lipschitz continuity   
4) Strict feasibility of constraints   
5) Diagonal dominance of the Hessian   
6) Gradient being zero everywhere   
7) Convexity with positive semidefinite Hessian everywhere </div>

        <div class="answer-section">
            <div class="answer-label">‚úì Correct Answer:</div>
            <div class="answer-text">The correct answer is 7) Convexity with positive semidefinite Hessian everywhere.</div>
        </div>

        <div class="reference-section">
            <div class="reference-label">üìö Reference Text:</div>
            <button class="toggle-button" onclick="toggleReference(65)">
                Show/Hide Reference
            </button>
            <div id="ref65" class="reference-text hidden">function.UsingtheeigendecompositionoftheHessianmatrix,wecangeneralize thesecondderivativetesttomultipledimensions.Atacriticalpoint,where ‚àá x f( x) = 0,wecanexaminetheeigenvaluesoftheHessiantodeterminewhether thecriticalpointisalocalmaximum,localminimum,orsaddlepoint.Whenthe HessianispositivedeÔ¨Ånite(allitseigenvaluesarepositive),thepointisalocal minimum.Thiscanbeseenbyobservingthatthedirectionalsecondderivative inanydirectionmustbepositive,andmakingreferencetotheunivariatesecond derivativetest.Likewise,whentheHessianisnegativedeÔ¨Ånite(allitseigenvalues arenegative),thepointisalocalmaximum.Inmultipledimensions,itisactually possibletoÔ¨Åndpositiveevidenceofsaddlepointsinsomecases. Whenatleast oneeigenvalueispositiveandatleastoneeigenvalueisnegative,weknowthat xisalocalmaximumononecrosssectionof fbutalocalminimumonanother crosssection.SeeÔ¨Ågureforanexample.Finally,themultidimensionalsecond 4.5 derivativetestcanbeinconclusive,justliketheunivariateversion.Thetestis inconclusivewheneverallofthenon-zeroeigenvalueshavethesamesign,butat leastoneeigenvalueiszero.Thisisbecausetheunivariatesecondderivativetestis inconclusiveinthecrosssectioncorrespondingtothezeroeigenvalue. Inmultipledimensions,thereisadiÔ¨Äerentsecondderivativeforeachdirection atasinglepoint.TheconditionnumberoftheHessianatthispointmeasures howmuchthesecondderivativesdiÔ¨Äerfromeachother.WhentheHessianhasa poorconditionnumber,gradientdescentperformspoorly.Thisisbecauseinone direction,thederivativeincreasesrapidly,whileinanotherdirection,itincreases slowly.Gradientdescentisunawareofthischangeinthederivativesoitdoesnot knowthatitneedstoexplorepreferentially inthedirectionwherethederivative remainsnegativeforlonger.ItalsomakesitdiÔ¨Éculttochooseagoodstepsize. Thestepsizemustbesmallenoughtoavoidovershootingtheminimumandgoing uphillindirectionswithstrongpositivecurvature.Thisusuallymeansthatthe stepsizeistoosmalltomakesigniÔ¨Åcantprogressinotherdirectionswithless curvature.SeeÔ¨Ågureforanexample.4.6 ThisissuecanberesolvedbyusinginformationfromtheHessianmatrixtoguide 8 9 CHAPTER4.NUMERICALCOMPUTATION ÓÅ∏ÓÄ± ÓÄ± ÓÄµÓÄ∞ÓÄ± ÓÄµÓÅ∏ ÓÄ≤ ÓÄ± ÓÄµÓÄ∞ÓÄ± ÓÄµÓÅ¶ÓÅ∏ÓÄ®ÓÄ±ÓÄª ÓÅ∏ÓÄ≤ÓÄ©  ÓÄµ ÓÄ∞ ÓÄ∞ÓÄ∞ÓÄµ ÓÄ∞ ÓÄ∞ Figure4.5:Asaddlepointcontainingbothpositiveandnegativecurvature.Thefunction inthisexampleis f( x)= x2 1‚àí x2 2.Alongtheaxiscorrespondingto x 1,thefunction curvesupward.ThisaxisisaneigenvectoroftheHessianandhasapositiveeigenvalue. Alongtheaxiscorrespondingto x 2,thefunctioncurvesdownward.Thisdirectionisan eigenvectoroftheHessianwithnegativeeigenvalue.Thename‚Äúsaddlepoint‚Äùderivesfrom thesaddle-likeshapeofthisfunction.Thisisthequintessentialexampleofafunction withasaddlepoint.Inmorethanonedimension,itisnotnecessarytohaveaneigenvalue of0inordertogetasaddlepoint:itisonlynecessarytohavebothpositiveandnegative eigenvalues.Wecanthinkofasaddlepointwithbothsignsofeigenvaluesasbeingalocal maximumwithinonecrosssectionandalocalminimumwithinanothercrosssection. 9 0 CHAPTER4.NUMERICALCOMPUTATION ‚àí ‚àí ‚àí 3 0 2 0 1 0 0 1 0 2 0 x 1‚àí 3 0‚àí 2 0‚àí 1 001 02 0x 2 Figure4.6:Gradientdescentfailstoexploitthecurvatureinformationcontainedinthe Hessianmatrix.Hereweusegradientdescenttominimizeaquadraticfunction f( x) whose Hessianmatrixhasconditionnumber5.Thismeansthatthedirectionofmostcurvature hasÔ¨Åvetimesmorecurvaturethanthedirectionofleastcurvature.Inthiscase,themost curvatureisinthedirection[1 ,1]ÓÄæandtheleastcurvatureisinthedirection[1 ,‚àí1]ÓÄæ.The redlinesindicatethepathfollowedbygradientdescent.Thisveryelongatedquadratic functionresemblesalongcanyon.Gradientdescentwastestimerepeatedlydescending canyonwalls,becausetheyarethesteepestfeature.Becausethestepsizeissomewhat toolarge,ithasatendencytoovershootthebottomofthefunctionandthusneedsto descendtheoppositecanyonwallonthenextiteration.Thelargepositiveeigenvalue oftheHessiancorrespondingtotheeigenvectorpointedinthisdirectionindicatesthat thisdirectionalderivativeisrapidlyincreasing,soanoptimizationalgorithmbasedon theHessiancouldpredictthatthesteepestdirectionisnotactuallyapromisingsearch directioninthiscontext. 9 1 CHAPTER4.NUMERICALCOMPUTATION thesearch.Thesimplestmethodfordoingsoisknownas Newt o n‚Äô s m e t ho d. Newton‚Äôsmethodisbasedonusingasecond-orderTaylorseriesexpansionto approximatenearsomepoint f() x x( 0 ): f f () x‚âà( x( 0 ))+( x x‚àí( 0 ))ÓÄæ‚àá x f( x( 0 ))+1 2( x x‚àí( 0 ))ÓÄæH x()( f( 0 ))( x x‚àí( 0 )) .(4.11) Ifwethensolveforthecriticalpointofthisfunction,weobtain: x‚àó= x( 0 )‚àí H x()( f( 0 ))‚àí 1‚àá x f( x( 0 )) . (4.12) When fisapositivedeÔ¨Ånitequadraticfunction,Newton‚Äôsmethodconsistsof applyingequationoncetojumptotheminimumofthefunctiondirectly. 4.12 When fisnottrulyquadraticbutcanbelocallyapproximatedasapositive deÔ¨Ånitequadratic,Newton‚Äôsmethodconsistsofapplyingequationmultiple4.12 times. Iterativelyupdatingtheapproximation andjumpingtotheminimumof theapproximation canreachthecriticalpointmuchfasterthangradientdescent would.Thisisausefulpropertynearalocalminimum,butitcanbeaharmful propertynearasaddlepoint.Asdiscussedinsection,Newton‚Äôsmethodis 8.2.3 onlyappropriatewhenthenearbycriticalpointisaminimum(alltheeigenvalues oftheHessianarepositive),whereasgradientdescentisnotattractedtosaddle pointsunlessthegradientpointstowardthem. Optimization algorithmsthatuseonlythegradient,suchasgradientdescent, arecalled Ô¨År st - o r d e r o pt i m i z a t i o n al g o r i t hms.Optimization algorithmsthat alsousetheHessianmatrix,suchasNewton‚Äôsmethod,arecalled se c o nd-or d e r o pt i m i z a t i o n al g o r i t hms(NocedalandWright2006,). The optimization algorithms employedin mostcontextsin this book are applicabletoawidevarietyoffunctions,butcomewithalmostnoguarantees. Deeplearningalgorithmstendtolackguaranteesbecausethefamilyoffunctions usedindeeplearningisquitecomplicated.InmanyotherÔ¨Åelds,thedominant approachtooptimization istodesignoptimization algorithmsforalimitedfamily offunctions. Inthecontextofdeeplearning,wesometimesgainsomeguaranteesbyrestrict- ingourselvestofunctionsthatareeither L i psc hi t z c o n t i n uousorhaveLipschitz continuousderivatives.ALipschitzcontinuousfunctionisafunction fwhoserate ofchangeisboundedbya L i psc hi t z c o nst antL: ‚àÄ‚àÄ| ‚àí |‚â§L||‚àí|| x , y , f() x f() y x y 2 . (4.13) Thispropertyisusefulbecauseitallowsustoquantifyourassumptionthata smallchangeintheinputmadebyanalgorithmsuchasgradientdescentwillhave 9 2 CHAPTER4.NUMERICALCOMPUTATION asmallchangeintheoutput.Lipschitzcontinuityisalsoafairlyweakconstraint, andmanyoptimizationproblemsindeeplearningcanbemadeLipschitzcontinuous withrelativelyminormodiÔ¨Åcations. PerhapsthemostsuccessfulÔ¨Åeldofspecializedoptimization is c o n v e x o p- t i m i z at i o n.Convexoptimization algorithmsareabletoprovidemanymore guaranteesbymakingstrongerrestrictions.Convexoptimization algorithmsare applicableonlytoconvexfunctions‚ÄîfunctionsforwhichtheHessianispositive semideÔ¨Åniteeverywhere.Suchfunctionsarewell-behavedbecausetheylacksaddle pointsandalloftheirlocalminimaarenecessarilyglobalminima.However,most problemsindeeplearningarediÔ¨Éculttoexpressintermsofconvexoptimization. Convexoptimization isusedonlyasasubroutineofsomedeeplearningalgorithms. Ideasfromtheanalysisofconvexoptimization algorithmscanbeusefulforproving theconvergenceofdeeplearningalgorithms.However,ingeneral,theimportance ofconvexoptimization isgreatlydiminishedinthecontextofdeeplearning.For moreinformationaboutconvexoptimization, seeBoydandVandenberghe2004() orRockafellar1997(). 4. 4 C on s t ra i n ed O p t i m i z a t i o n Sometimeswewishnotonlytomaximizeorminimizeafunction f( x)overall possible values of x.Insteadwemay wishto Ô¨Ånd themaximal or minimal value of f( x)for valuesof xinsome set S.Thisis known as c o nst r ai n e d o pt i m i z a t i o n.Points xthatliewithintheset Sarecalled f e asi bl epointsin constrainedoptimization terminology. WeoftenwishtoÔ¨Åndasolutionthatissmallinsomesense.Acommon approachinsuchsituationsistoimposeanormconstraint,suchas. ||||‚â§ x 1 Onesimpleapproachtoconstrainedoptimization issimplytomodifygradient descenttakingtheconstraintintoaccount.Ifweuseasmallconstantstepsize ÓÄè, wecanmakegradientdescentsteps,thenprojecttheresultbackinto S.Ifweuse alinesearch,wecansearchonlyoverstepsizes ÓÄèthatyieldnew xpointsthatare feasible,orwecanprojecteachpointonthelinebackintotheconstraintregion. Whenpossible,thismethodcanbemademoreeÔ¨Écientbyprojectingthegradient intothetangentspaceofthefeasibleregionbeforetakingthesteporbeginning thelinesearch(,).Rosen1960 AmoresophisticatedapproachistodesignadiÔ¨Äerent,unconstrainedopti- mizationproblemwhosesolutioncanbeconvertedintoasolutiontotheoriginal, constrainedoptimization problem.Forexample,ifwewanttominimize f( x)for 9 3 CHAPTER4.NUMERICALCOMPUTATION x‚àà R2with xconstrainedtohaveexactlyunit L2norm,wecaninsteadminimize g( Œ∏) = f([cossin Œ∏ , Œ∏]ÓÄæ)withrespectto Œ∏,thenreturn[cossin Œ∏ , Œ∏]asthesolution totheoriginalproblem.Thisapproachrequirescreativity;thetransformation betweenoptimization problemsmustbedesignedspeciÔ¨Åcallyforeachcasewe encounter. The K ar ush‚Äì K u h n ‚Äì T uc k e r(KKT)approach1providesaverygeneralso- lutiontoconstrainedoptimization. WiththeKKTapproach,weintroducea</div>
        </div>
    </div>

    <div class="question-card" id="q66">
        <div class="question-header">
            <span class="question-number">Question 66</span>
            <span class="question-type">multiple-choice</span>
        </div>

        <div class="question-text">In deep learning, Recurrent Neural Networks (RNNs) are widely used for modeling sequences with potentially complex dependencies. Their architecture leverages hidden states and parameter sharing to efficiently capture relationships across varying sequence lengths.

Which property of RNNs allows them to represent joint distributions over long sequences efficiently, without the number of parameters increasing with sequence length?

1) Using different weight matrices for each timestep   
2) Sharing parameters across all timesteps   
3) Expanding the hidden state size linearly with sequence length   
4) Ignoring long-term dependencies   
5) Utilizing a tabular representation for transitions   
6) Adding new layers for each additional timestep   
7) Restricting connections to immediate predecessors</div>

        <div class="answer-section">
            <div class="answer-label">‚úì Correct Answer:</div>
            <div class="answer-text">The correct answer is 2) Sharing parameters across all timesteps.</div>
        </div>

        <div class="reference-section">
            <div class="reference-label">üìö Reference Text:</div>
            <button class="toggle-button" onclick="toggleReference(66)">
                Show/Hide Reference
            </button>
            <div id="ref66" class="reference-text hidden">variables.Manygraphicalmodelsaimtoachievestatisticalandcomputational eÔ¨Éciencybyomittingedgesthatdonotcorrespondtostronginteractions.For 3 8 8 CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS example,itiscommontomaketheMarkovassumptionthatthegraphicalmodel shouldonlycontainedgesfrom{y( ) t k ‚àí, . . . ,y( 1 ) t ‚àí}toy( ) t,ratherthancontaining edgesfromtheentirepasthistory.However,insomecases,webelievethatallpast inputsshouldhaveaninÔ¨Çuenceonthenextelementofthesequence.RNNsare usefulwhenwebelievethatthedistributionovery( ) tmaydependonavalueofy( ) i fromthedistantpastinawaythatisnotcapturedbytheeÔ¨Äectofy( ) iony( 1 ) t ‚àí. OnewaytointerpretanRNNasagraphicalmodelistoviewtheRNNas deÔ¨Åningagraphicalmodelwhosestructureisthecompletegraph,abletorepresent directdependenciesbetweenanypairofyvalues.Thegraphicalmodeloverthey valueswiththecompletegraphstructureisshowninÔ¨Ågure.Thecomplete10.7 graphinterpretationoftheRNNisbasedonignoringthehiddenunitsh( ) tby marginalizing themoutofthemodel. ItismoreinterestingtoconsiderthegraphicalmodelstructureofRNNsthat resultsfromregardingthehiddenunitsh( ) tasrandomvariables.1Includingthe hiddenunitsinthegraphicalmodelrevealsthattheRNNprovidesaveryeÔ¨Écient parametrization ofthejointdistributionovertheobservations.Supposethatwe representedanarbitraryjointdistributionoverdiscretevalueswithatabular representation‚Äîanarraycontainingaseparateentryforeachpossibleassignment ofvalues,withthevalueofthatentrygivingtheprobabilityofthatassignment occurring. If ycantakeon kdiÔ¨Äerentvalues,thetabularrepresentationwould have O( kœÑ)parameters.Bycomparison,duetoparametersharing,thenumberof parametersintheRNNis O(1)asafunctionofsequencelength.Thenumberof parametersintheRNNmaybeadjustedtocontrolmodelcapacitybutisnotforced toscalewithsequencelength.EquationshowsthattheRNNparametrizes 10.5 long-termrelationshipsbetweenvariableseÔ¨Éciently,usingrecurrentapplications ofthesamefunction fandsameparametersŒ∏ateachtimestep.Figure10.8 illustratesthegraphicalmodelinterpretation.Incorporating theh( ) tnodesin thegraphicalmodeldecouplesthepastandthefuture,actingasanintermediate quantitybetweenthem.Avariable y( ) iinthedistantpastmayinÔ¨Çuenceavariable y( ) tviaitseÔ¨Äectonh.Thestructureofthisgraphshowsthatthemodelcanbe eÔ¨Écientlyparametrized byusingthesameconditionalprobabilitydistributionsat eachtimestep,andthatwhenthevariablesareallobserved,theprobabilityofthe jointassignmentofallvariablescanbeevaluatedeÔ¨Éciently. EvenwiththeeÔ¨Écientparametrization ofthegraphicalmodel,someoperations remaincomputationally challenging.Forexample,itisdiÔ¨Éculttopredictmissing 1Th e c o n d i t i o n a l d i s t rib u t i o n o v e r t h e s e v a ria b l e s g i v e n t h e i r p a re n t s i s d e t e rm i n i s t i c . Th i s i s p e rfe c t l y l e g i t i m a t e , t h o u g h i t i s s o m e wh a t ra re t o d e s i g n a g ra p h i c a l m o d e l with s u c h d e t e rm i n i s t i c h i d d e n u n i t s . 3 8 9 CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS valuesinthemiddleofthesequence. Thepricerecurrentnetworkspayfortheirreducednumberofparametersis that theparametersmaybediÔ¨Écult. o p t i m i z i ng Theparametersharingusedinrecurrentnetworksreliesontheassumption thatthesameparameterscanbeusedfordiÔ¨Äerenttimesteps.Equivalently,the assumptionisthattheconditionalprobabilitydistributionoverthevariablesat time t+1 giventhevariablesattime tisstationary,meaningthattherelationship betweentheprevioustimestepandthenexttimestepdoesnotdependon t.In principle,itwouldbepossibletouse tasanextrainputateachtimestepandlet thelearnerdiscoveranytime-dependencewhilesharingasmuchasitcanbetween diÔ¨Äerenttimesteps.ThiswouldalreadybemuchbetterthanusingadiÔ¨Äerent conditionalprobabilitydistributionforeach t,butthenetworkwouldthenhaveto extrapolatewhenfacedwithnewvaluesof. t TocompleteourviewofanRNNasagraphicalmodel,wemustdescribehow todrawsamplesfromthemodel.Themainoperationthatweneedtoperformis simplytosamplefromtheconditionaldistributionateachtimestep. However, thereisoneadditionalcomplication. The RNNmusthavesomemechanismfor determiningthelengthofthesequence.Thiscanbeachievedinvariousways. Inthecasewhentheoutputisasymboltakenfromavocabulary,onecan addaspecialsymbolcorrespondingtotheendofasequence(Schmidhuber2012,). Whenthatsymbolisgenerated,thesamplingprocessstops.Inthetrainingset, weinsertthissymbolasanextramemberofthesequence,immediatelyafterx( ) œÑ ineachtrainingexample. AnotheroptionistointroduceanextraBernoullioutputtothemodelthat representsthedecisiontoeithercontinuegenerationorhaltgenerationateach timestep.Thisapproachismoregeneralthantheapproachofaddinganextra symboltothevocabulary,becauseitmaybeappliedtoanyRNN,ratherthan onlyRNNsthatoutputasequenceofsymbols.Forexample,itmaybeappliedto anRNNthatemitsasequenceofrealnumbers.Thenewoutputunitisusuallya sigmoidunittrainedwiththecross-entropyloss.Inthisapproachthesigmoidis trainedtomaximizethelog-probabilit yofthecorrectpredictionastowhetherthe sequenceendsorcontinuesateachtimestep. Anotherwaytodeterminethesequencelength œÑistoaddanextraoutputto themodelthatpredictstheinteger œÑitself.Themodelcansampleavalueof œÑ andthensample œÑstepsworthofdata.Thisapproachrequiresaddinganextra inputtotherecurrentupdateateachtimestepsothattherecurrentupdateis awareofwhetheritisneartheendofthegeneratedsequence.Thisextrainput caneitherconsistofthevalueof œÑorcanconsistof œÑ t‚àí,thenumberofremaining 3 9 0 CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS timesteps.Withoutthisextrainput,theRNNmightgeneratesequencesthat endabruptly,suchasasentencethatendsbeforeitiscomplete.Thisapproachis basedonthedecomposition P(x( 1 ), . . . ,x( ) œÑ) = ()( P œÑ Px( 1 ), . . . ,x( ) œÑ| œÑ .)(10.34) Thestrategyofpredicting œÑdirectlyisusedforexamplebyGoodfellow e t a l . ().2014d 10.2.4ModelingSequencesConditionedonContextwithRNNs IntheprevioussectionwedescribedhowanRNNcouldcorrespondtoadirected graphicalmodeloverasequenceofrandomvariables y( ) twithnoinputsx.Of course,ourdevelopmentofRNNsasinequationincludedasequenceof 10.8 inputsx( 1 ),x( 2 ), . . . ,x( ) œÑ.Ingeneral,RNNsallowtheextensionofthegraphical modelviewtorepresentnotonlyajointdistributionoverthe yvariablesbut alsoaconditionaldistributionover ygivenx.Asdiscussedinthecontextof feedforwardnetworksinsection,anymodelrepresentingavariable 6.2.1.1 P(y;Œ∏) canbereinterpretedasamodelrepresentingaconditionaldistribution P(yœâ|) withœâ=Œ∏.Wecanextendsuchamodeltorepresentadistribution P(yx|)by usingthesame P(yœâ|)asbefore,butmakingœâafunctionofx.Inthecaseof anRNN,thiscanbeachievedindiÔ¨Äerentways.Wereviewherethemostcommon andobviouschoices. Previously,wehavediscussedRNNsthattakeasequenceofvectorsx( ) tfor t=1 , . . . , œÑasinput. Anotheroptionistotakeonlyasinglevectorxasinput. WhenxisaÔ¨Åxed-sizevector,wecansimplymakeitanextrainputoftheRNN thatgeneratesthe ysequence.Somecommonwaysofprovidinganextrainputto anRNNare: 1. asanextrainputateachtimestep,or 2. astheinitialstateh( 0 ),or 3. both. TheÔ¨ÅrstandmostcommonapproachisillustratedinÔ¨Ågure.Theinteraction10.9 betweentheinputxandeachhiddenunitvectorh( ) tisparametrized byanewly introducedweightmatrixRthatwasabsentfromthemodelofonlythesequence of yvalues. ThesameproductxÓÄæRisaddedasadditionalinputtothehidden unitsateverytimestep.Wecanthinkofthechoiceofxasdeterminingthevalue 3 9 1 CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS ofxÓÄæRthatiseÔ¨Äectivelyanewbiasparameterusedforeachofthehiddenunits. Theweightsremainindependentoftheinput.Wecanthinkofthismodelastaking theparametersŒ∏ofthenon-conditional modelandturningthemintoœâ,where thebiasparameterswithinarenowafunctionoftheinput. œâ o( t ‚àí 1 )o( t ‚àí 1 )o( ) to( ) to( + 1 ) to( + 1 ) tL( t ‚àí 1 )L( t ‚àí 1 )L( ) tL( ) tL( + 1 ) tL( + 1 ) ty( t ‚àí 1 )y( t ‚àí 1 )y( ) ty( ) ty( +1 ) ty( +1 ) t</div>
        </div>
    </div>

    <div class="question-card" id="q67">
        <div class="question-header">
            <span class="question-number">Question 67</span>
            <span class="question-type">multiple-choice</span>
        </div>

        <div class="question-text">In deep learning, regularization techniques are essential for preventing overfitting and improving model generalization. Early stopping is widely utilized due to its simplicity and effectiveness.

Which of the following statements best describes the mechanism by which early stopping regularizes a deep learning model?

1) It increases the size of the training dataset to reduce variance.   
2) It modifies the loss function to add a penalty for large weights.   
3) It restricts model parameters to remain close to their initial values by limiting training iterations.   
4) It applies dropout to randomly remove units during training.   
5) It augments data by generating synthetic examples to increase diversity.   
6) It freezes a subset of layers to maintain pretrained representations.   
7) It reduces the learning rate after a fixed number of epochs.</div>

        <div class="answer-section">
            <div class="answer-label">‚úì Correct Answer:</div>
            <div class="answer-text">The correct answer is 3) It restricts model parameters to remain close to their initial values by limiting training iterations..</div>
        </div>

        <div class="reference-section">
            <div class="reference-label">üìö Reference Text:</div>
            <button class="toggle-button" onclick="toggleReference(67)">
                Show/Hide Reference
            </button>
            <div id="ref67" class="reference-text hidden">CHAPTER7.REGULARIZATIONFORDEEPLEARNING 0 50 100 150 200 250 Time(epochs)000 .005 .010 .015 .020 .Loss(negative log-likelihood)T r a i n i n g s e t l o s s V a l i d a t i o n s e t l o s s Figure7.3:Learningcurvesshowinghowthenegativelog-likelihoodlosschangesover time(indicatedasnumberoftrainingiterationsoverthedataset,or e p o c h s).Inthis example,wetrainamaxoutnetworkonMNIST.Observethatthetrainingobjective decreasesconsistentlyovertime,butthevalidationsetaveragelosseventuallybeginsto increaseagain,forminganasymmetricU-shapedcurve. greatlyimproved(inproportionwiththeincreasednumberofexamplesforthe sharedparameters,comparedtothescenarioofsingle-taskmodels).Ofcoursethis willhappenonlyifsomeassumptionsaboutthestatisticalrelationshipbetween thediÔ¨Äerenttasksarevalid,meaningthatthereissomethingsharedacrosssome ofthetasks. Fromthepointofviewofdeeplearning,theunderlyingpriorbeliefisthe following:amongthefactorsthat explainthevariations observed inthedata associatedwiththediÔ¨Äerenttasks,somearesharedacrosstwoormoretasks. 7.8EarlyStopping WhentraininglargemodelswithsuÔ¨ÉcientrepresentationalcapacitytooverÔ¨Åt thetask,weoftenobservethattrainingerrordecreasessteadilyovertime,but validationseterrorbeginstoriseagain.SeeÔ¨Ågureforanexampleofthis 7.3 behavior.Thisbehavioroccursveryreliably. Thismeanswecanobtainamodelwithbettervalidationseterror(andthus, hopefullybettertestseterror)byreturningtotheparametersettingatthepointin timewiththelowestvalidationseterror.Everytimetheerroronthevalidationset improves,westoreacopyofthemodelparameters.Whenthetrainingalgorithm terminates,wereturntheseparameters,ratherthanthelatestparameters.The 2 4 6 CHAPTER7.REGULARIZATIONFORDEEPLEARNING algorithmterminateswhennoparametershaveimprovedoverthebestrecorded validationerrorforsomepre-speciÔ¨Åednumberofiterations.Thisprocedureis speciÔ¨Åedmoreformallyinalgorithm .7.1 Algorithm 7.1Theearlystopping meta-algorithmfor determiningthe best amountoftimetotrain.Thismeta-algorithm isageneralstrategythatworks wellwithavarietyoftrainingalgorithmsandwaysofquantifyingerroronthe validationset. Letbethenumberofstepsbetweenevaluations. n Let pbethe‚Äúpatience,‚Äùthenumberoftimestoobserveworseningvalidationset errorbeforegivingup. LetŒ∏ obetheinitialparameters. Œ∏Œ∏‚Üê o i‚Üê0 j‚Üê0 v‚Üê‚àû Œ∏‚àó‚ÜêŒ∏ i‚àó‚Üê i whiledo j < p Updatebyrunningthetrainingalgorithmforsteps. Œ∏ n i i n ‚Üê+ vÓÄ∞‚ÜêValidationSetError ()Œ∏ if vÓÄ∞< vthen j‚Üê0 Œ∏‚àó‚ÜêŒ∏ i‚àó‚Üê i v v‚ÜêÓÄ∞ else j j‚Üê+1 endif endwhile BestparametersareŒ∏‚àó,bestnumberoftrainingstepsis i‚àó Thisstrategyisknownasearlystopping.Itisprobablythemostcommonly usedformofregularizationindeeplearning.Itspopularityisduebothtoits eÔ¨Äectivenessanditssimplicity. OnewaytothinkofearlystoppingisasaveryeÔ¨Écienthyperparameter selection algorithm.Inthisview,thenumberoftrainingstepsisjustanotherhyperparameter. WecanseeinÔ¨Ågurethatthishyperparameter hasaU-shapedvalidationset 7.3 2 4 7 CHAPTER7.REGULARIZATIONFORDEEPLEARNING performancecurve.Mosthyperparameters thatcontrolmodelcapacityhavesucha U-shapedvalidationsetperformancecurve,asillustratedinÔ¨Ågure.Inthecaseof 5.3 earlystopping,wearecontrollingtheeÔ¨Äectivecapacityofthemodelbydetermining howmanystepsitcantaketoÔ¨Åtthetrainingset.Mosthyperparametersmustbe chosenusinganexpensiveguessandcheckprocess,wherewesetahyperparameter atthestartoftraining,thenruntrainingforseveralstepstoseeitseÔ¨Äect.The ‚Äútrainingtime‚Äù hyperparam eterisuniqueinthatbydeÔ¨Ånitionasinglerunof trainingtriesoutmanyvaluesofthehyperparameter.TheonlysigniÔ¨Åcantcost tochoosingthishyperparameter automatically viaearlystoppingisrunningthe validationsetevaluationperiodicallyduringtraining.Ideally,thisisdonein paralleltothetrainingprocessonaseparatemachine,separateCPU,orseparate GPUfromthemaintrainingprocess.Ifsuchresourcesarenotavailable,thenthe costoftheseperiodicevaluationsmaybereducedbyusingavalidationsetthatis smallcomparedtothetrainingsetorbyevaluatingthevalidationseterrorless frequentlyandobtainingalowerresolutionestimateoftheoptimaltrainingtime. Anadditionalcosttoearlystoppingistheneedtomaintainacopyofthe bestparameters.Thiscostisgenerallynegligible,becauseitisacceptabletostore theseparametersinaslowerandlargerformofmemory(forexample,trainingin GPUmemory,butstoringtheoptimalparametersinhostmemoryoronadisk drive).Sincethebestparametersarewrittentoinfrequentlyandneverreadduring training,theseoccasionalslowwriteshavelittleeÔ¨Äectonthetotaltrainingtime. Earlystoppingisaveryunobtrusiveformofregularization, inthatitrequires almostnochangeintheunderlyingtrainingprocedure,theobjectivefunction, orthesetofallowableparametervalues.Thismeansthatitiseasytouseearly stoppingwithoutdamagingthelearningdynamics.Thisisincontrasttoweight decay,whereonemustbecarefulnottousetoomuchweightdecayandtrapthe networkinabadlocalminimumcorrespondingtoasolutionwithpathologically smallweights. Earlystoppingmaybeusedeitheraloneorinconjunctionwithotherregulariza- tionstrategies.Evenwhenusingregularizationstrategiesthatmodifytheobjective functiontoencouragebettergeneralization, itisrareforthebestgeneralization to occuratalocalminimumofthetrainingobjective. Earlystoppingrequiresavalidationset,whichmeanssometrainingdataisnot fedtothemodel.Tobestexploitthisextradata,onecanperformextratraining aftertheinitialtrainingwithearlystoppinghascompleted.Inthesecond,extra trainingstep,allofthetrainingdataisincluded.Therearetwobasicstrategies onecanuseforthissecondtrainingprocedure. Onestrategy(algorithm )istoinitializethemodelagainandretrainonall 7.2 2 4 8 CHAPTER7.REGULARIZATIONFORDEEPLEARNING ofthedata.Inthissecondtrainingpass,wetrainforthesamenumberofstepsas theearlystoppingproceduredeterminedwasoptimalintheÔ¨Årstpass.Thereare somesubtletiesassociatedwiththisprocedure.Forexample,thereisnotagood wayofknowingwhethertoretrainforthesamenumberofparameterupdatesor thesamenumberofpassesthroughthedataset.Onthesecondroundoftraining, eachpassthroughthedatasetwillrequiremoreparameterupdatesbecausethe trainingsetisbigger. Algorithm7.2Ameta-algorithm forusingearlystoppingtodeterminehowlong totrain,thenretrainingonallthedata. LetX( ) t r a i nandy( ) t r a i nbethetrainingset. SplitX( ) t r a i nandy( ) t r a i ninto(X( ) s ubtr a i n,X( v a l i d )) (andy( ) s ubtr a i n,y( v a l i d )) respectively. Runearlystopping(algorithm )startingfromrandom 7.1 Œ∏usingX( ) s ubtr a i nand y( ) s ubtr a i nfortrainingdataandX( v a l i d )andy( v a l i d )forvalidationdata.This returns i‚àó,theoptimalnumberofsteps. Settorandomvaluesagain. Œ∏ TrainonX( ) t r a i nandy( ) t r a i nfor i‚àósteps. Anotherstrategyforusingallofthedataistokeeptheparametersobtained fromtheÔ¨Årstroundoftrainingandthencontinuetrainingbutnowusingallof thedata.Atthisstage,wenownolongerhaveaguideforwhentostopinterms ofanumberofsteps. Instead,wecanmonitortheaveragelossfunctiononthe validationset,andcontinuetraininguntilitfallsbelowthevalueofthetraining setobjectiveatwhichtheearlystoppingprocedurehalted.Thisstrategyavoids thehighcostofretrainingthemodelfromscratch,butisnotaswell-behaved.For example,thereisnotanyguaranteethattheobjectiveonthevalidationsetwill everreachthetargetvalue,sothisstrategyisnotevenguaranteedtoterminate. Thisprocedureispresentedmoreformallyinalgorithm .7.3 Earlystoppingisalsousefulbecauseitreducesthecomputational costofthe trainingprocedure.Besidestheobviousreductionincostduetolimitingthenumber oftrainingiterations,italsohasthebeneÔ¨Åtofprovidingregularizationwithout requiringtheadditionofpenaltytermstothecostfunctionorthecomputationof thegradientsofsuchadditionalterms. Howearlystoppingactsasaregularizer:Sofarwehavestatedthatearly stoppingaregularizationstrategy,butwehavesupportedthisclaimonlyby is showinglearningcurveswherethevalidationseterrorhasaU-shapedcurve.What 2 4 9 CHAPTER7.REGULARIZATIONFORDEEPLEARNING Algorithm7.3Meta-algorithm usingearlystoppingtodetermineatwhatobjec- tivevaluewestarttooverÔ¨Åt,thencontinuetraininguntilthatvalueisreached. LetX( ) t r a i nandy( ) t r a i nbethetrainingset. SplitX( ) t r a i nandy( ) t r a i ninto(X( ) s ubtr a i n,X( v a l i d )) (andy( ) s ubtr a i n,y( v a l i d )) respectively. Runearlystopping(algorithm )startingfromrandom 7.1 Œ∏usingX( ) s ubtr a i nand y( ) s ubtr a i nfortrainingdataandX( v a l i d )andy( v a l i d )forvalidationdata.This updates.Œ∏ ÓÄè J , ‚Üê(Œ∏X( ) s ubtr a i n,y( ) s ubtr a i n) while J ,(Œ∏X( v a l i d ),y( v a l i d )) > ÓÄèdo TrainonX( ) t r a i nandy( ) t r a i nforsteps. n endwhile istheactualmechanismbywhichearlystoppingregularizesthemodel?Bishop ()and ()arguedthatearlystoppinghastheeÔ¨Äectof 1995aSj√∂bergandLjung1995 restrictingtheoptimization proceduretoarelativelysmallvolumeofparameter spaceintheneighborhoodoftheinitialparametervalueŒ∏ o,asillustratedin Ô¨Ågure.MorespeciÔ¨Åcally,imaginetaking 7.4 œÑoptimization steps(corresponding to œÑtrainingiterations)andwithlearningrate ÓÄè.Wecanviewtheproduct ÓÄè œÑ asameasureofeÔ¨Äectivecapacity.Assumingthegradientisbounded,restricting boththenumberofiterationsandthelearningratelimitsthevolumeofparameter spacereachablefromŒ∏ o.Inthissense, ÓÄè œÑbehavesasifitwerethereciprocalof thecoeÔ¨Écientusedforweightdecay. Indeed,wecanshowhow‚Äîinthecaseofasimplelinearmodelwithaquadratic errorfunctionandsimplegradientdescent‚Äîearlystoppingisequivalentto L2</div>
        </div>
    </div>

    <div class="question-card" id="q68">
        <div class="question-header">
            <span class="question-number">Question 68</span>
            <span class="question-type">multiple-choice</span>
        </div>

        <div class="question-text">Training undirected probabilistic models, such as Restricted Boltzmann Machines (RBMs), involves complex optimization procedures driven by the properties of the partition function and gradient calculations. Sampling and expectation estimation techniques play a crucial role in practical learning algorithms for these models.

Which method efficiently approximates the negative phase in energy-based models by initializing Markov chains from data samples, thereby reducing the computational cost associated with burn-in?

1) Stochastic Gradient Descent   
2) Expectation-Maximization   
3) Variational Inference   
4) Hamiltonian Monte Carlo   
5) Contrastive Divergence   
6) Simulated Annealing   
7) Persistent Markov Chain Sampling</div>

        <div class="answer-section">
            <div class="answer-label">‚úì Correct Answer:</div>
            <div class="answer-text">The correct answer is 5) Contrastive Divergence.</div>
        </div>

        <div class="reference-section">
            <div class="reference-label">üìö Reference Text:</div>
            <button class="toggle-button" onclick="toggleReference(68)">
                Show/Hide Reference
            </button>
            <div id="ref68" class="reference-text hidden">CHAPTER18.CONFRONTINGTHEPARTITIONFUNCTION 18.1TheLog-LikelihoodGradient What makes learning undirectedmodels bymaximumlikelihood particularly diÔ¨Écultisthatthepartitionfunctiondependsontheparameters.Thegradientof thelog-likelihoodwithrespecttotheparametershasatermcorrespondingtothe gradientofthepartitionfunction: ‚àá Œ∏log(;) = px Œ∏ ‚àá Œ∏log Àú p(;)x Œ∏‚àí‚àá Œ∏log() Z Œ∏ .(18.4) Thisisawell-knowndecompositionintothe p o si t i v e phaseand negat i v e phaseoflearning. Formostundirectedmodelsofinterest,thenegativephaseisdiÔ¨Écult.Models withnolatentvariablesorwithfewinteractionsbetweenlatentvariablestypically haveatractablepositivephase.Thequintessentialexampleofamodelwitha straightforwardpositivephaseanddiÔ¨ÉcultnegativephaseistheRBM,whichhas hiddenunitsthatareconditionallyindependentfromeachothergiventhevisible units.ThecasewherethepositivephaseisdiÔ¨Écult,withcomplicatedinteractions betweenlatentvariables,isprimarilycoveredinchapter.Thischapterfocuses 19 onthediÔ¨Écultiesofthenegativephase. Letuslookmorecloselyatthegradientof: log Z ‚àá Œ∏log Z (18.5) =‚àá Œ∏ Z Z(18.6) =‚àá Œ∏ÓÅê xÀú p()x Z(18.7) =ÓÅê x‚àá Œ∏Àú p()x Z. (18.8) Formodelsthatguarantee p(x) >0forallx,wecansubstitute exp(log Àú p())x forÀú p()x:ÓÅê x‚àá Œ∏exp(log Àú p())x Z(18.9) =ÓÅê xexp(log Àú p())x‚àá Œ∏log Àú p()x Z(18.10) =ÓÅê xÀú p()x‚àá Œ∏log Àú p()x Z(18.11) =ÓÅò xp()x‚àá Œ∏log Àú p()x (18.12) 606 CHAPTER18.CONFRONTINGTHEPARTITIONFUNCTION = E x x ‚àº p ( )‚àá Œ∏log Àú p .()x (18.13) Thisderivationmadeuseofsummationoverdiscrete x,butasimilarresult appliesusingintegrationovercontinuous x.Inthecontinuousversionofthe derivation,weuseLeibniz‚ÄôsrulefordiÔ¨Äerentiationundertheintegralsigntoobtain theidentity ‚àá Œ∏ÓÅö Àú p d()x x=ÓÅö ‚àá Œ∏Àú p d . ()x x (18.14) ThisidentityisapplicableonlyundercertainregularityconditionsonÀú pand‚àá Œ∏Àú p(x). Inmeasuretheoreticterms,theconditionsare:(i)Theunnormalized distributionÀú p mustbeaLebesgue-integrablefunctionof xforeveryvalueof Œ∏;(ii)Thegradient ‚àá Œ∏Àú p(x)mustexistforall Œ∏andalmostall x;(iii)Theremustexistanintegrable function R( x)thatbounds ‚àá Œ∏Àú p(x)inthesensethatmax i|‚àÇ ‚àÇ Œ∏ iÀú p(x)|‚â§ R( x)forall Œ∏andalmostall x.Fortunately,mostmachinelearningmodelsofinteresthave theseproperties. Thisidentity ‚àá Œ∏log = Z E x x ‚àº p ( )‚àá Œ∏log Àú p()x (18.15) isthebasisforavarietyofMonteCarlomethodsforapproximatelymaximizing thelikelihoodofmodelswithintractablepartitionfunctions. TheMonteCarloapproachtolearningundirectedmodelsprovidesanintuitive frameworkinwhichwecanthinkofboththepositivephaseandthenegative phase.Inthepositivephase,weincreaselog Àú p(x)for xdrawnfromthedata.In thenegativephase,wedecreasethepartitionfunctionbydecreasinglog Àú p(x) drawn fromthemodeldistribution. Inthedeeplearningliterature,itiscommontoparametrize log Àú pintermsof anenergyfunction(equation).Inthiscase,wecaninterpretthepositive 16.7 phaseaspushingdownontheenergyoftrainingexamplesandthenegativephase aspushingupontheenergyofsamplesdrawnfromthemodel,asillustratedin Ô¨Ågure.18.1 18.2StochasticMaximumLikelihoodandContrastive Divergence Thenaivewayofimplementing equation istocomputeitbyburningin 18.15 asetofMarkovchainsfromarandominitialization everytimethegradientis needed.Whenlearningisperformedusingstochasticgradientdescent,thismeans thechainsmustbeburnedinoncepergradientstep.Thisapproachleadstothe 607 CHAPTER18.CONFRONTINGTHEPARTITIONFUNCTION trainingprocedurepresentedinalgorithm .Thehighcostofburninginthe 18.1 Markovchainsintheinnerloopmakesthisprocedurecomputationally infeasible, butthisprocedureisthestartingpointthatothermorepracticalalgorithmsaim toapproximate. Al g o r i t hm 1 8 . 1AnaiveMCMCalgorithmformaximizingthelog-likelihood withanintractablepartitionfunctionusinggradientascent. Set,thestepsize,toasmallpositivenumber. ÓÄè Set k,thenumberofGibbssteps,highenoughtoallowburnin.Perhaps100to trainanRBMonasmallimagepatch. whi l enotconverged do Sampleaminibatchofexamples m {x( 1 ), . . . ,x( ) m}fromthetrainingset. g‚Üê1 mÓÅêm i = 1‚àá Œ∏log Àú p(x( ) i;) Œ∏. Initializeasetof msamples {Àúx( 1 ), . . . ,Àúx( ) m}torandomvalues(e.g.,from auniformornormaldistribution,orpossiblyadistributionwithmarginals matchedtothemodel‚Äôsmarginals). f o r do i k = 1to f o r do j m = 1to Àúx( ) j‚Üêgibbs_update(Àúx( ) j) . e nd f o r e nd f o r gg‚Üê‚àí1 mÓÅêm i = 1‚àá Œ∏log Àú p(Àúx( ) i;) Œ∏ . Œ∏ Œ∏‚Üê + ÓÄè .g e nd whi l e WecanviewtheMCMCapproachtomaximumlikelihoodastryingtoachieve balancebetweentwoforces,onepushinguponthemodeldistributionwherethe dataoccurs,andanotherpushingdownonthemodeldistributionwherethemodel samplesoccur.Figureillustratesthisprocess.Thetwoforcescorrespondto 18.1 maximizing log Àú pandminimizing log Z.Severalapproximations tothenegative phasearepossible.Eachoftheseapproximationscanbeunderstoodasmaking thenegativephasecomputationally cheaperbutalsomakingitpushdowninthe wronglocations. Becausethenegativephaseinvolvesdrawingsamplesfromthemodel‚Äôsdistri- bution,wecanthinkofitasÔ¨Åndingpointsthatthemodelbelievesinstrongly. Becausethenegativephaseactstoreducetheprobabilityofthosepoints,they aregenerallyconsideredtorepresentthemodel‚Äôsincorrectbeliefsabouttheworld. Theyarefrequentlyreferredtointheliteratureas‚Äúhallucinations‚Äù or‚Äúfantasy particles.‚ÄùInfact,thenegativephasehasbeenproposedasapossibleexplanation 608 CHAPTER18.CONFRONTINGTHEPARTITIONFUNCTION xp(x )The p o s i t i v e p h a s e p mo d e l ( ) x p d a t a ( ) x xp(x )The n eg a t i v e p h a s e p mo d e l ( ) x p d a t a ( ) x Figure18.1:Theviewofalgorithmashavinga‚Äúpositivephase‚Äùand‚Äúnegativephase.‚Äù 18.1 ( L e f t )Inthepositivephase,wesamplepointsfromthedatadistribution,andpushupon theirunnormalizedprobability.Thismeanspointsthatarelikelyinthedatagetpushed uponmore. ( R i g h t )Inthenegativephase,wesamplepointsfromthemodeldistribution, andpushdownontheirunnormalizedprobability.Thiscounteractsthepositivephase‚Äôs tendencytojustaddalargeconstanttotheunnormalizedprobabilityeverywhere.When thedatadistributionandthemodeldistributionareequal,thepositivephasehasthe samechancetopushupatapointasthenegativephasehastopushdown.Whenthis occurs,thereisnolongeranygradient(inexpectation)andtrainingmustterminate. fordreaminginhumansandotheranimals(CrickandMitchison1983,),theidea beingthatthebrainmaintainsaprobabilisticmodeloftheworldandfollows thegradientoflog Àú pwhileexperiencingrealeventswhileawakeandfollowsthe negativegradientoflog Àú ptominimize log Zwhilesleepingandexperiencingevents sampledfromthecurrentmodel.Thisviewexplainsmuchofthelanguageusedto describealgorithmswithapositiveandnegativephase,butithasnotbeenproven tobecorrectwithneuroscientiÔ¨Åcexperiments.Inmachinelearningmodels,itis usuallynecessarytousethepositiveandnegativephasesimultaneously,rather thaninseparatetimeperiodsofwakefulnessandREMsleep. Aswewillseein section,othermachinelearningalgorithmsdrawsamplesfromthemodel 19.5 distributionforotherpurposesandsuchalgorithmscouldalsoprovideanaccount forthefunctionofdreamsleep. Giventhisunderstandingoftheroleofthepositiveandnegativephaseof learning,wecanattempttodesignalessexpensivealternativetoalgorithm .18.1 ThemaincostofthenaiveMCMCalgorithmisthecostofburningintheMarkov chainsfromarandominitialization ateachstep. Anaturalsolutionistoinitialize theMarkovchainsfromadistributionthatisveryclosetothemodeldistribution, 609 CHAPTER18.CONFRONTINGTHEPARTITIONFUNCTION sothattheburninoperationdoesnottakeasmanysteps. The c o n t r ast i v e di v e r g e nc e(CD,orCD- ktoindicateCDwith kGibbssteps) algorithminitializestheMarkovchainateachstepwithsamplesfromthedata distribution(Hinton20002010,,).Thisapproachispresentedasalgorithm .18.2 Obtainingsamplesfromthedatadistributionisfree,becausetheyarealready</div>
        </div>
    </div>

    <div class="question-card" id="q69">
        <div class="question-header">
            <span class="question-number">Question 69</span>
            <span class="question-type">multiple-choice</span>
        </div>

        <div class="question-text">Probability theory and statistics provide tools to describe randomness and relationships between variables, including measures like expectation, variance, covariance, and distributions such as Bernoulli, Categorical (Multinoulli), and Gaussian. Understanding the distinction between statistical and causal concepts is crucial for data analysis and modeling.

Which statement correctly describes the difference between statistical conditioning and intervention in causal inference?

1) Statistical conditioning always implies a change in the underlying data-generating process.   
2) Intervention queries can be answered solely using conditional probabilities.   
3) Conditioning on an event with zero probability yields meaningful results.   
4) Statistical conditioning and intervention both refer to restricting analysis to observed cases.   
5) Intervention changes the probability distribution by forcing a variable to take a specific value, unlike conditioning which only updates beliefs based on observed information.   
6) Intervention involves actively manipulating a variable to observe effects, while conditioning updates probabilities based on passive observation.   
7) Statistical conditioning is only meaningful for continuous variables, not discrete ones.</div>

        <div class="answer-section">
            <div class="answer-label">‚úì Correct Answer:</div>
            <div class="answer-text">The correct answer is 6) Intervention involves actively manipulating a variable to observe effects, while conditioning updates probabilities based on passive observation..</div>
        </div>

        <div class="reference-section">
            <div class="reference-label">üìö Reference Text:</div>
            <button class="toggle-button" onclick="toggleReference(69)">
                Show/Hide Reference
            </button>
            <div id="ref69" class="reference-text hidden">x ). (3.5) TheconditionalprobabilityisonlydeÔ¨ÅnedwhenP(x=x)>0.Wecannotcompute theconditionalprobabilityconditionedonaneventthatneverhappens. Itisimportantnottoconfuseconditionalprobabilitywithcomputingwhat wouldhappenifsomeactionwereundertaken.Theconditionalprobabilitythat apersonisfromGermanygiventhattheyspeakGermanisquitehigh,butif arandomlyselectedpersonistaughttospeakGerman,theircountryoforigin doesnotchange.Computingtheconsequencesofanactioniscalledmakingan i n t e r v e n t i o n q uer y.Interventionqueriesarethedomainof c ausal m o del i ng, whichwedonotexploreinthisbook. 3.6TheChainRuleofConditionalProbabilities Anyjointprobabilitydistributionovermanyrandomvariablesmaybedecomposed intoconditionaldistributionsoveronlyonevariable: P(x( 1 ),...,x( ) n) = (Px( 1 ))Œ†n i = 2P(x( ) i|x( 1 ),...,x( 1 ) i ‚àí).(3.6) Thisobservationisknownasthe c hai n r ul eor pr o duc t r ul eofprobability. ItfollowsimmediatelyfromthedeÔ¨Ånitionofconditionalprobabilityinequation.3.5 59 CHAPTER3.PROBABILITYANDINFORMATIONTHEORY Forexample,applyingthedeÔ¨Ånitiontwice,weget P,,P,P, (abc)= (ab|c)(bc) P,PP (bc)= ( )bc| ()c P,,P,PP. (abc)= (ab|c)( )bc| ()c 3.7IndependenceandConditionalIndependence Tworandomvariablesxandyare i ndep e nden tiftheirprobabilitydistribution canbeexpressedasaproductoftwofactors,oneinvolvingonlyxandoneinvolving onlyy: ‚àÄ‚àà ‚ààxx,yyxyxy (3.7) ,p(= x,= ) = (yp= )(xp= )y. Tworandomvariablesxandyare c o ndi t i o n a l l y i ndep e nden tgivenarandom variableziftheconditionalprobabilitydistributionoverxandyfactorizesinthis wayforeveryvalueofz: ‚àÄ‚àà ‚àà ‚àà | | | xx,yy,zzxy,p(= x,= yzx = ) = (zp= xzy = )(zp= yz= )z. (3.8) We candenoteindependence andconditionalindependence with compact notation:xy‚ä•meansthatxandyareindependent,whilexyz ‚ä•|meansthatx andyareconditionallyindependentgivenz. 3.8Expectation,VarianceandCovariance The e x p e c t at i o nor e x p e c t e d v al ueofsomefunctionf(x)withrespecttoa probabilitydistributionP(x)istheaverageormeanvaluethatftakesonwhenx isdrawnfrom.Fordiscretevariablesthiscanbecomputedwithasummation: P E x ‚àº P[()] =fxÓÅò xPxfx, ()() (3.9) whileforcontinuousvariables,itiscomputedwithanintegral: E x ‚àº p[()] =fxÓÅö pxfxdx. ()() (3.10) 60 CHAPTER3.PROBABILITYANDINFORMATIONTHEORY Whentheidentityofthedistributionisclearfromthecontext,wemaysimply writethenameoftherandomvariablethattheexpectationisover,asin E x[f(x)]. Ifitisclearwhichrandomvariabletheexpectationisover,wemayomitthe subscriptentirely,asin E[f(x)].Bydefault,wecanassumethat E[¬∑]averagesover thevaluesofalltherandomvariablesinsidethebrackets.Likewise,whenthereis noambiguity,wemayomitthesquarebrackets. Expectationsarelinear,forexample, E x[()+ ()] = Œ±fxŒ≤gxŒ± E x[()]+fxŒ≤ E x[()]gx, (3.11) whenandarenotdependenton. Œ±Œ≤ x The v ar i anc egivesameasureofhowmuchthevaluesofafunctionofarandom variablexvaryaswesamplediÔ¨Äerentvaluesofxfromitsprobabilitydistribution: Var(()) = fx EÓÅ® (() [()]) fx‚àí Efx2ÓÅ© . (3.12) Whenthevarianceislow,thevaluesoff(x)clusterneartheirexpectedvalue.The squarerootofthevarianceisknownasthe . st andar d dev i at i o n The c o v ar i anc egivessomesenseofhowmuchtwovaluesarelinearlyrelated toeachother,aswellasthescaleofthesevariables: Cov(()()) = [(() [()])(() [()])] fx,gy Efx‚àí Efxgy‚àí Egy.(3.13) Highabsolutevaluesofthecovariancemeanthatthevalueschangeverymuch andarebothfarfromtheirrespectivemeansatthesametime.Ifthesignofthe covarianceispositive,thenbothvariablestendtotakeonrelativelyhighvalues simultaneously.Ifthesignofthecovarianceisnegative,thenonevariabletendsto takeonarelativelyhighvalueatthetimesthattheothertakesonarelatively lowvalueandviceversa.Othermeasuressuchas c o r r e l at i o nnormalizethe contributionofeachvariableinordertomeasureonlyhowmuchthevariablesare related,ratherthanalsobeingaÔ¨Äectedbythescaleoftheseparatevariables. Thenotionsofcovarianceanddependencearerelated,butareinfactdistinct concepts.Theyarerelatedbecausetwovariablesthatareindependenthavezero covariance,andtwovariablesthathavenon-zerocovariancearedependent.How- ever,independence isadistinctpropertyfromcovariance.Fortwovariablestohave zerocovariance,theremustbenolineardependencebetweenthem.Independence isastrongerrequirementthanzerocovariance,becauseindependencealsoexcludes nonlinearrelationships.Itispossiblefortwovariablestobedependentbuthave zerocovariance.Forexample,supposeweÔ¨Årstsamplearealnumberxfroma uniformdistributionovertheinterval[‚àí1,1].Wenextsamplearandomvariable 61 CHAPTER3.PROBABILITYANDINFORMATIONTHEORY s.Withprobability1 2,wechoosethevalueofstobe.Otherwise,wechoose 1 thevalueofstobe‚àí1.Wecanthengeneratearandomvariableybyassigning y=sx.Clearly,xandyarenotindependent,becausexcompletelydetermines themagnitudeof.However,y Cov() = 0x,y. The c o v ar i anc e m at r i xofarandomvector x‚àà Rnisannn√ómatrix,such that Cov() x i , j= Cov(x i,x j). (3.14) Thediagonalelementsofthecovariancegivethevariance: Cov(x i,x i) = Var(x i). (3.15) 3.9CommonProbabilityDistributions Severalsimpleprobabilitydistributionsareusefulinmanycontextsinmachine learning. 3.9.1BernoulliDistribution The B e r noul l idistributionisadistributionoverasinglebinaryrandomvariable. ItiscontrolledbyasingleparameterœÜ‚àà[0,1],whichgivestheprobabilityofthe randomvariablebeingequalto1.Ithasthefollowingproperties: P œÜ (= 1) = x (3.16) P œÜ (= 0) = 1x ‚àí (3.17) PxœÜ (= x ) = x(1 )‚àíœÜ1 ‚àí x(3.18) E x[] = xœÜ (3.19) Var x() = (1 )xœÜ‚àíœÜ (3.20) 3.9.2MultinoulliDistribution The m ul t i noull ior c at e g o r i c a ldistributionisadistributionoverasinglediscrete variablewithkdiÔ¨Äerentstates,wherekisÔ¨Ånite.1Themultinoullidistributionis 1‚ÄúMultinoulli‚ÄùisatermthatwasrecentlycoinedbyGustavoLacerdoandpopularizedby Murphy2012().Themultinoullidistributionisaspecialcaseofthe m u lt in om ia ldistribution. Amultinomialdistributionisthedistributionovervectorsin{0,...,n}krepresentinghowmany timeseachofthekcategoriesisvisitedwhennsamplesaredrawnfromamultinoullidistribution. Manytextsusetheterm‚Äúmultinomial‚Äùtorefertomultinoullidistributionswithoutclarifying thattheyreferonlytothecase. n= 1 62 CHAPTER3.PROBABILITYANDINFORMATIONTHEORY parametrized byavector p‚àà[0,1]k ‚àí 1,wherep igivestheprobabilityofthei-th state.TheÔ¨Ånal,k-thstate‚Äôsprobabilityisgivenby1‚àí 1ÓÄæp.Notethatwemust constrain 1ÓÄæp‚â§1.Multinoullidistributionsareoftenusedtorefertodistributions overcategoriesofobjects,sowedonotusuallyassumethatstate1hasnumerical value1,etc.Forthisreason,wedonotusuallyneedtocomputetheexpectation orvarianceofmultinoulli-dis tributedrandomvariables. TheBernoulliandmultinoullidistributionsaresuÔ¨Écienttodescribeanydistri- butionovertheirdomain. They areabletodescribeanydistributionovertheir domainnotsomuchbecausetheyareparticularlypowerfulbutratherbecause theirdomainissimple;theymodeldiscretevariablesforwhichitisfeasibleto enumerateallofthestates.Whendealingwithcontinuousvariables,thereare uncountablymanystates,soanydistributiondescribedbyasmallnumberof parametersmustimposestrictlimitsonthedistribution. 3.9.3GaussianDistribution Themostcommonlyuseddistributionoverrealnumbersisthe nor m al di st r i bu- t i o n,alsoknownasthe : G aussian di st r i but i o n N(;x¬µ,œÉ2) =ÓÅ≤ 1 2œÄœÉ2expÓÄí ‚àí1 2œÉ2( )x¬µ‚àí2ÓÄì .(3.21) SeeÔ¨Ågureforaplotofthedensityfunction. 3.1 Thetwoparameters ¬µ‚àà RandœÉ‚àà(0,‚àû)controlthenormaldistribution. Theparameter¬µgivesthecoordinateofthecentralpeak.Thisisalsothemeanof thedistribution: E[x] =¬µ.Thestandarddeviationofthedistributionisgivenby œÉ,andthevariancebyœÉ2. WhenweevaluatethePDF,weneedtosquareandinvertœÉ.Whenweneedto frequentlyevaluatethePDFwithdiÔ¨Äerentparametervalues,amoreeÔ¨Écientway ofparametrizing thedistributionistouseaparameterŒ≤‚àà(0,‚àû)tocontrolthe pr e c i si o</div>
        </div>
    </div>

    <div class="question-card" id="q70">
        <div class="question-header">
            <span class="question-number">Question 70</span>
            <span class="question-type">multiple-choice</span>
        </div>

        <div class="question-text">In probabilistic machine learning, approximate inference methods are essential for dealing with intractable posteriors, especially in models involving complex latent variable structures. Techniques such as MAP inference, variational inference, and sparse coding each employ different assumptions and optimization strategies to achieve tractable learning and inference.

Which approach to approximate inference restricts the family of distributions to a Dirac delta, yielding point estimates of latent variables and reducing inference to deterministic optimization?

1) Maximum A Posteriori (MAP) inference   
2) Structured variational inference   
3) Mean field variational inference   
4) Expectation-Maximization (EM) with full posterior inference   
5) Gibbs sampling   
6) Importance sampling   
7) Markov Chain Monte Carlo (MCMC) with Gaussian approximations</div>

        <div class="answer-section">
            <div class="answer-label">‚úì Correct Answer:</div>
            <div class="answer-text">The correct answer is 1) Maximum A Posteriori (MAP) inference.</div>
        </div>

        <div class="reference-section">
            <div class="reference-label">üìö Reference Text:</div>
            <button class="toggle-button" onclick="toggleReference(70)">
                Show/Hide Reference
            </button>
            <div id="ref70" class="reference-text hidden">approximateinferencebyrestrictingthefamilyofdistributions qmaybedrawn from.SpeciÔ¨Åcally,werequiretotakeonaDiracdistribution: q q Œ¥ . ( ) = h v| ( ) h ¬µ‚àí (19.11) Thismeansthatwecannowcontrol qentirelyvia ¬µ.DroppingtermsofLthat donotvarywith,weareleftwiththeoptimization problem ¬µ ¬µ‚àó= argmax ¬µlog(= ) p h ¬µ v , , (19.12) whichisequivalenttotheMAPinferenceproblem h‚àó= argmax hp . ( ) h v| (19.13) WecanthusjustifyalearningproceduresimilartoEM,inwhichwealternate betweenperformingMAPinferencetoinfer h‚àóandthenupdate Œ∏toincrease log p( h‚àó, v).AswithEM,thisisaformofcoordinateascentonL,wherewe alternatebetweenusing inference to optimize Lwithrespect to qandusing parameterupdatestooptimize Lwithrespectto Œ∏.Theprocedureasawholecan bejustiÔ¨ÅedbythefactthatLisalowerboundonlog p( v).InthecaseofMAP inference,thisjustiÔ¨Åcationisrathervacuous,becausetheboundisinÔ¨Ånitelyloose, duetotheDiracdistribution‚ÄôsdiÔ¨ÄerentialentropyofnegativeinÔ¨Ånity.However, addingnoisetowouldmaketheboundmeaningfulagain. ¬µ 6 3 6 CHAPTER19.APPROXIMATEINFERENCE MAPinferenceiscommonlyusedindeeplearningasbothafeatureextractor andalearningmechanism.Itisprimarilyusedforsparsecodingmodels. Recallfromsectionthatsparsecodingisalinearfactormodelthatimposes 13.4 asparsity-inducingprioronitshiddenunits.AcommonchoiceisafactorialLaplace prior,with p h( i) =Œª 2e‚àí| Œª h i|. (19.14) Thevisibleunitsarethengeneratedbyperformingalineartransformationand addingnoise: p , Œ≤ ( ) = (; + x h| N v W h b‚àí 1I) . (19.15) Computingorevenrepresenting p( h v|)isdiÔ¨Écult.Everypairofvariables h i and h jarebothparentsof v.Thismeansthatwhen visobserved,thegraphical modelcontainsanactivepathconnecting h iand h j.Allofthehiddenunitsthus participateinonemassivecliquein p( h v|).IfthemodelwereGaussianthen theseinteractionscouldbemodeledeÔ¨Écientlyviathecovariancematrix,butthe sparsepriormakestheseinteractionsnon-Gaussian. Because p( h v|)isintractable,soisthecomputationofthelog-likelihoodand itsgradient.Wethuscannotuseexactmaximumlikelihoodlearning.Instead,we useMAPinferenceandlearntheparametersbymaximizingtheELBOdeÔ¨Ånedby theDiracdistributionaroundtheMAPestimateof. h Ifweconcatenateallofthe hvectorsinthetrainingsetintoamatrix H,and concatenateallofthevectorsintoamatrix,thenthesparsecodinglearning v V processconsistsofminimizing J ,( H W) =ÓÅò i , j| H i , j|+ÓÅò i , jÓÄê V H W‚àíÓÄæÓÄë2 i , j.(19.16) Mostapplicationsofsparsecodingalsoinvolveweightdecayoraconstrainton thenormsofthecolumnsof W,inordertopreventthepathologicalsolutionwith extremelysmallandlarge. H W Wecanminimize Jbyalternatingbetweenminimization withrespectto H andminimization withrespectto W.Bothsub-problemsareconvex.Infact, theminimization withrespectto Wisjustalinearregressionproblem.However, minimization of Jwithrespecttobothargumentsisusuallynotaconvexproblem. Minimization withrespectto Hrequiresspecializedalgorithmssuchasthe feature-signsearchalgorithm(,). Lee e t a l .2007 6 3 7 CHAPTER19.APPROXIMATEINFERENCE 19.4VariationalInferenceandLearning We have seen how the evidence lo wer bound L( v Œ∏ , , q)is a lower bound on log p( v; Œ∏),howinferencecanbeviewedasmaximizing Lwithrespectto q,and howlearningcanbeviewedasmaximizing Lwithrespectto Œ∏.Wehaveseen thattheEMalgorithmallowsustomakelargelearningstepswithaÔ¨Åxed qand thatlearningalgorithmsbasedonMAPinferenceallowustolearnusingapoint estimateof p( h v|)ratherthaninferringtheentiredistribution.Nowwedevelop themoregeneralapproachtovariationallearning. Thecoreideabehindvariationallearningisthatwecanmaximize Lovera restrictedfamilyofdistributions q.Thisfamilyshouldbechosensothatitiseasy tocompute E qlog p( h v ,). Atypicalwaytodothisistointroduceassumptions abouthowfactorizes. q Acommonapproachtovariationallearningistoimposetherestrictionthat q isafactorialdistribution: q( ) = h v|ÓÅô iq h( i| v) . (19.17) ThisiscalledthemeanÔ¨Åeldapproach.Moregenerally,wecanimposeanygraphi- calmodelstructurewechooseon q,toÔ¨Çexiblydeterminehowmanyinteractionswe wantourapproximationtocapture.Thisfullygeneralgraphicalmodelapproach iscalledstructuredvariationalinference( ,). SaulandJordan1996 Thebeautyofthevariationalapproachisthatwedonotneedtospecifya speciÔ¨Åcparametricformfor q.Wespecifyhowitshouldfactorize,butthenthe optimization problemdeterminestheoptimalprobabilitydistributionwithinthose factorizationconstraints.Fordiscretelatentvariables,thisjustmeansthatwe usetraditionaloptimization techniquestooptimizeaÔ¨Ånitenumberofvariables describingthe qdistribution.Forcontinuouslatentvariables,thismeansthatwe useabranchofmathematics calledcalculusofvariationstoperformoptimization overaspaceoffunctions,andactuallydeterminewhichfunctionshouldbeused torepresent q.Calculusof variations istheorigin ofthenames ‚Äúvariational learning‚Äùand‚Äúvariationalinference,‚Äùthoughthesenamesapplyevenwhenthe latentvariablesarediscreteandcalculusofvariationsisnotneeded.Inthecase ofcontinuouslatentvariables,calculusofvariationsisapowerfultechniquethat removesmuchoftheresponsibilityfromthehumandesignerofthemodel,who nowmustspecifyonlyhow qfactorizes,ratherthanneedingtoguesshowtodesign aspeciÔ¨Åcthatcanaccuratelyapproximate theposterior. q BecauseL( v Œ∏ , , q)isdeÔ¨Ånedtobelog p( v; Œ∏)‚àí D K L( q( h v|)ÓÅ´ p( h v|; Œ∏)),we canthinkofmaximizing Lwithrespectto qasminimizing D K L( q( h v|)ÓÅ´ p( h v|)). 6 3 8 CHAPTER19.APPROXIMATEINFERENCE Inthissense,weareÔ¨Åtting qto p. However,wearedoingsowiththeopposite directionoftheKLdivergencethanweareusedtousingforÔ¨Åttinganapproximation. WhenweusemaximumlikelihoodlearningtoÔ¨Åtamodeltodata,weminimize D K L( p da t aÓÅ´ p m o de l).AsillustratedinÔ¨Ågure,thismeansthatmaximumlikelihood 3.6 encouragesthemodeltohavehighprobabilityeverywherethatthedatahashigh probability, whileouroptimization-based inferenceprocedureencourages qto havelowprobabilityeverywherethetrueposteriorhaslowprobability.Both directionsoftheKLdivergencecanhavedesirableandundesirableproperties.The choiceofwhichtousedependsonwhichpropertiesarethehighestpriorityfor eachapplication. Inthecaseoftheinferenceoptimization problem,wechoose touse D K L( q( h v|)ÓÅ´ p( h v|))forcomputational reasons.SpeciÔ¨Åcally,computing D K L( q( h v|)ÓÅ´ p( h v|))involvesevaluatingexpectationswithrespectto q,soby designing qtobesimple,wecansimplifytherequiredexpectations.Theopposite directionoftheKLdivergencewouldrequirecomputingexpectationswithrespect tothetrueposterior.Becausetheformofthetrueposteriorisdeterminedby thechoiceofmodel,wecannotdesignareduced-costapproachtocomputing D K L(( )( )) p h v|ÓÅ´ q h v|exactly. 19.4.1DiscreteLatentVariables Variationalinferencewithdiscretelatentvariablesisrelativelystraightforward. WedeÔ¨Åneadistribution q,typicallyonewhereeachfactorof qisjustdeÔ¨Åned byalookuptableoverdiscretestates. Inthesimplestcase, hisbinaryandwe makethemeanÔ¨Åeldassumptionthatfactorizesovereachindividual q h i.Inthis casewecanparametrize qwithavectorÀÜ hwhoseentriesareprobabilities. Then q h( i= 1 ) =| v ÀÜh i. Afterdetermininghowtorepresent q,wesimplyoptimizeitsparameters.In thecaseofdiscretelatentvariables,thisisjustastandardoptimization problem. Inprincipletheselectionof qcouldbedonewithanyoptimization algorithm,such asgradientdescent. Becausethisoptimization mustoccurintheinnerloopofalearningalgorithm, itmustbeveryfast.Toachievethisspeed,wetypicallyusespecialoptimization algorithmsthataredesignedtosolvecomparativelysmallandsimpleproblemsin veryfewiterations.ApopularchoiceistoiterateÔ¨Åxedpointequations,inother words,tosolve ‚àÇ ‚àÇÀÜh iL= 0 (19.18) forÀÜh i.WerepeatedlyupdatediÔ¨ÄerentelementsofÀÜhuntilwesatisfyaconvergence 6 3 9 CHAPTER19.APPROXIMATEINFERENCE criterion. Tomakethismoreconcrete,weshowhowtoapplyvariationalinferencetothe binarysparsecodingmodel(wepresentherethemodeldevelopedbyHenniges e t a l .()butdemonstratetraditional,genericmeanÔ¨Åeldappliedtothemodel, 2010 whiletheyintroduceaspecializedalgorithm).Thisderivationgoesintoconsiderable mathematical detailandisintendedforthereaderwhowishestofullyresolve anyambiguityinthehigh-levelconceptualdescriptionofvariationalinferenceand learningwehavepresentedsofar.Readerswhodonotplantoderiveorimplement variationallearningalgorithmsmaysafelyskiptothenextsectionwithoutmissing anynewhigh-levelconcepts.Readerswhoproceedwiththebinarysparsecoding exampleareencouragedtoreviewthelistofusefulpropertiesoffunctionsthat commonlyariseinprobabilisticmodelsinsection.Weusetheseproperties 3.10 liberallythroughoutthefollowingderivationswithouthighlightingexactlywhere weuseeachone. Inthebinarysparsecodingmodel,theinput v‚àà Rnisgeneratedfromthe modelbyaddingGaussiannoisetothesumof mdiÔ¨Äerentcomponentswhich caneachbepresentorabsent.EachcomponentisswitchedonoroÔ¨Äbythe correspondinghiddenunitin h‚àà{}01 ,m: p h( i= 1) = ( œÉ b i) (19.19) p , ( ) = (; v h| N</div>
        </div>
    </div>

    <div class="question-card" id="q71">
        <div class="question-header">
            <span class="question-number">Question 71</span>
            <span class="question-type">multiple-choice</span>
        </div>

        <div class="question-text">Convolutional neural networks (CNNs) utilize unique mathematical operations to efficiently process grid-like data such as images. Understanding the properties of convolution is crucial for appreciating why CNNs are effective in large-scale machine learning tasks.

Which property of convolutional layers in CNNs enables the same set of kernel weights to be applied across all spatial positions of the input, thereby improving efficiency and statistical power?

1) Sparse interactions   
2) Equivariant representations   
3) Handling variable input sizes   
4) Matrix sparsity   
5) Parameter sharing   
6) Deep layer abstraction   
7) Cross-correlation implementation</div>

        <div class="answer-section">
            <div class="answer-label">‚úì Correct Answer:</div>
            <div class="answer-text">The correct answer is 5) Parameter sharing.</div>
        </div>

        <div class="reference-section">
            <div class="reference-label">üìö Reference Text:</div>
            <button class="toggle-button" onclick="toggleReference(71)">
                Show/Hide Reference
            </button>
            <div id="ref71" class="reference-text hidden">j I K i , j () = ( ‚àó)() =ÓÅò mÓÅò nI m , n K i m , j n . ( )( ‚àí ‚àí)(9.4) Convolutioniscommutative,meaningwecanequivalentlywrite: S i , j K I i , j () = ( ‚àó)() =ÓÅò mÓÅò nI i m , j n K m , n . ( ‚àí ‚àí)( )(9.5) Usuallythelatterformulaismorestraightforwardtoimplementinamachine learninglibrary,becausethereislessvariationintherangeofvalidvaluesof m and. n Thecommutativepropertyofconvolutionarisesbecausewehave Ô¨Çi pp e dthe kernelrelativetotheinput,inthesensethatas mincreases,theindexintothe inputincreases,buttheindexintothekerneldecreases.TheonlyreasontoÔ¨Çip thekernelistoobtainthecommutativeproperty.Whilethecommutativeproperty 3 3 2 CHAPTER9.CONVOLUTIONALNETWORKS isusefulforwritingproofs,itisnotusuallyanimportantpropertyofaneural networkimplementation.Instead,manyneuralnetworklibrariesimplementa relatedfunctioncalledthe c r o ss-c o r r e l a t i o n,whichisthesameasconvolution butwithoutÔ¨Çippingthekernel: S i , j I K i , j () = ( ‚àó)() =ÓÅò mÓÅò nI i m , j n K m , n . (+ +)( )(9.6) Manymachinelearninglibrariesimplementcross-correlationbutcallitconvolution. Inthistextwewillfollowthisconventionofcallingbothoperationsconvolution, andspecifywhetherwemeantoÔ¨Çipthekernelornotincontextswherekernel Ô¨Çippingisrelevant.Inthecontextofmachinelearning,thelearningalgorithmwill learntheappropriatevaluesofthekernelintheappropriateplace,soanalgorithm basedonconvolutionwithkernelÔ¨ÇippingwilllearnakernelthatisÔ¨Çippedrelative tothekernellearnedbyanalgorithmwithouttheÔ¨Çipping.Itisalsorarefor convolutiontobeusedaloneinmachinelearning;insteadconvolutionisused simultaneouslywithotherfunctions,andthecombinationofthesefunctionsdoes notcommuteregardlessofwhethertheconvolutionoperationÔ¨Çipsitskernelor not. SeeÔ¨Ågureforanexampleofconvolution(withoutkernelÔ¨Çipping)applied 9.1 toa2-Dtensor. Discreteconvolutioncanbeviewedasmultiplicationbyamatrix.However,the matrixhasseveralentriesconstrainedtobeequaltootherentries.Forexample, forunivariatediscreteconvolution,eachrowofthematrixisconstrainedtobe equaltotherowaboveshiftedbyoneelement.Thisisknownasa T o e pl i t z m at r i x.Intwodimensions,a doubly bl o c k c i r c ul an t m at r i xcorrespondsto convolution.Inadditiontotheseconstraintsthatseveralelementsbeequalto eachother,convolutionusuallycorrespondstoaverysparsematrix(amatrix whoseentriesaremostlyequaltozero).Thisisbecausethekernelisusuallymuch smallerthantheinputimage.Anyneuralnetworkalgorithmthatworkswith matrixmultiplication anddoesnotdependonspeciÔ¨Åcpropertiesofthematrix structureshouldworkwithconvolution,withoutrequiringanyfurtherchanges totheneuralnetwork.Typicalconvolutionalneuralnetworksdomakeuseof furtherspecializationsinordertodealwithlargeinputseÔ¨Éciently,buttheseare notstrictlynecessaryfromatheoreticalperspective. 3 3 3 CHAPTER9.CONVOLUTIONALNETWORKS a b c d e f g h i j k lw x y z a w + b x + e y + f za w + b x + e y + f zb w + c x + f y + g zb w + c x + f y + g zc w + d x + g y + h zc w + d x + g y + h z e w + f x + i y + j ze w + f x + i y + j zf w + g x + j y + k zf w + g x + j y + k zg w + h x + k y + l zg w + h x + k y + l zI nput K e r ne l O ut put Figure9.1:Anexampleof2-Dconvolutionwithoutkernel-Ô¨Çipping.Inthiscasewerestrict theoutputtoonlypositionswherethekernelliesentirelywithintheimage,called‚Äúvalid‚Äù convolutioninsomecontexts.Wedrawboxeswitharrowstoindicatehowtheupper-left elementoftheoutputtensorisformedbyapplyingthekerneltothecorresponding upper-leftregionoftheinputtensor. 3 3 4 CHAPTER9.CONVOLUTIONALNETWORKS 9.2Motivation Convolutionleveragesthreeimportantideasthatcanhelpimproveamachine learningsystem: spar se i nt e r ac t i o n s, par ameter shar i ngand e q ui v ar i an t r e pr e se n t at i o ns.Moreover, convolutionprovidesameansforworkingwith inputsofvariablesize.Wenowdescribeeachoftheseideasinturn. Traditionalneuralnetworklayersusematrixmultiplicationbyamatrixof parameterswithaseparateparameterdescribingtheinteractionbetweeneachinput unitandeachoutputunit.Thismeanseveryoutputunitinteractswitheveryinput unit.Convolutionalnetworks,however,typicallyhave spar se i n t e r ac t i o ns(also referredtoas spar se c o nnec t i v i t yor spar se wei g h t s).Thisisaccomplishedby makingthekernelsmallerthantheinput.Forexample,whenprocessinganimage, theinputimagemighthavethousandsormillionsofpixels,butwecandetectsmall, meaningfulfeaturessuchasedgeswithkernelsthatoccupyonlytensorhundredsof pixels.Thismeansthatweneedtostorefewerparameters,whichbothreducesthe memoryrequirementsofthemodelandimprovesitsstatisticaleÔ¨Éciency.Italso meansthatcomputingtheoutputrequiresfeweroperations.Theseimprovements ineÔ¨Éciencyareusuallyquitelarge.Ifthereare minputsand noutputs,then matrixmultiplication requires m n √óparametersandthealgorithmsusedinpractice have O( m n √ó)runtime(perexample).Ifwelimitthenumberofconnections eachoutputmayhaveto k,thenthesparselyconnectedapproachrequiresonly k n √óparametersand O( k n √ó)runtime.Formanypracticalapplications,itis possibletoobtaingoodperformanceonthemachinelearningtaskwhilekeeping kseveralordersofmagnitudesmallerthan m. Forgraphicaldemonstrationsof sparseconnectivity,seeÔ¨ÅgureandÔ¨Ågure.Inadeepconvolutionalnetwork, 9.2 9.3 unitsinthedeeperlayersmayindirectlyinteractwithalargerportionoftheinput, asshowninÔ¨Ågure.ThisallowsthenetworktoeÔ¨Écientlydescribecomplicated 9.4 interactionsbetweenmanyvariablesbyconstructingsuchinteractionsfromsimple buildingblocksthateachdescribeonlysparseinteractions. P ar amet e r shar i ngreferstousingthesameparameterformorethanone functioninamodel.Inatraditionalneuralnet,eachelementoftheweightmatrix isusedexactlyoncewhencomputingtheoutputofalayer.Itismultipliedby oneelementoftheinputandthenneverrevisited.Asasynonymforparameter sharing,onecansaythatanetworkhas t i e d w e i g h t s,becausethevalueofthe weightappliedtooneinputistiedtothevalueofaweightappliedelsewhere.In aconvolutionalneuralnet,eachmemberofthekernelisusedateveryposition oftheinput(exceptperhapssomeoftheboundarypixels, dependingonthe designdecisionsregardingtheboundary).Theparametersharingusedbythe convolutionoperationmeansthatratherthanlearningaseparatesetofparameters 3 3 5 CHAPTER9.CONVOLUTIONALNETWORKS x 1 x 1 x 2 x 2 x 3 x 3s</div>
        </div>
    </div>

    <div class="question-card" id="q72">
        <div class="question-header">
            <span class="question-number">Question 72</span>
            <span class="question-type">multiple-choice</span>
        </div>

        <div class="question-text">In machine learning optimization, the choice of gradient descent algorithm and learning rate schedule plays a critical role in both convergence speed and generalization performance. Techniques such as momentum and Nesterov momentum further modify the update rules to accelerate training and improve stability.

Which statement best explains why stochastic gradient descent (SGD) can improve generalization compared to batch gradient descent in large-scale machine learning tasks?

1) SGD uses adaptive learning rates that automatically prevent overfitting during training.   
2) The inherent noise from minibatch sampling in SGD introduces persistent fluctuations that help escape shallow local minima and reduce overfitting.   
3) Batch gradient descent always converges faster than SGD, leading to better generalization on unseen data.   
4) SGD guarantees zero gradient variance at the cost function minimum, ensuring optimal parameter selection.   
5) The convergence rate of SGD on convex problems is always O(1/k), regardless of problem structure.   
6) Increasing the minibatch size during SGD training eliminates noise and ensures perfect generalization.   
7) Nesterov momentum applied with SGD always results in superior generalization due to its lookahead property.</div>

        <div class="answer-section">
            <div class="answer-label">‚úì Correct Answer:</div>
            <div class="answer-text">The correct answer is 2) The inherent noise from minibatch sampling in SGD introduces persistent fluctuations that help escape shallow local minima and reduce overfitting..</div>
        </div>

        <div class="reference-section">
            <div class="reference-label">üìö Reference Text:</div>
            <button class="toggle-button" onclick="toggleReference(72)">
                Show/Hide Reference
            </button>
            <div id="ref72" class="reference-text hidden">ThisisbecausetheSGDgradientestimatorintroducesasourceofnoise(the randomsamplingof mtrainingexamples)thatdoesnotvanishevenwhenwearrive ataminimum.Bycomparison,thetruegradientofthetotalcostfunctionbecomes smallandthen 0whenweapproachandreachaminimumusingbatchgradient descent,sobatchgradientdescentcanuseaÔ¨Åxedlearningrate.AsuÔ¨Écient conditiontoguaranteeconvergenceofSGDisthat ‚àûÓÅò k = 1ÓÄè k= and ‚àû , (8.12) 2 9 4 CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS ‚àûÓÅò k = 1ÓÄè2 k < .‚àû (8.13) Inpractice,itiscommontodecaythelearningratelinearlyuntiliteration: œÑ ÓÄè k= (1 )‚àí Œ± ÓÄè 0+ Œ± ÓÄè œÑ (8.14) with Œ±=k œÑ.Afteriteration,itiscommontoleaveconstant. œÑ ÓÄè Thelearningratemaybechosenbytrialanderror,butitisusuallybest tochooseitbymonitoringlearningcurvesthatplottheobjectivefunctionasa functionoftime.Thisismoreofanartthanascience,andmostguidanceonthis subjectshouldberegardedwithsomeskepticism.Whenusingthelinearschedule, theparameterstochooseare ÓÄè 0, ÓÄè œÑ,and œÑ.Usually œÑmaybesettothenumberof iterationsrequiredtomakeafewhundredpassesthroughthetrainingset.Usually ÓÄè œÑshouldbesettoroughlythevalueof 1% ÓÄè 0.Themainquestionishowtoset ÓÄè 0. Ifitistoolarge,thelearningcurvewillshowviolentoscillations,withthecost functionoftenincreasingsigniÔ¨Åcantly.GentleoscillationsareÔ¨Åne,especiallyif trainingwithastochasticcostfunctionsuchasthecostfunctionarisingfromthe useofdropout.Ifthelearningrateistoolow,learningproceedsslowly,andifthe initiallearningrateistoolow,learningmaybecomestuckwithahighcostvalue. Typically,theoptimalinitiallearningrate,intermsoftotaltrainingtimeandthe Ô¨Ånalcostvalue,ishigherthanthelearningratethatyieldsthebestperformance aftertheÔ¨Årst100iterationsorso.Therefore,itisusuallybesttomonitortheÔ¨Årst severaliterationsandusealearningratethatishigherthanthebest-performing learningrateatthistime,butnotsohighthatitcausessevereinstability. ThemostimportantpropertyofSGDandrelatedminibatchoronlinegradient- basedoptimization isthatcomputationtimeperupdatedoesnotgrowwiththe numberoftrainingexamples.Thisallowsconvergenceevenwhenthenumber oftrainingexamplesbecomesverylarge.Foralargeenoughdataset,SGDmay convergetowithinsomeÔ¨ÅxedtoleranceofitsÔ¨Ånaltestseterrorbeforeithas processedtheentiretrainingset. Tostudytheconvergencerateofanoptimization algorithmitiscommonto measuretheexcesserror J(Œ∏)‚àímin Œ∏ J(Œ∏),whichistheamountthatthecurrent costfunctionexceedstheminimumpossiblecost.WhenSGDisappliedtoaconvex problem,theexcesserroris O(1‚àö k)after kiterations,whileinthestronglyconvex caseitis O(1 k).Theseboundscannotbeimprovedunlessextraconditionsare assumed.Batchgradientdescentenjoysbetterconvergenceratesthanstochastic gradientdescentintheory.However,theCram√©r-Raobound(,;, Cram√©r1946Rao 1945)statesthatgeneralization errorcannotdecreasefasterthan O(1 k).Bottou 2 9 5 CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS andBousquet2008()arguethatitthereforemaynotbeworthwhiletopursue anoptimization algorithmthatconvergesfasterthan O(1 k)formachinelearning tasks‚ÄîfasterconvergencepresumablycorrespondstooverÔ¨Åtting.Moreover,the asymptoticanalysisobscuresmanyadvantagesthatstochasticgradientdescent hasafterasmallnumberofsteps.Withlargedatasets,theabilityofSGDtomake rapidinitialprogresswhileevaluatingthegradientforonlyveryfewexamples outweighsitsslowasymptoticconvergence.Mostofthealgorithmsdescribedin theremainderofthischapterachievebeneÔ¨Åtsthatmatterinpracticebutarelost intheconstantfactorsobscuredbythe O(1 k)asymptoticanalysis.Onecanalso tradeoÔ¨ÄthebeneÔ¨Åtsofbothbatchandstochasticgradientdescentbygradually increasingtheminibatchsizeduringthecourseoflearning. FormoreinformationonSGD,see(). Bottou1998 8.3.2Momentum Whilestochasticgradientdescentremainsaverypopularoptimization strategy, learningwithitcansometimesbeslow.Themethodofmomentum(Polyak1964,) isdesignedtoacceleratelearning,especiallyinthefaceofhighcurvature,smallbut consistentgradients,ornoisygradients.Themomentumalgorithmaccumulates anexponentiallydecayingmovingaverageofpastgradientsandcontinuestomove intheirdirection.TheeÔ¨ÄectofmomentumisillustratedinÔ¨Ågure.8.5 Formally,themomentumalgorithmintroducesavariablevthatplaystherole ofvelocity‚Äîitisthedirectionandspeedatwhichtheparametersmovethrough parameterspace.Thevelocityissettoanexponentiallydecayingaverageofthe negativegradient.Thenamemomentumderivesfromaphysicalanalogy,in whichthenegativegradientisaforcemovingaparticlethroughparameterspace, accordingtoNewton‚Äôslawsofmotion.Momentuminphysicsismasstimesvelocity. Inthemomentumlearningalgorithm,weassumeunitmass,sothevelocityvectorv mayalsoberegardedasthemomentumoftheparticle.Ahyperparameter Œ±‚àà[0 ,1) determineshowquicklythecontributionsofpreviousgradientsexponentiallydecay. Theupdateruleisgivenby: vv‚Üê Œ±‚àí‚àá ÓÄè Œ∏ÓÄ† 1 mm ÓÅò i = 1L((fx( ) i;)Œ∏ ,y( ) i)ÓÄ° , (8.15) Œ∏Œ∏v ‚Üê + . (8.16) Thevelocityvaccumulatesthegradientelements‚àá Œ∏ÓÄÄ1 mÓÅêm i = 1 L((fx( ) i;)Œ∏ ,y( ) i)ÓÄÅ . Thelarger Œ±isrelativeto ÓÄè,themorepreviousgradientsaÔ¨Äectthecurrentdirection. TheSGDalgorithmwithmomentumisgiveninalgorithm .8.2 2 9 6 CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS ‚àí ‚àí ‚àí 3 0 2 0 1 0 0 1 0 2 0‚àí 3 0‚àí 2 0‚àí 1 001 02 0 Figure8.5:Momentumaimsprimarilytosolvetwoproblems:poorconditioningofthe Hessianmatrixandvarianceinthestochasticgradient.Here,weillustratehowmomentum overcomestheÔ¨Årstofthesetwoproblems.Thecontourlinesdepictaquadraticloss functionwithapoorlyconditionedHessianmatrix.Theredpathcuttingacrossthe contoursindicatesthepathfollowedbythemomentumlearningruleasitminimizesthis function.Ateachstepalongtheway,wedrawanarrowindicatingthestepthatgradient descentwouldtakeatthatpoint.Wecanseethatapoorlyconditionedquadraticobjective lookslikealong,narrowvalleyorcanyonwithsteepsides.Momentumcorrectlytraverses thecanyonlengthwise,whilegradientstepswastetimemovingbackandforthacrossthe narrowaxisofthecanyon.ComparealsoÔ¨Ågure,whichshowsthebehaviorofgradient 4.6 descentwithoutmomentum. 2 9 7 CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS Previously,thesizeofthestepwassimplythenormofthegradientmultiplied bythelearningrate.Now,thesizeofthestepdependsonhowlargeandhow alignedasequenceofgradientsare.Thestepsizeislargestwhenmanysuccessive gradientspointinexactlythesamedirection.Ifthemomentumalgorithmalways observesgradientg,thenitwillaccelerateinthedirectionof‚àíg,untilreachinga terminalvelocitywherethesizeofeachstepis ÓÄè||||g 1‚àí Œ±. (8.17) Itisthushelpfultothinkofthemomentumhyperparameterintermsof1 1 ‚àí Œ±.For example, Œ±= .9correspondstomultiplyingthemaximumspeedbyrelativeto 10 thegradientdescentalgorithm. Commonvaluesof Œ±usedinpracticeinclude .5, .9,and .99.Likethelearning rate, Œ±mayalsobeadaptedovertime.Typicallyitbeginswithasmallvalueand islaterraised.Itislessimportanttoadapt Œ±overtimethantoshrink ÓÄèovertime. Algorithm8.2Stochasticgradientdescent(SGD)withmomentum Require:Learningrate,momentumparameter. ÓÄè Œ± Require:Initialparameter,initialvelocity. Œ∏ v while do stoppingcriterionnotmet Sampleaminibatchof mexamplesfromthetrainingset{x( 1 ), . . . ,x( ) m}with correspondingtargetsy( ) i. Computegradientestimate:g‚Üê1 m‚àá Œ∏ÓÅê i L f((x( ) i;)Œ∏ ,y( ) i) Computevelocityupdate:vvg ‚Üê Œ±‚àí ÓÄè Applyupdate:Œ∏Œ∏v ‚Üê + endwhile Wecanviewthemomentumalgorithmassimulatingaparticlesubjectto continuous-timeNewtoniandynamics.Thephysicalanalogycanhelptobuild intuitionforhowthemomentumandgradientdescentalgorithmsbehave. ThepositionoftheparticleatanypointintimeisgivenbyŒ∏( t).Theparticle experiencesnetforce.Thisforcecausestheparticletoaccelerate: f() t f() = t‚àÇ2 ‚àÇ t2Œ∏() t . (8.18) Ratherthanviewingthisasasecond-orderdiÔ¨Äerentialequationoftheposition, wecanintroducethevariablev( t)representingthevelocityoftheparticleattime tandrewritetheNewtoniandynamicsasaÔ¨Årst-orderdiÔ¨Äerentialequation: v() = t‚àÇ ‚àÇ tŒ∏() t , (8.19) 2 9 8 CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS f() = t‚àÇ ‚àÇ tv() t . (8.20) ThemomentumalgorithmthenconsistsofsolvingthediÔ¨Äerentialequationsvia numericalsimulation.AsimplenumericalmethodforsolvingdiÔ¨Äerentialequations isEuler‚Äôsmethod,whichsimplyconsistsofsimulatingthedynamicsdeÔ¨Ånedby theequationbytakingsmall,Ô¨Ånitestepsinthedirectionofeachgradient. Thisexplainsthebasicformofthemomentumupdate,butwhatspeciÔ¨Åcallyare theforces?Oneforceisproportionaltothenegativegradientofthecostfunction: ‚àí‚àá Œ∏ J(Œ∏).Thisforcepushestheparticledownhillalongthecostfunctionsurface. Thegradientdescentalgorithmwouldsimplytakeasinglestepbasedoneach gradient,buttheNewtonianscenariousedbythemomentumalgorithminstead usesthisforcetoalterthevelocityoftheparticle.Wecanthinkoftheparticle asbeinglikeahockeypuckslidingdownanicysurface.Wheneveritdescendsa steeppartofthesurface,itgathersspeedandcontinuesslidinginthatdirection untilitbeginstogouphillagain. Oneotherforceisnecessary.Iftheonlyforceisthegradientofthecostfunction, thentheparticlemightnevercometorest.Imagineahockeypuckslidingdown onesideofavalleyandstraightuptheotherside,oscillatingbackandforthforever, assumingtheiceisperfectlyfrictionless.Toresolvethisproblem,weaddone otherforce,proportionalto‚àív( t).Inphysicsterminology,thisforcecorresponds toviscousdrag,asiftheparticlemustpushthrougharesistantmediumsuchas syrup.Thiscausestheparticletograduallyloseenergyovertimeandeventually convergetoalocalminimum. Whydoweuse‚àív( t)andviscousdraginparticular? Partofthereasonto use‚àív( t)ismathematical convenience‚Äîanintegerpowerofthevelocityiseasy toworkwith.However,otherphysicalsystemshaveotherkindsofdragbased onotherintegerpowersofthevelocity.Forexample,aparticletravelingthrough theairexperiencesturbulentdrag,withforceproportionaltothesquareofthe velocity,whileaparticlemovingalongthegroundexperiencesdryfriction,witha forceofconstantmagnitude.Wecanrejecteachoftheseoptions.Turbulentdrag, proportionaltothesquareofthevelocity,becomesveryweakwhenthevelocityis small.Itisnotpowerfulenoughtoforcetheparticletocometorest.Aparticle withanon-zeroinitialvelocitythatexperiencesonlytheforceofturbulentdrag willmoveawayfromitsinitialpositionforever,withthedistancefromthestarting pointgrowinglike O(log t).Wemustthereforeusealowerpowerofthevelocity. Ifweuseapowerofzero,representingdryfriction,thentheforceistoostrong. Whentheforceduetothegradientofthecostfunctionissmallbutnon-zero,the constantforceduetofrictioncancausetheparticletocometorestbeforereaching alocalminimum.Viscousdragavoidsbothoftheseproblems‚Äîitisweakenough 2 9 9 CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS thatthegradientcancontinuetocausemotionuntilaminimumisreached,but strongenoughtopreventmotionifthegradientdoesnotjustifymoving. 8.3.3NesterovMomentum Sutskever2013etal.()introducedavariantofthemomentumalgorithmthatwas inspiredbyNesterov‚Äôsacceleratedgradientmethod(,,).The Nesterov19832004 updaterulesinthiscasearegivenby: vv‚Üê Œ±‚àí‚àá ÓÄè Œ∏ÓÄ¢ 1 mm ÓÅò i = 1LÓÄê fx(( ) i;+ )Œ∏ Œ±v ,y( ) iÓÄëÓÄ£ ,(8.21) Œ∏Œ∏v ‚Üê + , (8.22) wheretheparameters Œ±and ÓÄèplayasimilarroleasinthestandardmomentum method.ThediÔ¨ÄerencebetweenNesterovmomentumandstandardmomentumis wherethegradientisevaluated.WithNesterovmomentumthegradientisevaluated afterthecurrentvelocityisapplied.ThusonecaninterpretNesterovmomentum asattemptingtoaddacorrectionfactortothestandardmethodofmomentum. ThecompleteNesterovmomentumalgorithmispresentedinalgorithm .8.3 Intheconvexbatchgradientcase,Nesterovmomentumbringstherateof convergenceoftheexcesserrorfrom O(1 /k)(after ksteps)to O(1 /k2)asshown byNesterov1983().Unfortunately, inthestochasticgradientcase,Nesterov momentumdoesnotimprovetherateofconvergence. Algorithm8.3Stochasticgradientdescent(SGD)withNesterovmomentum Require:Learningrate,momentumparameter. ÓÄè Œ± Require:Initialparameter,initialvelocity. Œ∏ v while do stoppingcriterionnotmet Sampleaminibatchof mexamplesfromthetrainingset{x( 1 ), . . . ,x( ) m}with correspondinglabelsy( ) i. Applyinterimupdate: ÀúŒ∏Œ∏v ‚Üê + Œ± Computegradient(atinterimpoint):g‚Üê1 m‚àá Àú Œ∏ÓÅê i L f((x( ) i;ÀúŒ∏y) ,( ) i) Computevelocityupdate:vvg ‚Üê Œ±‚àí ÓÄè Applyupdate:Œ∏Œ∏v ‚Üê + endwhile 3 0 0 CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS 8.4ParameterInitializationStrategies Someoptimization</div>
        </div>
    </div>

    <div class="question-card" id="q73">
        <div class="question-header">
            <span class="question-number">Question 73</span>
            <span class="question-type">multiple-choice</span>
        </div>

        <div class="question-text">Deep learning methods can face challenges when labeled data is scarce or expensive to obtain. Researchers have developed various approaches to reduce reliance on labeled data and enhance model adaptability across diverse tasks.

Which learning paradigm focuses on extracting patterns from unlabeled data and includes techniques such as clustering, dimensionality reduction, generative models, and self-supervised learning?

1) Unsupervised learning   
2) Reinforcement learning   
3) Supervised learning   
4) Transfer learning   
5) Multitask learning   
6) Active learning   
7) Federated learning</div>

        <div class="answer-section">
            <div class="answer-label">‚úì Correct Answer:</div>
            <div class="answer-text">The correct answer is 1) Unsupervised learning.</div>
        </div>

        <div class="reference-section">
            <div class="reference-label">üìö Reference Text:</div>
            <button class="toggle-button" onclick="toggleReference(73)">
                Show/Hide Reference
            </button>
            <div id="ref73" class="reference-text hidden">P a rt I I I DeepLearningResearch 486 This part of t he b o ok des c r ib e s t he more am bitious and adv anced approac hes t o deep learning, c urren t ly purs ued b y t he r e s e arc h c omm unit y . In t he previous parts of t he b o ok, we ha v e s ho wn how t o s olv e s up e r v is e d learning problems ‚Äî how t o learn t o map one v e c t or t o another, given e nough e x amples of t he mapping. N ot all problems w e might w ant t o s olve f all in t o t his c ategory . W e ma y wis h t o generate new e x amples , or determine how likely s ome p oin t is , or handle mis s ing v alues and t ake adv an t age of a large s e t of unlab e led e x amples or e x amples f r om r e lated t as k s . A s hortcom ing of t he c urren t s t ate of t he art f or indus t r ial applications is t hat our learning algorithms r e q uire large amounts of s up e r v is e d data t o ac hieve go o d accuracy . In t his part of t he b o ok, w e dis c us s s ome of t he s p e c ulative approac hes t o r e ducing t he amoun t of lab e led data neces s ary f or e x is t ing mo dels t o work w e ll and b e applicable acros s a broader r ange of t as k s . A c c omplis hing t hes e goals us ually r e q uires s ome f orm of uns up e r v is e d or s e mi-s up e r v is e d learning. Man y deep learning algorithms ha v e b e e n des igned t o t ackle uns upervis e d learning problems , but none hav e t r uly s olved t he problem in t he s ame w a y t hat deep learning has largely s olv e d t he s up e r v is e d learning problem f or a wide v ariet y of t as k s . In t his part of t he b o ok, we des c r ibe t he e x is t ing approaches t o uns upervis e d learning and s ome of t he p opular t hought ab</div>
        </div>
    </div>

    <div class="question-card" id="q74">
        <div class="question-header">
            <span class="question-number">Question 74</span>
            <span class="question-type">multiple-choice</span>
        </div>

        <div class="question-text">Energy-based models such as Boltzmann machines and neural networks are foundational in deep generative modeling, providing flexible structures for learning complex data distributions. Gradient-based optimization in these models often requires specialized techniques to handle stochastic operations and intricate parameter dependencies.

Which method enables gradient-based optimization in generative neural networks with continuous latent variables by expressing sampled values as deterministic functions of random noise and model parameters?

1) Contrastive divergence   
2) Backpropagation through time   
3) Monte Carlo sampling   
4) The reparameterization trick   
5) Dropout regularization   
6) Baseline offset technique   
7) Categorical cross-entropy</div>

        <div class="answer-section">
            <div class="answer-label">‚úì Correct Answer:</div>
            <div class="answer-text">The correct answer is 4) The reparameterization trick.</div>
        </div>

        <div class="reference-section">
            <div class="reference-label">üìö Reference Text:</div>
            <button class="toggle-button" onclick="toggleReference(74)">
                Show/Hide Reference
            </button>
            <div id="ref74" class="reference-text hidden">CHAPTER20.DEEPGENERATIVEMODELS ofmusicalnotesusedtocomposesongs.Boulanger-Lewandowski2012 e t a l .() introducedtheRNN-RBM sequencemodelandappliedittothistask.The RNN-RBMisagenerativemodelofasequenceofframes x( ) tconsistingofanRNN thatemitstheRBMparametersforeachtimestep.Unlikepreviousapproaches inwhichonlythebiasparametersoftheRBMvariedfromonetimesteptothe next,theRNN-RBMusestheRNNtoemitalloftheparametersoftheRBM, includingtheweights.Totrainthemodel,weneedtobeabletoback-propagate thegradientofthelossfunctionthroughtheRNN.Thelossfunctionisnotapplied directlytotheRNNoutputs.Instead,itisappliedtotheRBM.Thismeansthat wemustapproximately diÔ¨ÄerentiatethelosswithrespecttotheRBMparameters usingcontrastivedivergenceorarelatedalgorithm. This approximate gradient maythenbeback-propagated throughtheRNNusingtheusualback-propagation throughtimealgorithm. 20.8OtherBoltzmannMachines ManyothervariantsofBoltzmannmachinesarepossible. BoltzmannmachinesmaybeextendedwithdiÔ¨Äerenttrainingcriteria.Wehave focusedonBoltzmannmachinestrainedtoapproximately maximizethegenerative criterion log p( v).ItisalsopossibletotraindiscriminativeRBMsthataimto maximize log p( y| v)instead( ,).Thisapproachoften LarochelleandBengio2008 performsthebestwhenusingalinearcombinationofboththegenerativeand thediscriminativecriteria.Unfortunately,RBMsdonotseemtobeaspowerful supervisedlearnersasMLPs,atleastusingexistingmethodology. MostBoltzmannmachinesusedinpracticehaveonlysecond-orderinteractions intheirenergyfunctions,meaningthattheirenergyfunctionsarethesumofmany termsandeachindividualtermonlyincludestheproductbetweentworandom variables.Anexampleofsuchatermis v i W i , j h j.Itisalsopossibletotrain higher-orderBoltzmannmachines(,)whoseenergyfunctionterms Sejnowski1987 involvetheproductsbetweenmanyvariables.Three-wayinteractionsbetweena hiddenunitandtwodiÔ¨Äerentimagescanmodelspatialtransformationsfromone frameofvideotothenext(MemisevicandHinton20072010,,).Multiplication bya one-hotclassvariablecanchangetherelationshipbetweenvisibleandhiddenunits dependingonwhichclassispresent( ,).Onerecentexample NairandHinton2009 oftheuseofhigher-orderinteractionsisaBoltzmannmachinewithtwogroupsof hiddenunits,withonegroupofhiddenunitsthatinteractwithboththevisible units vandtheclasslabel y,andanothergroupofhiddenunitsthatinteractonly withthe vinputvalues(,).Thiscanbeinterpretedasencouraging Luo e t a l .2011 6 8 6 CHAPTER20.DEEPGENERATIVEMODELS somehiddenunitstolearntomodeltheinputusingfeaturesthatarerelevantto theclassbutalsotolearnextrahiddenunitsthatexplainnuisancedetailsthat arenecessaryforthesamplesof vtoberealisticbutdonotdeterminetheclass oftheexample.Anotheruseofhigher-orderinteractionsistogatesomefeatures. Sohn2013 e t a l .()introducedaBoltzmannmachinewiththird-orderinteractions withbinarymaskvariablesassociatedwitheachvisibleunit.Whenthesemasking variablesaresettozero,theyremovetheinÔ¨Çuenceofavisibleunitonthehidden units.ThisallowsvisibleunitsthatarenotrelevanttotheclassiÔ¨Åcationproblem toberemovedfromtheinferencepathwaythatestimatestheclass. Moregenerally,theBoltzmannmachineframeworkisarichspaceofmodels permittingmanymoremodelstructuresthanhavebeenexploredsofar.Developing anewformofBoltzmannmachinerequiressomemorecareandcreativitythan developinganewneuralnetworklayer,becauseitisoftendiÔ¨ÉculttoÔ¨Åndanenergy functionthatmaintainstractabilityofallofthediÔ¨Äerentconditionaldistributions neededtousetheBoltzmannmachine,butdespitethisrequiredeÔ¨ÄorttheÔ¨Åeld remainsopentoinnovation. 20.9Back-PropagationthroughRandomOperations Traditionalneuralnetworksimplementadeterministictransformationofsome inputvariables x.Whendevelopinggenerativemodels,weoftenwishtoextend neuralnetworkstoimplementstochastictransformationsof x.Onestraightforward waytodothisistoaugmenttheneuralnetworkwithextrainputs zthatare sampledfromsomesimpleprobabilitydistribution,suchasauniformorGaussian distribution.Theneuralnetworkcanthencontinuetoperformdeterministic computationinternally, butthefunction f( x z ,)willappearstochasticto an observerwhodoesnothaveaccessto z.Providedthat fiscontinuousand diÔ¨Äerentiable,wecanthencomputethegradientsnecessaryfortrainingusing back-propagationasusual. Asanexample,letusconsidertheoperationconsistingofdrawingsamplesy fromaGaussiandistributionwithmeanandvariance ¬µ œÉ2: y‚àºN( ¬µ , œÉ2) . (20.54) Becauseanindividualsampleofyisnotproducedbyafunction,butratherby asamplingprocesswhoseoutputchangeseverytimewequeryit,itmayseem counterintuitivetotakethederivativesof ywithrespecttotheparametersof itsdistribution, ¬µand œÉ2.However, wecanrewritethesamplingprocessas 6 8 7 CHAPTER20.DEEPGENERATIVEMODELS transforminganunderlyingrandomvaluez‚àºN( z;0 ,1)toobtainasamplefrom thedesireddistribution: y ¬µ œÉ z = + (20.55) Wearenowabletoback-propagatethroughthesamplingoperation,byregard- ingitasadeterministicoperationwithanextrainputz.Crucially,theextrainput isarandomvariablewhosedistributionisnotafunctionofanyofthevariables whosederivativeswewanttocalculate. The resulttellsushowaninÔ¨Ånitesimal changein ¬µor œÉwouldchangetheoutputifwecouldrepeatthesamplingoperation againwiththesamevalueofz. Beingabletoback-propagate throughthissamplingoperationallowsusto incorporateitintoalargergraph.Wecanbuildelementsofthegraphontopofthe outputofthesamplingdistribution.Forexample,wecancomputethederivatives ofsomelossfunction J( y).Wecanalsobuildelementsofthegraphwhoseoutputs aretheinputsortheparametersofthesamplingoperation.Forexample,wecould buildalargergraphwith ¬µ= f( x; Œ∏)and œÉ= g( x; Œ∏).Inthisaugmentedgraph, wecanuseback-propagationthroughthesefunctionstoderive‚àá Œ∏ J y(). TheprincipleusedinthisGaussiansamplingexampleismoregenerallyappli- cable.Wecanexpressanyprobabilitydistributionoftheform p(y; Œ∏)or p(y| x; Œ∏) as p(y| œâ),where œâisavariablecontainingbothparameters Œ∏,andifapplicable, theinputs x.Givenavalue ysampledfromdistribution p(y| œâ),where œâmayin turnbeafunctionofothervariables,wecanrewrite y y ‚àº p(| œâ) (20.56) as y z œâ = ( f;) , (20.57) where zisasourceofrandomness.Wemaythencomputethederivativesof ywith respectto œâusingtraditionaltoolssuchastheback-propagation algorithmapplied to f,solongas fiscontinuousanddiÔ¨Äerentiable almosteverywhere.Crucially, œâ mustnotbeafunctionof z,and zmustnotbeafunctionof œâ.Thistechnique isoftencalledthereparametrizationtrick,stochasticback-propagationor perturbationanalysis. Therequirementthat fbecontinuousanddiÔ¨Äerentiableofcourserequires y tobecontinuous.Ifwewishtoback-propagate throughasamplingprocessthat producesdiscrete-valuedsamples,itmaystillbepossibletoestimateagradienton œâ,usingreinforcementlearningalgorithmssuchasvariantsoftheREINFORCE algorithm(,),discussedinsection. Williams1992 20.9.1 6 8 8 CHAPTER20.DEEPGENERATIVEMODELS Inneuralnetworkapplications,wetypicallychoose ztobedrawnfromsome simpledistribution,suchasaunituniformorunitGaussiandistribution,and achievemorecomplexdistributionsbyallowingthedeterministicportionofthe networktoreshapeitsinput. Theideaofpropagatinggradientsoroptimizingthroughstochasticoperations datesbacktothemid-twentiethcentury(,;,)andwas Price1958Bonnet1964 Ô¨Årstusedformachinelearninginthecontextofreinforcementlearning(,Williams 1992). Morerecently,ithasbeenappliedtovariationalapproximations(Opper andArchambeau2009,)andstochasticorgenerativeneuralnetworks(Bengio e t a l .,;,; 2013bKingma2013KingmaandWelling2014baRezende2014 ,,; e t a l .,; Goodfellow2014c e t a l .,).Manynetworks,suchasdenoisingautoencodersor networksregularized with dropout, are also naturally designedto take noise asaninputwithoutrequiringanyspecialreparametrization tomakethenoise independentfromthemodel. 20.9.1Back-PropagatingthroughDiscreteStochasticOperations Whenamodelemitsadiscretevariable y,thereparametrization trickisnot applicable.Suppose thatthemodel takesinputs xandparameters Œ∏, both encapsulatedinthevector œâ,andcombinesthemwithrandomnoise ztoproduce y: y z œâ = ( f;) . (20.58) Because yisdiscrete, fmustbeastepfunction.Thederivativesofastepfunction arenotusefulatanypoint.Rightateachstepboundary,thederivativesare undeÔ¨Åned,butthatisasmallproblem.Thelargeproblemisthatthederivatives arezeroalmosteverywhere,ontheregionsbetweenstepboundaries.Thederivatives ofanycostfunction J( y)thereforedonotgiveanyinformationforhowtoupdate themodelparameters . Œ∏ TheREINFORCEalgorithm(REwardIncrement=Non-negativeFactor √ó OÔ¨ÄsetReinforcement√óCharacteristic Eligibility)providesaframeworkdeÔ¨Åninga familyofsimplebutpowerfulsolutions(,). Thecoreideaisthat Williams1992 eventhough J( f( z; œâ))isastepfunctionwithuselessderivatives,theexpected cost E z z‚àº p ( ) J f((;)) z œâisoftenasmoothfunctionamenabletogradientdescent. Althoughthatexpectationistypicallynottractablewhen yishigh-dimensional (oristheresultofthecompositionofmanydiscretestochasticdecisions),itcanbe estimatedwithoutbiasusingaMonteCarloaverage.Thestochasticestimateof thegradientcanbeusedwithSGDorotherstochasticgradient-basedoptimization techniques. 6 8 9 CHAPTER20.DEEPGENERATIVEMODELS ThesimplestversionofREINFORCEcanbederivedbysimplydiÔ¨Äerentiating theexpectedcost: E z[()] = J yÓÅò yJ p() y() y (20.59) ‚àÇ J E[()] y ‚àÇ œâ=ÓÅò yJ() y‚àÇ p() y ‚àÇ œâ(20.60) =ÓÅò yJ p() y() y‚àÇ plog() y ‚àÇ œâ(20.61) ‚âà1 mmÓÅò y( ) i‚àº p , i ( ) y = 1J( y( ) i)‚àÇ plog( y( ) i) ‚àÇ œâ.(20.62) Equationreliesontheassumptionthat 20.60 Jdoesnotreference œâdirectly.Itis trivialtoextendtheapproachtorelaxthisassumption.Equationexploits20.61 thederivativeruleforthelogarithm,‚àÇ p l o g ( ) y ‚àÇ œâ=1 p ( ) y‚àÇ p ( ) y ‚àÇ œâ.Equation gives20.62 anunbiasedMonteCarloestimatorofthegradient. Anywherewewrite p( y)inthissection,onecouldequallywrite p( y x|).This isbecause p( y)isparametrized by œâ,and œâcontainsboth Œ∏and x,if xispresent. OneissuewiththeabovesimpleREINFORCEestimatoristhatithasavery highvariance,sothatmanysamplesof yneedtobedrawntoobtainagood estimatorofthegradient,orequivalently,ifonlyonesampleisdrawn,SGDwill convergeveryslowlyandwillrequireasmallerlearningrate.Itispossibleto considerablyreducethevarianceofthatestimatorbyusingvariancereduction methods(,;,).Theideaistomodifytheestimatorso Wilson1984L‚ÄôEcuyer1994 thatitsexpectedvalueremainsunchangedbutitsvariancegetreduced. Inthe contextofREINFORCE,theproposedvariancereductionmethodsinvolvethe computationofabaselinethatisusedtooÔ¨Äset J( y).NotethatanyoÔ¨Äset b( œâ) thatdoesnotdependon ywouldnotchangetheexpectationoftheestimated gradientbecause E p ( ) yÓÄî‚àÇ plog() y ‚àÇ œâÓÄï =ÓÅò yp() y‚àÇ plog() y ‚àÇ œâ(20.63)</div>
        </div>
    </div>

    <div class="question-card" id="q75">
        <div class="question-header">
            <span class="question-number">Question 75</span>
            <span class="question-type">multiple-choice</span>
        </div>

        <div class="question-text">Modern natural language processing models have evolved from using basic word-level representations to sophisticated architectures leveraging distributed embeddings and attention mechanisms. These approaches have significantly impacted related fields such as machine translation and recommender systems.

Which neural network architecture was specifically introduced to allow models to dynamically focus on relevant parts of an input sequence during output generation, leading to significant improvements in translating longer sentences?

1) Convolutional Neural Networks (CNNs)   
2) Restricted Boltzmann Machines (RBMs)   
3) Singular Value Decomposition (SVD)   
4) Latent Semantic Analysis (LSA)   
5) Encoder-Decoder without attention   
6) Attention Mechanisms   
7) Byte-level Recurrent Neural Networks</div>

        <div class="answer-section">
            <div class="answer-label">‚úì Correct Answer:</div>
            <div class="answer-text">The correct answer is 6) Attention Mechanisms.</div>
        </div>

        <div class="reference-section">
            <div class="reference-label">üìö Reference Text:</div>
            <button class="toggle-button" onclick="toggleReference(75)">
                Show/Hide Reference
            </button>
            <div id="ref75" class="reference-text hidden">l .,)oraconvolutional network(KalchbrennerandBlunsom2013,). Asecondmodel,usuallyanRNN, thenreadsthecontext Candgeneratesasentenceinthetargetlanguage.This generalideaofanencoder-decoderframeworkformachinetranslationisillustrated inÔ¨Ågure.12.5 Inordertogenerateanentiresentenceconditionedonthesourcesentence,the modelmusthaveawaytorepresenttheentiresourcesentence. Earliermodels wereonlyabletorepresentindividualwordsorphrases. Fromarepresentation 4 7 4 CHAPTER12.APPLICATIONS learningpointofview,itcanbeusefultolearnarepresentationinwhichsentences thathavethesamemeaninghavesimilarrepresentationsregardlessofwhether theywerewritteninthesourcelanguageorthetargetlanguage.Thisstrategywas exploredÔ¨ÅrstusingacombinationofconvolutionsandRNNs(Kalchbrennerand Blunsom2013,).LaterworkintroducedtheuseofanRNNforscoringproposed translations(,)andforgeneratingtranslatedsentences( Cho e t a l .2014a Sutskever e t a l . e t a l . ,).2014Jean()scaledthesemodelstolargervocabularies. 2014 12.4.5.1UsinganAttentionMechanismandAligningPiecesofData Œ±( t ‚àí 1 )Œ±( t ‚àí 1 )Œ±( ) tŒ±( ) tŒ±( + 1 ) tŒ±( + 1 ) t h( t ‚àí 1 )h( t ‚àí 1 )h( ) th( ) th( + 1 ) th( + 1 ) tc c √ó √ó √ó √ó √ó √ó+ Figure12.6:Amodernattentionmechanism,asintroducedby (),is Bahdanau e t a l .2015 essentiallyaweightedaverage.Acontextvectorcisformedbytakingaweightedaverage offeaturevectorsh( ) twithweights Œ±( ) t.Insomeapplications,thefeaturevectorshare hiddenunitsofaneuralnetwork,buttheymayalsoberawinputtothemodel.The weights Œ±( ) tareproducedbythemodelitself.Theyareusuallyvaluesintheinterval [0 ,1]andareintendedtoconcentratearoundjustoneh( ) tsothattheweightedaverage approximatesreadingthatonespeciÔ¨Åctimestepprecisely.Theweights Œ±( ) tareusually producedbyapplyingasoftmaxfunctiontorelevancescoresemittedbyanotherportion ofthemodel.Theattentionmechanismismoreexpensivecomputationallythandirectly indexingthedesiredh( ) t,butdirectindexingcannotbetrainedwithgradientdescent.The attentionmechanismbasedonweightedaveragesisasmooth,diÔ¨Äerentiableapproximation thatcanbetrainedwithexistingoptimizationalgorithms. UsingaÔ¨Åxed-sizerepresentationtocaptureallthesemanticdetailsofavery longsentenceofsay60wordsisverydiÔ¨Écult. Itcanbeachievedbytraininga suÔ¨ÉcientlylargeRNNwellenoughandforlongenough,asdemonstratedbyCho e t a l .()and2014aSutskever2014 e t a l .().However,amoreeÔ¨Écientapproachis toreadthewholesentenceorparagraph(togetthecontextandthegistofwhat 4 7 5 CHAPTER12.APPLICATIONS isbeingexpressed),thenproducethetranslatedwordsoneatatime,eachtime focusingonadiÔ¨Äerentpartoftheinputsentenceinordertogatherthesemantic detailsthatarerequiredtoproducethenextoutputword. Thatisexactlythe ideathat ()Ô¨Årstintroduced.Theattentionmechanismused Bahdanau e t a l .2015 tofocusonspeciÔ¨Åcpartsoftheinputsequenceateachtimestepisillustratedin Ô¨Ågure.12.6 Wecanthinkofanattention-basedsystemashavingthreecomponents: 1.Aprocessthat‚Äú r e a d s‚Äùrawdata(suchassourcewordsinasourcesentence), andconvertsthemintodistributedrepresentations,withonefeaturevector associatedwitheachwordposition. 2.Alistoffeaturevectorsstoringtheoutputofthereader.Thiscanbe understoodasa‚Äú‚Äù containingasequenceoffacts,whichcanbe m e m o r y retrievedlater,notnecessarilyinthesameorder,withouthavingtovisitall ofthem. 3.Aprocessthat‚Äú‚Äùthecontentofthememorytosequentiallyperform e x p l o i t s atask,ateachtimestephavingtheabilityputattentiononthecontentof onememoryelement(orafew,withadiÔ¨Äerentweight). Thethirdcomponentgeneratesthetranslatedsentence. Whenwordsinasentencewritteninonelanguagearealignedwithcorrespond- ingwordsinatranslatedsentenceinanotherlanguage,itbecomespossibletorelate thecorrespondingwordembeddings.Earlierworkshowedthatonecouldlearna kindoftranslationmatrixrelatingthewordembeddingsinonelanguagewiththe wordembeddingsinanother(Koƒçisk√Ω2014 e t a l .,),yieldingloweralignmenterror ratesthantraditionalapproachesbasedonthefrequencycountsinthephrasetable. Thereisevenearlierworkonlearningcross-lingualwordvectors(Klementiev e t a l ., 2012).Manyextensionstothisapproacharepossible.Forexample,moreeÔ¨Écient cross-lingualalignment( ,)allowstrainingonlargerdatasets. Gouws e t a l .2014 12.4.6HistoricalPerspective TheideaofdistributedrepresentationsforsymbolswasintroducedbyRumelhart e t a l .()inoneoftheÔ¨Årstexplorationsofback-propagation, withsymbols 1986a correspondingtotheidentityoffamilymembersandtheneuralnetworkcapturing therelationshipsbetweenfamilymembers,withtrainingexamplesformingtriplets suchas(Colin,Mother,Victoria). The Ô¨Årstlayeroftheneuralnetworklearned arepresentationofeachfamilymember.Forexample, thefeaturesforColin 4 7 6 CHAPTER12.APPLICATIONS mightrepresentwhichfamilytreeColinwasin,whatbranchofthattreehewas in,whatgenerationhewasfrom,etc.Onecanthinkoftheneuralnetworkas computinglearnedrulesrelatingtheseattributestogetherinordertoobtainthe desiredpredictions.Themodelcanthenmakepredictionssuchasinferringwhois themotherofColin. Theideaofforminganembeddingforasymbolwasextendedtotheideaofan embeddingforawordbyDeerwester1990 e t a l .().Theseembeddingswerelearned usingtheSVD.Later,embeddingswouldbelearnedbyneuralnetworks. Thehistoryofnaturallanguageprocessingismarkedbytransitionsinthe popularityofdiÔ¨Äerentwaysofrepresentingtheinputtothemodel.Following thisearlyworkonsymbolsorwords,someoftheearliestapplicationsofneural networkstoNLP( ,; Miikkulainen andDyer1991Schmidhuber1996,)represented theinputasasequenceofcharacters. Bengio2001 e t a l .()returnedthefocustomodelingwordsandintroduced neurallanguagemodels,whichproduceinterpretable wordembeddings.These neuralmodelshavescaledupfromdeÔ¨Åningrepresentationsofasmallsetofsymbols inthe1980stomillionsofwords(includingpropernounsandmisspellings)in modernapplications.Thiscomputational scalingeÔ¨Äortledtotheinventionofthe techniquesdescribedaboveinsection.12.4.3 Initially,theuseofwordsasthefundamentalunitsoflanguagemodelsyielded improvedlanguage modeling performance( ,).Tothisday, Bengio e t a l .2001 newtechniquescontinuallypushbothcharacter-based models(Sutskever e t a l ., 2011)andword-basedmodelsforward,withrecentwork( ,)even Gillick e t a l .2015 modelingindividualbytesofUnicodecharacters. Theideasbehindneurallanguagemodelshavebeenextendedintoseveral naturallanguageprocessingapplications,suchasparsing(,,; Henderson20032004 Collobert2011,),part-of-speechtagging,semanticrolelabeling,chunking,etc, sometimesusingasinglemulti-tasklearningarchitecture(CollobertandWeston, 2008aCollobert2011a ; e t a l .,)inwhichthewordembeddingsaresharedacross tasks. Two-dimensionalvisualizationsofembeddingsbecameapopulartoolforan- alyzinglanguagemodelsfollowingthedevelopmentofthet-SNEdimensionality reductionalgorithm(vanderMaatenandHinton2008,)anditshigh-proÔ¨Åleappli- cationtovisualizationwordembeddingsbyJosephTurianin2009. 4 7 7 CHAPTER12.APPLICATIONS 12. 5 O t h er A p p l i c a t i o n s Inthissectionwecoverafewothertypesofapplicationsofdeeplearningthat arediÔ¨Äerentfromthestandardobjectrecognition,speechrecognitionandnatural languageprocessingtasksdiscussedabove.Partofthisbookwillexpandthat III scopeevenfurthertotasksthatremainprimarilyresearchareas. 12.5.1RecommenderSystems Oneofthemajorfamiliesofapplicationsofmachinelearningintheinformation technologysectoristheabilitytomakerecommendations ofitemstopotential usersorcustomers.Twomajortypesofapplicationscanbedistinguished:online advertisinganditemrecommendations (oftentheserecommendations arestillfor thepurposeofsellingaproduct).Bothrelyonpredictingtheassociationbetween auserandanitem,eithertopredicttheprobabilityofsomeaction(theuser buyingtheproduct,orsomeproxyforthisaction)ortheexpectedgain(which maydependonthevalueoftheproduct)ifanadisshownorarecommendation is maderegardingthatproducttothatuser.TheinternetiscurrentlyÔ¨Ånancedin greatpartbyvariousformsofonlineadvertising. Therearemajorpartsofthe economythatrelyononlineshopping. CompaniesincludingAmazonandeBay usemachinelearning,includingdeeplearning,fortheirproductrecommendations . Sometimes,theitemsarenotproductsthatareactuallyforsale.Examplesinclude selectingpoststodisplayonsocialnetworknewsfeeds,recommendingmoviesto watch,recommendingjokes,recommendingadvicefromexperts,matchingplayers forvideogames,ormatchingpeopleindatingservices. Often,thisassociationproblemishandledlikeasupervisedlearningproblem: givensomeinformationabouttheitemandabouttheuser,predicttheproxyof interest(userclicksonad,userentersarating,userclicksona‚Äúlike‚Äùbutton,user buysproduct,userspendssomeamountofmoneyontheproduct,userspends timevisitingapagefortheproduct,etc).Thisoftenendsupbeingeithera regressionproblem(predictingsomeconditionalexpectedvalue)oraprobabilistic classiÔ¨Åcationproblem(predictingtheconditionalprobabilityofsomediscrete event). Theearlyworkonrecommendersystemsreliedonminimalinformationas inputsforthesepredictions:theuserIDandtheitemID.Inthiscontext,the onlywaytogeneralizeistorelyonthesimilaritybetweenthepatternsofvaluesof thetargetvariablefordiÔ¨ÄerentusersorfordiÔ¨Äerentitems.Supposethatuser1 anduser2bothlikeitemsA,BandC.Fromthis,wemayinferthatuser1and 4 7 8 CHAPTER12.APPLICATIONS user2havesimilartastes.Ifuser1likesitemD,thenthisshouldbeastrong cuethatuser2willalsolikeD.Algorithmsbasedonthisprinciplecomeunder thenameofcollaborativeÔ¨Åltering.Bothnon-parametric approaches(suchas nearest-neighbormethodsbasedontheestimatedsimilaritybetweenpatternsof preferences)andparametricmethodsarepossible.Parametricmethodsoftenrely onlearningadistributedrepresentation(alsocalledanembedding)foreachuser andforeachitem.Bilinearpredictionofthetargetvariable(suchasarating)isa simpleparametricmethodthatishighlysuccessfulandoftenfoundasacomponent ofstate-of-the-art systems.Thepredictionisobtainedbythedotproductbetween theuserembeddingandtheitemembedding(possiblycorrectedbyconstantsthat dependonlyoneithertheuserIDortheitemID).LetÀÜRbethematrixcontaining ourpredictions,AamatrixwithuserembeddingsinitsrowsandBamatrixwith itemembeddingsinitscolumns.Letbandcbevectorsthatcontainrespectively akindofbiasforeachuser(representinghowgrumpyorpositivethatuseris ingeneral)andforeachitem(representingitsgeneralpopularity).Thebilinear predictionisthusobtainedasfollows: ÀÜ R u , i= b u+ c i+ÓÅò jA u , j B j , i . (12.20) Typicallyonewantstominimizethesquarederrorbetweenpredictedratings ÀÜ R u , iandactualratings R u , i.Userembeddingsanditemembeddingscanthenbe convenientlyvisualizedwhentheyareÔ¨Årstreducedtoalowdimension(twoor three),ortheycanbeusedtocompareusersoritemsagainsteachother,just likewordembeddings. One waytoobtaintheseembeddingsisbyperforminga singularvaluedecompositionofthematrixRofactualtargets(suchasratings). ThiscorrespondstofactorizingR=UDVÓÄ∞(oranormalizedvariant)intothe productoftwofactors,thelowerrankmatricesA=UDandB=VÓÄ∞.One problemwiththeSVDisthatittreatsthemissingentriesinanarbitraryway, asiftheycorrespondedtoatargetvalueof0.Insteadwewouldliketoavoid payinganycostforthepredictionsmadeonmissingentries.Fortunately,thesum ofsquarederrorsontheobservedratingscanalsobeeasilyminimizedbygradient- basedoptimization. TheSVDandthebilinearpredictionofequation both12.20 performedverywellinthecompetitionfortheNetÔ¨Çixprize( , BennettandLanning 2007),aimingatpredictingratingsforÔ¨Ålms,basedonlyonpreviousratingsby alargesetofanonymoususers. Manymachinelearningexpertsparticipatedin thiscompetition,whichtookplacebetween2006and2009.Itraisedthelevelof researchinrecommendersystemsusingadvancedmachinelearningandyielded improvementsinrecommendersystems.Eventhoughitdidnotwinbyitself, thesimplebilinearpredictionorSVDwasacomponentoftheensemblemodels 4 7 9 CHAPTER12.APPLICATIONS presentedbymostofthecompetitors,includingthewinners( ,; T√∂scher e t a l .2009 Koren2009,). Beyondthesebilinearmodelswithdistributedrepresentations,oneoftheÔ¨Årst usesofneuralnetworksforcollaborativeÔ¨ÅlteringisbasedontheRBMundirected probabilisticmodel(Salakhutdinov2007 e t a l .,).RBMswereanimportantelement oftheensembleofmethodsthatwontheNetÔ¨Çixcompetition(T√∂scher2009 e t a l .,; Koren2009,).Moreadvancedvariantsontheideaoffactorizingtheratingsmatrix havealsobeenexploredintheneuralnetworkscommunity(Salakhutdinovand Mnih2008,). However,thereisabasiclimitationofcollaborativeÔ¨Ålteringsystems:whena newitemoranewuserisintroduced,itslackofratinghistorymeansthatthere isnowaytoevaluateitssimilaritywithotheritemsorusers(respectively),or thedegreeofassociationbetween,say,thatnewuserandexistingitems.This iscalledtheproblemofcold-startrecommendations .Ageneralwayofsolving thecold-startrecommendation problemistointroduceextrainformationabout theindividualusersanditems.Forexample,thisextrainformationcouldbeuser proÔ¨Åleinformationorfeaturesofeachitem. Systems thatusesuchinformation arecalledcontent-basedrecommendersystems.Themappingfromarich setofuserfeaturesoritemfeaturestoanembeddingcanbelearnedthrougha</div>
        </div>
    </div>

    <div class="question-card" id="q76">
        <div class="question-header">
            <span class="question-number">Question 76</span>
            <span class="question-type">multiple-choice</span>
        </div>

        <div class="question-text">Hyperparameter optimization is a crucial process in building high-performing machine learning models. Several strategies exist, each with trade-offs in efficiency, coverage, and computational cost.

Which approach to hyperparameter optimization is designed to balance exploration and exploitation using probabilistic modeling of validation error and uncertainty?

1) Grid search   
2) Random search   
3) Early stopping   
4) Cross-validation   
5) Bayesian optimization   
6) Ensemble averaging   
7) Manual tuning</div>

        <div class="answer-section">
            <div class="answer-label">‚úì Correct Answer:</div>
            <div class="answer-text">The correct answer is 5) Bayesian optimization.</div>
        </div>

        <div class="reference-section">
            <div class="reference-label">üìö Reference Text:</div>
            <button class="toggle-button" onclick="toggleReference(76)">
                Show/Hide Reference
            </button>
            <div id="ref76" class="reference-text hidden">ri d S ea rch Whentherearethreeorfewerhyperparameters ,thecommonpracticeistoperform g r i d se ar c h.Foreachhyperparameter, the userselectsasmallÔ¨Ånitesetof valuestoexplore.Thegridsearchalgorithmthentrainsamodelforeveryjoint speciÔ¨ÅcationofhyperparametervaluesintheCartesianproductofthesetofvalues foreachindividualhyperparameter.Theexperimentthatyieldsthebestvalidation 4 3 2 CHAPTER11.PRACTICALMETHODOLOGY Grid Random Figure11.2:Comparisonofgridsearchandrandomsearch.Forillustrationpurposeswe displaytwohyperparametersbutwearetypicallyinterestedinhavingmanymore. ( L e f t )To performgridsearch,weprovideasetofvaluesforeachhyperparameter.Thesearch algorithmrunstrainingforeveryjointhyperparametersettinginthecrossproductofthese sets.Toperformrandomsearch,weprovideaprobabilitydistributionoverjoint ( R i g h t ) hyperparameterconÔ¨Ågurations.Usuallymostofthesehyperparametersareindependent fromeachother.Commonchoicesforthedistributionoverasinglehyperparameterinclude uniformandlog-uniform(tosamplefromalog-uniformdistribution,taketheexpofa samplefromauniformdistribution).Thesearchalgorithmthenrandomlysamplesjoint hyperparameterconÔ¨Ågurationsandrunstrainingwitheachofthem.Bothgridsearch andrandomsearchevaluatethevalidationseterrorandreturnthebestconÔ¨Åguration. TheÔ¨ÅgureillustratesthetypicalcasewhereonlysomehyperparametershaveasigniÔ¨Åcant inÔ¨Çuenceontheresult.Inthisillustration,onlythehyperparameteronthehorizontalaxis hasasigniÔ¨ÅcanteÔ¨Äect.Gridsearchwastesanamountofcomputationthatisexponential inthenumberofnon-inÔ¨Çuentialhyperparameters,whilerandomsearchtestsaunique valueofeveryinÔ¨Çuentialhyperparameteronnearlyeverytrial.Figurereproducedwith permissionfrom (). BergstraandBengio2012 4 3 3 CHAPTER11.PRACTICALMETHODOLOGY seterroristhenchosenashavingfoundthebesthyperparameters .Seetheleftof Ô¨Ågureforanillustrationofagridofhyperparameter values. 11.2 Howshouldthelistsofvaluestosearchoverbechosen?Inthecaseofnumerical (ordered)hyperparameters ,thesmallestandlargestelementofeachlistischosen conservatively,basedonpriorexperiencewithsimilarexperiments,tomakesure thattheoptimalvalueisverylikelytobeintheselectedrange.Typically,agrid searchinvolvespickingvaluesapproximately onalogarithmicscale,e.g.,alearning ratetakenwithintheset{ .1 , .01 ,10‚àí3,10‚àí4,10‚àí5},oranumberofhiddenunits takenwiththeset . { } 501002005001000 2000 , , , , , Gridsearchusuallyperformsbestwhenitisperformedrepeatedly.Forexample, supposethatweranagridsearchoverahyperparameter Œ±usingvaluesof{‚àí1 ,0 ,1}. Ifthebestvaluefoundis,thenweunderestimatedtherangeinwhichthebest 1 Œ± liesandweshouldshiftthegridandrunanothersearchwith Œ±in,forexample, {1 ,2 ,3}.IfweÔ¨Åndthatthebestvalueof Œ±is,thenwemaywishtoreÔ¨Åneour 0 estimatebyzoominginandrunningagridsearchover. {‚àí } . , , .101 Theobviousproblemwithgridsearchisthatitscomputational costgrows exponentiallywiththenumberofhyperparameters .Ifthereare mhyperparameters, eachtakingatmost nvalues,thenthenumberoftrainingandevaluationtrials requiredgrowsas O( nm).Thetrialsmayberuninparallelandexploitloose parallelism(withalmostnoneedforcommunication betweendiÔ¨Äerentmachines carryingoutthesearch)Unfortunately,duetotheexponentialcostofgridsearch, evenparallelization maynotprovideasatisfactorysizeofsearch. 1 1 . 4 . 4 Ra n d o m S ea rch Fortunately,thereisanalternativetogridsearchthatisassimpletoprogram,more convenienttouse,andconvergesmuchfastertogoodvaluesofthehyperparameters : randomsearch( ,). BergstraandBengio2012 Arandomsearchproceedsasfollows.FirstwedeÔ¨Åneamarginaldistribution foreachhyperparameter, e.g.,aBernoulliormultinoulliforbinaryordiscrete hyperparameters,orauniformdistributiononalog-scaleforpositivereal-valued hyperparameters.Forexample, l o g l e a r n i n g r a t e __ ‚àº‚àí‚àí u(1 ,5) (11.2) l e a r n i n g r a t e_ = 10loglearningrate _ _. (11.3) where u( a , b)indicatesasampleoftheuniformdistributionintheinterval( a , b). Similarlythe l o g n u m b e r o f h i d d e n u n i t s ____maybesampledfrom u(log(50) , log(2000) ). 4 3 4 CHAPTER11.PRACTICALMETHODOLOGY Unlikeinthecaseofagridsearch,oneshouldnotdiscretizeorbinthevalues ofthehyperparameters.Thisallowsonetoexplorealargersetofvalues,anddoes notincuradditionalcomputational cost. Infact,asillustratedinÔ¨Ågure,a11.2 randomsearchcanbeexponentiallymoreeÔ¨Écientthanagridsearch,whenthere areseveralhyperparametersthatdonotstronglyaÔ¨Äecttheperformancemeasure. Thisisstudiedatlengthin (),whofoundthatrandom BergstraandBengio2012 searchreducesthevalidationseterrormuchfasterthangridsearch,intermsof thenumberoftrialsrunbyeachmethod. Aswithgridsearch,onemayoftenwanttorunrepeatedversionsofrandom search,toreÔ¨ÅnethesearchbasedontheresultsoftheÔ¨Årstrun. ThemainreasonwhyrandomsearchÔ¨Åndsgoodsolutionsfasterthangridsearch isthattherearenowastedexperimentalruns,unlikeinthecaseofgridsearch, whentwovaluesofahyperparameter(givenvaluesoftheotherhyperparameters ) wouldgivethesameresult.Inthecaseofgridsearch,theotherhyperparameters wouldhavethesamevaluesforthesetworuns,whereaswithrandomsearch,they wouldusuallyhavediÔ¨Äerentvalues.Henceifthechangebetweenthesetwovalues doesnotmarginallymakemuchdiÔ¨Äerenceintermsofvalidationseterror,grid searchwillunnecessarilyrepeattwoequivalentexperimentswhilerandomsearch willstillgivetwoindependentexplorationsoftheotherhyperparameters . 1 1 . 4 . 5 Mo d el - B a s ed Hyp erp a ra m et er O p t i m i za t i o n Thesearchforgoodhyperparameters canbecastasanoptimization problem. Thedecisionvariablesarethehyperparameters.Thecosttobeoptimizedisthe validationseterrorthatresultsfromtrainingusingthesehyperparameters .In simpliÔ¨ÅedsettingswhereitisfeasibletocomputethegradientofsomediÔ¨Äerentiable errormeasureonthevalidationsetwithrespecttothehyperparameters ,wecan simplyfollowthisgradient( ,;,; , Bengioetal.1999Bengio2000Maclaurin etal. 2015).Unfortunately,inmostpracticalsettings,thisgradientisunavailable,either duetoitshighcomputationandmemorycost,orduetohyperparametershaving intrinsicallynon-diÔ¨Äerentiable interactionswiththevalidationseterror,asinthe caseofdiscrete-valuedhyperparameters . Tocompensateforthislackofagradient,wecanbuildamodelofthevalidation seterror,thenproposenewhyperparameterguessesbyperformingoptimization withinthismodel.Mostmodel-basedalgorithmsforhyperparameter searchusea Bayesianregressionmodeltoestimateboththeexpectedvalueofthevalidationset errorforeachhyperparameterandtheuncertaintyaroundthisexpectation.Opti- mizationthusinvolvesatradeoÔ¨Äbetweenexploration(proposinghyperparameters 4 3 5 CHAPTER11.PRACTICALMETHODOLOGY forwhichthereishighuncertainty,whichmayleadtoalargeimprovementbutmay alsoperformpoorly)andexploitation(proposinghyperparameters whichthemodel isconÔ¨Ådentwillperformaswellasanyhyperparameters ithasseensofar‚Äîusually hyperparametersthatareverysimilartoonesithasseenbefore).Contemporary approachestohyperparameter optimizationincludeSpearmint(,), Snoeketal.2012 TPE( ,)andSMAC( ,). Bergstraetal.2011 Hutteretal.2011 Currently,wecannotunambiguously recommendBayesianhyperparameter optimization asanestablishedtoolforachievingbetterdeeplearningresultsor forobtainingthoseresultswithlesseÔ¨Äort.Bayesianhyperparameteroptimization sometimesperformscomparablytohumanexperts,sometimesbetter,butfails catastrophicallyonotherproblems.Itmaybeworthtryingtoseeifitworkson aparticularproblembutisnotyetsuÔ¨Écientlymatureorreliable.Thatbeing said,hyperparameter optimization isanimportantÔ¨Åeldofresearchthat,while oftendrivenprimarilybytheneedsofdeeplearning,holdsthepotentialtobeneÔ¨Åt notonlytheentireÔ¨Åeldofmachinelearningbutthedisciplineofengineeringin general. Onedrawbackcommontomosthyperparameter optimization algorithmswith moresophisticationthanrandomsearchisthattheyrequireforatrainingex- perimenttoruntocompletionbeforetheyareabletoextractanyinformation fromtheexperiment.ThisismuchlesseÔ¨Écient,inthesenseofhowmuchinfor- mationcanbegleanedearlyinanexperiment,thanmanualsearchbyahuman practitioner,sinceonecanusuallytellearlyonifsomesetofhyperparameters is completelypathological. ()haveintroducedanearlyversion Swerskyetal.2014 ofanalgorithmthatmaintainsasetofmultipleexperiments.Atvarioustime points,thehyperparameter optimization algorithmcanchoosetobeginanew experiment,to‚Äúfreeze‚Äùarunningexperimentthatisnotpromising,orto‚Äúthaw‚Äù andresumeanexperimentthatwasearlierfrozenbutnowappearspromisinggiven moreinformation. 11.5DebuggingStrategies Whenamachinelearningsystemperformspoorly,itisusuallydiÔ¨Éculttotell whetherthepoorperformanceisintrinsictothealgorithmitselforwhetherthere isabugintheimplementation ofthealgorithm. Machine learningsystemsare diÔ¨Éculttodebugforavarietyofreasons. Inmostcases,wedonotknowaprioriwhattheintendedbehaviorofthe algorithmis.Infact,theentirepointofusingmachinelearningisthatitwill discoverusefulbehaviorthatwewerenotabletospecifyourselves.Ifwetraina 4 3 6 CHAPTER11.PRACTICALMETHODOLOGY neuralnetworkonaclassiÔ¨Åcationtaskanditachieves5%testerror,wehave new nostraightforwardwayofknowingifthisistheexpectedbehaviororsub-optimal behavior. AfurtherdiÔ¨Écultyisthatmostmachinelearningmodelshavemultipleparts thatareeachadaptive.Ifonepartisbroken,theotherpartscanadaptandstill achieveroughlyacceptableperformance.Forexample,supposethatwearetraining aneuralnetwithseverallayersparametrized byweights Wandbiases b.Suppose furtherthatwehavemanuallyimplemented thegradientdescentruleforeach parameterseparately,andwemadeanerrorintheupdateforthebiases: b b‚Üê‚àí Œ± (11.4) where Œ±isthelearningrate.Thiserroneousupdatedoesnotusethegradientat all.Itcausesthebiasestoconstantlybecomenegativethroughoutlearning,which isclearlynotacorrectimplementation ofanyreasonablelearningalgorithm.The bugmaynotbeapparentjustfromexaminingtheoutputofthemodelthough. Dependingonthedistributionoftheinput,theweightsmaybeabletoadaptto compensateforthenegativebiases. Mostdebuggingstrategiesforneuralnetsaredesignedtogetaroundoneor bothofthesetwodiÔ¨Éculties.Eitherwedesignacasethatissosimplethatthe correctbehavioractuallycanbepredicted,orwedesignatestthatexercisesone partoftheneuralnetimplementationinisolation. Someimportantdebuggingtestsinclude: Visualizethemodelinaction:Whentrainingamodeltodetectobjectsin images,viewsomeimageswiththedetectionsproposedbythemodeldisplayed superimposedontheimage.Whentrainingagenerativemodelofspeech,listento someofthespeechsamplesitproduces.Thismayseemobvious,butitiseasyto fallintothepracticeofonlylookingatquantitativeperformancemeasurements likeaccuracyorlog-likelihood.Directlyobservingthemachinelearningmodel performingitstaskwillhelptodeterminewhetherthequantitativeperformance numbersitachievesseemreasonable.Evaluationbugscanbesomeofthemost devastatingbugsbecausetheycanmisleadyouintobelievingyoursystemis performingwellwhenitisnot. Visualizetheworstmistakes: Mostmodelsareabletooutputsomesortof conÔ¨Ådencemeasureforthetasktheyperform.Forexample,classiÔ¨Åersbasedona softmaxoutputlayerassignaprobabilitytoeachclass.Theprobabilityassigned tothemostlikelyclassthusgivesanestimateoftheconÔ¨Ådencethemodelhasin itsclassiÔ¨Åcationdecision.Typically,maximumlikelihoodtrainingresultsinthese valuesbeingoverestimatesratherthanaccurateprobabilitiesofcorrectprediction, 4 3 7 CHAPTER11.PRACTICALMETHODOLOGY buttheyaresomewhatusefulinthesensethatexamplesthatareactuallyless likelytobecorrectlylabeledreceivesmallerprobabilities underthemodel.By viewingthetrainingsetexamplesthatarethehardesttomodelcorrectly,onecan oftendiscoverproblemswiththewaythedatahasbeenpreprocessedorlabeled. Forexample,theStreetViewtranscriptionsystemoriginallyhadaproblemwhere theaddressnumberdetectionsystemwouldcroptheimagetootightlyandomit someofthedigits.Thetranscriptionnetworkthenassignedverylowprobability tothecorrectanswerontheseimages.Sortingtheimagestoidentifythemost conÔ¨Ådentmistakesshowedthattherewasasystematicproblemwiththecropping. Modifyingthedetectionsystemtocropmuchwiderimagesresultedinmuchbetter performanceoftheoverallsystem,eventhoughthetranscriptionnetworkneeded tobeabletoprocessgreatervariationinthepositionandscaleoftheaddress numbers. Reasoningaboutsoftwareusingtrainandtesterror:ItisoftendiÔ¨Écultto determinewhethertheunderlyingsoftwareiscorrectlyimplemented. Someclues canbeobtainedfromthetrainandtesterror.Iftrainingerrorislowbuttesterror ishigh,thenitislikelythatthatthetrainingprocedureworkscorrectly,andthe modelisoverÔ¨Åttingforfundamentalalgorithmicreasons.Analternativepossibility isthatthetesterrorismeasuredincorrectlyduetoaproblemwithsavingthe modelaftertrainingthenreloadingitfortestsetevaluation,orifthetestdata wasprepareddiÔ¨Äerentlyfromthetrainingdata.Ifbothtrainandtesterrorare high,thenitisdiÔ¨Éculttodeterminewhetherthereisasoftwaredefectorwhether themodelisunderÔ¨Åttingduetofundamentalalgorithmicreasons.Thisscenario requiresfurthertests,describednext. Fitatinydataset:Ifyouhavehigherroronthetrainingset,determinewhether itisduetogenuineunderÔ¨Åttingorduetoasoftwaredefect.Usuallyevensmall modelscanbeguaranteedtobeableÔ¨ÅtasuÔ¨Écientlysmalldataset.Forexample, aclassiÔ¨ÅcationdatasetwithonlyoneexamplecanbeÔ¨Åtjustbysettingthebiases oftheoutputlayercorrectly.UsuallyifyoucannottrainaclassiÔ¨Åertocorrectly labelasingleexample,anautoencodertosuccessfullyreproduceasingleexample withhighÔ¨Ådelity,oragenerativemodeltoconsistentlyemitsamplesresemblinga singleexample,thereisasoftwaredefectpreventingsuccessfuloptimization onthe trainingset.Thistestcanbeextendedtoasmalldatasetwithfewexamples. Compareback-propagatedderivativestonumericalderivatives:Ifyouareusing asoftwareframeworkthatrequiresyoutoimplementyourowngradientcom- putations,orifyouareaddinganewoperationtoadiÔ¨Äerentiation libraryand mustdeÔ¨Åneitsbpropmethod,thenacommonsourceoferrorisimplementingthis gradientexpressionincorrectly.Onewaytoverifythatthesederivativesarecorrect 4 3 8 CHAPTER11.PRACTICALMETHODOLOGY istocomparethederivativescomputedbyyourimplementation ofautomatic diÔ¨Äerentiationtothederivativescomputedbya .Because Ô¨Åni t e di Ô¨Ä e r e nc e s fÓÄ∞() =lim x ÓÄè ‚Üí0f x ÓÄè f</div>
        </div>
    </div>

    <div class="question-card" id="q77">
        <div class="question-header">
            <span class="question-number">Question 77</span>
            <span class="question-type">multiple-choice</span>
        </div>

        <div class="question-text">Regularization techniques such as L1 and L2 are commonly used in machine learning to improve model generalization and control overfitting. These methods interact with the optimization landscape in distinct ways, affecting parameters based on the curvature and dimensionality of the problem.

In the context of weight decay (L2 regularization) applied to a quadratic loss function, which statement best describes the effect on model parameters along directions defined by the Hessian's eigenvectors?

1) Parameters are scaled equally regardless of their corresponding eigenvalues.   
2) Parameters associated with large Hessian eigenvalues are shrunk more than those with small eigenvalues.   
3) Parameters along directions with small Hessian eigenvalues are shrunk more aggressively than those with large eigenvalues.   
4) All parameters are set exactly to zero due to regularization.   
5) Regularization only affects parameters with positive values, leaving negative parameters unchanged.   
6) Regularization increases all eigenvalues of the Hessian matrix by subtracting the regularization strength.   
7) Parameters along directions with large Hessian eigenvalues are unaffected by regularization.</div>

        <div class="answer-section">
            <div class="answer-label">‚úì Correct Answer:</div>
            <div class="answer-text">The correct answer is 3) Parameters along directions with small Hessian eigenvalues are shrunk more aggressively than those with large eigenvalues..</div>
        </div>

        <div class="reference-section">
            <div class="reference-label">üìö Reference Text:</div>
            <button class="toggle-button" onclick="toggleReference(77)">
                Show/Hide Reference
            </button>
            <div id="ref77" class="reference-text hidden">b u t b e t t e r re s u l t s will b e o b t a i n e d f o r a v a l u e c l o s e r t o t h e t ru e o n e , with z e ro b e i n g a d e f a u l t v a l u e t h a t m a k e s s e n s e wh e n we d o n o t k n o w i f t h e c o rre c t v a l u e s h o u l d b e p o s i t i v e o r n e g a t i v e . S i n c e i t i s f a r m o re c o m m o n t o re g u l a riz e t h e m o d e l p a ra m e t e rs t o w a rd s z e ro , w e will f o c u s o n t h i s s p e c i a l c a s e i n o u r e x p o s i t i o n . 2 3 1 CHAPTER7.REGULARIZATIONFORDEEPLEARNING meansquarederror,thentheapproximationisperfect.Theapproximation ÀÜ Jis givenby ÀÜ J J () = Œ∏ (w‚àó)+1 2(ww‚àí‚àó)ÓÄæHww (‚àí‚àó) , (7.6) whereHistheHessianmatrixof Jwithrespecttowevaluatedatw‚àó.Thereis noÔ¨Årst-orderterminthisquadraticapproximation, becausew‚àóisdeÔ¨Ånedtobea minimum,wherethegradientvanishes.Likewise,becausew‚àóisthelocationofa minimumof,wecanconcludethatispositivesemideÔ¨Ånite. J H TheminimumofÀÜ Joccurswhereitsgradient ‚àá wÀÜ J() = (wHww‚àí‚àó) (7.7) isequalto. 0 TostudytheeÔ¨Äectofweightdecay,wemodifyequationbyaddingthe 7.7 weightdecaygradient.Wecannowsolvefortheminimumoftheregularized versionofÀÜ J.Weusethevariable Àúwtorepresentthelocationoftheminimum. Œ±ÀúwH+ (Àúww‚àí‚àó) = 0 (7.8) (+ )H Œ±IÀúwHw = ‚àó(7.9) ÀúwHI = (+ Œ±)‚àí 1Hw‚àó. (7.10) As Œ±approaches0,theregularizedsolution Àúwapproachesw‚àó.Butwhat happensas Œ±grows?BecauseHisrealandsymmetric,wecandecomposeit intoadiagonalmatrix Œõandanorthonormal basisofeigenvectors,Q,suchthat HQQ = ŒõÓÄæ.Applyingthedecompositiontoequation,weobtain:7.10 ÀúwQQ = ( ŒõÓÄæ+ ) Œ±I‚àí 1QQ ŒõÓÄæw‚àó(7.11) =ÓÅ® QIQ (+ Œõ Œ±)ÓÄæÓÅ©‚àí 1 QQ ŒõÓÄæw‚àó(7.12) = (+ )Q Œõ Œ±I‚àí 1ŒõQÓÄæw‚àó. (7.13) WeseethattheeÔ¨Äectofweightdecayistorescalew‚àóalongtheaxesdeÔ¨Ånedby theeigenvectorsofH.SpeciÔ¨Åcally,thecomponentofw‚àóthatisalignedwiththe i-theigenvectorofHisrescaledbyafactorofŒª i Œª i + Œ±.(Youmaywishtoreview howthiskindofscalingworks,Ô¨ÅrstexplainedinÔ¨Ågure).2.3 AlongthedirectionswheretheeigenvaluesofHarerelativelylarge,forexample, where Œª iÓÄù Œ±,theeÔ¨Äectofregularizationisrelativelysmall.However,components with Œª iÓÄú Œ±willbeshrunktohavenearlyzeromagnitude.ThiseÔ¨Äectisillustrated inÔ¨Ågure.7.1 2 3 2 CHAPTER7.REGULARIZATIONFORDEEPLEARNING w 1w 2w‚àó Àú w Figure7.1:AnillustrationoftheeÔ¨Äectof L2(orweightdecay)regularizationonthevalue oftheoptimalw.Thesolidellipsesrepresentcontoursofequalvalueoftheunregularized objective.Thedottedcirclesrepresentcontoursofequalvalueofthe L2regularizer.At thepointÀúw,thesecompetingobjectivesreachanequilibrium.IntheÔ¨Årstdimension,the eigenvalueoftheHessianof Jissmall. Theobjectivefunctiondoesnotincreasemuch whenmovinghorizontallyawayfromw‚àó.Becausetheobjectivefunctiondoesnotexpress astrongpreferencealongthisdirection,theregularizerhasastrongeÔ¨Äectonthisaxis. Theregularizerpulls w1closetozero.Intheseconddimension,theobjectivefunction isverysensitivetomovementsawayfromw‚àó.Thecorrespondingeigenvalueislarge, indicatinghighcurvature.Asaresult,weightdecayaÔ¨Äectsthepositionof w2relatively little. OnlydirectionsalongwhichtheparameterscontributesigniÔ¨Åcantlytoreducing theobjectivefunctionarepreservedrelativelyintact.Indirectionsthatdonot contributetoreducingtheobjectivefunction,asmalleigenvalueoftheHessian tellsusthatmovementinthisdirectionwillnotsigniÔ¨Åcantlyincreasethegradient. Componentsoftheweightvectorcorrespondingtosuchunimportant directions aredecayedawaythroughtheuseoftheregularizationthroughouttraining. SofarwehavediscussedweightdecayintermsofitseÔ¨Äectontheoptimization ofanabstract,general,quadraticcostfunction.HowdotheseeÔ¨Äectsrelateto machinelearninginparticular?WecanÔ¨Åndoutbystudyinglinearregression,a modelforwhichthetruecostfunctionisquadraticandthereforeamenabletothe samekindofanalysiswehaveusedsofar.Applyingtheanalysisagain,wewill beabletoobtainaspecialcaseofthesameresults,butwiththesolutionnow phrasedintermsofthetrainingdata.Forlinearregression,thecostfunctionis 2 3 3 CHAPTER7.REGULARIZATIONFORDEEPLEARNING thesumofsquarederrors: ( )Xwy‚àíÓÄæ( )Xwy‚àí . (7.14) Whenweadd L2regularization, theobjectivefunctionchangesto ( )Xwy‚àíÓÄæ( )+Xwy‚àí1 2Œ±wÓÄæw . (7.15) Thischangesthenormalequationsforthesolutionfrom wX= (ÓÄæX)‚àí 1XÓÄæy (7.16) to wX= (ÓÄæXI+ Œ±)‚àí 1XÓÄæy . (7.17) ThematrixXÓÄæXinequationisproportionaltothecovariancematrix 7.161 mXÓÄæX. Using L2regularizationreplacesthismatrixwithÓÄÄ XÓÄæXI+ Œ±ÓÄÅ‚àí 1inequation.7.17 Thenewmatrixisthesameastheoriginalone,butwiththeadditionof Œ±tothe diagonal.Thediagonalentriesofthismatrixcorrespondtothevarianceofeach inputfeature.Wecanseethat L2regularizationcausesthelearningalgorithm to‚Äúperceive‚ÄùtheinputXashavinghighervariance,whichmakesitshrinkthe weightsonfeatureswhosecovariancewiththeoutputtargetislowcomparedto thisaddedvariance. 7 . 1 . 2 L1Regu l a ri z a t i o n While L2weightdecayisthemostcommonformofweightdecay,thereareother waystopenalizethesizeofthemodelparameters. Anotheroptionistouse L1 regularization. Formally, L1regularizationonthemodelparameter isdeÔ¨Ånedas:w ‚Ñ¶() = Œ∏ ||||w 1=ÓÅò i| w i| , (7.18) thatis,asthesumofabsolutevaluesoftheindividualparameters.2Wewill nowdiscusstheeÔ¨Äectof L1regularizationonthesimplelinearregressionmodel, withnobiasparameter,thatwestudiedinouranalysisof L2regularization. In particular,weareinterestedindelineatingthediÔ¨Äerencesbetween L1and L2forms 2As with L2re g u l a riz a t i o n , w e c</div>
        </div>
    </div>

    <div class="question-card" id="q78">
        <div class="question-header">
            <span class="question-number">Question 78</span>
            <span class="question-type">multiple-choice</span>
        </div>

        <div class="question-text">Optimization techniques and performance evaluation are central to developing effective machine learning algorithms. Understanding the mathematical foundations and appropriate metrics is essential for tackling diverse machine learning tasks.

Which machine learning task involves modeling the underlying probability distribution of data, often used for generative modeling and anomaly detection?

1) Density estimation   
2) Classification   
3) Regression   
4) Transcription   
5) Machine translation   
6) Structured output prediction   
7) Denoising</div>

        <div class="answer-section">
            <div class="answer-label">‚úì Correct Answer:</div>
            <div class="answer-text">The correct answer is 1) Density estimation.</div>
        </div>

        <div class="reference-section">
            <div class="reference-label">üìö Reference Text:</div>
            <button class="toggle-button" onclick="toggleReference(78)">
                Show/Hide Reference
            </button>
            <div id="ref78" class="reference-text hidden">i t hm 4 . 1Analgorithmtominimize f( x) =1 2||‚àí|| A x b2 2withrespectto x usinggradientdescent,startingfromanarbitraryvalueof. x Setthestepsize()andtolerance()tosmall,positivenumbers. ÓÄè Œ¥ whi l e|| AÓÄæA x A‚àíÓÄæb|| 2 > Œ¥ do x x‚Üê ‚àí ÓÄèÓÄÄ AÓÄæA x A‚àíÓÄæbÓÄÅ e nd whi l e OnecanalsosolvethisproblemusingNewton‚Äôsmethod.Inthiscase,because thetruefunctionisquadratic,thequadraticapproximation employedbyNewton‚Äôs methodisexact,andthealgorithmconvergestotheglobalminimuminasingle step. Nowsuppose we wishto minimizethesame function,butsubjectto the constraint xÓÄæx‚â§1.Todoso,weintroducetheLagrangian L , Œª f Œª ( x) = ()+ xÓÄê xÓÄæx‚àí1ÓÄë . (4.23) Wecannowsolvetheproblem min xmax Œª , Œª ‚â• 0L , Œª . ( x) (4.24) 9 6 CHAPTER4.NUMERICALCOMPUTATION Thesmallest-normsolutiontotheunconstrainedleastsquaresproblemmaybe foundusingtheMoore-Penrosepseudoinverse: x= A+b.Ifthispointisfeasible, thenitisthesolutiontotheconstrainedproblem.Otherwise,wemustÔ¨Ånda solutionwheretheconstraintisactive.BydiÔ¨Äerentiating theLagrangianwith respectto,weobtaintheequation x AÓÄæA x A‚àíÓÄæb x+2 Œª= 0 . (4.25) Thistellsusthatthesolutionwilltaketheform x A= (ÓÄæA I+2 Œª)‚àí 1AÓÄæb . (4.26) Themagnitudeof Œªmustbechosensuchthattheresultobeystheconstraint.We canÔ¨Åndthisvaluebyperforminggradientascenton.Todoso,observe Œª ‚àÇ ‚àÇ ŒªL , Œª( x) = xÓÄæx‚àí1 . (4.27) Whenthenormof xexceeds1,thisderivativeispositive,sotofollowthederivative uphillandincreasetheLagrangianwithrespectto Œª,weincrease Œª.Becausethe coeÔ¨Écientonthe xÓÄæxpenaltyhasincreased,solvingthelinearequationfor xwill nowyieldasolutionwithsmallernorm.Theprocessofsolvingthelinearequation andadjusting Œªcontinuesuntil xhasthecorrectnormandthederivativeon Œªis 0. Thisconcludesthemathematical preliminaries thatweusetodevelopmachine learningalgorithms.Wearenowreadytobuildandanalyzesomefull-Ô¨Çedged learningsystems. 9 7 C h a p t e r 5 Mac h i n e L e ar n i n g B asics DeeplearningisaspeciÔ¨Åckindofmachinelearning.Inordertounderstand deeplearningwell,onemusthaveasolidunderstandingofthebasicprinciplesof machinelearning.Thischapterprovidesabriefcourseinthemostimportantgeneral principlesthatwillbeappliedthroughouttherestofthebook.Novicereadersor thosewhowantawiderperspectiveareencouragedtoconsidermachinelearning textbookswithamorecomprehensivecoverageofthefundamentals,suchasMurphy ()or().Ifyouarealreadyfamiliarwithmachinelearningbasics, 2012Bishop2006 feelfreetoskipaheadtosection.Thatsectioncoverssomeperspectives 5.11 on traditional machinelearning techniques thathavestrongly inÔ¨Çuenced the developmentofdeeplearningalgorithms. WebeginwithadeÔ¨Ånitionofwhatalearningalgorithmis,andpresentan example:thelinearregressionalgorithm. W ethenproceedtodescribehowthe challengeofÔ¨ÅttingthetrainingdatadiÔ¨ÄersfromthechallengeofÔ¨Åndingpatterns thatgeneralizetonewdata.Mostmachinelearningalgorithmshavesettings calledhyperparametersthatmustbedeterminedexternaltothelearningalgorithm itself;wediscusshowtosettheseusingadditionaldata.Machinelearningis essentiallyaformofappliedstatisticswithincreasedemphasisontheuseof computerstostatisticallyestimatecomplicatedfunctionsandadecreasedemphasis onprovingconÔ¨Ådenceintervalsaroundthesefunctions;wethereforepresentthe twocentralapproachestostatistics:frequentistestimatorsandBayesianinference. Mostmachinelearningalgorithmscanbedividedintothecategoriesofsupervised learningandunsupervisedlearning;wedescribethesecategoriesandgivesome examplesofsimplelearningalgorithmsfromeachcategory. Mostdeeplearning algorithmsare basedonan optimization algorithmcalled stochasticgradient descent.Wedescribehowtocombinevariousalgorithmcomponentssuchas 98 CHAPTER5.MACHINELEARNINGBASICS anoptimization algorithm,acostfunction,amodel,andadatasettobuilda machinelearningalgorithm.Finally,insection,wedescribesomeofthe 5.11 factorsthathavelimitedtheabilityoftraditionalmachinelearningtogeneralize. Thesechallengeshavemotivatedthedevelopmentofdeeplearningalgorithmsthat overcometheseobstacles. 5.1LearningAlgorithms Amachinelearningalgorithmisanalgorithmthatisabletolearnfromdata.But whatdowemeanbylearning?Mitchell1997()providesthedeÔ¨Ånition‚ÄúAcomputer programissaidtolearnfromexperienceEwithrespecttosomeclassoftasksT andperformancemeasureP,ifitsperformanceattasksinT,asmeasuredbyP, improveswithexperienceE.‚ÄùOnecanimagineaverywidevarietyofexperiences E,tasksT,andperformancemeasuresP,andwedonotmakeanyattemptinthis booktoprovideaformaldeÔ¨Ånitionofwhatmaybeusedforeachoftheseentities. Instead,thefollowingsectionsprovideintuitivedescriptionsandexamplesofthe diÔ¨Äerentkindsoftasks,performance measuresandexperiencesthatcanbeused toconstructmachinelearningalgorithms. 5.1.1TheTask, T MachinelearningallowsustotackletasksthataretoodiÔ¨Éculttosolvewith Ô¨Åxedprogramswrittenanddesignedbyhumanbeings.FromascientiÔ¨Åcand philosophicalpointofview,machinelearningisinterestingbecausedevelopingour understandingofmachinelearningentailsdevelopingourunderstandingofthe principlesthatunderlieintelligence. InthisrelativelyformaldeÔ¨Ånitionoftheword‚Äútask,‚Äùtheprocessoflearning itselfisnotthetask.Learningisourmeansofattainingtheabilitytoperformthe task.Forexample,ifwewantarobottobeabletowalk,thenwalkingisthetask. Wecouldprogramtherobottolearntowalk,orwecouldattempttodirectlywrite aprogramthatspeciÔ¨Åeshowtowalkmanually. Machinelearningtasksareusuallydescribedintermsofhowthemachine learningsystemshouldprocessanexample.Anexampleisacollectionoffeatures thathavebeenquantitativelymeasuredfromsomeobjectoreventthatwewant themachinelearningsystemtoprocess.Wetypicallyrepresentanexampleasa vectorx‚àà Rnwhereeachentryx iofthevectorisanotherfeature.Forexample, thefeaturesofanimageareusuallythevaluesofthepixelsintheimage. 9 9 CHAPTER5.MACHINELEARNINGBASICS Manykindsoftaskscanbesolvedwithmachinelearning.Someofthemost commonmachinelearningtasksincludethefollowing: ‚Ä¢ClassiÔ¨Åcation:Inthistypeoftask,thecomputerprogramisaskedtospecify whichofkcategoriessomeinputbelongsto.Tosolvethistask,thelearning algorithmisusuallyaskedtoproduceafunctionf: Rn‚Üí{1,...,k}.When y=f(x),themodelassignsaninputdescribedbyvectorxtoacategory identiÔ¨Åedbynumericcodey.ThereareothervariantsoftheclassiÔ¨Åcation task,forexample,wherefoutputsaprobabilitydistributionoverclasses. AnexampleofaclassiÔ¨Åcationtaskisobjectrecognition,wheretheinput isanimage(usuallydescribedasasetofpixelbrightnessvalues),andthe outputisanumericcodeidentifyingtheobjectintheimage.Forexample, theWillowGaragePR2robotisabletoactasawaiterthatcanrecognize diÔ¨Äerentkindsofdrinksanddeliverthemtopeopleoncommand(Good- fellow2010etal.,).Modernobjectrecognitionisbestaccomplishedwith deeplearning( ,; ,).Object Krizhevskyetal.2012IoÔ¨ÄeandSzegedy2015 recognitionisthesamebasictechnologythatallowscomputerstorecognize faces(Taigman 2014etal.,),whichcanbeusedtoautomatically tagpeople inphotocollectionsandallowcomputerstointeractmorenaturallywith theirusers. ‚Ä¢ClassiÔ¨Åcationwithmissinginputs:ClassiÔ¨Åcationbecomesmorechal- lengingifthecomputerprogramisnotguaranteedthateverymeasurement initsinputvectorwillalwaysbeprovided.InordertosolvetheclassiÔ¨Åcation task,thelearningalgorithmonlyhastodeÔ¨Åneafunctionmapping single fromavectorinputtoacategoricaloutput.Whensomeoftheinputsmay bemissing,ratherthanprovidingasingleclassiÔ¨Åcationfunction,thelearning algorithmmustlearnaoffunctions.Eachfunctioncorrespondstoclassi- set fyingxwithadiÔ¨Äerentsubsetofitsinputsmissing.Thiskindofsituation arisesfrequentlyinmedicaldiagnosis,becausemanykindsofmedicaltests areexpensiveorinvasive.OnewaytoeÔ¨ÉcientlydeÔ¨Ånesuchalargeset offunctionsistolearnaprobabilitydistributionoveralloftherelevant variables,thensolvetheclassiÔ¨Åcationtaskbymarginalizing outthemissing variables.Withninputvariables,wecannowobtainall2ndiÔ¨ÄerentclassiÔ¨Å- cationfunctionsneededforeachpossiblesetofmissinginputs,butweonly needtolearnasinglefunctiondescribingthejointprobabilitydistribution. SeeGoodfellow2013betal.()foranexampleofadeepprobabilisticmodel appliedtosuchataskinthisway.Manyoftheothertasksdescribedinthis sectioncanalsobegeneralizedtoworkwithmissinginputs;classiÔ¨Åcation withmissinginputsisjustoneexampleofwhatmachinelearningcando. 1 0 0 CHAPTER5.MACHINELEARNINGBASICS ‚Ä¢Regression:Inthistypeoftask,thecomputerprogramisaskedtopredicta numericalvaluegivensomeinput.Tosolvethistask,thelearningalgorithm isaskedtooutputafunctionf: Rn‚Üí R.Thistypeoftaskissimilarto classiÔ¨Åcation,exceptthattheformatofoutputisdiÔ¨Äerent.Anexampleof aregressiontaskisthepredictionoftheexpectedclaimamountthatan insuredpersonwillmake(usedtosetinsurancepremiums),ortheprediction offuturepricesofsecurities.Thesekindsofpredictionsarealsousedfor algorithmictrading. ‚Ä¢Transcription:Inthistypeoftask,themachinelearningsystemisasked toobservearelativelyunstructuredrepresentationofsomekindofdataand transcribeitintodiscrete,textualform.Forexample,inopticalcharacter recognition,thecomputerprogramisshownaphotographcontainingan imageoftextandisaskedtoreturnthistextintheformofasequence ofcharacters(e.g.,inASCIIorUnicodeformat).GoogleStreetViewuses deeplearningtoprocessaddressnumbersinthisway( , Goodfellow etal. 2014d).Anotherexampleisspeechrecognition,wherethecomputerprogram isprovidedanaudiowaveformandemitsasequenceofcharactersorword IDcodesdescribingthewordsthatwerespokenintheaudiorecording.Deep learningisacrucialcomponentofmodernspeechrecognitionsystemsused atmajorcompaniesincludingMicrosoft,IBMandGoogle( ,Hintonetal. 2012b). ‚Ä¢Machinetranslation:Inamachinetranslationtask,theinputalready consistsofasequenceofsymbolsinsomelanguage,andthecomputerprogram mustconvertthisintoasequenceofsymbolsinanotherlanguage.Thisis commonlyappliedtonaturallanguages,suchastranslatingfromEnglishto French.Deeplearninghasrecentlybeguntohaveanimportantimpacton thiskindoftask(Sutskever2014Bahdanau 2015 etal.,; etal.,). ‚Ä¢Structuredoutput:Structuredoutputtasksinvolveanytaskwherethe outputisavector(orotherdatastructurecontainingmultiplevalues)with importantrelationshipsbetweenthediÔ¨Äerentelements.Thisisabroad category,andsubsumesthetranscriptionandtranslationtasksdescribed above,butalsomanyothertasks.Oneexampleisparsing‚Äîmappinga naturallanguagesentenceintoatreethatdescribesitsgrammaticalstructure andtaggingnodesofthetreesasbeingverbs,nouns,oradverbs,andsoon. See ()foranexampleofdeeplearningappliedtoaparsing Collobert2011 task.Anotherexampleispixel-wisesegmentationofimages, wherethe computerprogramassignseverypixelinanimagetoaspeciÔ¨Åccategory.For 1 0 1 CHAPTER5.MACHINELEARNINGBASICS example,deeplearningcanbeusedtoannotatethelocationsofroadsin aerialphotographs(MnihandHinton2010,).Theoutputneednothaveits formmirrorthestructureoftheinputascloselyasintheseannotation-style tasks.Forexample,inimagecaptioning,thecomputerprogramobservesan imageandoutputsanaturallanguagesentencedescribingtheimage(Kiros etal. etal. ,,;2014abMao,;2015Vinyals2015bDonahue2014 etal.,; etal.,; KarpathyandLi2015Fang2015Xu2015 ,;etal.,;etal.,).Thesetasksare calledstructuredoutputtasksbecausetheprogrammustoutputseveral valuesthatarealltightlyinter-related.Forexample,thewordsproducedby animagecaptioningprogrammustformavalidsentence. ‚Ä¢Anomalydetection:Inthistypeoftask,thecomputerprogramsifts throughasetofeventsorobjects,andÔ¨Çagssomeofthemasbeingunusual oratypical.Anexampleofananomalydetectiontaskiscreditcardfraud detection.Bymodelingyourpurchasinghabits,acreditcardcompanycan detectmisuseofyourcards.Ifathiefstealsyourcreditcardorcreditcard information,thethief‚ÄôspurchaseswilloftencomefromadiÔ¨Äerentprobability distributionoverpurchasetypesthanyourown.Thecreditcardcompany canpreventfraudbyplacingaholdonanaccountassoonasthatcardhas beenusedforanuncharacteris ticpurchase.See ()fora Chandola etal.2009 surveyofanomalydetectionmethods. ‚Ä¢Synthesisandsampling:Inthistypeoftask,themachinelearningal- gorithmisaskedtogeneratenewexamplesthataresimilartothoseinthe trainingdata. Synthesisandsamplingviamachinelearningcanbeuseful formediaapplicationswhereitcanbeexpensiveorboringforanartistto generatelargevolumesofcontentbyhand.Forexample,videogamescan automatically generatetexturesforlargeobjectsorlandscapes,ratherthan requiringanartisttomanuallylabeleachpixel(,).Insome Luoetal.2013 cases,wewantthesamplingorsynthesisproceduretogeneratesomespeciÔ¨Åc kindofoutputgiventheinput.Forexample,inaspeechsynthesistask,we provideawrittensentenceandasktheprogramtoemitanaudiowaveform containingaspokenversionofthatsentence. Thisisakindofstructured outputtask,butwiththeaddedqualiÔ¨Åcationthatthereisnosinglecorrect outputforeachinput,andweexplicitlydesirealargeamountofvariationin theoutput,inorderfortheoutputtoseemmorenaturalandrealistic. ‚Ä¢Imputationofmissingvalues:Inthistypeoftask,themachinelearning algorithmisgivenanewexamplex‚àà Rn,butwithsomeentriesx iofx missing.Thealgorithmmustprovideapredictionofthevaluesofthemissing entries. 1 0 2 CHAPTER5.MACHINELEARNINGBASICS ‚Ä¢Denoising:Inthistypeoftask,themachinelearningalgorithmisgivenin inputacorruptedexampleÀúx‚àà Rnobtainedbyanunknowncorruptionprocess fromacleanexamplex‚àà Rn.Thelearnermustpredictthecleanexample xfromitscorruptedversionÀúx,ormoregenerallypredicttheconditional probabilitydistributionp(x|Àúx). ‚Ä¢Densityestimationorprobabilitymassfunctionestimation:In thedensityestimationproblem,themachinelearningalgorithmisasked tolearnafunctionpmodel: Rn‚Üí R,wherepmodel(x)canbeinterpreted asaprobabilitydensityfunction(if xiscontinuous)oraprobabilitymass function(if xisdiscrete)onthespacethattheexamplesweredrawnfrom. Todosuchataskwell(wewillspecifyexactlywhatthatmeanswhenwe discussperformancemeasuresP),thealgorithmneedstolearnthestructure ofthedataithasseen.Itmustknowwhereexamplesclustertightlyand wheretheyareunlikelytooccur.Mostofthetasksdescribedaboverequire thelearningalgorithmtoatleastimplicitlycapturethestructureofthe probabilitydistribution.Densityestimationallowsustoexplicitlycapture thatdistribution.Inprinciple,wecanthenperformcomputations onthat distributioninordertosolvetheothertasksaswell.Forexample,ifwe haveperformeddensityestimationtoobtainaprobabilitydistributionp(x), wecanusethatdistributiontosolvethemissingvalueimputationtask.If avaluex iismissingandalloftheothervalues,denotedx ‚àí i,aregiven, thenweknowthedistributionoveritisgivenbyp(x i|x ‚àí i).Inpractice, densityestimationdoesnotalwaysallowustosolvealloftheserelatedtasks, becauseinmanycasestherequiredoperationsonp(x)arecomputationally intractable. Ofcourse,manyothertasksandtypesoftasksarepossible.Thetypesoftasks welisthereareintendedonlytoprovideexamplesofwhatmachinelearningcan do,nottodeÔ¨Ånearigidtaxonomyoftasks. 5.1.2ThePerformanceMeasure, P Inordertoevaluatetheabilitiesofamachinelearningalgorithm,wemustdesign aquantitativemeasureofitsperformance.UsuallythisperformancemeasurePis speciÔ¨Åctothetaskbeingcarriedoutbythesystem. T FortaskssuchasclassiÔ¨Åcation,classiÔ¨Åcationwithmissinginputs,andtran- scription,weoftenmeasuretheaccuracyofthemodel.Accuracyisjustthe proportionofexamplesforwhichthemodelproducesthecorrectoutput.Wecan 1 0 3 CHAPTER5.MACHINELEARNINGBASICS alsoobtainequivalentinformationbymeasuringtheerrorrate,theproportion ofexamplesforwhichthemodelproducesanincorrectoutput.Weoftenreferto theerrorrateastheexpected0-1loss.The0-1lossonaparticularexampleis0 ifitiscorrectlyclassiÔ¨Åedand1ifitisnot.Fortaskssuchasdensityestimation, itdoesnotmakesensetomeasureaccuracy,errorrate,oranyotherkindof0-1 loss.Instead,wemustuseadiÔ¨Äerentperformancemetricthatgivesthemodel acontinuous-valuedscoreforeachexample.Themostcommonapproachisto reporttheaveragelog-probabilit ythemodelassignstosomeexamples. Usuallyweareinterestedinhowwellthemachinelearningalgorithmperforms ondatathatithasnotseenbefore,sincethisdetermineshowwellitwillworkwhen deployedintherealworld.Wethereforeevaluatetheseperformancemeasuresusing atestsetofdatathatisseparatefromthedatausedfortrainingthemachine learningsystem. Thechoiceofperformancemeasuremayseemstraightforwardandobjective, butitisoftendiÔ¨Éculttochooseaperformancemeasurethatcorrespondswellto thedesiredbehaviorofthesystem. Insomecases,thisisbecauseitisdiÔ¨Éculttodecidewhatshouldbemeasured. Forexample,whenperformingatranscriptiontask,shouldwemeasuretheaccuracy ofthesystemattranscribingentiresequences,orshouldweuseamoreÔ¨Åne-grained performancemeasurethatgivespartialcreditforgettingsomeelementsofthe sequencecorrect?Whenperformingaregressiontask,shouldwepenalizethe systemmoreifitfrequentlymakesmedium-sizedmistakesorifitrarelymakes verylargemistakes?Thesekindsofdesignchoicesdependontheapplication. Inothercases,weknowwhatquantitywewouldideallyliketomeasure,but measuringitisimpractical.Forexample,thisarisesfrequentlyinthecontextof densityestimation.Manyofthebestprobabilisticmodelsrepresentprobability distributionsonlyimplicitly.Computingtheactualprobabilityvalueassignedto aspeciÔ¨Åcpointinspaceinmanysuchmodelsisintractable.Inthesecases,one mustdesignanalternativecriterionthatstillcorrespondstothedesignobjectives, ordesignagoodapproximationtothedesiredcriterion. 5.1.3TheExperience, E</div>
        </div>
    </div>

    <div class="question-card" id="q79">
        <div class="question-header">
            <span class="question-number">Question 79</span>
            <span class="question-type">multiple-choice</span>
        </div>

        <div class="question-text">Probabilistic models with latent variables are foundational in machine learning, but performing inference in these models can be challenging due to complex dependencies and intractable posterior distributions. Various algorithms and approximations have been developed to address tractable inference and learning.

Which statement best characterizes the role of the Evidence Lower Bound (ELBO) in variational inference for models with latent variables?

1) ELBO provides a tractable lower bound on the log-likelihood that is maximized to approximate the true posterior.   
2) ELBO is a measure of the independence between observed and latent variables in graphical models.   
3) ELBO always equals the exact log-likelihood in models with any choice of variational distribution \( q(h|v) \).   
4) ELBO is only used in deterministic models without latent variables.   
5) ELBO directly computes the marginal likelihood by integrating over all latent variables.   
6) ELBO is maximized by restricting \( q(h|v) \) to the most complex family of distributions possible.   
7) ELBO serves as an upper bound on the likelihood and is minimized during training.</div>

        <div class="answer-section">
            <div class="answer-label">‚úì Correct Answer:</div>
            <div class="answer-text">The correct answer is 1) ELBO provides a tractable lower bound on the log-likelihood that is maximized to approximate the true posterior..</div>
        </div>

        <div class="reference-section">
            <div class="reference-label">üìö Reference Text:</div>
            <button class="toggle-button" onclick="toggleReference(79)">
                Show/Hide Reference
            </button>
            <div id="ref79" class="reference-text hidden">intractableduetolargecliquesoflatentvariables.AdeepBoltzmannmachine, ( C e n t e r ) organizedintolayersofvariableswithoutintra-layerconnections,stillhasanintractable posteriordistributionduetotheconnectionsbetweenlayers.Thisdirectedmodel ( R i g h t ) hasinteractionsbetweenlatentvariableswhenthevisiblevariablesareobserved,because everytwolatentvariablesareco-parents.Someprobabilisticmodelsareabletoprovide tractableinferenceoverthelatentvariablesdespitehavingoneofthegraphstructures depictedabove.Thisispossibleiftheconditionalprobabilitydistributionsarechosento introduceadditionalindependencesbeyondthosedescribedbythegraph.Forexample, probabilisticPCAhasthegraphstructureshownintheright,yetstillhassimpleinference duetospecialpropertiesofthespeciÔ¨Åcconditionaldistributionsituses(linear-Gaussian conditionalswithmutuallyorthogonalbasisvectors). 6 3 2 CHAPTER19.APPROXIMATEINFERENCE 19.1InferenceasOptimization ManyapproachestoconfrontingtheproblemofdiÔ¨Écultinferencemakeuseof theobservationthatexactinferencecanbedescribedasanoptimization problem. Approximateinferencealgorithmsmaythenbederivedbyapproximatingthe underlyingoptimization problem. Toconstructtheoptimization problem,assumewehaveaprobabilisticmodel consistingofobservedvariables vandlatentvariables h.Wewouldliketocompute thelogprobabilityoftheobserveddata, log p( v; Œ∏).SometimesitistoodiÔ¨Écult tocompute log p( v; Œ∏)ifitiscostlytomarginalizeout h.Instead,wecancompute alowerbound L( v Œ∏ , , q)onlog p( v; Œ∏).Thisboundiscalledtheevidencelower bound(ELBO).Anothercommonlyusednameforthislowerboundisthenegative variationalfreeenergy.SpeciÔ¨Åcally,theevidencelowerboundisdeÔ¨Ånedtobe L ‚àí ( ) = log(;) v Œ∏ , , q p v Œ∏ D K L(( )( ;)) q h v|ÓÅ´ p h v| Œ∏(19.1) whereisanarbitraryprobabilitydistributionover. q h BecausethediÔ¨Äerencebetweenlog p( v)and L( v Œ∏ , , q)isgivenbytheKL divergenceandbecausetheKLdivergenceisalwaysnon-negative,wecanseethat Lalwayshasatmostthesamevalueasthedesiredlogprobability.Thetwoare equalifandonlyifisthesamedistributionas. q p( ) h v| Surprisingly,Lcanbeconsiderablyeasiertocomputeforsomedistributions q. Simplealgebrashowsthatwecanrearrange Lintoamuchmoreconvenientform: L ‚àí ( ) =log(;) v Œ∏ , , q p v Œ∏ D K L(( )( ;)) q h v|ÓÅ´ p h v| Œ∏ (19.2) =log(;) p v Œ∏‚àí E h‚àº qlogq( ) h v| p( ) h v|(19.3) =log(;) p v Œ∏‚àí E h‚àº qlogq( ) h v| p , ( h v Œ∏ ; ) p ( ; ) v Œ∏(19.4) =log(;) p v Œ∏‚àí E h‚àº q[log( )log(;)+log(;)] q h v|‚àí p h v , Œ∏ p v Œ∏(19.5) =‚àí E h‚àº q[log( )log(;)] q h v|‚àí p h v , Œ∏ . (19.6) ThisyieldsthemorecanonicaldeÔ¨Ånitionoftheevidencelowerbound, L( ) = v Œ∏ , , q E h‚àº q[log( )]+() p h v , H q . (19.7) Foranappropriatechoiceof q,Listractabletocompute.Foranychoice of q,Lprovidesalowerboundonthelikelihood.For q( h v|)thatarebetter 6 3 3 CHAPTER19.APPROXIMATEINFERENCE approximationsof p( h v|),thelowerbound Lwillbetighter,inotherwords, closertolog p( v). When q( h v|)= p( h v|),theapproximation isperfect,and L( ) = log(;) v Œ∏ , , q p v Œ∏ . WecanthusthinkofinferenceastheprocedureforÔ¨Åndingthe qthatmaximizes L.ExactinferencemaximizesLperfectlybysearchingoverafamilyoffunctions qthatincludes p( h v|).Throughoutthischapter,wewillshowhowtoderive diÔ¨Äerentformsofapproximateinferencebyusingapproximateoptimization to Ô¨Ånd q.Wecanmaketheoptimization procedurelessexpensivebutapproximate byrestrictingthefamilyofdistributions qtheoptimization isallowedtosearch overorbyusinganimperfectoptimization procedurethatmaynotcompletely maximizebutmerelyincreaseitbyasigniÔ¨Åcantamount. L Nomatterwhatchoiceof qweuse,Lisalowerbound.Wecangettighter orlooserboundsthatarecheaperormoreexpensivetocomputedependingon howwechoosetoapproachthisoptimization problem. Wecanobtainapoorly matched qbutreducethecomputational costbyusinganimperfectoptimization procedure,orbyusingaperfectoptimization procedureoverarestrictedfamilyof qdistributions. 19.2ExpectationMaximization TheÔ¨Årstalgorithmweintroducebasedonmaximizingalowerbound Listhe expectationmaximization(EM)algorithm,apopulartrainingalgorithmfor modelswithlatentvariables.WedescribehereaviewontheEMalgorithm developedby ().Unlikemostoftheotheralgorithmswe NealandHinton1999 describeinthischapter,EMisnotanapproachtoapproximateinference,but ratheranapproachtolearningwithanapproximate posterior. TheEMalgorithmconsistsofalternatingbetweentwostepsuntilconvergence: ‚Ä¢TheE-step(Expectationstep):Let Œ∏( 0 )denotethevalueoftheparameters atthebeginningofthestep.Set q( h( ) i| v)= p( h( ) i| v( ) i; Œ∏( 0 ))forall indices iofthetrainingexamples v( ) iwewanttotrainon(bothbatchand minibatchvariantsarevalid).Bythiswemean qisdeÔ¨Ånedintermsofthe c u r r e ntparametervalueof Œ∏( 0 );ifwevary Œ∏then p( h v|; Œ∏)willchangebut q p ( ) h v|willremainequalto( ; h v| Œ∏( 0 )). ‚Ä¢The (Maximization step):Completelyorpartiallymaximize M-step ÓÅò iL( v( ) i, , q Œ∏) (19.8) 6 3 4 CHAPTER19.APPROXIMATEINFERENCE withrespecttousingyouroptimization algorithmofchoice. Œ∏ Thiscanbeviewedasacoordinateascentalgorithmtomaximize L.Onone step,wemaximize Lwithrespectto q,andontheother,wemaximize Lwith respectto. Œ∏ Stochasticgradientascentonlatentvariablemodelscanbeseenasaspecial caseoftheEMalgorithmwheretheMstepconsistsoftakingasinglegradient step.OthervariantsoftheEMalgorithmcanmakemuchlargersteps.Forsome modelfamilies,theMstepcanevenbeperformedanalytically,jumpingallthe waytotheoptimalsolutionforgiventhecurrent. Œ∏ q EventhoughtheE-stepinvolvesexactinference,wecanthinkoftheEM algorithmasusingapproximate inferenceinsomesense.SpeciÔ¨Åcally,theM-step assumesthatthesamevalueof qcanbeusedforallvaluesof Œ∏.Thiswillintroduce agapbetweenLandthetruelog p( v)astheM-stepmovesfurtherandfurther awayfromthevalue Œ∏( 0 )usedintheE-step.Fortunately,theE-stepreducesthe gaptozeroagainasweentertheloopforthenexttime. TheEMalgorithmcontainsafewdiÔ¨Äerentinsights.First,thereisthebasic structureofthelearningprocess,inwhichweupdatethemodelparametersto improvethelikelihoodofacompleteddataset,whereallmissingvariableshave theirvaluesprovidedbyanestimateoftheposteriordistribution.Thisparticular insightisnotuniquetotheEMalgorithm.Forexample,usinggradientdescentto maximizethelog-likelihoodalsohasthissameproperty;thelog-likelihoodgradient computationsrequiretakingexpectationswithrespecttotheposteriordistribution overthehiddenunits. AnotherkeyinsightintheEMalgorithmisthatwecan continuetouseonevalueof qevenafterwehavemovedtoadiÔ¨Äerentvalueof Œ∏. Thisparticularinsightisusedthroughoutclassicalmachinelearningtoderivelarge M-stepupdates.Inthecontextofdeeplearning,mostmodelsaretoocomplex toadmitatractablesolutionforanoptimallargeM-stepupdate,sothissecond insightwhichismoreuniquetotheEMalgorithmisrarelyused. 19.3MAPInferenceandSparseCoding Weusuallyusetheterminferencetorefertocomputingtheprobabilitydistribution overonesetofvariablesgivenanother.Whentrainingprobabilisticmodelswith latentvariables,weareusuallyinterestedincomputing p( h v|).Analternative formofinferenceistocomputethesinglemostlikelyvalueofthemissingvariables, ratherthantoinfertheentiredistributionovertheirpossiblevalues.Inthecontext 6 3 5 CHAPTER19.APPROXIMATEINFERENCE oflatentvariablemodels,thismeanscomputing h‚àó= argmax hp . ( ) h v| (19.9) Thisisknownasmaximumaposterioriinference,abbreviatedMAPinference. MAPinferenceisusuallynotthoughtofasapproximate inference‚Äîit does computetheexactmostlikelyvalueof h‚àó.However,ifwewishtodevelopa learningprocessbasedonmaximizing L( v h , , q),thenitishelpfultothinkofMAP inferenceasaprocedurethatprovidesavalueof q.Inthissense,wecanthinkof MAPinferenceasapproximateinference,becauseitdoesnotprovidetheoptimal q. Recallfromsectionthatexactinferenceconsistsofmaximizing 19.1 L( ) = v Œ∏ , , q E h‚àº q[log( )]+() p h v , H q (19.10) withrespectto qoveranunrestrictedfamilyofprobabilitydistributions,using anexactoptimization algorithm.WecanderiveMAPinferenceasaformof</div>
        </div>
    </div>

    <div class="question-card" id="q80">
        <div class="question-header">
            <span class="question-number">Question 80</span>
            <span class="question-type">multiple-choice</span>
        </div>

        <div class="question-text">Deep Boltzmann Machines (DBMs) are powerful deep generative models, but their optimization can be challenging due to complex layer interactions and unfavorable training dynamics. Architectural and algorithmic innovations have been developed to address these issues.

Which technique specifically improves the conditioning of the optimization problem in DBMs by reparametrizing the energy function through mean subtraction, without altering the model's representational capacity?

1) Centered DBM   
2) Greedy layer-wise pretraining   
3) Multi-prediction DBM   
4) Persistent contrastive divergence   
5) Mean field inference in the negative phase   
6) Weight halving for middle layers   
7) Training an MLP classifier on DBM features</div>

        <div class="answer-section">
            <div class="answer-label">‚úì Correct Answer:</div>
            <div class="answer-text">The correct answer is 1) Centered DBM.</div>
        </div>

        <div class="reference-section">
            <div class="reference-label">üìö Reference Text:</div>
            <button class="toggle-button" onclick="toggleReference(80)">
                Show/Hide Reference
            </button>
            <div id="ref80" class="reference-text hidden">20.1 varientoftheDBMthatlacksbiasparameters;includingthemistrivial. 20.4.4Layer-WisePretraining Unfortunately,trainingaDBMusingstochasticmaximumlikelihood(asdescribed above)fromarandominitialization usuallyresultsinfailure.Insomecases,the modelfailstolearntorepresentthedistributionadequately.Inothercases,the DBMmayrepresentthedistributionwell,butwithnohigherlikelihoodthancould beobtainedwithjustanRBM.ADBMwithverysmallweightsinallbuttheÔ¨Årst layerrepresentsapproximatelythesamedistributionasanRBM. Varioustechniquesthatpermitjointtraininghavebeendevelopedandare describedinsection.However,theoriginalandmostpopularmethodfor 20.4.5 overcomingthejointtrainingproblemofDBMsisgreedylayer-wisepretraining. Inthismethod,eachlayeroftheDBMistrainedinisolationasanRBM.The Ô¨Årstlayeristrainedtomodeltheinputdata.EachsubsequentRBMistrainedto modelsamplesfromthepreviousRBM‚Äôsposteriordistribution. Afterallofthe 6 6 9 CHAPTER20.DEEPGENERATIVEMODELS Algorithm20.1Thevariationalstochasticmaximumlikelihoodalgorithmfor trainingaDBMwithtwohiddenlayers. Set,thestepsize,toasmallpositivenumber ÓÄè Set k,thenumberofGibbssteps,highenoughtoallowaMarkovchainof p( v h ,( 1 ), h( 2 ); Œ∏+ ÓÄè‚àÜ Œ∏)toburnin,startingfromsamplesfrom p( v h ,( 1 ), h( 2 ); Œ∏). Initializethreematrices,Àú V,Àú H( 1 )and Àú H( 2 )eachwith mrowssettorandom values(e.g.,fromBernoullidistributions,possiblywithmarginalsmatchedto themodel‚Äôsmarginals). whilenotconverged(learningloop)do Sampleaminibatchof mexamplesfromthetrainingdataandarrangethem astherowsofadesignmatrix. V Initializematrices ÀÜ H( 1 )and ÀÜ H( 2 ),possiblytothemodel‚Äôsmarginals. whilenotconverged(meanÔ¨Åeldinferenceloop)do ÀÜ H( 1 )‚Üê œÉÓÄê V W( 1 )+ÀÜ H( 2 )W( 2 )ÓÄæÓÄë . ÀÜ H( 2 )‚Üê œÉÓÄê ÀÜ H( 1 )W( 2 )ÓÄë . endwhile ‚àÜW( 1 )‚Üê1 mVÓÄæÀÜ H( 1 ) ‚àÜW( 2 )‚Üê1 mÀÜ H( 1 )ÓÄæÀÜ H( 2 ) for do l k = 1to(Gibbssampling) Gibbsblock1: ‚àÄ i , j ,Àú V i , jsampledfrom P(Àú V i , j= 1) = œÉÓÄí W( 1 ) j , :ÓÄê Àú H( 1 ) i , :ÓÄëÓÄæÓÄì . ‚àÄ i , j ,Àú H( 2 ) i , jsampledfrom P(Àú H( 2 ) i , j= 1) = œÉÓÄê Àú H( 1 ) i , : W( 2 ) : , jÓÄë . Gibbsblock2: ‚àÄ i , j ,Àú H( 1 ) i , jsampledfrom P(Àú H( 1 ) i , j= 1) = œÉÓÄê Àú V i , : W( 1 ) : , j+Àú H( 2 ) i , : W( 2 )ÓÄæ j , :ÓÄë . endfor ‚àÜW( 1 )‚Üê‚àÜW( 1 )‚àí1 mVÓÄæÀú H( 1 ) ‚àÜW( 2 )‚Üê‚àÜW( 2 )‚àí1 mÀú H( 1 )ÓÄæÀú H( 2 ) W( 1 )‚Üê W( 1 )+ ÓÄè‚àÜW( 1 )(thisisacartoonillustration,inpracticeuseamore eÔ¨Äectivealgorithm,suchasmomentumwithadecayinglearningrate) W( 2 )‚Üê W( 2 )+‚àÜ ÓÄèW( 2 ) endwhile 6 7 0 CHAPTER20.DEEPGENERATIVEMODELS RBMshavebeentrainedinthisway,theycanbecombinedtoformaDBM.The DBMmaythenbetrainedwithPCD.TypicallyPCDtrainingwillmakeonlya smallchangeinthemodel‚Äôsparametersanditsperformanceasmeasuredbythe log-likelihooditassignstothedata,oritsabilitytoclassifyinputs.SeeÔ¨Ågure20.4 foranillustrationofthetrainingprocedure. Thisgreedylayer-wisetrainingprocedureisnotjustcoordinateascent.Itbears somepassingresemblancetocoordinateascentbecauseweoptimizeonesubsetof theparametersateachstep.ThetwomethodsdiÔ¨Äerbecausethegreedylayer-wise trainingprocedureusesadiÔ¨Äerentobjectivefunctionateachstep. Greedylayer-wisepretrainingofaDBMdiÔ¨Äersfromgreedylayer-wisepre- trainingofaDBN.TheparametersofeachindividualRBMmaybecopiedto thecorrespondingDBNdirectly.InthecaseoftheDBM,theRBMparameters mustbemodiÔ¨ÅedbeforeinclusionintheDBM.Alayerinthemiddleofthestack ofRBMsistrainedwithonlybottom-upinput,butafterthestackiscombined toformtheDBM,thelayerwillhavebothbottom-upandtop-downinput. To accountforthiseÔ¨Äect,SalakhutdinovandHinton2009a()advocatedividingthe weightsofallbutthetopandbottomRBMinhalfbeforeinsertingthemintothe DBM.Additionally,thebottomRBMmustbetrainedusingtwo‚Äúcopies‚Äùofeach visibleunitandtheweightstiedtobeequalbetweenthetwocopies.Thismeans thattheweightsareeÔ¨Äectivelydoubledduringtheupwardpass.Similarly,thetop RBMshouldbetrainedwithtwocopiesofthetopmostlayer. ObtainingthestateoftheartresultswiththedeepBoltzmannmachinerequires amodiÔ¨ÅcationofthestandardSMLalgorithm,whichistouseasmallamountof meanÔ¨ÅeldduringthenegativephaseofthejointPCDtrainingstep(Salakhutdinov andHinton2009a,). SpeciÔ¨Åcally,theexpectationoftheenergygradientshould becomputedwithrespecttothemeanÔ¨Åelddistributioninwhichalloftheunits areindependentfromeachother.TheparametersofthismeanÔ¨Åelddistribution shouldbeobtainedbyrunningthemeanÔ¨ÅeldÔ¨Åxedpointequationsforjustone step.See ()foracomparisonoftheperformanceofcentered Goodfellow e t a l .2013b DBMswithandwithouttheuseofpartialmeanÔ¨Åeldinthenegativephase. 20.4.5JointlyTrainingDeepBoltzmannMachines ClassicDBMsrequiregreedyunsupervisedpretraining,andtoperformclassiÔ¨Åcation well,requireaseparateMLP-basedclassiÔ¨Åerontopofthehiddenfeaturesthey extract.Thishassomeundesirableproperties.Itishardtotrackperformance duringtrainingbecausewecannotevaluatepropertiesofthefullDBMwhile trainingtheÔ¨ÅrstRBM.Thus,itishardtotellhowwellourhyperparameters 6 7 1 CHAPTER20.DEEPGENERATIVEMODELS d)a) b) c ) Figure20.4:ThedeepBoltzmannmachinetrainingprocedureusedtoclassifytheMNIST dataset(SalakhutdinovandHinton2009aSrivastava2014 ,; e t a l .,).TrainanRBM ( a ) byusingCDtoapproximatelymaximizelog P( v).TrainasecondRBMthatmodels ( b ) h( 1 )andtargetclassybyusingCD- ktoapproximatelymaximizelog P( h( 1 ),y)where h( 1 )isdrawnfromtheÔ¨ÅrstRBM‚Äôsposteriorconditionedonthedata.Increase kfrom1 to20duringlearning.CombinethetwoRBMsintoaDBM.Trainittoapproximately ( c ) maximizelog P(v ,y)usingstochasticmaximumlikelihoodwith k= 5.Delete ( d )yfrom themodel.DeÔ¨Åneanewsetoffeatures h( 1 )and h( 2 )thatareobtainedbyrunningmean Ô¨Åeldinferenceinthemodellackingy.UsethesefeaturesasinputtoanMLPwhose structureisthesameasanadditionalpassofmeanÔ¨Åeld,withanadditionaloutputlayer fortheestimateofy.InitializetheMLP‚ÄôsweightstobethesameastheDBM‚Äôsweights. TraintheMLPtoapproximatelymaximizelog P(y|v)usingstochasticgradientdescent anddropout.Figurereprintedfrom( ,). Goodfellow e t a l .2013b 6 7 2 CHAPTER20.DEEPGENERATIVEMODELS areworkinguntilquitelateinthetrainingprocess.Softwareimplementations ofDBMsneedtohavemanydiÔ¨ÄerentcomponentsforCDtrainingofindividual RBMs,PCDtrainingofthefullDBM,andtrainingbasedonback-propagation throughtheMLP.Finally,theMLPontopoftheBoltzmannmachinelosesmany oftheadvantagesoftheBoltzmannmachineprobabilisticmodel,suchasbeing abletoperforminferencewhensomeinputvaluesaremissing. Therearetwomainwaystoresolvethejointtrainingproblemofthedeep Boltzmannmachine.The Ô¨ÅrstisthecentereddeepBoltzmann machine (MontavonandMuller2012,),whichreparametrizes themodelinordertomake theHessianofthecostfunctionbetter-conditionedatthebeginningofthelearning process.Thisyieldsamodelthatcanbetrainedwithoutagreedylayer-wise pretrainingstage.Theresultingmodelobtainsexcellenttestsetlog-likelihood andproduceshighqualitysamples.Unfortunately,itremainsunabletocompete withappropriately regularizedMLPsasaclassiÔ¨Åer.Thesecondwaytojointly trainadeepBoltzmannmachineistouseamulti-predictiondeepBoltzmann machine(Goodfellow2013b e t a l .,).Thismodelusesanalternativetraining criterionthatallowstheuseoftheback-propagationalgorithminordertoavoid theproblemswithMCMCestimatesofthegradient.Unfortunately, thenew criteriondoesnotleadtogoodlikelihoodorsamples,but,comparedtotheMCMC approach,itdoesleadtosuperiorclassiÔ¨Åcationperformanceandabilitytoreason wellaboutmissinginputs. ThecenteringtrickfortheBoltzmannmachineiseasiesttodescribeifwe returntothegeneralviewofaBoltzmannmachineasconsistingofasetofunits xwithaweightmatrix Uandbiases b.Recallfromequationthatheenergy 20.2 functionisgivenby E() = x ‚àí xÓÄæU x b‚àíÓÄæx . (20.36) Using diÔ¨Äerent sparsity patternsin theweight matrix U, wecan implemen t structuresofBoltzmannmachines,suchasRBMs,orDBMswithdiÔ¨Äerentnumbers oflayers.Thisisaccomplishedbypartitioning xintovisibleandhiddenunitsand zeroingoutelementsof Uforunitsthatdonotinteract.ThecenteredBoltzmann machineintroducesavectorthatissubtractedfromallofthestates: ¬µ EÓÄ∞(; ) = ( ) x U b , ‚àí x ¬µ‚àíÓÄæU x ¬µ x ¬µ (‚àí)(‚àí ‚àí)ÓÄæb .(20.37) Typically ¬µisahyperparameterÔ¨Åxedatthebeginningoftraining.Itisusu- allychosentomakesurethat x ¬µ‚àí ‚âà0whenthemodelisinitialized.This reparametrization doesnotchangethesetofprobabilitydistributionsthatthe modelcanrepresent,butitdoeschangethedynamicsofstochasticgradientdescent appliedtothelikelihood.SpeciÔ¨Åcally,inmanycases,thisreparametrization results 6</div>
        </div>
    </div>

    <div class="question-card" id="q81">
        <div class="question-header">
            <span class="question-number">Question 81</span>
            <span class="question-type">multiple-choice</span>
        </div>

        <div class="question-text">In deep learning, training neural networks involves computing gradients efficiently using specialized algorithms and understanding the underlying mathematical structures. Advanced techniques are often needed for memory optimization and higher-order derivative calculations.

Which statement most accurately describes why computing the full Hessian matrix is typically infeasible in deep learning applications?

1) The Hessian matrix requires non-differentiable activation functions.   
2) The Hessian matrix grows quadratically with the number of parameters, making both storage and computation impractical for large models.   
3) The Hessian matrix is not used in any optimization algorithms for neural networks.   
4) The backward pass in neural networks only supports computation of first derivatives.   
5) The Hessian matrix can only be calculated after training is completed.   
6) Storing inputs to non-linearities eliminates the need for Hessian computation.   
7) Krylov methods are designed to compute full Hessians directly.</div>

        <div class="answer-section">
            <div class="answer-label">‚úì Correct Answer:</div>
            <div class="answer-text">The correct answer is 2) The Hessian matrix grows quadratically with the number of parameters, making both storage and computation impractical for large models..</div>
        </div>

        <div class="reference-section">
            <div class="reference-label">üìö Reference Text:</div>
            <button class="toggle-button" onclick="toggleReference(81)">
                Show/Hide Reference
            </button>
            <div id="ref81" class="reference-text hidden">E J M L E c r o s s _ e n t r o p y U( 5 )U( 5 ) s q ru( 6 )u( 6 ) s u mu( 8 )u( 8 )J J + √ó + Figure6.11:Thecomputationalgraphusedtocomputethecostusedtotrainourexample ofasingle-layerMLPusingthecross-entropylossandweightdecay. Theotherpaththroughthecross-entropycostisslightlymorecomplicated. LetGbethegradientontheunnormalized logprobabilitiesU( 2 )providedby thecross_entropyoperation.Theback-propagation algorithmnowneedsto exploretwodiÔ¨Äerentbranches.Ontheshorterbranch,itaddsHÓÄæGtothe gradientonW( 2 ),usingtheback-propagation ruleforthesecondargumentto thematrixmultiplication operation.Theotherbranchcorrespondstothelonger chaindescendingfurtheralongthenetwork.First,theback-propagationalgorithm computes ‚àá H J=GW( 2 )ÓÄæusingtheback-propagationrulefortheÔ¨Årstargument tothematrixmultiplication operation.Next,thereluoperationusesitsback- propagationruletozerooutcomponentsofthegradientcorrespondingtoentries ofU( 1 )thatwerelessthan.Lettheresultbecalled 0 GÓÄ∞.Thelaststepofthe back-propagationalgorithmistousetheback-propagation ruleforthesecond argumentoftheoperationtoadd matmul XÓÄæGÓÄ∞tothegradientonW( 1 ). Afterthesegradientshavebeencomputed,itistheresponsibilityofthegradient descentalgorithm,oranotheroptimization algorithm,tousethesegradientsto updatetheparameters. FortheMLP,thecomputational costisdominatedbythecostofmatrix multiplication. Duringtheforwardpropagationstage,wemultiplybyeachweight 2 2 0 CHAPTER6.DEEPFEEDFORWARDNETWORKS matrix,resultingin O( w) multiply-adds,where wisthenumberofweights.During thebackwardpropagationstage,wemultiplybythetransposeofeachweight matrix,whichhasthesamecomputational cost.Themainmemorycostofthe algorithmisthatweneedtostoretheinputtothenonlinearityofthehiddenlayer. Thisvalueisstoredfromthetimeitiscomputeduntilthebackwardpasshas returnedtothesamepoint.Thememorycostisthus O( m n h),where misthe numberofexamplesintheminibatchand n histhenumberofhiddenunits. 6.5.8Complications Ourdescriptionoftheback-propagation algorithmhereissimplerthantheimple- mentationsactuallyusedinpractice. Asnotedabove,wehaverestrictedthedeÔ¨Ånitionofanoperationtobea functionthatreturnsasingletensor.Mostsoftwareimplementations needto supportoperationsthatcanreturnmorethanonetensor.Forexample,ifwewish tocomputeboththemaximumvalueinatensorandtheindexofthatvalue,itis besttocomputebothinasinglepassthroughmemory,soitismosteÔ¨Écientto implementthisprocedureasasingleoperationwithtwooutputs. We havenot described how tocontrolthememoryconsumption ofback- propagation. Back-propagati onofteninvolvessummationofmanytensorstogether. Inthenaiveapproach,eachofthesetensorswouldbecomputedseparately,then allofthemwouldbeaddedinasecondstep.Thenaiveapproachhasanoverly highmemorybottleneckthatcanbeavoidedbymaintainingasinglebuÔ¨Äerand addingeachvaluetothatbuÔ¨Äerasitiscomputed. Real-worldimplementationsofback-propagation alsoneedtohandlevarious datatypes,suchas32-bitÔ¨Çoatingpoint,64-bitÔ¨Çoatingpoint,andintegervalues. Thepolicyforhandlingeachofthesetypestakesspecialcaretodesign. SomeoperationshaveundeÔ¨Ånedgradients,anditisimportanttotrackthese casesanddeterminewhetherthegradientrequestedbytheuserisundeÔ¨Åned. Variousothertechnicalitiesmakereal-worlddiÔ¨Äerentiation morecomplicated. Thesetechnicalitiesarenotinsurmountable,andthischapterhasdescribedthekey intellectualtoolsneededtocomputederivatives,butitisimportanttobeaware thatmanymoresubtletiesexist. 6.5.9DiÔ¨ÄerentiationoutsidetheDeepLearningCommunity The deeplearning comm unityhas beensomewhat isolatedfrom the broader computersciencecommunityandhaslargelydevelopeditsownculturalattitudes 2 2 1 CHAPTER6.DEEPFEEDFORWARDNETWORKS concerninghowtoperformdiÔ¨Äerentiation. Moregenerally,theÔ¨Åeldofautomatic diÔ¨Äerentiationisconcernedwithhowtocomputederivativesalgorithmically . Theback-propagationalgorithmdescribedhereisonlyoneapproachtoautomatic diÔ¨Äerentiation.Itisaspecialcaseofabroaderclassoftechniquescalledreverse modeaccumulation.Otherapproachesevaluatethesubexpressionsofthechain ruleindiÔ¨Äerentorders.Ingeneral, determining theorderofevaluationthat resultsinthelowestcomputational costisadiÔ¨Écultproblem.Findingtheoptimal sequenceofoperationstocomputethegradientisNP-complete(,), Naumann2008 inthesensethatitmayrequiresimplifyingalgebraicexpressionsintotheirleast expensiveform. Forexample,supposewehavevariables p 1 , p 2 , . . . , p nrepresentingprobabilities andvariables z 1 , z 2 , . . . , z nrepresentingunnormalized logprobabilities. Suppose wedeÔ¨Åne q i=exp( z i)ÓÅê iexp( z i), (6.57) wherewebuildthesoftmaxfunctionoutofexponentiation,summationanddivision operations, and construct a cross-entropyloss J=‚àíÓÅê i p ilog q i.Ahuman mathematician canobservethatthederivativeof Jwithrespectto z itakesavery simpleform: q i‚àí p i.Theback-propagation algorithmisnotcapableofsimplifying thegradientthisway,andwillinsteadexplicitlypropagategradientsthroughallof thelogarithmandexponentiationoperationsintheoriginalgraph.Somesoftware librariessuchasTheano( ,; ,)areableto Bergstra e t a l .2010Bastien e t a l .2012 performsomekindsofalgebraicsubstitutiontoimproveoverthegraphproposed bythepureback-propagation algorithm. Whentheforwardgraph Ghasasingleoutputnodeandeachpartialderivative ‚àÇ u() i ‚àÇ u() jcanbecomputedwithaconstantamountofcomputation,back-propagation guaranteesthatthenumberofcomputations forthegradientcomputationisof thesameorderasthenumberofcomputations fortheforwardcomputation: this canbeseeninalgorithm becauseeachlocalpartialderivative 6.2‚àÇ u() i ‚àÇ u() jneedsto becomputedonlyoncealongwithanassociatedmultiplication andadditionfor therecursivechain-ruleformulation(equation).Theoverallcomputationis 6.49 therefore O(#edges).However,itcanpotentiallybereducedbysimplifyingthe computational graphconstructedbyback-propagation,andthisisanNP-complete task. ImplementationssuchasTheanoandTensorFlowuseheuristicsbasedon matchingknownsimpliÔ¨Åcationpatternsinordertoiterativelyattempttosimplify thegraph.WedeÔ¨Ånedback-propagation onlyforthecomputationofagradientofa scalaroutputbutback-propagationcanbeextendedtocomputeaJacobian(either of kdiÔ¨Äerentscalarnodesinthegraph,orofatensor-valuednodecontaining k values).Anaiveimplementation maythenneed ktimesmorecomputation: for 2 2 2 CHAPTER6.DEEPFEEDFORWARDNETWORKS eachscalarinternalnodeintheoriginalforwardgraph,thenaiveimplementation computes kgradientsinsteadofasinglegradient.Whenthenumberofoutputsof thegraphislargerthanthenumberofinputs,itissometimespreferabletouse anotherformofautomaticdiÔ¨Äerentiationcalledforwardmodeaccumulation. Forwardmodecomputationhasbeenproposedforobtainingreal-timecomputation ofgradientsinrecurrentnetworks,forexample( ,).This WilliamsandZipser1989 alsoavoidstheneedtostorethevaluesandgradientsforthewholegraph,trading oÔ¨Äcomputational eÔ¨Éciencyformemory.Therelationshipbetweenforwardmode andbackwardmodeisanalogoustotherelationshipbetweenleft-multiplyingversus right-multiplyingasequenceofmatrices,suchas ABCD , (6.58) wherethematricescanbethoughtofasJacobianmatrices.Forexample,ifD isacolumnvectorwhileAhasmanyrows,thiscorrespondstoagraphwitha singleoutputandmanyinputs,andstartingthemultiplications fromtheend andgoingbackwardsonlyrequiresmatrix-vector products.Thiscorrespondsto thebackwardmode.Instead,startingtomultiplyfromtheleftwouldinvolvea seriesofmatrix-matrix products,whichmakesthewholecomputationmuchmore expensive.However,ifAhasfewerrowsthanDhascolumns,itischeapertorun themultiplications left-to-right,correspondingtotheforwardmode. Inmanycommunitiesoutsideofmachinelearning,itismorecommontoim- plementdiÔ¨Äerentiationsoftwarethatactsdirectlyontraditionalprogramming languagecode,suchasPythonorCcode,andautomatically generatesprograms thatdiÔ¨Äerentiatefunctionswrittenintheselanguages.Inthedeeplearningcom- munity,computational graphsareusuallyrepresentedbyexplicitdatastructures createdbyspecializedlibraries.Thespecializedapproachhasthedrawbackof requiringthelibrarydevelopertodeÔ¨Ånethebpropmethodsforeveryoperation andlimitingtheuserofthelibrarytoonlythoseoperationsthathavebeendeÔ¨Åned. However,thespecializedapproachalsohasthebeneÔ¨Åtofallowingcustomized back-propagationrulestobedevelopedforeachoperation,allowingthedeveloper toimprovespeedorstabilityinnon-obviouswaysthatanautomaticprocedure wouldpresumablybeunabletoreplicate. Back-propagationisthereforenottheonlywayortheoptimalwayofcomputing thegradient,butitisaverypracticalmethodthatcontinuestoservethedeep learningcommunityverywell.Inthefuture,diÔ¨Äerentiation technologyfordeep networksmayimproveasdeeplearningpractitionersbecomemoreawareofadvances inthebroaderÔ¨ÅeldofautomaticdiÔ¨Äerentiation. 2 2 3 CHAPTER6.DEEPFEEDFORWARDNETWORKS 6.5.10Higher-OrderDerivatives Somesoftwareframeworkssupporttheuseofhigher-orderderivatives.Amongthe deeplearningsoftwareframeworks,thisincludesatleastTheanoandTensorFlow. Theselibrariesusethesamekindofdatastructuretodescribetheexpressionsfor derivativesastheyusetodescribetheoriginalfunctionbeingdiÔ¨Äerentiated.This meansthatthesymbolicdiÔ¨Äerentiation machinerycanbeappliedtoderivatives. Inthecontextofdeeplearning,itisraretocomputeasinglesecondderivative ofascalarfunction.Instead,weareusuallyinterestedinpropertiesoftheHessian matrix.Ifwehaveafunction f: Rn‚Üí R,thentheHessianmatrixisofsize n n√ó. Intypicaldeeplearningapplications, nwillbethenumberofparametersinthe model,whichcouldeasilynumberinthebillions.TheentireHessianmatrixis thusinfeasibletoevenrepresent. InsteadofexplicitlycomputingtheHessian,thetypicaldeeplearningapproach istouseKrylovmethods.Krylovmethodsareasetofiterativetechniquesfor performingvariousoperationslikeapproximately invertingamatrixorÔ¨Ånding approximationstoitseigenvectorsoreigenvalues,withoutusinganyoperation otherthanmatrix-vector products. InordertouseKrylovmethodsontheHessian,weonlyneedtobeableto computetheproductbetweentheHessianmatrixHandanarbitraryvectorv.A straightforwardtechnique( ,)fordoingsoistocompute Christianson1992 Hv= ‚àá xÓÅ® (‚àá x f x())ÓÄævÓÅ© . (6.59) Bothofthegradientcomputations inthisexpressionmaybecomputedautomati- callybytheappropriatesoftwarelibrary.Notethattheoutergradientexpression takesthegradientofafunctionoftheinnergradientexpression. Ifvisitselfavectorproducedbyacomputational graph,itisimportantto specifythattheautomaticdiÔ¨ÄerentiationsoftwareshouldnotdiÔ¨Äerentiatethrough thegraphthatproduced.v WhilecomputingtheHessianisusuallynotadvisable,itispossibletodowith Hessianvectorproducts.OnesimplycomputesHe( ) iforall i= 1 , . . . , n ,where e( ) iistheone-hotvectorwith e( ) i i= 1andallotherentriesequalto0. 6. 6 Hi s t or i c a l Not es FeedforwardnetworkscanbeseenaseÔ¨Écientnonlinearfunctionapproximators basedonusinggradientdescenttominimizetheerrorinafunctionapproximation. 2 2 4 CHAPTER6.DEEPFEEDFORWARDNETWORKS Fromthispointofview,themodernfeedforwardnetworkistheculminationof centuriesofprogressonthegeneralfunctionapproximationtask. Thechainrulethatunderliestheback-propagation algorithmwasinvented inthe17thcentury(,;,).Calculusandalgebrahave Leibniz1676L‚ÄôH√¥pital1696 longbeenusedtosolveoptimization problemsinclosedform,butgradientdescent wasnotintroducedasatechniqueforiterativelyapproximating thesolutionto optimization problemsuntilthe19thcentury(Cauchy1847,). Beginninginthe1940s,thesefunctionapproximation techniqueswereusedto motivatemachinelearningmodelssuchastheperceptron.However,theearliest modelswerebasedonlinearmodels.CriticsincludingMarvinMinskypointedout severaloftheÔ¨Çawsofthelinearmodelfamily,suchasitsinabilitytolearnthe XORfunction,whichledtoabacklashagainsttheentireneuralnetworkapproach. Learningnonlinearfunctionsrequiredthedevelopmentofamultilayerper- ceptronandameansofcomputingthegradientthroughsuchamodel.EÔ¨Écient applicationsofthechainrulebasedondynamicprogramming begantoappear inthe1960sand1970s,mostlyforcontrolapplications(,;Kelley1960Brysonand Denham1961Dreyfus1962BrysonandHo1969Dreyfus1973 ,;,; ,;,)butalsofor sensitivityanalysis(,). Linnainmaa1976Werbos1981()proposedapplyingthese techniquestotrainingartiÔ¨Åcialneuralnetworks.TheideawasÔ¨Ånallydeveloped inpracticeafterbeingindependentlyrediscoveredindiÔ¨Äerentways(,;LeCun1985 Parker1985Rumelhart 1986a ,; e t a l .,).ThebookParallelDistributedPro- cessingpresentedtheresultsofsomeoftheÔ¨Årstsuccessfulexperimentswith back-propagationinachapter( ,)thatcontributedgreatly Rumelhart e t a l .1986b tothepopularization ofback-propagation andinitiatedaveryactiveperiodof researchinmulti-layerneuralnetworks. However,theideasputforwardbythe authorsofthatbookandinparticularbyRumelhartandHintongomuchbeyond back-propagation. Theyincludecrucialideasaboutthepossiblecomputational implementationofseveralcentralaspectsofcognitionandlearning,whichcame underthenameof‚Äúconnectionism‚Äù becauseoftheimportancethisschoolofthought placesontheconnectionsbetweenneuronsasthelocusoflearningandmemory. Inparticular,theseideasincludethenotionofdistributedrepresentation(Hinton</div>
        </div>
    </div>

    <div class="question-card" id="q82">
        <div class="question-header">
            <span class="question-number">Question 82</span>
            <span class="question-type">multiple-choice</span>
        </div>

        <div class="question-text">Deep generative models such as VAEs, GANs, and GMMNs have transformed unsupervised learning by enabling artificial neural networks to synthesize complex data and learn interpretable latent representations. Their training objectives and architectures differ significantly, impacting model flexibility, sample diversity, and theoretical guarantees.

Which generative modeling approach explicitly tightens the variational lower bound on log-likelihood by leveraging multiple samples and importance sampling, thereby improving posterior approximation and robustness?

1) Generative Adversarial Networks (GANs)   
2) Deep Convolutional GANs (DCGANs)   
3) Generative Moment Matching Networks (GMMNs)   
4) Importance Weighted Autoencoders (IWAE)   
5) Conditional GANs (cGANs)   
6) Laplacian Pyramid GANs (LAPGAN)   
7) Restricted Boltzmann Machines (RBMs)</div>

        <div class="answer-section">
            <div class="answer-label">‚úì Correct Answer:</div>
            <div class="answer-text">The correct answer is 4) Importance Weighted Autoencoders (IWAE).</div>
        </div>

        <div class="reference-section">
            <div class="reference-label">üìö Reference Text:</div>
            <button class="toggle-button" onclick="toggleReference(82)">
                Show/Hide Reference
            </button>
            <div id="ref82" class="reference-text hidden">| q ( z x )ÓÄ¢ log1 kkÓÅò i = 1p m o de l( x z ,( ) i) q( z( ) i| x)ÓÄ£ .(20.79) Thisnewobjectiveisequivalenttothetraditionallowerbound Lwhen k=1. However,itmayalsobeinterpretedasforminganestimateofthetruelog p m o de l( x) usingimportancesamplingof zfromproposaldistribution q( z x|).Theimportance weightedautoencoderobjectiveisalsoalowerboundonlog p m o de l( x) andbecomes tighterasincreases. k VariationalautoencodershavesomeinterestingconnectionstotheMP-DBM andotherapproachesthatinvolveback-propagationthroughtheapproximate inferencegraph(Goodfellow2013bStoyanov2011Brakel2013 e t a l .,; e t a l .,; e t a l .,). ThesepreviousapproachesrequiredaninferenceproceduresuchasmeanÔ¨ÅeldÔ¨Åxed pointequationstoprovidethecomputational graph.Thevariationalautoencoder isdeÔ¨Ånedforarbitrarycomputational graphs,whichmakesitapplicabletoawider rangeofprobabilisticmodelfamiliesbecausethereisnoneedtorestrictthechoice 6 9 8 CHAPTER20.DEEPGENERATIVEMODELS ofmodelstothosewithtractablemeanÔ¨ÅeldÔ¨Åxedpointequations.Thevariational autoencoderalsohastheadvantagethatitincreasesaboundonthelog-likelihood ofthemodel,whilethecriteriafortheMP-DBMandrelatedmodelsaremore heuristicandhavelittleprobabilisticinterpretation beyondmakingtheresultsof approximateinferenceaccurate.Onedisadvantageofthevariationalautoencoder isthatitlearnsaninferencenetworkforonlyoneproblem,inferring zgiven x. Theoldermethodsareabletoperformapproximateinferenceoveranysubsetof variablesgivenanyothersubsetofvariables,becausethemeanÔ¨ÅeldÔ¨Åxedpoint equationsspecifyhowtoshareparametersbetweenthecomputational graphsfor allofthesediÔ¨Äerentproblems. Oneverynicepropertyofthevariationalautoencoderisthatsimultaneously trainingaparametricencoderincombinationwiththegeneratornetworkforcesthe modeltolearnapredictablecoordinatesystemthattheencodercancapture.This makesitanexcellentmanifoldlearningalgorithm.SeeÔ¨Ågureforexamplesof 20.6 low-dimensionalmanifoldslearnedbythevariationalautoencoder.Inoneofthe casesdemonstratedintheÔ¨Ågure,thealgorithmdiscoveredtwoindependentfactors ofvariationpresentinimagesoffaces:angleofrotationandemotionalexpression. 20.10.4GenerativeAdversarialNetworks GenerativeadversarialnetworksorGANs( ,)areanother Goodfellow e t a l .2014c generativemodelingapproachbasedondiÔ¨Äerentiablegeneratornetworks. Generativeadversarialnetworksarebasedonagametheoreticscenarioin whichthegeneratornetworkmustcompeteagainstanadversary.Thegenerator networkdirectlyproducessamples x= g( z; Œ∏( ) g).Itsadversary,thediscriminator network,attemptstodistinguishbetweensamplesdrawnfromthetrainingdata andsamplesdrawnfromthegenerator.Thediscriminatoremitsaprobabilityvalue givenby d( x; Œ∏( ) d),indicatingtheprobabilitythat xisarealtrainingexample ratherthanafakesampledrawnfromthemodel. Thesimplestwaytoformulatelearningingenerativeadversarialnetworksis asazero-sumgame,inwhichafunction v( Œ∏( ) g, Œ∏( ) d)determinesthepayoÔ¨Äofthe discriminator.Thegeneratorreceives‚àí v( Œ∏( ) g, Œ∏( ) d)asitsownpayoÔ¨Ä.During learning,eachplayerattemptstomaximizeitsownpayoÔ¨Ä,sothatatconvergence g‚àó= argmin gmax dv g , d . () (20.80) Thedefaultchoiceforis v v( Œ∏( ) g, Œ∏( ) d) = E x‚àº pdatalog()+ d x E x‚àº pmodellog(1 ()) ‚àí d x .(20.81) 6 9 9 CHAPTER20.DEEPGENERATIVEMODELS Figure20.6:Examplesoftwo-dimensionalcoordinatesystemsforhigh-dimensionalmani- folds,learnedbyavariationalautoencoder(KingmaandWelling2014a,).Twodimensions maybeplotteddirectlyonthepageforvisualization,sowecangainanunderstandingof howthemodelworksbytrainingamodelwitha2-Dlatentcode,evenifwebelievethe intrinsicdimensionalityofthedatamanifoldismuchhigher.Theimagesshownarenot examplesfromthetrainingsetbutimages xactuallygeneratedbythemodel p( x z|), simplybychangingthe2-D‚Äúcode‚Äù z(eachimagecorrespondstoadiÔ¨Äerentchoiceof‚Äúcode‚Äù zona2-Duniformgrid). ( L e f t )Thetwo-dimensionalmapoftheFreyfacesmanifold. Onedimensionthathasbeendiscovered(horizontal)mostlycorrespondstoarotationof theface,whiletheother(vertical)correspondstotheemotionalexpression.The ( R i g h t ) two-dimensionalmapoftheMNISTmanifold. Thisdrivesthediscriminatortoattempttolearntocorrectlyclassifysamplesasreal orfake.Simultaneous ly,thegeneratorattemptstofooltheclassiÔ¨Åerintobelieving itssamplesarereal.Atconvergence,thegenerator‚Äôssamplesareindistinguishable fromrealdata,andthediscriminatoroutputs1 2everywhere.Thediscriminator maythenbediscarded. ThemainmotivationforthedesignofGANsisthatthelearningprocess requiresneitherapproximateinferencenorapproximation ofapartitionfunction gradient.Inthecasewhere max d v( g , d)isconvexin Œ∏( ) g(suchasthecasewhere optimization isperformeddirectlyinthespaceofprobabilitydensityfunctions) theprocedureisguaranteedtoconvergeandisasymptoticallyconsistent. Unfortunately,learninginGANscanbediÔ¨Écultinpracticewhen gand d arerepresentedbyneuralnetworksandmax d v( g , d)isnotconvex.Goodfellow 7 0 0 CHAPTER20.DEEPGENERATIVEMODELS ()identiÔ¨Åednon-convergenceasanissuethatmaycauseGANstounderÔ¨Åt. 2014 Ingeneral,simultaneousgradientdescentontwoplayers‚Äôcostsisnotguaranteed toreachanequilibrium.Considerforexamplethevaluefunction v( a , b)= a b, whereoneplayercontrols aandincurscost a b,whiletheotherplayercontrols b andreceivesacost‚àí a b.IfwemodeleachplayerasmakinginÔ¨Ånitesimallysmall gradientsteps,eachplayerreducingtheirowncostattheexpenseoftheother player,then aand bgointoastable,circularorbit,ratherthanarrivingatthe equilibriumpointattheorigin.Notethattheequilibriaforaminimaxgameare notlocalminimaof v.Instead,theyarepointsthataresimultaneouslyminima forbothplayers‚Äôcosts.Thismeansthattheyaresaddlepointsof vthatarelocal minimawithrespecttotheÔ¨Årstplayer‚Äôsparametersandlocalmaximawithrespect tothesecondplayer‚Äôsparameters.Itispossibleforthetwoplayerstotaketurns increasingthendecreasing vforever,ratherthanlandingexactlyonthesaddle pointwhereneitherplayeriscapableofreducingitscost.Itisnotknowntowhat extentthisnon-convergenceproblemaÔ¨ÄectsGANs. Goodfellow2014()identiÔ¨ÅedanalternativeformulationofthepayoÔ¨Äs,inwhich thegameisnolongerzero-sum,thathasthesameexpectedgradientasmaximum likelihoodlearningwheneverthediscriminatorisoptimal.Becausemaximum likelihoodtrainingconverges,thisreformulationoftheGANgameshouldalso converge,givenenoughsamples.Unfortunately,thisalternativeformulationdoes notseemtoimproveconvergenceinpractice,possiblyduetosuboptimalityofthe discriminator,orpossiblyduetohighvariancearoundtheexpectedgradient. Inrealisticexperiments,thebest-performingformulationoftheGANgame isadiÔ¨Äerentformulationthatisneitherzero-sumnorequivalenttomaximum likelihood,introducedby ()withaheuristicmotivation.In Goodfellow e t a l .2014c thisbest-performingformulation,thegeneratoraimstoincreasethelogprobability thatthediscriminatormakesamistake,ratherthanaimingtodecreasethelog probabilitythatthediscriminatormakesthecorrectprediction.Thisreformulation ismotivatedsolelybytheobservationthatitcausesthederivativeofthegenerator‚Äôs costfunctionwithrespecttothediscriminator‚Äôslogitstoremainlargeeveninthe situationwherethediscriminatorconÔ¨Ådentlyrejectsallgeneratorsamples. Stabilization ofGANlearningremainsanopenproblem. Fortunately,GAN learningperformswellwhenthemodelarchitectureandhyperparametersarecare- fullyselected. ()craftedadeepconvolutionalGAN(DCGAN) Radford e t a l .2015 thatperformsverywellforimagesynthesistasks,andshowedthatitslatentrepre- sentationspacecapturesimportantfactorsofvariation,asshowninÔ¨Ågure.15.9 SeeÔ¨ÅgureforexamplesofimagesgeneratedbyaDCGANgenerator. 20.7 TheGANlearningproblemcanalsobesimpliÔ¨Åedbybreakingthegeneration 7 0 1 CHAPTER20.DEEPGENERATIVEMODELS Figure20.7:ImagesgeneratedbyGANstrainedontheLSUNdataset. ( L e f t )Images ofbedroomsgeneratedbyaDCGANmodel,reproducedwithpermissionfromRadford e t a l .().ImagesofchurchesgeneratedbyaLAPGANmodel,reproducedwith 2015 ( R i g h t ) permissionfrom (). Denton e t a l .2015 processintomanylevelsofdetail.ItispossibletotrainconditionalGANs(Mirza andOsindero2014,)thatlearntosamplefromadistribution p( x y|)rather thansimplysamplingfromamarginaldistribution p( x). () Denton e t a l .2015 showedthataseriesofconditionalGANscanbetrainedtoÔ¨Årstgenerateavery low-resolutionversionofanimage,thenincrementally adddetailstotheimage. ThistechniqueiscalledtheLAPGANmodel,duetotheuseofaLaplacianpyramid togeneratetheimagescontainingvaryinglevelsofdetail.LAPGANgenerators areabletofoolnotonlydiscriminatornetworksbutalsohumanobservers,with experimentalsubjectsidentifyingupto40%oftheoutputsofthenetworkas beingrealdata.SeeÔ¨ÅgureforexamplesofimagesgeneratedbyaLAPGAN 20.7 generator. OneunusualcapabilityoftheGANtrainingprocedureisthatitcanÔ¨Åtproba- bilitydistributionsthatassignzeroprobabilitytothetrainingpoints.Ratherthan maximizingthelogprobabilityofspeciÔ¨Åcpoints,thegeneratornetlearnstotrace outamanifoldwhosepointsresembletrainingpointsinsomeway.Somewhatpara- doxically,thismeansthatthemodelmayassignalog-likelihoodofnegativeinÔ¨Ånity tothetestset,whilestillrepresentingamanifoldthatahumanobserverjudges tocapturetheessenceofthegenerationtask.Thisisnotclearlyanadvantageor adisadvantage,andonemayalsoguaranteethatthegeneratornetworkassigns non-zeroprobabilitytoallpointssimplybymakingthelastlayerofthegenerator networkaddGaussiannoisetoallofthegeneratedvalues. Generatornetworks thataddGaussiannoiseinthismannersamplefromthesamedistributionthatone obtainsbyusingthegeneratornetworktoparametrizethemeanofaconditional 7 0 2 CHAPTER20.DEEPGENERATIVEMODELS Gaussiandistribution. Dropoutseemstobeimportantinthediscriminatornetwork. Inparticular, units shouldbe stochasticallydropped whilecomputingthe gradientfor the generatornetworktofollow.Followingthegradientofthedeterministicversionof thediscriminatorwithitsweightsdividedbytwodoesnotseemtobeaseÔ¨Äective. Likewise,neverusingdropoutseemstoyieldpoorresults. WhiletheGANframeworkisdesignedfordiÔ¨Äerentiablegeneratornetworks, similarprinciplescanbeusedtotrainotherkindsofmodels.Forexample,self- supervisedboostingcanbeusedtotrainanRBMgeneratortofoolalogistic regressiondiscriminator(Welling2002 e t a l .,). 20.10.5GenerativeMomentMatchingNetworks Generativemomentmatchingnetworks(,; , Li e t a l .2015Dziugaite e t a l . 2015)areanotherformofgenerativemodelbasedondiÔ¨Äerentiablegenerator networks.UnlikeVAEsandGANs,theydonotneedtopairthegeneratornetwork withanyothernetwork‚Äîneither aninferencenetworkasusedwithVAEsnora discriminatornetworkasusedwithGANs. Thesenetworksaretrainedwithatechniquecalledmomentmatching.The basicideabehindmomentmatchingistotrainthegeneratorinsuchawaythat manyofthestatisticsofsamplesgeneratedbythemodelareassimilaraspossible tothoseofthestatisticsoftheexamplesinthetrainingset.Inthiscontext,a momentisanexpectationofdiÔ¨Äerentpowersofarandomvariable.Forexample, theÔ¨Årstmomentisthemean,thesecondmomentisthemeanofthesquared values,andsoon.Inmultipledimensions,eachelementoftherandomvectormay beraisedtodiÔ¨Äerentpowers,sothatamomentmaybeanyquantityoftheform E xŒ† i xn i i (20.82) where n= [ n 1 , n 2 , . . . , n d]ÓÄæisavectorofnon-negativeintegers. UponÔ¨Årstexamination,thisapproachseemstobecomputationally infeasible. Forexample,ifwewanttomatchallthemomentsoftheform x i x j,thenweneed tominimizethediÔ¨Äerencebetweenanumberofvaluesthatisquadraticinthe dimensionof x.Moreover,evenmatchingalloftheÔ¨Årstandsecondmoments wouldonlybesuÔ¨ÉcienttoÔ¨ÅtamultivariateGaussiandistribution,whichcaptures onlylinearrelationshipsbetweenvalues.Ourambitionsforneuralnetworksareto capturecomplexnonlinearrelationships,whichwouldrequirefarmoremoments. GANsavoidthisproblemofexhaustivelyenumeratingallmomentsbyusinga 7 0 3 CHAPTER20.DEEPGENERATIVEMODELS dynamicallyupdateddiscriminatorthatautomatically focusesitsattentionon whicheverstatisticthegeneratornetworkismatchingtheleasteÔ¨Äectively. Instead,generativemomentmatchingnetworkscanbetrainedbyminimizing acostfunctioncalledmaximummeandiscrepancy(Sch√∂lkopfandSmola, 2002Gretton2012 ; e t a l .,)orMMD.Thiscostfunctionmeasurestheerrorin theÔ¨ÅrstmomentsinaninÔ¨Ånite-dimens ionalspace,usinganimplicitmapping</div>
        </div>
    </div>

    <div class="question-card" id="q83">
        <div class="question-header">
            <span class="question-number">Question 83</span>
            <span class="question-type">multiple-choice</span>
        </div>

        <div class="question-text">Regularization and data augmentation are fundamental techniques in deep learning, used to improve model generalization and stability. Different strategies, such as norm penalties, explicit constraints, and various forms of noise injection, address specific problems related to overfitting and well-posedness.

Which technique specifically constrains the norm of each column in a neural network weight matrix to prevent individual hidden units from dominating learning, and is typically implemented via projection after each update?

1) Frobenius norm penalty on the entire weight matrix   
2) L1 regularization applied globally   
3) Dropout applied to hidden units   
4) Column norm constraints with projection   
5) Label smoothing across output classes   
6) Data augmentation using input transformations   
7) Input noise injection during training</div>

        <div class="answer-section">
            <div class="answer-label">‚úì Correct Answer:</div>
            <div class="answer-text">The correct answer is 4) Column norm constraints with projection.</div>
        </div>

        <div class="reference-section">
            <div class="reference-label">üìö Reference Text:</div>
            <button class="toggle-button" onclick="toggleReference(83)">
                Show/Hide Reference
            </button>
            <div id="ref83" class="reference-text hidden">whileothersmayuseanalyticalsolutionsforwherethegradientiszero‚Äîbutinall procedures Œ±mustincreasewhenever‚Ñ¶(Œ∏) > kanddecreasewhenever‚Ñ¶(Œ∏) < k. Allpositive Œ±encourage ‚Ñ¶(Œ∏)toshrink.Theoptimalvalue Œ±‚àówillencourage ‚Ñ¶(Œ∏) toshrink,butnotsostronglytomakebecomelessthan. ‚Ñ¶()Œ∏ k TogainsomeinsightintotheeÔ¨Äectoftheconstraint,wecanÔ¨Åx Œ±‚àóandview theproblemasjustafunctionof:Œ∏ Œ∏‚àó= argmin Œ∏L(Œ∏ , Œ±‚àó) = argmin Œ∏J , Œ± (;Œ∏Xy)+‚àó‚Ñ¶()Œ∏ .(7.28) Thisisexactlythesameastheregularizedtrainingproblemofminimizing Àú J. Wecanthusthinkofaparameternormpenaltyasimposingaconstraintonthe weights.Ifisthe ‚Ñ¶ L2norm,thentheweightsareconstrainedtolieinan L2 ball. Ifisthe ‚Ñ¶ L1norm,thentheweightsareconstrainedtolieinaregionof 2 3 7 CHAPTER7.REGULARIZATIONFORDEEPLEARNING limited L1norm.Usuallywedonotknowthesizeoftheconstraintregionthatwe imposebyusingweightdecaywithcoeÔ¨Écient Œ±‚àóbecausethevalueof Œ±‚àódoesnot directlytellusthevalueof k.Inprinciple,onecansolvefor k,buttherelationship between kand Œ±‚àódependsontheformof J.Whilewedonotknowtheexactsize oftheconstraintregion,wecancontrolitroughlybyincreasingordecreasing Œ± inordertogroworshrinktheconstraintregion.Larger Œ±willresultinasmaller constraintregion.Smallerwillresultinalargerconstraintregion. Œ± Sometimeswemaywishtouseexplicitconstraintsratherthanpenalties.As describedinsection,wecanmodifyalgorithmssuchasstochasticgradient 4.4 descenttotakeastepdownhillon J(Œ∏)andthenprojectŒ∏backtothenearest pointthatsatisÔ¨Åes‚Ñ¶(Œ∏) < k.Thiscanbeusefulifwehaveanideaofwhatvalue of kisappropriateanddonotwanttospendtimesearchingforthevalueof Œ±that correspondstothis. k Anotherreasontouseexplicitconstraintsandreprojectionratherthanenforcing constraintswithpenaltiesisthatpenaltiescancausenon-convexoptimization procedurestogetstuckinlocalminimacorrespondingtosmallŒ∏.Whentraining neuralnetworks,thisusuallymanifestsasneuralnetworksthattrainwithseveral ‚Äúdeadunits.‚ÄùTheseareunitsthatdonotcontributemuchtothebehaviorofthe functionlearnedbythenetworkbecausetheweightsgoingintooroutofthemare allverysmall. Whentrainingwithapenaltyonthenormoftheweights,these conÔ¨Ågurations canbelocallyoptimal,evenifitispossibletosigniÔ¨Åcantlyreduce Jbymakingtheweightslarger.Explicitconstraintsimplementedbyre-projection canworkmuchbetterinthesecasesbecausetheydonotencouragetheweights toapproachtheorigin.Explicitconstraintsimplemented byre-projectiononly haveaneÔ¨Äectwhentheweightsbecomelargeandattempttoleavetheconstraint region. Finally,explicitconstraintswithreprojectioncanbeusefulbecausetheyimpose somestabilityontheoptimization procedure.Whenusinghighlearningrates,it ispossibletoencounterapositivefeedbackloopinwhichlargeweightsinduce largegradientswhichtheninducealargeupdatetotheweights.Iftheseupdates consistentlyincreasethesizeoftheweights,thenŒ∏rapidlymovesawayfrom theoriginuntilnumericaloverÔ¨Çowoccurs.Explicitconstraintswithreprojection preventthisfeedbackloopfromcontinuingtoincreasethemagnitudeoftheweights withoutbound. ()recommendusingconstraintscombinedwith Hintonetal.2012c ahighlearningratetoallowrapidexplorationofparameterspacewhilemaintaining somestability. Inparticular,Hinton2012cetal.()recommendastrategyintroducedbySrebro andShraibman2005():constrainingthenormofeachcolumnoftheweightmatrix 2 3 8 CHAPTER7.REGULARIZATIONFORDEEPLEARNING ofaneuralnetlayer,ratherthanconstrainingtheFrobeniusnormoftheentire weightmatrix.Constrainingthenormofeachcolumnseparatelypreventsanyone hiddenunitfromhavingverylargeweights.Ifweconvertedthisconstraintintoa penaltyinaLagrangefunction,itwouldbesimilarto L2weightdecaybutwitha separateKKTmultiplierfortheweightsofeachhiddenunit.EachoftheseKKT multiplierswouldbedynamicallyupdatedseparatelytomakeeachhiddenunit obeytheconstraint.Inpractice,columnnormlimitationisalwaysimplementedas anexplicitconstraintwithreprojection. 7.3RegularizationandUnder-ConstrainedProblems Insomecases,regularizationisnecessaryformachinelearningproblemstobeprop- erlydeÔ¨Åned.Manylinearmodelsinmachinelearning,includinglinearregression andPCA,dependoninvertingthematrixXÓÄæX.Thisisnotpossiblewhenever XÓÄæXissingular.Thismatrixcanbesingularwheneverthedatageneratingdistri- butiontrulyhasnovarianceinsomedirection,orwhennovarianceisobservedin somedirectionbecausetherearefewerexamples(rowsofX)thaninputfeatures (columnsofX).Inthiscase,manyformsofregularizationcorrespondtoinverting XÓÄæXI+ Œ±instead.Thisregularizedmatrixisguaranteedtobeinvertible. Theselinearproblemshaveclosedformsolutionswhentherelevantmatrix isinvertible.Itisalsopossibleforaproblemwithnoclosedformsolutiontobe underdetermined. Anexampleislogisticregressionappliedtoaproblemwhere theclassesarelinearlyseparable.Ifaweightvectorwisabletoachieveperfect classiÔ¨Åcation,then2wwillalsoachieveperfectclassiÔ¨Åcationandhigherlikelihood. Aniterativeoptimization procedurelikestochasticgradientdescentwillcontinually increasethemagnitudeofwand,intheory,willneverhalt.Inpractice,anumerical implementationofgradientdescentwilleventuallyreachsuÔ¨Écientlylargeweights tocausenumericaloverÔ¨Çow,atwhichpointitsbehaviorwilldependonhowthe programmerhasdecidedtohandlevaluesthatarenotrealnumbers. Mostformsofregularizationareabletoguaranteetheconvergenceofiterative methodsappliedtounderdetermined problems. Forexample,weightdecaywill causegradientdescenttoquitincreasingthemagnitudeoftheweightswhenthe slopeofthelikelihoodisequaltotheweightdecaycoeÔ¨Écient. Theideaofusingregularizationtosolveunderdetermined problemsextends beyondmachinelearning.Thesameideaisusefulforseveralbasiclinearalgebra problems. Aswesawinsection,wecansolveunderdetermined linearequationsusing 2.9 2 3 9 CHAPTER7.REGULARIZATIONFORDEEPLEARNING theMoore-Penrosepseudoinverse.RecallthatonedeÔ¨Ånitionofthepseudoinverse X+ofamatrixisX X+=lim Œ±ÓÄ¶ 0(XÓÄæXI+ Œ±)‚àí 1XÓÄæ. (7.29) Wecannowrecognizeequationasperforminglinearregressionwithweight 7.29 decay.SpeciÔ¨Åcally,equationisthelimitofequationastheregularization 7.29 7.17 coeÔ¨Écientshrinkstozero.Wecanthusinterpretthepseudoinverseasstabilizing underdetermined problemsusingregularization. 7.4DatasetAugmentation Thebestwaytomakeamachinelearningmodelgeneralizebetteristotrainiton moredata.Ofcourse,inpractice,theamountofdatawehaveislimited.Oneway togetaroundthisproblemistocreatefakedataandaddittothetrainingset. Forsomemachinelearningtasks,itisreasonablystraightforwardtocreatenew fakedata. ThisapproachiseasiestforclassiÔ¨Åcation.AclassiÔ¨Åerneedstotakeacompli- cated,highdimensionalinputxandsummarizeitwithasinglecategoryidentity y. ThismeansthatthemaintaskfacingaclassiÔ¨Åeristobeinvarianttoawidevariety oftransformations.Wecangeneratenew(x , y)pairseasilyjustbytransforming theinputsinourtrainingset. x Thisapproachisnotasreadilyapplicabletomanyothertasks.Forexample,it isdiÔ¨Éculttogeneratenewfakedataforadensityestimationtaskunlesswehave alreadysolvedthedensityestimationproblem. DatasetaugmentationhasbeenaparticularlyeÔ¨ÄectivetechniqueforaspeciÔ¨Åc classiÔ¨Åcationproblem:objectrecognition.Imagesarehighdimensionalandinclude anenormousvarietyoffactorsofvariation,manyofwhichcanbeeasilysimulated. Operationsliketranslatingthetrainingimagesafewpixelsineachdirectioncan oftengreatlyimprovegeneralization, evenifthemodelhasalreadybeendesignedto bepartiallytranslationinvariantbyusingtheconvolutionandpoolingtechniques describedinchapter.Manyotheroperationssuchasrotatingtheimageorscaling 9 theimagehavealsoprovenquiteeÔ¨Äective. Onemustbecarefulnottoapplytransformationsthatwouldchangethecorrect class.Forexample,opticalcharacterrecognitiontasksrequirerecognizingthe diÔ¨Äerencebetween‚Äòb‚Äôand‚Äòd‚ÄôandthediÔ¨Äerencebetween‚Äò6‚Äôand‚Äò9‚Äô,sohorizontal Ô¨Çipsand180‚ó¶rotationsarenotappropriatewaysofaugmentingdatasetsforthese tasks. 2 4 0 CHAPTER7.REGULARIZATIONFORDEEPLEARNING TherearealsotransformationsthatwewouldlikeourclassiÔ¨Åerstobeinvariant to,butwhicharenoteasytoperform.Forexample,out-of-planerotationcannot beimplementedasasimplegeometricoperationontheinputpixels. DatasetaugmentationiseÔ¨Äectiveforspeechrecognitiontasksaswell(Jaitly andHinton2013,). Injectingnoiseintheinputtoaneuralnetwork(SietsmaandDow1991,) canalsobeseenasaformofdataaugmentation.FormanyclassiÔ¨Åcationand evensomeregressiontasks,thetaskshouldstillbepossibletosolveevenifsmall randomnoiseisaddedtotheinput.Neuralnetworksprovenottobeveryrobust tonoise,however(TangandEliasmith2010,).Onewaytoimprovetherobustness ofneuralnetworksissimplytotrainthemwithrandomnoiseappliedtotheir inputs.Inputnoiseinjectionispartofsomeunsupervisedlearningalgorithmssuch asthedenoisingautoencoder(Vincent2008etal.,).Noiseinjectionalsoworks whenthenoiseisappliedtothehiddenunits,whichcanbeseenasdoingdataset augmentationatmultiplelevelsofabstraction.Poole2014etal.()recentlyshowed thatthisapproachcanbehighlyeÔ¨Äectiveprovidedthatthemagnitudeofthe noiseiscarefullytuned.Dropout,apowerfulregularizationstrategythatwillbe describedinsection,canbeseenasaprocessofconstructingnewinputsby 7.12 multiplyingbynoise. Whencomparingmachinelearningbenchmarkresults,itisimportanttotake theeÔ¨Äectofdatasetaugmentationintoaccount.Often,hand-designeddataset augmentationschemescandramaticallyreducethegeneralization errorofamachine learningtechnique.Tocomparetheperformanceofonemachinelearningalgorithm toanother,itisnecessarytoperformcontrolledexperiments.Whencomparing machinelearningalgorithmAandmachinelearningalgorithmB,itisnecessary tomakesurethatbothalgorithmswereevaluatedusingthesamehand-designed datasetaugmentationschemes.SupposethatalgorithmAperformspoorlywith nodatasetaugmentationandalgorithmBperformswellwhencombinedwith numeroussynthetictransformationsoftheinput.Insuchacaseitislikelythe synthetictransformationscausedtheimprovedperformance,ratherthantheuse ofmachinelearningalgorithmB.Sometimesdecidingwhetheranexperiment hasbeenproperlycontrolledrequiressubjectivejudgment.Forexample,machine learningalgorithmsthatinjectnoiseintotheinputareperformingaformofdataset augmentation.Usually,operationsthataregenerallyapplicable(suchasadding Gaussiannoisetotheinput)areconsideredpartofthemachinelearningalgorithm, whileoperationsthatarespeciÔ¨Åctooneapplicationdomain(suchasrandomly croppinganimage)areconsideredtobeseparatepre-processingsteps. 2 4 1 CHAPTER7.REGULARIZATIONFORDEEPLEARNING 7.5NoiseRobustness Sectionhasmotivatedtheuseofnoiseappliedtotheinputsasadataset 7.4 augmentationstrategy.Forsomemodels,theadditionofnoisewithinÔ¨Ånitesimal varianceattheinputofthemodelisequivalenttoimposingapenaltyonthe normoftheweights(,,).Inthegeneralcase,itisimportantto Bishop1995ab rememberthatnoiseinjectioncanbemuchmorepowerfulthansimplyshrinking theparameters,especiallywhenthenoiseisaddedtothehiddenunits.Noise appliedtothehiddenunitsissuchanimportanttopicthatitmerititsownseparate discussion;thedropoutalgorithmdescribedinsectionisthemaindevelopment 7.12 ofthatapproach. Anotherwaythatnoisehasbeenusedintheserviceofregularizingmodels isbyaddingittotheweights.Thistechniquehasbeenusedprimarilyinthe contextofrecurrentneuralnetworks(,; Jimetal.1996Graves2011,). Thiscan beinterpretedasa stochasticimplementation of Bayesianinference overthe weights. TheBayesiantreatmentoflearningwouldconsiderthemodelweights tobeuncertainandrepresentableviaaprobabilitydistributionthatreÔ¨Çectsthis uncertainty.Addingnoisetotheweightsisapractical,stochasticwaytoreÔ¨Çect thisuncertainty. Noiseappliedtotheweightscanalsobeinterpretedasequivalent(undersome assumptions)toamoretraditionalformofregularization, encouragingstabilityof thefunctiontobelearned.Considertheregressionsetting,wherewewishtotrain afunction ÀÜ y(x)thatmapsasetoffeaturesxtoascalarusingtheleast-squares costfunctionbetweenthemodelpredictions ÀÜ y()xandthetruevalues: y J= E p x , y ( )ÓÄÇ(ÀÜ y y ()x‚àí)2ÓÄÉ . (7.30) Thetrainingsetconsistsoflabeledexamples m {(x( 1 ), y( 1 )) ( , . . . ,x( ) m, y( ) m)}. Wenowassumethatwitheachinputpresentationwealsoincludearandom perturbation ÓÄè W‚àºN(ÓÄè; 0 , Œ∑I)ofthenetworkweights.Letusimaginethatwe haveastandard l-layerMLP.WedenotetheperturbedmodelasÀÜ y ÓÄè W(x).Despite theinjectionofnoise,wearestillinterestedinminimizingthesquarederrorofthe outputofthenetwork.Theobjectivefunctionthusbecomes: Àú J W= E p , y , ( x ÓÄè W )ÓÅ® (ÀÜ y ÓÄè W() )x‚àí y2ÓÅ© (7.31) = E p , y , ( x ÓÄè W )ÓÄÇ ÀÜ y2 ÓÄè W()2ÀÜx‚àí y y ÓÄè W()+x y2ÓÄÉ .(7.32) Forsmall Œ∑,theminimization of Jwithaddedweightnoise(withcovariance Œ∑I)isequivalenttominimization of Jwithanadditionalregularizationterm: 2 4 2 CHAPTER7.REGULARIZATIONFORDEEPLEARNING Œ∑ E p , y ( x )ÓÄÇÓÅ´‚àá WÀÜ y()xÓÅ´2ÓÄÉ .Thisformofregularizationencouragestheparametersto gotoregionsofparameterspacewheresmallperturbationsoftheweightshave arelativelysmallinÔ¨Çuenceontheoutput.Inotherwords,itpushesthemodel intoregionswherethemodelisrelativelyinsensitivetosmallvariationsinthe weights,Ô¨Åndingpointsthatarenotmerelyminima,butminimasurroundedby Ô¨Çatregions(HochreiterandSchmidhuber1995,).InthesimpliÔ¨Åedcaseoflinear regression(where,forinstance, ÀÜ y(x) =wÓÄæx+ b),thisregularizationtermcollapses into Œ∑ E p ( ) xÓÄÇ ÓÅ´ÓÅ´x2ÓÄÉ ,whichisnotafunctionofparametersandthereforedoesnot contributetothegradientofÀú J Wwithrespecttothemodelparameters. 7 . 5 . 1 In j ect i n g No i s e a t t h e O u t p u t T a rg et s Mostdatasetshavesomeamountofmistakesinthe ylabels.Itcanbeharmfulto maximize log p( y|x)when yisamistake.Onewaytopreventthisistoexplicitly modelthenoiseonthelabels.Forexample,wecanassumethatforsomesmall constant ÓÄè,thetrainingsetlabel yiscorrectwithprobability 1‚àí ÓÄè,andotherwise anyoftheotherpossiblelabelsmightbecorrect.Thisassumptioniseasyto incorporateintothecostfunctionanalytically,ratherthanbyexplicitlydrawing noisesamples.Forexample,labelsmoothingregularizesamodelbasedona softmaxwith koutputvaluesbyreplacingthehardandclassiÔ¨Åcationtargets 0 1 withtargetsofÓÄè k‚àí 1and1‚àí ÓÄè,respectively.Thestandardcross-entropylossmay thenbeusedwiththesesofttargets.Maximumlikelihoodlearningwithasoftmax classiÔ¨Åerandhardtargetsmayactuallyneverconverge‚Äîthesoftmaxcannever predictaprobabilityofexactlyorexactly,soitwillcontinuetolearnlarger 0 1 andlargerweights,makingmoreextremepredictionsforever.Itispossibleto preventthisscenariousingotherregularizationstrategieslikeweightdecay.Label smoothinghastheadvantageofpreventingthepursuitofhardprobabilitieswithout discouragingcorrectclassiÔ¨Åcation.Thisstrategyhasbeenusedsincethe1980s andcontinuestobefeaturedprominentlyinmodernneuralnetworks(Szegedy etal.,).2015 7.6Semi-SupervisedLearning Intheparadigmofsemi-supervisedlearning,bothunlabeledexamplesfrom P( x) andlabeledexamplesfrom P( x y</div>
        </div>
    </div>

    <div class="question-card" id="q84">
        <div class="question-header">
            <span class="question-number">Question 84</span>
            <span class="question-type">multiple-choice</span>
        </div>

        <div class="question-text">In machine learning, selecting an appropriate model involves balancing complexity with the amount of available data to achieve optimal generalization performance. Overfitting and underfitting are key challenges that practitioners must address when training predictive algorithms.

Which scenario best exemplifies overfitting in a supervised learning regression task using polynomial models?

1) Using a linear model to fit data generated by a quadratic function, resulting in high bias.   
2) Applying a quadratic model to fit data generated by a quadratic function, achieving low error on both training and test sets.   
3) Training a linear model on a large dataset and observing similar errors on both training and test sets.   
4) Employing a ninth-degree polynomial to fit data generated by a quadratic function, resulting in excellent training accuracy but poor performance on the test set.   
5) Selecting a model with fewer parameters than necessary, causing consistently high error on all data splits.   
6) Using regularization techniques to penalize large weights and obtain a smoother fit.   
7) Training a model with insufficient data, leading to unstable parameter estimates and poor generalization.</div>

        <div class="answer-section">
            <div class="answer-label">‚úì Correct Answer:</div>
            <div class="answer-text">The correct answer is 4) Employing a ninth-degree polynomial to fit data generated by a quadratic function, resulting in excellent training accuracy but poor performance on the test set..</div>
        </div>

        <div class="reference-section">
            <div class="reference-label">üìö Reference Text:</div>
            <button class="toggle-button" onclick="toggleReference(84)">
                Show/Hide Reference
            </button>
            <div id="ref84" class="reference-text hidden">CHAPTER5.MACHINELEARNINGBASICS cansolvethesupervisedlearningproblemoflearningp(y| x)byusingtraditional unsupervised learningtechnologiesto learn thejointdistributionp( x,y)and inferring py(| x) =p,y( x)ÓÅê yÓÄ∞p,y( xÓÄ∞). (5.2) Thoughunsupervisedlearningandsupervisedlearningarenotcompletelyformalor distinctconcepts,theydohelptoroughlycategorizesomeofthethingswedowith machinelearningalgorithms.Traditionally,peoplerefertoregression,classiÔ¨Åcation andstructuredoutputproblemsassupervisedlearning.Densityestimationin supportofothertasksisusuallyconsideredunsupervisedlearning. Othervariantsofthelearningparadigmarepossible.Forexample,insemi- supervisedlearning,someexamplesincludeasupervisiontargetbutothersdo not.Inmulti-instancelearning,anentirecollectionofexamplesislabeledas containingornotcontaininganexampleofaclass,buttheindividualmembers ofthecollectionarenotlabeled.Forarecentexampleofmulti-instancelearning withdeepmodels,seeKotzias 2015etal.(). SomemachinelearningalgorithmsdonotjustexperienceaÔ¨Åxeddataset.For example,reinforcementlearningalgorithmsinteractwithanenvironment,so thereisafeedbackloopbetweenthelearningsystemanditsexperiences. Such algorithmsarebeyondthescopeofthisbook.Pleasesee () SuttonandBarto1998 orBertsekasandTsitsiklis1996()forinformationaboutreinforcementlearning, and ()forthedeeplearningapproachtoreinforcementlearning. Mnihetal.2013 Mostmachinelearningalgorithmssimplyexperienceadataset.Adatasetcan bedescribedinmanyways.Inallcases,adatasetisacollectionofexamples, whichareinturncollectionsoffeatures. Onecommonwayofdescribingadatasetiswitha .Adesign designmatrix matrixisamatrixcontainingadiÔ¨Äerentexampleineachrow.Eachcolumnofthe matrixcorrespondstoadiÔ¨Äerentfeature.Forinstance,theIrisdatasetcontains 150exampleswithfourfeaturesforeachexample.Thismeanswecanrepresent thedatasetwithadesignmatrixX‚àà R1504 √ó,whereX i ,1isthesepallengthof planti,X i ,2isthesepalwidthofplanti,etc.Wewilldescribemostofthelearning algorithmsinthisbookintermsofhowtheyoperateondesignmatrixdatasets. Ofcourse,todescribeadatasetasadesignmatrix,itmustbepossibleto describeeachexampleasavector,andeachofthesevectorsmustbethesamesize. Thisisnotalwayspossible.Forexample,ifyouhaveacollectionofphotographs withdiÔ¨Äerentwidthsandheights,thendiÔ¨ÄerentphotographswillcontaindiÔ¨Äerent numbersofpixels,sonotallofthephotographs maybedescribedwiththesame lengthofvector.SectionandchapterdescribehowtohandlediÔ¨Äerent 9.7 10 1 0 6 CHAPTER5.MACHINELEARNINGBASICS typesofsuchheterogeneous data.Incaseslikethese,ratherthandescribingthe datasetasamatrixwithmrows,wewilldescribeitasasetcontainingmelements: {x(1),x(2),...,x() m}.Thisnotationdoesnotimplythatanytwoexamplevectors x() iandx() jhavethesamesize. Inthecaseofsupervisedlearning,theexamplecontainsalabelortargetas wellasacollectionoffeatures.Forexample,ifwewanttousealearningalgorithm toperformobjectrecognitionfromphotographs, weneedtospecifywhichobject appearsineachofthephotos.Wemightdothiswithanumericcode,with0 signifyingaperson,1signifyingacar,2signifyingacat,etc.Oftenwhenworking withadatasetcontainingadesignmatrixoffeatureobservationsX,wealso provideavectoroflabels,withyy iprovidingthelabelforexample.i Ofcourse,sometimesthelabelmaybemorethanjustasinglenumber.For example,ifwewanttotrainaspeechrecognitionsystemtotranscribeentire sentences,thenthelabelforeachexamplesentenceisasequenceofwords. JustasthereisnoformaldeÔ¨Ånitionofsupervisedandunsupervisedlearning, thereisnorigidtaxonomyofdatasetsorexperiences.Thestructuresdescribedhere covermostcases,butitisalwayspossibletodesignnewonesfornewapplications. 5.1.4Example:LinearRegression OurdeÔ¨Ånitionofamachinelearningalgorithmasanalgorithmthatiscapable ofimprovingacomputerprogram‚Äôsperformanceatsometaskviaexperienceis somewhatabstract.Tomakethismoreconcrete,wepresentanexampleofa simplemachinelearningalgorithm:linearregression.Wewillreturntothis examplerepeatedlyasweintroducemoremachinelearningconceptsthathelpto understanditsbehavior. Asthenameimplies,linearregressionsolvesaregressionproblem. Inother words,thegoalistobuildasystemthatcantakeavectorx‚àà Rnasinputand predictthevalueofascalary‚àà Rasitsoutput.Inthecaseoflinearregression, theoutputisalinearfunctionoftheinput.LetÀÜybethevaluethatourmodel predictsshouldtakeon.WedeÔ¨Ånetheoutputtobe y ÀÜy= wÓÄæx (5.3) wherew‚àà Rnisavectorof .parameters Parametersarevaluesthatcontrolthebehaviorofthesystem.Inthiscase,w iis thecoeÔ¨Écientthatwemultiplybyfeaturex ibeforesummingupthecontributions fromallthefeatures.Wecanthinkofwasasetofweightsthatdeterminehow eachfeatureaÔ¨Äectstheprediction. If afeaturex ireceivesapositiveweightw i, 1 0 7 CHAPTER5.MACHINELEARNINGBASICS thenincreasingthevalueofthatfeatureincreasesthevalueofourprediction ÀÜy. Ifafeaturereceivesanegativeweight,thenincreasingthevalueofthatfeature decreasesthevalueofourprediction.Ifafeature‚Äôsweightislargeinmagnitude, thenithasalargeeÔ¨Äectontheprediction.Ifafeature‚Äôsweightiszero,ithasno eÔ¨Äectontheprediction. WethushaveadeÔ¨ÅnitionofourtaskT: topredictyfromxbyoutputting ÀÜy= wÓÄæx.NextweneedadeÔ¨Ånitionofourperformancemeasure,.P Supposethatwehaveadesignmatrixofmexampleinputsthatwewillnot usefortraining,onlyforevaluatinghowwellthemodelperforms.Wealsohave avectorofregressiontargetsprovidingthecorrectvalueofyforeachofthese examples.Becausethisdatasetwillonlybeusedforevaluation,wecallitthetest set.WerefertothedesignmatrixofinputsasX()testandthevectorofregression targetsasy()test. Onewayofmeasuringtheperformanceofthemodelistocomputethemean squarederrorofthemodelonthetestset.IfÀÜy()testgivesthepredictionsofthe modelonthetestset,thenthemeansquarederrorisgivenby MSEtest=1 mÓÅò i(ÀÜy()test‚àíy()test)2 i. (5.4) Intuitively,onecanseethatthiserrormeasuredecreasesto0when ÀÜy()test=y()test. Wecanalsoseethat MSEtest=1 m||ÀÜy()test‚àíy()test||2 2, (5.5) sotheerrorincreaseswhenevertheEuclideandistancebetweenthepredictions andthetargetsincreases. Tomakeamachinelearningalgorithm,weneedtodesignanalgorithmthat willimprovetheweightswinawaythatreducesMSEtestwhenthealgorithm isallowedtogainexperiencebyobservingatrainingset(X()train,y()train).One intuitivewayofdoingthis(whichwewilljustifylater,insection)isjustto 5.5.1 minimizethemeansquarederroronthetrainingset,MSEtrain. TominimizeMSEtrain,wecansimplysolveforwhereitsgradientis: 0 ‚àá wMSEtrain= 0 (5.6) ‚áí‚àá w1 m||ÀÜy()train‚àíy()train||2 2= 0 (5.7) ‚áí1 m‚àá w||X()trainwy‚àí()train||2 2= 0 (5.8) 1 0 8 CHAPTER5.MACHINELEARNINGBASICS ‚àí ‚àí 1 0 . 0 5 0 0 0 5 1 0 . . . . x1‚àí 3‚àí 2‚àí 10123yL i n ea r r eg r es s i o n ex a m p l e 0 5 1 0 1 5 . . . w10 2 0 .0 2 5 .0 3 0 .0 3 5 .0 4 0 .0 4 5 .0 5 0 .0 5 5 .MSE(train)O p t i m i za t i o n o f w Figure5.1:Alinearregressionproblem,withatrainingsetconsistingoftendatapoints, eachcontainingonefeature.Becausethereisonlyonefeature,theweightvectorw containsonlyasingleparametertolearn,w 1. ( L e f t )Observethatlinearregressionlearns tosetw 1suchthattheliney=w 1xcomesascloseaspossibletopassingthroughallthe trainingpoints.Theplottedpointindicatesthevalueof ( R i g h t ) w 1foundbythenormal equations,whichwecanseeminimizesthemeansquarederroronthetrainingset. ‚áí‚àá wÓÄê X()trainwy‚àí()trainÓÄëÓÄæÓÄê X()trainwy‚àí()trainÓÄë = 0(5.9) ‚áí‚àá wÓÄê wÓÄæX()train ÓÄæX()trainww‚àí2ÓÄæX()train ÓÄæy()train+y()train ÓÄæy()trainÓÄë = 0 (5.10) ‚áí2X()train ÓÄæX()trainwX‚àí2()train ÓÄæy()train= 0(5.11) ‚áíw=ÓÄê X()train ÓÄæX()trainÓÄë‚àí1 X()train ÓÄæy()train(5.12) Thesystemofequationswhosesolutionisgivenbyequationisknownas 5.12 thenormalequations.Evaluatingequationconstitutesasimplelearning 5.12 algorithm.Foranexampleofthelinearregressionlearningalgorithminaction, seeÔ¨Ågure.5.1 Itisworthnotingthatthetermlinearregressionisoftenusedtoreferto aslightlymoresophisticatedmodelwithoneadditionalparameter‚Äîan intercept term.Inthismodelb ÀÜy= wÓÄæx+b (5.13) sothemappingfromparameterstopredictionsisstillalinearfunctionbutthe mappingfromfeaturestopredictionsisnowanaÔ¨Énefunction.Thisextensionto aÔ¨Énefunctionsmeansthattheplotofthemodel‚Äôspredictionsstilllookslikea line,butitneednotpassthroughtheorigin.Insteadofaddingthebiasparameter 1 0 9 CHAPTER5.MACHINELEARNINGBASICS b,onecancontinuetousethemodelwithonlyweightsbutaugmentxwithan extraentrythatisalwayssetto.Theweightcorrespondingtotheextraentry 1 1 playstheroleofthebiasparameter.Wewillfrequentlyusetheterm‚Äúlinear‚Äùwhen referringtoaÔ¨Énefunctionsthroughoutthisbook. TheintercepttermbisoftencalledthebiasparameteroftheaÔ¨Énetransfor- mation.Thisterminologyderivesfromthepointofviewthattheoutputofthe transformationisbiasedtowardbeingbintheabsenceofanyinput.Thisterm isdiÔ¨Äerentfromtheideaofastatisticalbias,inwhichastatisticalestimation algorithm‚Äôsexpectedestimateofaquantityisnotequaltothetruequantity. Linearregressionisofcourseanextremelysimpleandlimitedlearningalgorithm, butitprovidesanexampleofhowalearningalgorithmcanwork.Inthesubsequent sectionswewilldescribesomeofthebasicprinciplesunderlyinglearningalgorithm designanddemonstratehowtheseprinciplescanbeusedtobuildmorecomplicated learningalgorithms. 5.2Capacity,OverÔ¨ÅttingandUnderÔ¨Åtting Thecentralchallengeinmachinelearningisthatwemustperformwellonnew, previouslyunseeninputs‚Äînotjustthoseonwhichourmodelwastrained. The abilitytoperformwellonpreviouslyunobservedinputsiscalledgeneralization. Typically,whentrainingamachinelearningmodel,wehaveaccesstoatraining set,wecancomputesomeerrormeasureonthetrainingsetcalledthetraining error,andwereducethistrainingerror.Sofar,whatwehavedescribedissimply anoptimization problem.Whatseparatesmachinelearningfromoptimization is thatwewantthegeneralizationerror,alsocalledthetesterror,tobelowas well. Thegeneralization errorisdeÔ¨Ånedastheexpectedvalueoftheerrorona newinput.HeretheexpectationistakenacrossdiÔ¨Äerentpossibleinputs,drawn fromthedistributionofinputsweexpectthesystemtoencounterinpractice. Wetypicallyestimatethegeneralization errorofamachinelearningmodelby measuringitsperformanceonatestsetofexamplesthatwerecollectedseparately fromthetrainingset. Inourlinearregressionexample,wetrainedthemodelbyminimizingthe trainingerror, 1 m()train||X()trainwy‚àí()train||2 2, (5.14) butweactuallycareaboutthetesterror,1 m()test||X()testwy‚àí()test||2 2. HowcanweaÔ¨Äectperformanceonthetestsetwhenwegettoobserveonlythe 1 1 0 CHAPTER5.MACHINELEARNINGBASICS trainingset?TheÔ¨Åeldofstatisticallearningtheoryprovidessomeanswers.If thetrainingandthetestsetarecollectedarbitrarily,thereisindeedlittlewecan do.Ifweareallowedtomakesomeassumptionsabouthowthetrainingandtest setarecollected,thenwecanmakesomeprogress. Thetrainandtestdataaregeneratedbyaprobabilitydistributionoverdatasets calledthedatageneratingprocess.Wetypicallymakeasetofassumptions knowncollectivelyasthei.i.d. assumptions. Theseassumptionsarethatthe examplesineachdatasetareindependentfromeachother,andthatthetrain setandtestsetareidenticallydistributed,drawnfromthesameprobability distributionaseachother. Thisassumptionallowsustodescribethedatagen- eratingprocesswithaprobabilitydistributionoverasingleexample.Thesame distributionisthenusedtogenerateeverytrainexampleandeverytestexample. Wecallthatsharedunderlyingdistributionthedatageneratingdistribution, denotedpdata.Thisprobabilisticframeworkandthei.i.d.assumptionsallowusto mathematically studytherelationshipbetweentrainingerrorandtesterror. Oneimmediateconnectionwecanobservebetweenthetrainingandtesterror isthattheexpectedtrainingerrorofarandomlyselectedmodelisequaltothe expectedtesterrorofthatmodel.Supposewehaveaprobabilitydistribution p(x,y)andwesamplefromitrepeatedlytogeneratethetrainsetandthetest set.ForsomeÔ¨Åxedvaluew,theexpectedtrainingseterrorisexactlythesameas theexpectedtestseterror,becausebothexpectationsareformedusingthesame datasetsamplingprocess.TheonlydiÔ¨Äerencebetweenthetwoconditionsisthe nameweassigntothedatasetwesample. Ofcourse, when weuseamachinelearning algorithm, w edonotÔ¨Åxthe parametersaheadoftime,thensamplebothdatasets.Wesamplethetrainingset, thenuseittochoosetheparameterstoreducetrainingseterror,thensamplethe testset.Underthisprocess,theexpectedtesterrorisgreaterthanorequalto theexpectedvalueoftrainingerror.Thefactorsdetermininghowwellamachine learningalgorithmwillperformareitsabilityto: 1. Makethetrainingerrorsmall. 2. Makethegapbetweentrainingandtesterrorsmall. Thesetwofactorscorrespondtothetwocentralchallengesinmachinelearning: underÔ¨ÅttingandoverÔ¨Åtting.UnderÔ¨Åttingoccurswhenthemodelisnotableto obtainasuÔ¨Écientlylowerrorvalueonthetrainingset.OverÔ¨Åttingoccurswhen thegapbetweenthetrainingerrorandtesterroristoolarge. WecancontrolwhetheramodelismorelikelytooverÔ¨ÅtorunderÔ¨Åtbyaltering itscapacity.Informally,amodel‚ÄôscapacityisitsabilitytoÔ¨Åtawidevarietyof 1 1 1 CHAPTER5.MACHINELEARNINGBASICS functions.ModelswithlowcapacitymaystruggletoÔ¨Åtthetrainingset.Models withhighcapacitycanoverÔ¨Åtbymemorizingpropertiesofthetrainingsetthatdo notservethemwellonthetestset. Onewaytocontrolthecapacityofalearningalgorithmisbychoosingits hypothesisspace,thesetoffunctionsthatthelearningalgorithmisallowedto selectasbeingthesolution.Forexample,thelinearregressionalgorithmhasthe setofalllinearfunctionsofitsinputasitshypothesisspace.Wecangeneralize linearregressiontoincludepolynomials,ratherthanjustlinearfunctions,inits hypothesisspace.Doingsoincreasesthemodel‚Äôscapacity. Apolynomialofdegreeonegivesusthelinearregressionmodelwithwhichwe arealreadyfamiliar,withprediction ÀÜybwx. = + (5.15) Byintroducingx2asanotherfeatureprovidedtothelinearregressionmodel,we canlearnamodelthatisquadraticasafunctionof:x ÀÜybw = +1xw+2x2. (5.16) Thoughthismodelimplementsaquadraticfunctionofits,theoutputis input stillalinearfunctionoftheparameters,sowecanstillusethenormalequations totrainthemodelinclosedform.Wecancontinuetoaddmorepowersofxas additionalfeatures,forexampletoobtainapolynomialofdegree9: ÀÜyb= +9ÓÅò i=1w ixi. (5.17) Machinelearningalgorithmswillgenerallyperformbestwhentheircapacity isappropriateforthetruecomplexityofthetasktheyneedtoperformandthe amountoftrainingdatatheyareprovidedwith.ModelswithinsuÔ¨Écientcapacity areunabletosolvecomplextasks.Modelswithhighcapacitycansolvecomplex tasks,butwhentheircapacityishigherthanneededtosolvethepresenttaskthey mayoverÔ¨Åt. Figureshowsthisprincipleinaction.Wecomparealinear,quadratic 5.2 anddegree-9predictorattemptingtoÔ¨Åtaproblemwherethetrueunderlying functionisquadratic. Thelinearfunctionisunabletocapturethecurvaturein thetrueunderlyingproblem,soitunderÔ¨Åts.Thedegree-9predictoriscapableof representingthecorrectfunction,butitisalsocapableofrepresentinginÔ¨Ånitely manyotherfunctionsthatpassexactlythroughthetrainingpoints,becausewe 1 1 2 CHAPTER5.MACHINELEARNINGBASICS havemoreparametersthantrainingexamples.Wehavelittlechanceofchoosing asolutionthatgeneralizeswellwhensomanywildlydiÔ¨Äerentsolutionsexist.In thisexample,thequadraticmodelisperfectlymatchedtothetruestructureof</div>
        </div>
    </div>

    <div class="question-card" id="q85">
        <div class="question-header">
            <span class="question-number">Question 85</span>
            <span class="question-type">multiple-choice</span>
        </div>

        <div class="question-text">Semi-supervised and multi-task learning are advanced strategies in deep learning designed to improve generalization by leveraging both labeled and unlabeled data, as well as shared structure between related tasks. These approaches often involve specialized parameter sharing and hybrid training objectives.

In multi-task deep learning architectures, which of the following best describes how model parameters are typically organized to maximize generalization across related tasks?

1) Lower layers consist of shared parameters across tasks, while upper layers contain task-specific parameters for specialization.   
2) All layers are fully task-specific, with no parameter sharing to prevent interference between tasks.   
3) Upper layers are shared across tasks, while lower layers are task-specific to capture input-specific variations.   
4) Only the output layer parameters are shared while all other layers are task-specific.   
5) Shared parameters are exclusively used for generative modeling, and discriminative tasks use separate networks.   
6) Each task uses completely independent networks with no shared parameters or representations.   
7) Only intermediate layers are shared, while both input and output layers remain task-specific.</div>

        <div class="answer-section">
            <div class="answer-label">‚úì Correct Answer:</div>
            <div class="answer-text">The correct answer is 1) Lower layers consist of shared parameters across tasks, while upper layers contain task-specific parameters for specialization..</div>
        </div>

        <div class="reference-section">
            <div class="reference-label">üìö Reference Text:</div>
            <button class="toggle-button" onclick="toggleReference(85)">
                Show/Hide Reference
            </button>
            <div id="ref85" class="reference-text hidden">,)areusedtoestimate P( y x|)orpredict yfrom x. Inthecontextofdeeplearning,semi-supervisedlearningusuallyrefersto learningarepresentationh= f(x) .Thegoalistolearnarepresentationso 2 4 3 CHAPTER7.REGULARIZATIONFORDEEPLEARNING thatexamplesfromthesameclasshavesimilarrepresentations.Unsupervised learningcanprovideusefulcuesforhowtogroupexamplesinrepresentation space.Examplesthatclustertightlyintheinputspaceshouldbemappedto similarrepresentations.AlinearclassiÔ¨Åerinthenewspacemayachievebetter generalization inmanycases(BelkinandNiyogi2002Chapelle2003 ,; etal.,).A long-standingvariantofthisapproachistheapplicationofprincipalcomponents analysisasapre-processingstepbeforeapplyingaclassiÔ¨Åer(ontheprojected data). Insteadofhavingseparateunsupervisedandsupervisedcomponentsinthe model,onecanconstructmodelsinwhichagenerativemodelofeither P( x)or P( x y ,)sharesparameterswithadiscriminativemodelof P( y x|).Onecan thentrade-oÔ¨Äthesupervisedcriterion ‚àílog P( y x|)withtheunsupervisedor generativeone(suchas‚àílog P( x)or‚àílog P( x y ,)).Thegenerativecriterionthen expressesaparticularformofpriorbeliefaboutthesolutiontothesupervised learningproblem( ,),namelythatthestructureof Lasserreetal.2006 P( x)is connectedtothestructureof P( y x|)inawaythatiscapturedbytheshared parametrization. Bycontrollinghowmuchofthegenerativecriterionisincluded inthetotalcriterion,onecanÔ¨Åndabettertrade-oÔ¨Äthanwithapurelygenerative orapurelydiscriminativetrainingcriterion( ,; Lasserreetal.2006Larochelleand Bengio2008,). SalakhutdinovandHinton2008()describeamethodforlearningthekernel functionofakernelmachineusedforregression,inwhichtheusageofunlabeled examplesformodeling improvesquitesigniÔ¨Åcantly. P() x P( ) y x| See ()formoreinformationaboutsemi-supervisedlearning. Chapelle etal.2006 7.7Multi-TaskLearning Multi-tasklearning(,)isawaytoimprovegeneralization bypooling Caruana1993 theexamples(whichcanbeseenassoftconstraintsimposedontheparameters) arisingoutofseveraltasks. Inthesamewaythatadditionaltrainingexamples putmorepressureontheparametersofthemodeltowardsvaluesthatgeneralize well,whenpartofamodelissharedacrosstasks,thatpartofthemodelismore constrainedtowardsgoodvalues(assumingthesharingisjustiÔ¨Åed),oftenyielding bettergeneralization. Figureillustratesaverycommonformofmulti-tasklearning,inwhich 7.2 diÔ¨Äerentsupervisedtasks(predicting y( ) igiven x)sharethesameinput x,aswell assomeintermediate-lev elrepresentationh( s ha r e d)capturingacommonpoolof 2 4 4 CHAPTER7.REGULARIZATIONFORDEEPLEARNING factors.Themodelcangenerallybedividedintotwokindsofpartsandassociated parameters: 1.Task-speciÔ¨Åcparameters(whichonlybeneÔ¨Åtfromtheexamplesoftheirtask toachievegoodgeneralization). Thesearetheupperlayersoftheneural networkinÔ¨Ågure.7.2 2.Genericparameters,sharedacrossallthetasks(whichbeneÔ¨Åtfromthe pooleddataofallthetasks).Thesearethelowerlayersoftheneuralnetwork inÔ¨Ågure.7.2 h( 1 )h( 1 )h( 2 )h( 2 )h( 3 )h( 3 )y( 1 )y( 1 )y( 2 )y( 2 ) h( s h a r e d )h( s h a r e d ) xx Figure7.2:Multi-tasklearningcanbecastinseveralwaysindeeplearningframeworks andthisÔ¨Ågureillustratesthecommonsituationwherethetasksshareacommoninputbut involvediÔ¨Äerenttargetrandomvariables.Thelowerlayersofadeepnetwork(whetherit issupervisedandfeedforwardorincludesagenerativecomponentwithdownwardarrows) canbesharedacrosssuchtasks,whiletask-speciÔ¨Åcparameters(associatedrespectively withtheweightsintoandfromh(1)andh(2))canbelearnedontopofthoseyieldinga sharedrepresentationh(shared).Theunderlyingassumptionisthatthereexistsacommon pooloffactorsthatexplainthevariationsintheinput x,whileeachtaskisassociated withasubsetofthesefactors.Inthisexample,itisadditionallyassumedthattop-level hiddenunitsh(1)andh(2)arespecializedtoeachtask(respectivelypredicting y(1)and y(2))whilesomeintermediate-levelrepresentationh(shared)issharedacrossalltasks.In theunsupervisedlearningcontext,itmakessenseforsomeofthetop-levelfactorstobe associatedwithnoneoftheoutputtasks(h(3)):thesearethefactorsthatexplainsomeof theinputvariationsbutarenotrelevantforpredicting y(1)or y(2). Improvedgeneralization andgeneralization errorbounds(,)canbe Baxter1995 achievedbecauseofthesharedparameters,forwhichstatisticalstrengthcanbe 2 4 5</div>
        </div>
    </div>

    <div class="question-card" id="q86">
        <div class="question-header">
            <span class="question-number">Question 86</span>
            <span class="question-type">multiple-choice</span>
        </div>

        <div class="question-text">Deep generative models leverage neural network architectures to produce realistic data samples, with various training objectives and structural components influencing their effectiveness. Understanding the differences between recognition and generation networks is crucial for designing models suited to specific tasks.

Which statement best explains why convolutional generator networks for image synthesis typically require fewer parameters than fully connected generator networks?

1) Convolutional networks exploit spatial weight sharing, allowing local patterns to be modeled efficiently with fewer unique weights.   
2) Fully connected networks use pooling layers that reduce the need for parameters in image generation.   
3) Convolutional networks always have fewer layers than fully connected ones, minimizing parameter count.   
4) Convolutional generator networks only produce low-resolution images, reducing required parameters.   
5) Fully connected networks use transposed convolutions, which increase the number of parameters.   
6) Convolutional networks restrict connections to adjacent pixels, preventing modeling of global image features.   
7) Fully connected networks force the use of autoencoders, which increases parameter numbers.</div>

        <div class="answer-section">
            <div class="answer-label">‚úì Correct Answer:</div>
            <div class="answer-text">The correct answer is 1) Convolutional networks exploit spatial weight sharing, allowing local patterns to be modeled efficiently with fewer unique weights..</div>
        </div>

        <div class="reference-section">
            <div class="reference-label">üìö Reference Text:</div>
            <button class="toggle-button" onclick="toggleReference(86)">
                Show/Hide Reference
            </button>
            <div id="ref86" class="reference-text hidden">tofeaturespacedeÔ¨Ånedbyakernelfunctioninordertomakecomputations on inÔ¨Ånite-dimens ionalvectorstractable.TheMMDcostiszeroifandonlyifthetwo distributionsbeingcomparedareequal. Visually,thesamplesfromgenerativemomentmatchingnetworksaresomewhat disappointing.Fortunately,theycanbeimprovedbycombiningthegenerator networkwithanautoencoder.First,anautoencoderistrainedtoreconstructthe trainingset.Next,theencoderoftheautoencoderisusedtotransformtheentire trainingsetintocodespace.Thegeneratornetworkisthentrainedtogenerate codesamples,whichmaybemappedtovisuallypleasingsamplesviathedecoder. UnlikeGANs,thecostfunctionisdeÔ¨Ånedonlywithrespecttoabatchof examplesfromboththetrainingsetandthegeneratornetwork.Itisnotpossible tomakeatrainingupdateasafunctionofonlyonetrainingexampleoronly onesamplefromthegeneratornetwork.Thisisbecausethemomentsmustbe computedasanempiricalaverageacrossmanysamples.Whenthebatchsizeistoo small,MMDcanunderestimatethetrueamountofvariationinthedistributions beingsampled.NoÔ¨ÅnitebatchsizeissuÔ¨Écientlylargetoeliminatethisproblem entirely,butlargerbatchesreducetheamountofunderestimation.Whenthebatch sizeistoolarge,thetrainingprocedurebecomesinfeasiblyslow,becausemany examplesmustbeprocessedinordertocomputeasinglesmallgradientstep. AswithGANs,itispossibletotrainageneratornetusingMMDevenifthat generatornetassignszeroprobabilitytothetrainingpoints. 20.10.6ConvolutionalGenerativeNetworks Whengeneratingimages,itisoftenusefultouseageneratornetworkthatincludes aconvolutionalstructure(seeforexampleGoodfellow2014cDosovitskiy e t a l .()or e t a l .()).Todoso, weusethe‚Äútranspose‚Äùoftheconvolutionoperator, 2015 describedinsection.Thisapproachoftenyieldsmorerealisticimagesanddoes 9.5 sousingfewerparametersthanusingfullyconnectedlayerswithoutparameter sharing. ConvolutionalnetworksforrecognitiontaskshaveinformationÔ¨Çowfromthe imagetosomesummarizationlayeratthetopofthenetwork,oftenaclasslabel. 7 0 4 CHAPTER20.DEEPGENERATIVEMODELS AsthisimageÔ¨Çowsupwardthroughthenetwork,informationisdiscardedasthe representationoftheimagebecomesmoreinvarianttonuisancetransformations. Inageneratornetwork, theoppositeistrue.Richdetailsmustbeaddedas therepresentationoftheimagetobegeneratedpropagatesthroughthenetwork, culminatingintheÔ¨Ånalrepresentationoftheimage,whichisofcoursetheimage itself,inallofitsdetailedglory,withobjectpositionsandposesandtexturesand lighting. Theprimarymechanismfordiscardinginformationinaconvolutional recognitionnetworkisthepoolinglayer.Thegeneratornetworkseemstoneedto addinformation. Wecannotputtheinverseofapoolinglayerintothegenerator networkbecausemostpoolingfunctionsarenotinvertible.Asimpleroperationis tomerelyincreasethespatialsizeoftherepresentation.Anapproachthatseems toperformacceptablyistousean‚Äúun-pooling‚ÄùasintroducedbyDosovitskiy e t a l . ().Thislayercorrespondstotheinverseofthemax-poolingoperationunder 2015 certainsimplifyingconditions. Firs t,thestrideofthemax-poolingoperationis constrainedtobeequaltothewidthofthepoolingregion.Second,themaximum inputwithineachpoolingregionisassumedtobetheinputintheupper-left corner.Finally,allnon-maximal inputswithineachpoolingregionareassumedto bezero.Theseareverystrongandunrealisticassumptions,buttheydoallowthe max-poolingoperatortobeinverted.Theinverseun-poolingoperationallocates atensorofzeros,thencopieseachvaluefromspatialcoordinate ioftheinput tospatialcoordinate i k√óoftheoutput.Theintegervalue kdeÔ¨Ånesthesize ofthepoolingregion.EventhoughtheassumptionsmotivatingthedeÔ¨Ånitionof theun-poolingoperatorareunrealistic,thesubsequentlayersareabletolearnto compensateforitsunusualoutput,sothesamplesgeneratedbythemodelasa wholearevisuallypleasing. 20.10.7Auto-RegressiveNetworks Auto-regressivenetworksaredirectedprobabilisticmodelswithnolatentrandom variables.Theconditionalprobabilitydistributionsinthesemodelsarerepresented byneuralnetworks(sometimesextremelysimpleneuralnetworkssuchaslogistic regression).Thegraphstructureofthesemodelsisthecompletegraph.They decomposeajointprobabilityovertheobservedvariablesusingthechainruleof probabilitytoobtainaproductofconditionalsoftheform P( x d| x d‚àí 1 , . . . , x 1). Suchmodelshavebeencalledfully-visibleBayesnetworks(FVBNs)andused successfully inmany forms, Ô¨Årstwith logistic regression foreachconditional distribution(Frey1998,)andthenwithneuralnetworkswithhiddenunits(Bengio andBengio2000bLarochelleandMurray2011 ,; ,).Insomeformsofauto- regressivenetworks,suchasNADE( ,),described LarochelleandMurray2011 7 0 5</div>
        </div>
    </div>

    <div class="question-card" id="q87">
        <div class="question-header">
            <span class="question-number">Question 87</span>
            <span class="question-type">multiple-choice</span>
        </div>

        <div class="question-text">Neural language models often struggle with computational efficiency when dealing with very large vocabularies, especially in tasks such as machine translation or dialogue systems. Solutions like hierarchical softmax and importance sampling have been developed to address these bottlenecks.

Which statement best describes how hierarchical softmax improves computational efficiency in large-vocabulary neural language models?

1) It replaces neural networks entirely with n-gram models for rare words.   
2) It restricts the output to a fixed shortlist of the most frequent words.   
3) It computes the probability for every word in parallel using multiple output layers.   
4) It organizes the vocabulary into a tree structure, allowing probabilities to be computed along root-to-leaf paths, reducing computation from O(|V|) to O(log|V|).   
5) It dynamically expands the vocabulary during training based on word frequencies.   
6) It eliminates the need for cross-entropy loss in language modeling tasks.   
7) It merges all word class computations into a single linear transformation.</div>

        <div class="answer-section">
            <div class="answer-label">‚úì Correct Answer:</div>
            <div class="answer-text">The correct answer is 4) It organizes the vocabulary into a tree structure, allowing probabilities to be computed along root-to-leaf paths, reducing computation from O(|V|) to O(log|V|)..</div>
        </div>

        <div class="reference-section">
            <div class="reference-label">üìö Reference Text:</div>
            <button class="toggle-button" onclick="toggleReference(87)">
                Show/Hide Reference
            </button>
            <div id="ref87" class="reference-text hidden">CHAPTER12.APPLICATIONS lossfunctions,thegradientcanbecomputedeÔ¨Éciently( ,),but Vincent e t a l .2015 thestandardcross-entropylossappliedtoatraditionalsoftmaxoutputlayerposes manydiÔ¨Éculties. Supposethathisthetophiddenlayerusedtopredicttheoutputprobabilities ÀÜy.IfweparametrizethetransformationfromhtoÀÜywithlearnedweightsW andlearnedbiasesb,thentheaÔ¨Éne-softmaxoutputlayerperformsthefollowing computations: a i= b i+ÓÅò jW i j h j‚àÄ‚àà{ ||} i1 , . . . , V , (12.8) ÀÜ y i=ea i ÓÅê|| V iÓÄ∞=1 eai ÓÄ∞. (12.9) Ifhcontains n helementsthentheaboveoperationis O(|| V n h).With n hinthe thousandsand|| Vinthehundredsofthousands,thisoperationdominatesthe computationofmostneurallanguagemodels. 12.4.3.1UseofaShortList TheÔ¨Årstneurallanguagemodels( ,,)dealtwiththehighcost Bengio e t a l .20012003 ofusingasoftmaxoveralargenumberofoutputwordsbylimitingthevocabulary sizeto10,000or20,000words.SchwenkandGauvain2002Schwenk2007 ()and () builtuponthisapproachbysplittingthevocabulary Vintoashortlist Lofmost frequentwords(handledbytheneuralnet)andatail T= V L\ofmorerarewords (handledbyan n-grammodel). Tobeabletocombinethetwopredictions,the neuralnetalsohastopredicttheprobabilitythatawordappearingaftercontext Cbelongstothetaillist.Thismaybeachievedbyaddinganextrasigmoidoutput unittoprovideanestimateof P( i C ‚àà| T ).Theextraoutputcanthenbeusedto achieveanestimateoftheprobabilitydistributionoverallwordsinasfollows: V P y i C (= |) =1 i‚àà L P y i C, i P i C (= | ‚àà ‚àí L)(1 (‚àà| T )) +1 i‚àà T P y i C, i P i C (= | ‚àà T)(‚àà| T )(12.10) where P( y= i C, i| ‚àà L)isprovidedbytheneurallanguagemodeland P( y= i| C, i‚àà T) isprovidedbythe n-grammodel.WithslightmodiÔ¨Åcation,thisapproach canalsoworkusinganextraoutputvalueintheneurallanguagemodel‚Äôssoftmax layer,ratherthanaseparatesigmoidunit. Anobviousdisadvantageoftheshortlistapproachisthatthepotentialgener- alizationadvantageoftheneurallanguagemodelsislimitedtothemostfrequent 4 6 6 CHAPTER12.APPLICATIONS words,where,arguably,itistheleastuseful. Thisdisadvantagehasstimulated theexplorationofalternativemethodstodealwithhigh-dimensionaloutputs, describedbelow. 12.4.3.2HierarchicalSoftmax Aclassicalapproach(,)toreducingthecomputational burden Goodman2001 ofhigh-dimensionaloutputlayersoverlargevocabularysets Vistodecompose probabilities hierarchically .Insteadofnecessitatinganumberofcomputations proportionalto|| V(andalsoproportionaltothenumberofhiddenunits, n h), the|| Vfactorcanbereducedtoaslowaslog|| V.()and Bengio2002Morinand Bengio2005()introducedthisfactorizedapproachtothecontextofneurallanguage models. Onecanthinkofthishierarchyasbuildingcategoriesofwords,thencategories ofcategoriesofwords,thencategoriesofcategoriesofcategoriesofwords,etc. Thesenestedcategoriesformatree,withwordsattheleaves.Inabalancedtree, thetreehasdepth O(log|| V). Theprobabilityofachoosingawordisgivenby theproductoftheprobabilities ofchoosingthebranchleadingtothatwordat everynodeonapathfromtherootofthetreetotheleafcontainingtheword. Figureillustratesasimpleexample. ()alsodescribe 12.4 MnihandHinton2009 howtousemultiplepathstoidentifyasinglewordinordertobettermodelwords thathavemultiplemeanings.Computingtheprobabilityofawordtheninvolves summationoverallofthepathsthatleadtothatword. Topredicttheconditionalprobabilities requiredateachnodeofthetree,we typicallyusealogisticregressionmodelateachnodeofthetree,andprovidethe samecontext Casinputtoallofthesemodels.Becausethecorrectoutputis encodedinthetrainingset,wecanusesupervisedlearningtotrainthelogistic regressionmodels.Thisistypicallydoneusingastandardcross-entropyloss, correspondingtomaximizingthelog-likelihoodofthecorrectsequenceofdecisions. Becausetheoutputlog-likelihoodcanbecomputedeÔ¨Éciently(aslowaslog|| V ratherthan|| V),itsgradientsmayalsobecomputedeÔ¨Éciently.Thisincludesnot onlythegradientwithrespecttotheoutputparametersbutalsothegradients withrespecttothehiddenlayeractivations. Itispossiblebutusuallynotpracticaltooptimizethetreestructuretominimize theexpectednumberofcomputations. Toolsfrominformationtheoryspecifyhow tochoosetheoptimalbinarycodegiventherelativefrequenciesofthewords.To doso,wecouldstructurethetreesothatthenumberofbitsassociatedwithaword isapproximatelyequaltothelogarithmofthefrequencyofthatword.However,in 4 6 7 CHAPTER12.APPLICATIONS ( 1) ( 0) ( 0, 0, 0) ( 0, 0, 1) ( 0, 1, 0) ( 0, 1, 1) ( 1, 0, 0) ( 1, 0, 1) ( 1, 1, 0) ( 1, 1, 1)( 1, 1) ( 1, 0) ( 0, 1) ( 0, 0) w 0 w 0 w 1 w 1 w 2 w 2 w 3 w 3 w 4 w 4 w 5 w 5 w 6 w 6 w 7 w 7 Figure12.4:Illustrationofasimplehierarchyofwordcategories,with8words w 0 , . . . , w 7 organizedintoathreelevelhierarchy.TheleavesofthetreerepresentactualspeciÔ¨Åcwords. Internalnodesrepresentgroupsofwords.Anynodecanbeindexedbythesequence ofbinarydecisions(0=left,1=right)toreachthenodefromtheroot.Super-class(0) containstheclasses(0 ,0) (0and ,1),whichrespectivelycontainthesetsofwords{ w 0 , w 1} and{ w 2 , w 3},andsimilarlysuper-classcontainstheclasses (1) (1 ,0) (1and ,1),which respectivelycontainthewords( w 4 , w 5) (and w 6 , w 7).IfthetreeissuÔ¨Écientlybalanced, themaximumdepth(numberofbinarydecisions)isontheorderofthelogarithmof thenumberofwords|| V: thechoiceofoneoutof|| Vwordscanbeobtainedbydoing O(log|| V)operations(oneforeachofthenodesonthepathfromtheroot).Inthisexample, computingtheprobabilityofaword ycanbedonebymultiplyingthreeprobabilities, associatedwiththebinarydecisionstomoveleftorrightateachnodeonthepathfrom theroottoanode y.Let bi( y)bethe i-thbinarydecisionwhentraversingthetree towardsthevalue y.Theprobabilityofsamplinganoutputydecomposesintoaproduct ofconditionalprobabilities,usingthechainruleforconditionalprobabilities,witheach nodeindexedbythepreÔ¨Åxofthesebits.Forexample,node(1 ,0)correspondstothe preÔ¨Åx( b 0( w4) = 1 , b1( w4) = 0),andtheprobabilityof w 4canbedecomposedasfollows: P w (= y 4) = ( Pb 0= 1 ,b 1= 0 ,b 2= 0) (12.11) = ( Pb 0= 1) ( Pb 1= 0 |b 0= 1) ( Pb 2= 0 |b 0= 1 ,b 1= 0) .(12.12) 4 6 8 CHAPTER12.APPLICATIONS practice,thecomputational savingsaretypicallynotworththeeÔ¨Äortbecausethe computationoftheoutputprobabilitiesisonlyonepartofthetotalcomputation intheneurallanguagemodel.Forexample,supposethereare lfullyconnected hiddenlayersofwidth n h.Let n bbetheweightedaverageofthenumberofbits requiredtoidentifyaword,withtheweightinggivenbythefrequencyofthese words.Inthisexample,thenumberofoperationsneededtocomputethehidden activationsgrowsasas O( l n2 h)whiletheoutputcomputations growas O( n h n b). Aslongas n b‚â§ l n h,wecanreducecomputationmorebyshrinking n hthanby shrinking n b.Indeed, n bisoftensmall.Becausethesizeofthevocabularyrarely exceedsamillionwordsandlog2(106)‚âà20,itispossibletoreduce n btoabout,20 but n hisoftenmuchlarger,around 103ormore.Ratherthancarefullyoptimizing atreewithabranchingfactorof,onecaninsteaddeÔ¨Åneatreewithdepthtwo 2 andabranchingfactorofÓÅ∞ || V.SuchatreecorrespondstosimplydeÔ¨Åningaset ofmutuallyexclusivewordclasses.Thesimpleapproachbasedonatreeofdepth twocapturesmostofthecomputational beneÔ¨Åtofthehierarchicalstrategy. OnequestionthatremainssomewhatopenishowtobestdeÔ¨Ånetheseword classes,orhowtodeÔ¨Ånethewordhierarchyingeneral.Earlyworkusedexisting hierarchies( ,)butthehierarchycanalsobelearned,ideally MorinandBengio2005 jointlywiththeneurallanguagemodel.LearningthehierarchyisdiÔ¨Écult.Anexact optimization ofthelog-likelihoodappearsintractablebecausethechoiceofaword hierarchyisadiscreteone,notamenabletogradient-basedoptimization. However, onecouldusediscreteoptimization toapproximately optimizethepartitionof wordsintowordclasses. Animportantadvantageofthehierarchicalsoftmaxisthatitbringscomputa- tionalbeneÔ¨Åtsbothattrainingtimeandattesttime,ifattesttimewewantto computetheprobabilityofspeciÔ¨Åcwords. Ofcourse,computingtheprobabilityofall|| Vwordswillremainexpensive evenwiththehierarchicalsoftmax.Anotherimportantoperationisselectingthe mostlikelywordinagivencontext.Unfortunatelythetreestructuredoesnot provideaneÔ¨Écientandexactsolutiontothisproblem. Adisadvantageisthatinpracticethehierarchicalsoftmaxtendstogiveworse testresultsthansampling-basedmethodswewilldescribenext.Thismaybedue toapoorchoiceofwordclasses. 12.4.3.3ImportanceSampling Onewaytospeedupthetrainingofneurallanguagemodelsistoavoidexplicitly computingthecontributionofthegradientfromallofthewordsthatdonotappear</div>
        </div>
    </div>

    <div class="question-card" id="q88">
        <div class="question-header">
            <span class="question-number">Question 88</span>
            <span class="question-type">multiple-choice</span>
        </div>

        <div class="question-text">Dropout is a widely used regularization technique in neural networks that randomly disables units during training to prevent overfitting. Understanding its theoretical properties and practical limitations is important for optimizing model performance.

In linear regression models, which statement accurately describes the relationship between dropout and L2 weight decay?

1) Dropout is mathematically equivalent to L2 regularization, but with a different decay coefficient determined by the dropout probability.   
2) Dropout increases the effective capacity of the model compared to L2 weight decay.   
3) Dropout and L2 weight decay are fundamentally unrelated in linear models.   
4) Dropout is only equivalent to L1 regularization in linear regression.   
5) Dropout leads to larger parameter magnitudes than L2 weight decay in all cases.   
6) Dropout does not provide any regularization effect in linear models.   
7) Dropout and L2 weight decay result in identical parameter values for any dropout probability.</div>

        <div class="answer-section">
            <div class="answer-label">‚úì Correct Answer:</div>
            <div class="answer-text">The correct answer is 1) Dropout is mathematically equivalent to L2 regularization, but with a different decay coefficient determined by the dropout probability..</div>
        </div>

        <div class="reference-section">
            <div class="reference-label">üìö Reference Text:</div>
            <button class="toggle-button" onclick="toggleReference(88)">
                Show/Hide Reference
            </button>
            <div id="ref88" class="reference-text hidden">e nse m bl e(= )y y| vÓÅê yÓÄ∞Àú P e nse m bl e(= y yÓÄ∞| v)(7.58) where Àú P e nse m bl e(= ) =y y| v2nÓÅ≥ÓÅô d‚àà{} 0 1 ,nP y . (= y | v;)d (7.59) 2 6 3 CHAPTER7.REGULARIZATIONFORDEEPLEARNING Toseethattheweightscalingruleisexact,wecansimplify Àú P e nse m bl e: Àú P e nse m bl e(= ) =y y| v2nÓÅ≥ÓÅô d‚àà{} 0 1 ,nP y (= y | v;)d(7.60) = 2nÓÅ≥ÓÅô d‚àà{} 0 1 ,nsoftmax (WÓÄæ( )+)dÓÄå vby (7.61) = 2nÓÅ∂ÓÅµÓÅµÓÅ¥ÓÅô d‚àà{} 0 1 ,nexpÓÄÄ WÓÄæy , :( )+dÓÄå v b yÓÄÅ ÓÅê yÓÄ∞expÓÄê WÓÄæ yÓÄ∞ , :( )+dÓÄå v b yÓÄ∞ÓÄë (7.62) =2nÓÅ±ÓÅë d‚àà{} 0 1 ,nexpÓÄÄ WÓÄæy , :( )+dÓÄå v b yÓÄÅ 2nÓÅ≤ÓÅë d‚àà{} 0 1 ,nÓÅê yÓÄ∞expÓÄê WÓÄæ yÓÄ∞ , :( )+dÓÄå v b yÓÄ∞ÓÄë(7.63) BecauseÀú Pwillbenormalized,wecansafelyignoremultiplication byfactorsthat areconstantwithrespectto: y Àú P e nse m bl e(= ) y y| v‚àù2nÓÅ≥ÓÅô d‚àà{} 0 1 ,nexpÓÄÄ WÓÄæy , :( )+dÓÄå v b yÓÄÅ (7.64) = expÔ£´ Ô£≠1 2nÓÅò d‚àà{} 0 1 ,nWÓÄæ y , :( )+dÓÄå v b yÔ£∂ Ô£∏ (7.65) = expÓÄí1 2WÓÄæ y , : v+ b yÓÄì . (7.66) SubstitutingthisbackintoequationweobtainasoftmaxclassiÔ¨Åerwithweights 7.58 1 2W. Theweightscalingruleisalsoexactinothersettings,includingregression networkswithconditionallynormaloutputs,anddeepnetworksthathavehidden layerswithoutnonlinearities. However,theweightscalingruleisonlyanapproxi- mationfordeepmodelsthathavenonlinearities. Thoughtheapproximationhas notbeentheoreticallycharacterized, itoftenworkswell,empirically.Goodfellow etal.()foundexperimentallythattheweightscalingapproximationcanwork 2013a better(intermsofclassiÔ¨Åcationaccuracy)thanMonteCarloapproximations tothe ensemblepredictor.ThisheldtrueevenwhentheMonteCarloapproximationwas allowedtosampleupto1,000sub-networks. ()found GalandGhahramani2015 thatsomemodelsobtainbetterclassiÔ¨Åcationaccuracyusingtwentysamplesand 2 6 4 CHAPTER7.REGULARIZATIONFORDEEPLEARNING theMonteCarloapproximation.Itappearsthattheoptimalchoiceofinference approximationisproblem-dependent. Srivastava2014etal.()showedthatdropoutismoreeÔ¨Äectivethanother standardcomputationally inexpensiveregularizers,suchasweightdecay,Ô¨Ålter normconstraintsandsparseactivityregularization. Dropoutmayalsobecombined withotherformsofregularizationtoyieldafurtherimprovement. Oneadvantageofdropoutisthatitisverycomputationally cheap.Using dropoutduringtrainingrequiresonly O( n)computationperexampleperupdate, togenerate nrandombinarynumbersandmultiplythembythestate.Depending ontheimplementation,itmayalsorequire O( n)memorytostorethesebinary numbersuntiltheback-propagationstage.Runninginferenceinthetrainedmodel hasthesamecostper-exampleasifdropoutwerenotused,thoughwemustpay thecostofdividingtheweightsby2oncebeforebeginningtoruninferenceon examples. AnothersigniÔ¨ÅcantadvantageofdropoutisthatitdoesnotsigniÔ¨Åcantlylimit thetypeofmodelortrainingprocedurethatcanbeused.Itworkswellwithnearly anymodelthatusesadistributedrepresentationandcanbetrainedwithstochastic gradientdescent.Thisincludesfeedforwardneuralnetworks,probabilisticmodels suchasrestrictedBoltzmannmachines(Srivastava2014etal.,),andrecurrent neuralnetworks(BayerandOsendorfer2014Pascanu2014a ,; etal.,).Manyother regularizationstrategiesofcomparablepowerimposemoresevererestrictionson thearchitectureofthemodel. Thoughthecostper-stepofapplyingdropouttoaspeciÔ¨Åcmodelisnegligible, thecostofusingdropoutinacompletesystemcanbesigniÔ¨Åcant.Becausedropout isaregularizationtechnique,itreducestheeÔ¨Äectivecapacityofamodel.TooÔ¨Äset thiseÔ¨Äect,wemustincreasethesizeofthemodel.Typicallytheoptimalvalidation seterrorismuchlowerwhenusingdropout,butthiscomesatthecostofamuch largermodelandmanymoreiterationsofthetrainingalgorithm.Forverylarge datasets,regularizationconferslittlereductioningeneralization error. Inthese cases,thecomputational costofusingdropoutandlargermodelsmayoutweigh thebeneÔ¨Åtofregularization. Whenextremelyfewlabeledtrainingexamplesareavailable,dropoutisless eÔ¨Äective.Bayesian neuralnetworks(, )outperform dropout onthe Neal1996 AlternativeSplicingDataset(,)wherefewerthan5,000examples Xiongetal.2011 areavailable(Srivastava2014etal.,).Whenadditionalunlabeleddataisavailable, unsupervisedfeaturelearningcangainanadvantageoverdropout. Wager2013etal.()showedthat,whenappliedtolinearregression,dropout isequivalentto L2weightdecay,withadiÔ¨ÄerentweightdecaycoeÔ¨Écientfor 2 6 5</div>
        </div>
    </div>

    <div class="question-card" id="q89">
        <div class="question-header">
            <span class="question-number">Question 89</span>
            <span class="question-type">multiple-choice</span>
        </div>

        <div class="question-text">Linear factor models are widely used in signal processing and machine learning to extract interpretable features from complex data. Techniques such as ICA, SFA, and sparse coding leverage statistical properties and priors to enhance feature learning and separation.

Which of the following statements correctly explains why Independent Component Analysis (ICA) requires the latent variable distribution to be non-Gaussian?

1) Non-Gaussian latent distributions ensure that the mixing matrix is unique, regardless of variable independence.   
2) Gaussian latent distributions allow for more efficient computation of determinants in ICA models.   
3) Non-Gaussianity guarantees that the observed data has zero mean and unit variance after mixing.   
4) ICA with Gaussian latent variables makes the model more suitable for generative tasks.   
5) Non-Gaussian latent distributions break rotational symmetry, allowing ICA to uniquely identify the original sources.   
6) Gaussian latent distributions encourage sparsity in the learned components, which is essential for ICA.   
7) Non-Gaussianity ensures that the mixing process is linear and reversible for ICA inference.</div>

        <div class="answer-section">
            <div class="answer-label">‚úì Correct Answer:</div>
            <div class="answer-text">The correct answer is 5) Non-Gaussian latent distributions break rotational symmetry, allowing ICA to uniquely identify the original sources..</div>
        </div>

        <div class="reference-section">
            <div class="reference-label">üìö Reference Text:</div>
            <button class="toggle-button" onclick="toggleReference(89)">
                Show/Hide Reference
            </button>
            <div id="ref89" class="reference-text hidden">recoverlow-levelsignalsthathavebeenmixedtogether.Inthissetting,each trainingexampleisonemomentintime,each x iisonesensor‚Äôsobservationof themixedsignals,andeach h iisoneestimateofoneoftheoriginalsignals.For example,wemighthave npeoplespeakingsimultaneously.Ifwehave ndiÔ¨Äerent microphonesplacedindiÔ¨Äerentlocations,ICAcandetectthechangesinthevolume betweeneachspeakerasheardbyeachmicrophone, andseparatethesignalsso thateach h icontainsonlyonepersonspeakingclearly.Thisiscommonlyused inneuroscienceforelectroencephalograph y,atechnologyforrecordingelectrical signalsoriginatinginthebrain.Manyelectrodesensorsplacedonthesubject‚Äôs headareusedtomeasuremanyelectricalsignalscomingfromthebody.The experimenteristypicallyonlyinterestedinsignalsfromthebrain,butsignalsfrom thesubject‚Äôsheartandeyesarestrongenoughtoconfoundmeasurementstaken atthesubject‚Äôsscalp.Thesignalsarriveattheelectrodesmixedtogether,so ICAisnecessarytoseparatetheelectricalsignatureoftheheartfromthesignals originatinginthebrain,andtoseparatesignalsindiÔ¨Äerentbrainregionsfrom eachother. Asmentionedbefore,manyvariantsofICAarepossible.Someaddsomenoise inthegenerationof xratherthanusingadeterministicdecoder.Mostdonot usethemaximumlikelihoodcriterion,butinsteadaimtomaketheelementsof h= W‚àí 1xindependentfromeachother.Manycriteriathataccomplishthisgoal arepossible.Equationrequirestakingthedeterminantof 3.47 W,whichcanbe anexpensiveandnumericallyunstableoperation.SomevariantsofICAavoidthis problematicoperationbyconstrainingtobeorthogonal. W AllvariantsofICArequirethat p( h)benon-Gaussian.Thisisbecauseif p( h) isanindependentpriorwithGaussiancomponents,then WisnotidentiÔ¨Åable. Wecanobtainthesamedistributionover p( x)formanyvaluesof W.Thisisvery diÔ¨ÄerentfromotherlinearfactormodelslikeprobabilisticPCAandfactoranalysis, thatoftenrequire p( h)tobeGaussianinordertomakemanyoperationsonthe modelhaveclosedformsolutions.Inthemaximumlikelihoodapproachwherethe userexplicitlyspeciÔ¨Åesthedistribution,atypicalchoiceistouse p( h i) =d d h iœÉ( h i). Typicalchoicesofthesenon-Gaussiandistributionshavelargerpeaksnear0than doestheGaussiandistribution,sowecanalsoseemostimplementations ofICA aslearningsparsefeatures. 492 CHAPTER13.LINEARFACTORMODELS ManyvariantsofICAarenotgenerativemodelsinthesensethatweusethe phrase.Inthisbook,agenerativemodeleitherrepresents p( x) orcandrawsamples fromit.ManyvariantsofICAonlyknowhowtotransformbetween xand h,but donothaveanywayofrepresenting p( h),andthusdonotimposeadistribution over p( x).Forexample,manyICAvariantsaimtoincreasethesamplekurtosisof h= W‚àí 1x,becausehighkurtosisindicatesthat p( h)isnon-Gaussian,butthisis accomplishedwithoutexplicitlyrepresenting p( h).ThisisbecauseICAismore oftenusedasananalysistoolforseparatingsignals,ratherthanforgenerating dataorestimatingitsdensity. JustasPCAcanbegeneralizedtothenonlinearautoencodersdescribedin chapter,ICAcanbegeneralizedtoanonlineargenerativemodel,inwhich 14 weuseanonlinearfunction ftogeneratetheobserveddata.SeeHyv√§rinenand Pajunen1999()fortheinitialworkonnonlinearICAanditssuccessfulusewith ensemblelearningby ()and (). RobertsandEverson2001Lappalainen e t a l .2000 AnothernonlinearextensionofICAistheapproachof nonlinear i ndep e ndent c o m p o nen t s e st i m at i o n,orNICE(,),whichstacksaseries Dinh e t a l .2014 ofinvertibletransformations(encoderstages)thathavethepropertythatthe determinantoftheJacobianofeachtransformationcanbecomputedeÔ¨Éciently. Thismakesitpossibletocomputethelikelihoodexactlyand,likeICA,attempts totransformthedataintoaspacewhereithasafactorizedmarginaldistribution, butismorelikelytosucceedthankstothenonlinearencoder.Becausetheencoder isassociatedwithadecoderthatisitsperfectinverse,itisstraightforwardto generatesamplesfromthemodel(byÔ¨Årstsamplingfrom p( h)andthenapplying thedecoder). Anothergeneralization ofICAistolearngroupsoffeatures,withstatistical dependenceallowedwithinagroupbutdiscouragedbetweengroups(Hyv√§rinenand Hoyer1999Hyv√§rinen 2001b ,; e t a l .,).Whenthegroupsofrelatedunitsarechosen tobenon-overlapping,thisiscalled i ndep e nden t subspac e analysis.Itisalso possibletoassignspatialcoordinatestoeachhiddenunitandformoverlapping groupsofspatiallyneighboringunits.Thisencouragesnearbyunitstolearnsimilar features.Whenappliedtonaturalimages,this t o p o g r aphic I CAapproachlearns GaborÔ¨Ålters,suchthatneighboringfeatureshavesimilarorientation,locationor frequency.ManydiÔ¨ÄerentphaseoÔ¨ÄsetsofsimilarGaborfunctionsoccurwithin eachregion,sothatpoolingoversmallregionsyieldstranslationinvariance. 13.3SlowFeatureAnalysis Sl o w f e at ur e analysis(SFA)isalinearfactormodelthatusesinformationfrom 493 CHAPTER13.LINEARFACTORMODELS timesignalstolearninvariantfeatures( ,). WiskottandSejnowski2002 Slowfeatureanalysisismotivatedbyageneralprinciplecalledtheslowness principle.Theideaisthattheimportantcharacteristicsofsceneschangevery slowlycomparedtotheindividualmeasurementsthatmakeupadescriptionofa scene.Forexample,incomputervision,individualpixelvaluescanchangevery rapidly.Ifazebramovesfromlefttorightacrosstheimage,anindividualpixel willrapidlychangefromblacktowhiteandbackagainasthezebra‚Äôsstripespass overthepixel.Bycomparison,thefeatureindicatingwhetherazebraisinthe imagewillnotchangeatall,andthefeaturedescribingthezebra‚Äôspositionwill changeslowly. Wethereforemaywishtoregularizeourmodeltolearnfeatures thatchangeslowlyovertime. Theslownessprinciplepredatesslowfeatureanalysisandhasbeenapplied toawidevarietyofmodels(,;,; ,; Hinton1989F√∂ldi√°k1989Mobahi e t a l .2009 BergstraandBengio2009,).Ingeneral,wecanapplytheslownessprincipletoany diÔ¨Äerentiablemodeltrainedwithgradientdescent.Theslownessprinciplemaybe introducedbyaddingatermtothecostfunctionoftheform ŒªÓÅò tL f(( x( + 1 ) t)( , f x( ) t)) (13.7) where Œªisahyperparameter determiningthestrengthoftheslownessregularization term, tistheindexintoatimesequenceofexamples, fisthefeatureextractor toberegularized,and Lisalossfunctionmeasuringthedistancebetween f( x( ) t) and f( x( + 1 ) t).AcommonchoiceforisthemeansquareddiÔ¨Äerence. L SlowfeatureanalysisisaparticularlyeÔ¨Écientapplicationoftheslowness principle.ItiseÔ¨Écientbecauseitisappliedtoalinearfeatureextractor,andcan thusbetrainedinclosedform.LikesomevariantsofICA,SFAisnotquitea generativemodelperse,inthesensethatitdeÔ¨Ånesalinearmapbetweeninput spaceandfeaturespacebutdoesnotdeÔ¨Åneaprioroverfeaturespaceandthus doesnotimposeadistributiononinputspace. p() x TheSFAalgorithm(WiskottandSejnowski2002,)consistsofdeÔ¨Åning f( x; Œ∏) tobealineartransformation,andsolvingtheoptimization problem min Œ∏E t(( f x( + 1 ) t) i‚àí f( x( ) t) i)2(13.8) subjecttotheconstraints E t f( x( ) t) i= 0 (13.9) and E t[( f x( ) t)2 i] = 1 . (13.10) 494 CHAPTER13.LINEARFACTORMODELS Theconstraintthatthelearnedfeaturehavezeromeanisnecessarytomakethe problemhaveauniquesolution;otherwisewecouldaddaconstanttoallfeature valuesandobtainadiÔ¨Äerentsolutionwithequalvalueoftheslownessobjective. Theconstraintthatthefeatureshaveunitvarianceisnecessarytopreventthe pathologicalsolutionwhereallfeaturescollapseto.LikePCA,theSFAfeatures 0 areordered,withtheÔ¨Årstfeaturebeingtheslowest.Tolearnmultiplefeatures,we mustalsoaddtheconstraint ‚àÄ i < j , E t[( f x( ) t) i f( x( ) t) j] = 0 . (13.11) ThisspeciÔ¨Åesthatthelearnedfeaturesmustbelinearlydecorrelated fromeach other.Withoutthisconstraint,allofthelearnedfeatureswouldsimplycapturethe oneslowestsignal.Onecouldimagineusingothermechanisms,suchasminimizing reconstructionerror, to forcethe featurestodiversify, but thisdecorrelation mechanismadmitsasimplesolutionduetothelinearityofSFAfeatures.TheSFA problemmaybesolvedinclosedformbyalinearalgebrapackage. SFAistypicallyusedtolearnnonlinearfeaturesbyapplyinganonlinearbasis expansionto xbeforerunningSFA.Forexample,itiscommontoreplace xbythe quadraticbasisexpansion,avectorcontainingelements x i x jforall iand j.Linear SFAmodulesmaythenbecomposedtolearndeepnonlinearslowfeatureextractors byrepeatedlylearningalinearSFAfeatureextractor,applyinganonlinearbasis expansiontoitsoutput,andthenlearninganotherlinearSFAfeatureextractoron topofthatexpansion. Whentrainedonsmallspatialpatchesofvideosofnaturalscenes,SFAwith quadraticbasisexpansionslearnsfeaturesthatsharemanycharacteristicswith thoseofcomplexcellsinV1cortex(BerkesandWiskott2005,).Whentrained onvideosofrandommotionwithin3-Dcomputerrenderedenvironments,deep SFAlearnsfeaturesthatsharemanycharacteristicswiththefeaturesrepresented byneuronsinratbrainsthatareusedfornavigation(Franzius 2007 e t a l .,).SFA thusseemstobeareasonablybiologicallyplausiblemodel. AmajoradvantageofSFAisthatitispossiblytotheoreticallypredictwhich featuresSFAwilllearn,eveninthedeep,nonlinearsetting.Tomakesuchtheoretical predictions,onemustknowaboutthedynamicsoftheenvironmentintermsof conÔ¨Åguration space (e.g., inthe caseofrandom motion inthe 3-Drendered environment,thetheoreticalanalysisproceedsfromknowledgeoftheprobability distributionoverpositionandvelocityofthecamera).Giventheknowledgeofhow theunderlyingfactorsactuallychange,itispossibletoanalyticallysolveforthe optimalfunctionsexpressingthesefactors.Inpractice,experimentswithdeepSFA appliedtosimulateddataseemtorecoverthetheoreticallypredictedfunctions. 495 CHAPTER13.LINEARFACTORMODELS Thisisincomparisontootherlearningalgorithmswherethecostfunctiondepends highlyonspeciÔ¨Åcpixelvalues,makingitmuchmorediÔ¨Éculttodeterminewhat featuresthemodelwilllearn. DeepSFAhasalsobeenusedtolearnfeaturesforobjectrecognitionandpose estimation(Franzius 2008 e t a l .,).Sofar,theslownessprinciplehasnotbecome thebasisforanystateoftheartapplications.Itisunclearwhatfactorhaslimited itsperformance.Wespeculatethatperhapstheslownessprioristoostrong,and that,ratherthanimposingapriorthatfeaturesshouldbeapproximatelyconstant, itwouldbebettertoimposeapriorthatfeaturesshouldbeeasytopredictfrom onetimesteptothenext.Thepositionofanobjectisausefulfeatureregardlessof whethertheobject‚Äôsvelocityishighorlow,buttheslownessprincipleencourages themodeltoignorethepositionofobjectsthathavehighvelocity. 13.4SparseCoding Spar se c o di ng( ,)isalinearfactormodelthathas OlshausenandField1996 beenheavilystudiedasanunsupervisedfeaturelearningandfeatureextraction mechanism. Strictlyspeaking,theterm‚Äúsparsecoding‚Äùreferstotheprocessof inferringthevalueof hinthismodel,while‚Äúsparsemodeling‚Äùreferstotheprocess ofdesigningandlearningthemodel,buttheterm‚Äúsparsecoding‚Äùisoftenusedto refertoboth. Likemostotherlinearfactormodels,itusesalineardecoderplusnoiseto obtainreconstructionsof x,asspeciÔ¨Åedinequation.MorespeciÔ¨Åcally,sparse 13.2 codingmodelstypicallyassumethatthelinearfactorshaveGaussiannoisewith isotropicprecision: Œ≤ p , ( ) = (; + x h| N x W h b1 Œ≤I) . (13.12) Thedistribution p( h)ischosentobeonewithsharppeaksnear0(Olshausen andField1996,).CommonchoicesincludefactorizedLaplace,Cauchyorfactorized Student- tdistributions.Forexample,theLaplacepriorparametrized intermsof thesparsitypenaltycoeÔ¨Écientisgivenby Œª p h( i) = Laplace( h i;0 ,2 Œª) =Œª 4e‚àí1 2Œª h| i|(13.13) andtheStudent-priorby t p h( i) ‚àù1 (1+h2 i ŒΩ)ŒΩ +1 2. (13.14) 496 CHAPTER13.LINEARFACTORMODELS Trainingsparsecodingwithmaximumlikelihoodisintractable.Instead,the trainingalternatesbetweenencodingthedataandtrainingthedecodertobetter reconstructthedatagiventheencoding.ThisapproachwillbejustiÔ¨Åedfurtheras aprincipledapproximation tomaximumlikelihoodlater,insection.19.3 FormodelssuchasPCA,wehaveseentheuseofaparametricencoderfunction thatpredicts handconsistsonlyofmultiplication byaweightmatrix.Theencoder thatweusewithsparsecodingisnotaparametricencoder.Instead,theencoder</div>
        </div>
    </div>

    <div class="question-card" id="q90">
        <div class="question-header">
            <span class="question-number">Question 90</span>
            <span class="question-type">multiple-choice</span>
        </div>

        <div class="question-text">Machine learning encompasses supervised and unsupervised learning techniques, with algorithms often relying on different forms of data representation and optimization. Understanding how various methods transform data and facilitate model training is critical for building effective AI systems.

Which method computes a linear transformation that aligns new feature axes with directions of greatest data variance, resulting in uncorrelated and lower-dimensional representations?

1) k-means clustering   
2) Independent Component Analysis (ICA)   
3) Principal Components Analysis (PCA)   
4) Gradient Boosted Trees   
5) Stochastic Gradient Descent (SGD)   
6) Random Forests   
7) Autoencoders</div>

        <div class="answer-section">
            <div class="answer-label">‚úì Correct Answer:</div>
            <div class="answer-text">The correct answer is 3) Principal Components Analysis (PCA).</div>
        </div>

        <div class="reference-section">
            <div class="reference-label">üìö Reference Text:</div>
            <button class="toggle-button" onclick="toggleReference(90)">
                Show/Hide Reference
            </button>
            <div id="ref90" class="reference-text hidden">CHAPTER5.MACHINELEARNINGBASICS Anothertypeoflearningalgorithmthatalsobreakstheinputspaceintoregions andhasseparateparametersforeachregionisthedecisiontree( , Breimanetal. 1984)anditsmanyvariants.AsshowninÔ¨Ågure,eachnodeofthedecision 5.7 treeisassociatedwitharegionintheinputspace,andinternalnodesbreakthat regionintoonesub-regionforeachchildofthenode(typicallyusinganaxis-aligned cut). Spaceisthussub-dividedintonon-overlappingregions,withaone-to-one correspondencebetweenleafnodesandinputregions.Eachleafnodeusuallymaps everypointinitsinputregiontothesameoutput.Decisiontreesareusually trainedwithspecializedalgorithmsthatarebeyondthescopeofthisbook.The learningalgorithmcanbeconsiderednon-parametric ifitisallowedtolearnatree ofarbitrarysize,thoughdecisiontreesareusuallyregularizedwithsizeconstraints thatturnthemintoparametricmodelsinpractice.Decisiontreesastheyare typicallyused,withaxis-alignedsplitsandconstantoutputswithineachnode, struggletosolvesomeproblemsthatareeasyevenforlogisticregression.For example,ifwehaveatwo-classproblemandthepositiveclassoccurswherever x2>x1,thedecisionboundaryisnotaxis-aligned.Thedecisiontreewillthus needtoapproximatethedecisionboundarywithmanynodes,implementingastep functionthatconstantlywalksbackandforthacrossthetruedecisionfunction withaxis-alignedsteps. Aswehaveseen,nearestneighborpredictorsanddecisiontreeshavemany limitations.Nonetheless,theyareusefullearningalgorithmswhencomputational resourcesareconstrained.Wecanalsobuildintuitionformoresophisticated learningalgorithmsbythinkingaboutthesimilaritiesanddiÔ¨Äerencesbetween sophisticatedalgorithmsand-NNordecisiontreebaselines. k See (), (), ()orothermachine Murphy2012Bishop2006Hastieetal.2001 learningtextbooksformorematerialontraditionalsupervisedlearningalgorithms. 5.8UnsupervisedLearningAlgorithms Recallfromsectionthatunsupervisedalgorithmsarethosethatexperience 5.1.3 only‚Äúfeatures‚Äùbutnotasupervisionsignal.Thedistinctionbetweensupervised andunsupervisedalgorithmsisnotformallyandrigidlydeÔ¨Ånedbecausethereisno objectivetestfordistinguishingwhetheravalueisafeatureoratargetprovidedby asupervisor.Informally,unsupervisedlearningreferstomostattemptstoextract informationfromadistributionthatdonotrequirehumanlabortoannotate examples.Thetermisusuallyassociatedwithdensityestimation,learningto drawsamplesfromadistribution,learningtodenoisedatafromsomedistribution, Ô¨Åndingamanifoldthatthedataliesnear,orclusteringthedataintogroupsof 1 4 6 CHAPTER5.MACHINELEARNINGBASICS relatedexamples. AclassicunsupervisedlearningtaskistoÔ¨Åndthe‚Äúbest‚Äùrepresentationofthe data.By‚Äòbest‚ÄôwecanmeandiÔ¨Äerentthings,butgenerallyspeakingwearelooking forarepresentationthatpreservesasmuchinformationaboutxaspossiblewhile obeyingsomepenaltyorconstraintaimedatkeepingtherepresentation or simpler moreaccessiblethanitself.x TherearemultiplewaysofdeÔ¨Åningarepresentation.Threeofthe simpler mostcommonincludelowerdimensionalrepresentations,sparserepresentations andindependentrepresentations.Low-dimensionalrepresentationsattemptto compressasmuchinformationaboutxaspossibleinasmallerrepresentation. Sparserepresentations(,; ,; Barlow1989OlshausenandField1996Hintonand Ghahramani1997,)embedthedatasetintoarepresentationwhoseentriesare mostlyzeroesformostinputs.Theuseofsparserepresentationstypicallyrequires increasingthedimensionalityoftherepresentation,sothattherepresentation becomingmostlyzeroesdoesnotdiscardtoomuchinformation. Thisresultsinan overallstructureoftherepresentationthattendstodistributedataalongtheaxes oftherepresentationspace.Independentrepresentationsattempttodisentangle thesourcesofvariationunderlyingthedatadistributionsuchthatthedimensions oftherepresentationarestatisticallyindependent. Of coursethese three criteriaare certainly notmutuallyexclusive.Low- dimensionalrepresentationsoftenyieldelementsthathavefewerorweakerde- pendenciesthantheoriginalhigh-dimensionaldata.Thisisbecauseonewayto reducethesizeofarepresentationistoÔ¨Åndandremoveredundancies.Identifying andremovingmoreredundancyallowsthedimensionalityreductionalgorithmto achievemorecompressionwhilediscardinglessinformation. Thenotionofrepresentationisoneofthecentralthemesofdeeplearningand thereforeoneofthecentralthemesinthisbook.Inthissection,wedevelopsome simpleexamplesofrepresentationlearningalgorithms.Together,theseexample algorithmsshowhowtooperationalizeallthreeofthecriteriaabove.Mostofthe remainingchaptersintroduceadditionalrepresentationlearningalgorithmsthat developthesecriteriaindiÔ¨Äerentwaysorintroduceothercriteria. 5.8.1PrincipalComponentsAnalysis Insection,wesawthattheprincipalcomponentsanalysisalgorithmprovides 2.12 ameansofcompressingdata.WecanalsoviewPCAasanunsupervisedlearning algorithmthatlearnsarepresentationofdata.Thisrepresentationisbasedon twoofthecriteriaforasimplerepresentationdescribedabove.PCAlearnsa 1 4 7 CHAPTER5.MACHINELEARNINGBASICS ‚àí ‚àí 2 0 1 0 0 1 0 2 0 x 1‚àí 2 0‚àí 1 001 02 0x 2 ‚àí ‚àí 2 0 1 0 0 1 0 2 0 z 1‚àí 2 0‚àí 1 001 02 0z 2 Figure5.8:PCAlearnsalinearprojectionthatalignsthedirectionofgreatestvariance withtheaxesofthenewspace. ( L e f t )Theoriginaldataconsistsofsamplesofx.Inthis space,thevariancemightoccuralongdirectionsthatarenotaxis-aligned. ( R i g h t )The transformeddataz=xÓÄæWnowvariesmostalongtheaxisz 1.Thedirectionofsecond mostvarianceisnowalongz 2. representationthathaslowerdimensionalitythantheoriginalinput.Italsolearns arepresentationwhoseelementshavenolinearcorrelationwitheachother.This isaÔ¨Årststeptowardthecriterionoflearningrepresentationswhoseelementsare statisticallyindependent.Toachievefullindependence,arepresentationlearning algorithmmustalsoremovethenonlinearrelationshipsbetweenvariables. PCAlearnsanorthogonal,lineartransformationofthedatathatprojectsan inputxtoarepresentationzasshowninÔ¨Ågure.Insection,wesawthat 5.8 2.12 wecouldlearnaone-dimensional representationthatbestreconstructstheoriginal data(inthesenseofmeansquarederror)andthatthisrepresentationactually correspondstotheÔ¨Årstprincipalcomponentofthedata.ThuswecanusePCA asasimpleandeÔ¨Äectivedimensionalityreductionmethodthatpreservesasmuch oftheinformationinthedataaspossible(again,asmeasuredbyleast-squares reconstructionerror).Inthefollowing,wewillstudyhowthePCArepresentation decorrelatestheoriginaldatarepresentation.X Letusconsiderthemn√ó-dimensionaldesignmatrixX.Wewillassumethat thedatahasameanofzero, E[x] = 0.Ifthisisnotthecase,thedatacaneasily becenteredbysubtractingthemeanfromallexamplesinapreprocessingstep. Theunbiasedsamplecovariancematrixassociatedwithisgivenby:X Var[] =x1 m‚àí1XÓÄæX. (5.85) 1 4 8 CHAPTER5.MACHINELEARNINGBASICS PCAÔ¨Åndsarepresentation(throughlineartransformation)z=xÓÄæWwhere Var[]zisdiagonal. Insection,wesawthattheprincipalcomponentsofadesignmatrix 2.12 X aregivenbytheeigenvectorsofXÓÄæX.Fromthisview, XÓÄæXWW = ŒõÓÄæ. (5.86) Inthissection,weexploitanalternativederivationoftheprincipalcomponents.The principalcomponentsmayalsobeobtainedviathesingularvaluedecomposition. SpeciÔ¨Åcally,theyaretherightsingularvectorsofX.Toseethis,letWbethe rightsingularvectorsinthedecompositionX=UW Œ£ÓÄæ. Wethenrecoverthe originaleigenvectorequationwithastheeigenvectorbasis: W XÓÄæX=ÓÄê UW Œ£ÓÄæÓÄëÓÄæ UW Œ£ÓÄæ= W Œ£2WÓÄæ.(5.87) TheSVDishelpfultoshowthatPCAresultsinadiagonal Var[z].Usingthe SVDof,wecanexpressthevarianceofas: X X Var[] =x1 m‚àí1XÓÄæX (5.88) =1 m‚àí1(UW Œ£ÓÄæ)ÓÄæUW Œ£ÓÄæ(5.89) =1 m‚àí1W Œ£ÓÄæUÓÄæUW Œ£ÓÄæ(5.90) =1 m‚àí1W Œ£2WÓÄæ, (5.91) whereweusethefactthatUÓÄæU=IbecausetheUmatrixofthesingularvalue decompositionisdeÔ¨Ånedtobeorthogonal.Thisshowsthatifwetakez=xÓÄæW, wecanensurethatthecovarianceofisdiagonalasrequired: z Var[] =z1 m‚àí1ZÓÄæZ (5.92) =1 m‚àí1WÓÄæXÓÄæXW (5.93) =1 m‚àí1WÓÄæW Œ£2WÓÄæW (5.94) =1 m‚àí1Œ£2, (5.95) wherethistimeweusethefactthatWÓÄæW=I,againfromthedeÔ¨Ånitionofthe SVD. 1 4 9 CHAPTER5.MACHINELEARNINGBASICS Theaboveanalysisshowsthatwhenweprojectthedataxtoz,viathelinear transformationW,theresultingrepresentationhasadiagonalcovariancematrix (asgivenby Œ£2)whichimmediatelyimpliesthattheindividualelementsofzare mutuallyuncorrelated. ThisabilityofPCAtotransformdataintoarepresentationwheretheelements aremutuallyuncorrelated isaveryimportantpropertyofPCA.Itisasimple exampleofarepresentationthatattemptstodisentangletheunknownfactorsof variationunderlyingthedata. InthecaseofPCA,thisdisentanglingtakesthe formofÔ¨Åndingarotationoftheinputspace(describedbyW)thatalignsthe principalaxesofvariancewiththebasisofthenewrepresentationspaceassociated with.z Whilecorrelationisanimportantcategoryofdependencybetweenelementsof thedata,wearealsointerestedinlearningrepresentationsthatdisentanglemore complicatedformsoffeaturedependencies.Forthis,wewillneedmorethanwhat canbedonewithasimplelineartransformation. 5.8.2-meansClustering k Anotherexampleofasimplerepresentationlearningalgorithmisk-meansclustering. Thek-meansclusteringalgorithmdividesthetrainingsetintokdiÔ¨Äerentclusters ofexamplesthatareneareachother.Wecanthusthinkofthealgorithmas providingak-dimensionalone-hotcodevectorhrepresentinganinputx.Ifx belongstoclusteri,thenh i= 1andallotherentriesoftherepresentationhare zero. Theone-hotcodeprovidedbyk-meansclusteringisanexampleofasparse representation,becausethemajorityofitsentriesarezeroforeveryinput.Later, wewilldevelopotheralgorithmsthatlearnmoreÔ¨Çexiblesparserepresentations, wheremorethanoneentrycanbenon-zeroforeachinputx.One-hotcodes areanextremeexampleofsparserepresentationsthatlosemanyofthebeneÔ¨Åts ofadistributedrepresentation.Theone-hotcodestillconferssomestatistical advantages(itnaturallyconveystheideathatallexamplesinthesameclusterare similartoeachother)anditconfersthecomputational advantagethattheentire representationmaybecapturedbyasingleinteger. Thek-meansalgorithmworksbyinitializingkdiÔ¨Äerentcentroids{¬µ(1),...,¬µ() k} todiÔ¨Äerentvalues,thenalternatingbetweentwodiÔ¨Äerentstepsuntilconvergence. Inonestep,eachtrainingexampleisassignedtoclusteri,whereiistheindexof thenearestcentroid¬µ() i.Intheotherstep,eachcentroid¬µ() iisupdatedtothe meanofalltrainingexamplesx() jassignedtocluster.i 1 5 0 CHAPTER5.MACHINELEARNINGBASICS OnediÔ¨Écultypertainingtoclusteringisthattheclusteringproblemisinherently ill-posed,inthesensethatthereisnosinglecriterionthatmeasureshowwella clusteringofthedatacorrespondstotherealworld.Wecanmeasurepropertiesof theclusteringsuchastheaverageEuclideandistancefromaclustercentroidtothe membersofthecluster.Thisallowsustotellhowwellweareabletoreconstruct thetrainingdatafromtheclusterassignments.Wedonotknowhowwellthe clusterassignmentscorrespondtopropertiesoftherealworld.Moreover,there maybemanydiÔ¨Äerentclusteringsthatallcorrespondwelltosomepropertyof therealworld.WemayhopetoÔ¨Åndaclusteringthatrelatestoonefeaturebut obtainadiÔ¨Äerent,equallyvalidclusteringthatisnotrelevanttoourtask.For example,supposethatweruntwoclusteringalgorithmsonadatasetconsistingof imagesofredtrucks,imagesofredcars,imagesofgraytrucks,andimagesofgray cars.IfweaskeachclusteringalgorithmtoÔ¨Åndtwoclusters,onealgorithmmay Ô¨Åndaclusterofcarsandaclusteroftrucks,whileanothermayÔ¨Åndaclusterof redvehiclesandaclusterofgrayvehicles.Supposewealsorunathirdclustering algorithm,whichisallowedtodeterminethenumberofclusters.Thismayassign theexamplestofourclusters,redcars,redtrucks,graycars,andgraytrucks.This newclusteringnowatleastcapturesinformationaboutbothattributes,butithas lostinformationaboutsimilarity.RedcarsareinadiÔ¨Äerentclusterfromgray cars,justastheyareinadiÔ¨Äerentclusterfromgraytrucks. Theoutputofthe clusteringalgorithmdoesnottellusthatredcarsaremoresimilartograycars thantheyaretograytrucks.TheyarediÔ¨Äerentfromboththings,andthatisall weknow. Theseissuesillustratesomeofthereasonsthatwemaypreferadistributed representationtoaone-hotrepresentation.Adistributedrepresentationcouldhave twoattributesforeachvehicle‚Äîonerepresentingitscolorandonerepresenting whetheritisacaroratruck.Itisstillnotentirelyclearwhattheoptimal distributedrepresentationis(howcanthelearningalgorithmknowwhetherthe twoattributesweareinterestedinarecolorandcar-versus-truckratherthan manufacturerandage?)buthavingmanyattributesreducestheburdenonthe algorithmtoguesswhichsingleattributewecareabout,andallowsustomeasure similaritybetweenobjectsinaÔ¨Åne-grainedwaybycomparingmanyattributes insteadofjusttestingwhetheroneattributematches. 5.9StochasticGradientDescent Nearlyallofdeeplearningispoweredbyoneveryimportantalgorithm:stochastic gradientdescentorSGD.Stochasticgradientdescentisanextensionofthe 1 5 1 CHAPTER5.MACHINELEARNINGBASICS gradientdescentalgorithmintroducedinsection.4.3 Arecurringprobleminmachinelearningisthatlargetrainingsetsarenecessary forgoodgeneralization, butlargetrainingsetsarealsomorecomputationally expensive. Thecostfunctionusedbyamachinelearningalgorithmoftendecomposesasa sumovertrainingexamplesofsomeper-examplelossfunction.Forexample,the negativeconditionallog-likelihoodofthetrainingdatacanbewrittenas J() = Œ∏ E x ,y ‚àºÀÜ pdataL,y,(xŒ∏) =1 mmÓÅò i=1L(x() i,y() i,Œ∏)(5.96) whereistheper-exampleloss L L,y,py. (xŒ∏) = log‚àí (|xŒ∏;) Fortheseadditivecostfunctions,gradientdescentrequirescomputing ‚àá Œ∏J() =Œ∏1 mmÓÅò i=1‚àá Œ∏L(x() i,y() i,.Œ∏) (5.97) Thecomputational costofthisoperationisO(m).Asthetrainingsetsizegrowsto billionsofexamples,thetimetotakeasinglegradientstepbecomesprohibitively long. Theinsightofstochasticgradientdescentisthatthegradientisanexpectation. Theexpectationmaybeapproximately estimatedusingasmallsetofsamples. SpeciÔ¨Åcally,oneachstepofthealgorithm,wecansampleaminibatchofexamples B={x(1),...,x( mÓÄ∞)}drawnuniformlyfromthetrainingset.Theminibatchsize mÓÄ∞istypicallychosentobearelativelysmallnumberofexamples,rangingfrom 1toafewhundred.Crucially,mÓÄ∞isusuallyheldÔ¨Åxedasthetrainingsetsizem grows.WemayÔ¨Åtatrainingsetwithbillionsofexamplesusingupdatescomputed ononlyahundredexamples. Theestimateofthegradientisformedas g=1 mÓÄ∞‚àá Œ∏mÓÄ∞ÓÅò i=1L(x() i,y() i,.Œ∏) (5.98) usingexamplesfromtheminibatch.Thestochasticgradientdescentalgorithm B thenfollowstheestimatedgradientdownhill: Œ∏Œ∏g ‚Üê ‚àíÓÄè, (5.99) whereisthelearningrate. ÓÄè 1 5 2 CHAPTER5.MACHINELEARNINGBASICS Gradientdescentingeneralhasoftenbeenregardedassloworunreliable.In thepast,theapplicationofgradientdescenttonon-convexoptimization problems wasregardedasfoolhardyorunprincipled. Today,weknowthatthemachine learningmodelsdescribedinpartworkverywellwhentrainedwithgradient II descent.Theoptimization algorithmmaynotbeguaranteedtoarriveatevena localminimuminareasonableamountoftime,butitoftenÔ¨Åndsaverylowvalue ofthecostfunctionquicklyenoughtobeuseful. Stochasticgradientdescenthasmanyimportantusesoutsidethecontextof deeplearning.Itisthemainwaytotrainlargelinearmodelsonverylarge datasets.ForaÔ¨Åxedmodelsize,thecostperSGDupdatedoesnotdependonthe trainingsetsizem.Inpractice,weoftenusealargermodelasthetrainingsetsize increases,butwearenotforcedtodoso.Thenumberofupdatesrequiredtoreach convergenceusuallyincreaseswithtrainingsetsize. However,asmapproaches inÔ¨Ånity,themodelwilleventuallyconvergetoitsbestpossibletesterrorbefore SGDhassampledeveryexampleinthetrainingset.Increasingmfurtherwillnot extendtheamountoftrainingtimeneededtoreachthemodel‚Äôsbestpossibletest error.Fromthispointofview,onecanarguethattheasymptoticcostoftraining amodelwithSGDisasafunctionof. O(1) m Priortotheadventofdeeplearning,themainwaytolearnnonlinearmodels wastousethekerneltrickincombinationwithalinearmodel.Manykernellearning algorithmsrequireconstructinganmm√ómatrixG i , j=k(x() i,x() j).Constructing thismatrixhascomputational costO(m2),whichisclearlyundesirablefordatasets with billions of examples. In academia, starting in2006,deep learning was initiallyinterestingbecauseitwasabletogeneralizetonewexamplesbetter thancompetingalgorithmswhentrainedonmedium-sizeddatasetswithtensof thousandsofexamples.Soonafter,deeplearninggarneredadditionalinterestin industry,becauseitprovidedascalablewayoftrainingnonlinearmodelsonlarge datasets. Stochasticgradientdescentandmanyenhancements toitaredescribedfurther inchapter.8 5.10BuildingaMachineLearningAlgorithm Nearlyalldeeplearningalgorithmscanbedescribedasparticularinstancesof afairlysimplerecipe:combineaspeciÔ¨Åcationofadataset,acostfunction,an optimization procedureandamodel. Forexample,thelinearregressionalgorithmcombinesadatasetconsistingof 1 5 3 CHAPTER5.MACHINELEARNINGBASICS Xyand,thecostfunction J,b(w) = ‚àí E x</div>
        </div>
    </div>

    <div class="question-card" id="q91">
        <div class="question-header">
            <span class="question-number">Question 91</span>
            <span class="question-type">multiple-choice</span>
        </div>

        <div class="question-text">In deep learning, computational graphs are used to organize the sequence of operations and facilitate efficient computation of gradients. Backpropagation leverages these graphs to enable training of neural networks using gradient-based optimization methods.

Which of the following best describes how dynamic programming techniques improve the efficiency of backpropagation in computational graphs?

1) By parallelizing all tensor operations to maximize throughput   
2) By minimizing the number of nodes required in the graph   
3) By automatically selecting activation functions that speed up gradient computation   
4) By caching intermediate results during forward and backward passes to avoid redundant computation   
5) By replacing the chain rule with finite difference approximations   
6) By restricting the graph to only scalar-valued operations   
7) By executing only the forward pass and skipping gradient calculations</div>

        <div class="answer-section">
            <div class="answer-label">‚úì Correct Answer:</div>
            <div class="answer-text">The correct answer is 4) By caching intermediate results during forward and backward passes to avoid redundant computation.</div>
        </div>

        <div class="reference-section">
            <div class="reference-label">üìö Reference Text:</div>
            <button class="toggle-button" onclick="toggleReference(91)">
                Show/Hide Reference
            </button>
            <div id="ref91" class="reference-text hidden">CHAPTER6.DEEPFEEDFORWARDNETWORKS z z xx yy ( a)√ó x x ww ( b)u( 1 )u( 1 ) d o t bbu( 2 )u( 2 ) +ÀÜ y ÀÜ y œÉ ( c )XX WWU( 1 )U( 1 ) m a t m u l b bU( 2 )U( 2 ) +HH r e l u x x ww ( d)ÀÜ yÀÜ y d o t Œª Œªu( 1 )u( 1 ) s q ru( 2 )u( 2 ) s u mu( 3 )u( 3 ) √ó Figure6.8:Examplesofcomputationalgraphs.Thegraphusingthe ( a ) √óoperationto compute z= x y.Thegraphforthelogisticregressionprediction ( b ) ÀÜ y= œÉÓÄÄ xÓÄæw+ bÓÄÅ . Someoftheintermediateexpressionsdonothavenamesinthealgebraicexpression butneednamesinthegraph.Wesimplynamethe i-thsuchvariableu( ) i.The ( c ) computationalgraphfortheexpressionH=max{0 ,XW+b},whichcomputesadesign matrixofrectiÔ¨ÅedlinearunitactivationsHgivenadesignmatrixcontainingaminibatch ofinputsX.Examplesa‚Äìcappliedatmostoneoperationtoeachvariable,butit ( d ) ispossibletoapplymorethanoneoperation.Hereweshowacomputationgraphthat appliesmorethanoneoperationtotheweightswofalinearregressionmodel.The weightsareusedtomakeboththepredictionÀÜ yandtheweightdecaypenalty ŒªÓÅê iw2 i. 2 0 6 CHAPTER6.DEEPFEEDFORWARDNETWORKS gmapsfrom Rmto Rn,and fmapsfrom Rnto R.Ify= g(x) and z= f(y),then ‚àÇ z ‚àÇ x i=ÓÅò j‚àÇ z ‚àÇ y j‚àÇ y j ‚àÇ x i. (6.45) Invectornotation,thismaybeequivalentlywrittenas ‚àá x z=ÓÄí‚àÇy ‚àÇxÓÄìÓÄæ ‚àá y z , (6.46) where‚àÇ y ‚àÇ xistheJacobianmatrixof. n m√ó g Fromthisweseethatthegradientofavariablexcanbeobtainedbymultiplying aJacobianmatrix‚àÇ y ‚àÇ xbyagradient‚àá y z.Theback-propagation algorithmconsists ofperformingsuchaJacobian-gradient productforeachoperationinthegraph. Usuallywedonotapplytheback-propagationalgorithmmerelytovectors, butrathertotensorsofarbitrarydimensionality.Conceptually,thisisexactlythe sameasback-propagation withvectors.TheonlydiÔ¨Äerenceishowthenumbers arearrangedinagridtoformatensor.WecouldimagineÔ¨Çatteningeachtensor intoavectorbeforewerunback-propagation,computingavector-valuedgradient, andthenreshapingthegradientbackintoatensor.Inthisrearrangedview, back-propagationisstilljustmultiplyingJacobiansbygradients. Todenotethegradientofavalue zwithrespecttoatensor X,wewrite ‚àá X z, justasif Xwereavector.Theindicesinto Xnowhavemultiplecoordinates‚Äîfor example,a3-Dtensorisindexedbythreecoordinates.Wecanabstractthisaway byusingasinglevariable itorepresentthecompletetupleofindices.Forall possibleindextuples i,(‚àá X z) igives‚àÇ z ‚àÇ X i.Thisisexactlythesameashowforall possibleintegerindices iintoavector,(‚àá x z) igives‚àÇ z ‚àÇ x i.Usingthisnotation,we canwritethechainruleasitappliestotensors.Ifand ,then Y X= ( g) z f= () Y ‚àá X z=ÓÅò j(‚àá X Y j)‚àÇ z ‚àÇ Y j. (6.47) 6.5.3RecursivelyApplyingtheChainRuletoObtainBackprop Usingthechainrule,itisstraightforwardtowritedownanalgebraicexpressionfor thegradientofascalarwithrespecttoanynodeinthecomputational graphthat producedthatscalar.However,actuallyevaluatingthatexpressioninacomputer introducessomeextraconsiderations. SpeciÔ¨Åcally,manysubexpressionsmayberepeatedseveraltimeswithinthe overallexpressionforthegradient.Anyprocedurethatcomputesthegradient 2 0 7 CHAPTER6.DEEPFEEDFORWARDNETWORKS willneedtochoosewhethertostorethesesubexpressionsortorecomputethem severaltimes.Anexampleofhowtheserepeatedsubexpressionsariseisgivenin Ô¨Ågure.Insomecases,computingthesamesubexpressiontwicewouldsimply 6.9 bewasteful. Forcomplicatedgraphs,therecanbeexponentiallymanyofthese wastedcomputations, makinganaiveimplementation ofthechainruleinfeasible. Inothercases,computingthesamesubexpressiontwicecouldbeavalidwayto reducememoryconsumptionatthecostofhigherruntime. WeÔ¨Årstbeginbyaversionoftheback-propagationalgorithmthatspeciÔ¨Åesthe actualgradientcomputationdirectly(algorithm alongwithalgorithm forthe 6.2 6.1 associatedforwardcomputation), intheorderitwillactuallybedoneandaccording totherecursiveapplicationofchainrule.Onecouldeitherdirectlyperformthese computations orviewthedescriptionofthealgorithmasasymbolicspeciÔ¨Åcation ofthecomputational graphforcomputingtheback-propagation. However,this formulationdoesnotmakeexplicitthemanipulation andtheconstructionofthe symbolicgraphthatperformsthegradientcomputation. Such aformulationis presentedbelowinsection,withalgorithm ,wherewealsogeneralizeto 6.5.6 6.5 nodesthatcontainarbitrarytensors. Firstconsideracomputational graphdescribinghowtocomputeasinglescalar u( ) n(saythelossonatrainingexample).Thisscalaristhequantitywhose gradientwewanttoobtain,withrespecttothe n iinputnodes u( 1 )to u( n i ). In otherwordswewishtocompute‚àÇ u() n ‚àÇ u() iforall i‚àà{1 ,2 , . . . , n i}.Intheapplication ofback-propagationtocomputinggradientsforgradientdescentoverparameters, u( ) nwillbethecostassociatedwithanexampleoraminibatch,while u( 1 )to u( n i ) correspondtotheparametersofthemodel. Wewillassumethatthenodesofthegraphhavebeenorderedinsuchaway thatwecancomputetheiroutputoneaftertheother,startingat u( n i + 1 )and goingupto u( ) n.AsdeÔ¨Ånedinalgorithm ,eachnode6.1 u( ) iisassociatedwithan operation f( ) iandiscomputedbyevaluatingthefunction u( ) i= ( f A( ) i) (6.48) where A( ) iisthesetofallnodesthatareparentsof u( ) i. ThatalgorithmspeciÔ¨Åestheforwardpropagationcomputation,whichwecould putinagraph G.Inordertoperformback-propagation, wecanconstructa computational graphthatdependsonGandaddstoitanextrasetofnodes.These formasubgraph BwithonenodepernodeofG.Computation inBproceedsin exactlythereverseoftheorderofcomputationinG,andeachnodeofBcomputes thederivative‚àÇ u() n ‚àÇ u() iassociatedwiththeforwardgraphnode u( ) i.Thisisdone 2 0 8 CHAPTER6.DEEPFEEDFORWARDNETWORKS Algorithm6.1Aprocedurethatperformsthecomputations mapping n iinputs u( 1 )to u( n i )toanoutput u( ) n.ThisdeÔ¨Ånesacomputational graphwhereeachnode computesnumericalvalue u( ) ibyapplyingafunction f( ) itothesetofarguments A( ) ithatcomprisesthevaluesofpreviousnodes u( ) j, j < i,with j P a‚àà ( u( ) i).The inputtothecomputational graphisthevectorx,andissetintotheÔ¨Årst n inodes u( 1 )to u( n i ).Theoutputofthecomputational graphisreadoÔ¨Äthelast(output) node u( ) n. for i , . . . , n = 1 ido u( ) i‚Üê x i endfor for i n= i+1 , . . . , ndo A( ) i‚Üê{ u( ) j|‚àà j P a u(( ) i)} u( ) i‚Üê f( ) i( A( ) i) endfor return u( ) n usingthechainrulewithrespecttoscalaroutput u(</div>
        </div>
    </div>

    <div class="question-card" id="q92">
        <div class="question-header">
            <span class="question-number">Question 92</span>
            <span class="question-type">multiple-choice</span>
        </div>

        <div class="question-text">Deep generative modeling leverages neural networks to synthesize data from complex distributions, with approaches like Variational Autoencoders (VAEs) playing a prominent role. Understanding the architecture and training objectives of VAEs is essential for grasping their strengths and limitations.

Which of the following statements best describes a key reason why Variational Autoencoders (VAEs) tend to produce blurry samples?

1) VAEs commonly use Gaussian likelihoods during maximum likelihood training, which averages out sharp details in the reconstructed data.   
2) VAEs utilize deterministic encoders that remove stochasticity from the reconstruction process.   
3) VAEs are unable to model discrete data due to their reliance on continuous latent spaces.   
4) VAEs require explicit supervision for each latent variable, limiting their expressiveness.   
5) VAEs lack regularization terms in their objective, leading to poor generalization.   
6) VAEs always match the prior and posterior distributions exactly during training.   
7) VAEs exclusively use binary outputs, causing loss of fine features in generated data.</div>

        <div class="answer-section">
            <div class="answer-label">‚úì Correct Answer:</div>
            <div class="answer-text">The correct answer is 1) VAEs commonly use Gaussian likelihoods during maximum likelihood training, which averages out sharp details in the reconstructed data..</div>
        </div>

        <div class="reference-section">
            <div class="reference-label">üìö Reference Text:</div>
            <button class="toggle-button" onclick="toggleReference(92)">
                Show/Hide Reference
            </button>
            <div id="ref92" class="reference-text hidden">v.Ifweareabletospecify p( x),integrateover x,andinvertthe resultingfunction,wecansamplefromwithoutusingmachinelearning. p x() TogeneratesamplesfrommorecomplicateddistributionsthatarediÔ¨Écult tospecifydirectly, diÔ¨Éculttointegrateover, orwhoseresultingintegralsare diÔ¨Éculttoinvert,weuseafeedforwardnetworktorepresentaparametricfamily ofnonlinearfunctions g,andusetrainingdatatoinfertheparametersselecting thedesiredfunction. Wecanthinkof gasprovidinganonlinearchangeofvariablesthattransforms thedistributionoverintothedesireddistributionover. z x Recallfromequationthat,forinvertible,diÔ¨Äerentiable,continuous, 3.47 g p z() = z p x(()) g zÓÄåÓÄåÓÄåÓÄådet(‚àÇ g ‚àÇ z)ÓÄåÓÄåÓÄåÓÄå. (20.72) 6 9 4 CHAPTER20.DEEPGENERATIVEMODELS Thisimplicitlyimposesaprobabilitydistributionover:x p x() = xp z( g‚àí 1()) xÓÄåÓÄåÓÄådet(‚àÇ g ‚àÇ z)ÓÄåÓÄåÓÄå. (20.73) Ofcourse,thisformulamaybediÔ¨Éculttoevaluate,dependingonthechoiceof g,soweoftenuseindirectmeansoflearning g,ratherthantryingtomaximize log() p xdirectly. Insomecases,ratherthanusing gtoprovideasampleof xdirectly,weuse g todeÔ¨Åneaconditionaldistributionover x.Forexample,wecoulduseagenerator netwhoseÔ¨Ånallayerconsistsofsigmoidoutputstoprovidethemeanparameters ofBernoullidistributions: p(x i= 1 ) = () | z g z i . (20.74) Inthiscase,whenweuse gtodeÔ¨Åne p( x z|),weimposeadistributionover xby marginalizing : z p() = x E z p . ( ) x z| (20.75) BothapproachesdeÔ¨Åneadistribution p g( x)andallowustotrainvarious criteriaof p gusingthereparametrization trickofsection.20.9 ThetwodiÔ¨Äerentapproachestoformulatinggeneratornets‚Äîemittingthe parametersofaconditionaldistributionversusdirectlyemittingsamples‚Äîhave complementarystrengthsandweaknesses.WhenthegeneratornetdeÔ¨Ånesa conditionaldistributionover x,itiscapableofgeneratingdiscretedataaswellas continuousdata.Whenthegeneratornetprovidessamplesdirectly,itiscapableof generatingonlycontinuousdata(wecouldintroducediscretizationintheforward propagation, butdoingsowouldmeanthemodelcouldnolongerbetrainedusing back-propagation).Theadvantagetodirectsamplingisthatwearenolonger forcedtouseconditionaldistributionswhoseformcanbeeasilywrittendownand algebraically manipulated byahumandesigner. ApproachesbasedondiÔ¨Äerentiable generatornetworksaremotivatedbythe successof gradient descentappliedtodiÔ¨Äerentiable feedforwardnetworksfor classiÔ¨Åcation. Inthecontextofsupervisedlearning,deepfeedforwardnetworks trainedwithgradient-basedlearningseempracticallyguaranteedtosucceedgiven enoughhiddenunitsandenoughtrainingdata.Canthissamerecipeforsuccess transfertogenerativemodeling? GenerativemodelingseemstobemorediÔ¨ÉcultthanclassiÔ¨Åcationorregression becausethelearningprocessrequiresoptimizingintractablecriteria.Inthecontext 6 9 5 CHAPTER20.DEEPGENERATIVEMODELS ofdiÔ¨Äerentiablegeneratornets,thecriteriaareintractablebecausethedatadoes notspecifyboththeinputs zandtheoutputs xofthegeneratornet.Inthecase ofsupervisedlearning,boththeinputs xandtheoutputs yweregiven,andthe optimization procedureneedsonlytolearnhowtoproducethespeciÔ¨Åedmapping. Inthecaseofgenerativemodeling,thelearningprocedureneedstodeterminehow toarrangespaceinausefulwayandadditionallyhowtomapfromto. z z x Dosovitskiy2015 e t a l .()studiedasimpliÔ¨Åedproblem,wherethecorrespondence between zand xisgiven.SpeciÔ¨Åcally,thetrainingdataiscomputer-rendered imageryofchairs.Thelatentvariables zareparametersgiventotherendering enginedescribingthechoiceofwhichchairmodeltouse,thepositionofthechair, andotherconÔ¨ÅgurationdetailsthataÔ¨Äecttherenderingoftheimage.Usingthis syntheticallygenerateddata,aconvolutionalnetworkisabletolearntomap z descriptionsofthecontentofanimageto xapproximationsofrenderedimages. ThissuggeststhatcontemporarydiÔ¨ÄerentiablegeneratornetworkshavesuÔ¨Écient modelcapacitytobegoodgenerativemodels,andthatcontemporaryoptimization algorithmshavetheabilitytoÔ¨Åtthem.ThediÔ¨Écultyliesindetermininghowto traingeneratornetworkswhenthevalueof zforeach xisnotÔ¨Åxedandknown aheadofeachtime. ThefollowingsectionsdescribeseveralapproachestotrainingdiÔ¨Äerentiable generatornetsgivenonlytrainingsamplesof. x 20.10.3VariationalAutoencoders ThevariationalautoencoderorVAE(,; ,)isa Kingma2013Rezende e t a l .2014 directedmodelthatuseslearnedapproximate inferenceandcanbetrainedpurely withgradient-basedmethods. Togenerateasamplefromthemodel,theVAEÔ¨Årstdrawsasample zfrom thecodedistribution p m o de l( z).ThesampleisthenrunthroughadiÔ¨Äerentiable generatornetwork g( z).Finally, xissampledfromadistribution p m o de l( x; g( z)) = p m o de l( x z|). However,duringtraining,theapproximate inferencenetwork(or encoder) q( z x|)isusedtoobtain zand p m o de l( x z|)isthenviewedasadecoder network. Thekeyinsightbehindvariationalautoencodersisthattheymaybetrained bymaximizingthevariationallowerboundassociatedwithdatapoint: L() q x L() = q E z z x ‚àº q (| )log p m o de l()+(( )) z x , H qz| x (20.76) = E z z x ‚àº q (| )log p m o de l( ) x z|‚àí D K L(( ) qz| x|| p m o de l())z(20.77) ‚â§log p m o de l() x . (20.78) 6 9 6 CHAPTER20.DEEPGENERATIVEMODELS Inequation,werecognizetheÔ¨Årsttermasthejointlog-likelihoodofthevisible 20.76 andhiddenvariablesundertheapproximateposterioroverthelatentvariables (justlikewithEM,exceptthatweuseanapproximateratherthantheexact posterior).Werecognizealsoasecondterm,theentropyoftheapproximate posterior. When qischosentobeaGaussiandistribution,withnoiseaddedto apredictedmeanvalue,maximizingthisentropytermencouragesincreasingthe standarddeviationofthisnoise.Moregenerally,thisentropytermencouragesthe variationalposteriortoplacehighprobabilitymassonmany zvaluesthatcould havegenerated x,ratherthancollapsingtoasinglepointestimateofthemost likelyvalue.Inequation,werecognizetheÔ¨Årsttermasthereconstruction 20.77 log-likelihoodfoundinotherautoencoders.Thesecondtermtriestomakethe approximateposteriordistribution q(z| x) andthemodelprior p m o de l( z) approach eachother. Traditionalapproachestovariationalinferenceandlearninginfer qviaanopti- mizationalgorithm,typicallyiteratedÔ¨Åxedpointequations(section).These19.4 approachesareslowandoftenrequiretheabilitytocompute E z‚àº qlog p m o de l( z x ,) inclosedform.Themainideabehindthevariationalautoencoderistotraina parametricencoder(alsosometimescalledaninferencenetworkorrecognition model)thatproducestheparametersof q.Solongas zisacontinuousvariable,we canthenback-propagate throughsamplesof zdrawnfrom q( z x|) = q( z; f( x; Œ∏)) inordertoobtainagradientwithrespectto Œ∏.Learningthenconsistssolelyof maximizing Lwithrespecttotheparametersoftheencoderanddecoder.Allof theexpectationsinmaybeapproximatedbyMonteCarlosampling. L Thevariationalautoencoderapproachiselegant,theoreticallypleasing,and simpletoimplement.Italsoobtainsexcellentresultsandisamongthestateofthe artapproachestogenerativemodeling.Itsmaindrawbackisthatsamplesfrom variationalautoencoderstrainedonimagestendtobesomewhatblurry.Thecauses ofthisphenomenon arenotyetknown.Onepossibilityisthattheblurrinessis anintrinsiceÔ¨Äectofmaximumlikelihood,whichminimizes D K L( p da t aÓÅ´ p m o de l).As illustratedinÔ¨Ågure,thismeansthatthemodelwillassignhighprobabilityto 3.6 pointsthatoccurinthetrainingset,butmayalsoassignhighprobabilitytoother points.Theseotherpointsmayincludeblurryimages.Partofthereasonthatthe modelwouldchoosetoputprobabilitymassonblurryimagesratherthansome otherpartofthespaceisthatthevariationalautoencodersusedinpracticeusually haveaGaussiandistributionfor p m o de l( x; g( z)). Maximizing alowerboundon thelikelihoodofsuchadistributionissimilartotrainingatraditionalautoencoder withmeansquarederror,inthesensethatithasatendencytoignorefeatures oftheinputthatoccupyfewpixelsorthatcauseonlyasmallchangeinthe brightnessofthepixelsthattheyoccupy.ThisissueisnotspeciÔ¨ÅctoVAEsand 6 9 7 CHAPTER20.DEEPGENERATIVEMODELS issharedwithgenerativemodelsthatoptimizealog-likelihood,orequivalently, D K L( p da t aÓÅ´ p m o de l),asarguedby ()andby().Another Theis e t a l .2015 Huszar2015 troublingissuewithcontemporary VAEmodelsisthattheytendtouseonlyasmall subsetofthedimensionsof z,asiftheencoderwasnotabletotransformenough ofthelocaldirectionsininputspacetoaspacewherethemarginaldistribution matchesthefactorizedprior. TheVAEframeworkisverystraightforwardtoextendtoawiderangeofmodel architectures.ThisisakeyadvantageoverBoltzmannmachines,whichrequire extremelycarefulmodeldesigntomaintaintractability.VAEsworkverywellwith adiversefamilyofdiÔ¨Äerentiable operators.OneparticularlysophisticatedVAE isthedeeprecurrentattentionwriterorDRAWmodel( ,). Gregor e t a l .2015 DRAWusesarecurrentencoderandrecurrentdecodercombinedwithanattention mechanism.ThegenerationprocessfortheDRAWmodelconsistsofsequentially visitingdiÔ¨Äerentsmallimagepatchesanddrawingthevaluesofthepixelsatthose points.VAEscanalsobeextendedtogeneratesequencesbydeÔ¨Åningvariational RNNs( ,)byusingarecurrentencoderanddecoderwithin Chung e t a l .2015b theVAEframework.GeneratingasamplefromatraditionalRNNinvolvesonly non-determinis ticoperationsattheoutputspace.VariationalRNNsalsohave randomvariabilityatthepotentiallymoreabstractlevelcapturedbytheVAE latentvariables. TheVAEframeworkhasbeenextendedtomaximizenotjustthetraditional variationallowerbound,butinsteadtheimportanceweightedautoencoder (,)objective: Burda e t a l .2015 L k() = x , q Ez( 1 ) , . . . , z( ) k‚àº</div>
        </div>
    </div>

    <div class="question-card" id="q93">
        <div class="question-header">
            <span class="question-number">Question 93</span>
            <span class="question-type">multiple-choice</span>
        </div>

        <div class="question-text">Probabilistic modeling often involves defining complex distributions over many variables, where normalization constants play a critical role in ensuring valid probabilities. Advances in generative modeling have led to architectures that avoid computational bottlenecks associated with normalization constants.

Which generative modeling approach is specifically designed to circumvent the need for computing intractable partition functions when learning data distributions?

1) Markov Random Fields   
2) Restricted Boltzmann Machines   
3) Gibbs Sampling-based Models   
4) Generative Adversarial Networks   
5) Metropolis-Hastings Algorithms   
6) Importance Sampling Models   
7) Annealed Importance Sampling</div>

        <div class="answer-section">
            <div class="answer-label">‚úì Correct Answer:</div>
            <div class="answer-text">The correct answer is 4) Generative Adversarial Networks.</div>
        </div>

        <div class="reference-section">
            <div class="reference-label">üìö Reference Text:</div>
            <button class="toggle-button" onclick="toggleReference(93)">
                Show/Hide Reference
            </button>
            <div id="ref93" class="reference-text hidden">well as c ompute a normalization c ons t an t whic h s ums o v e r t he v alues of a and c . ‚Ä¢ I nt r a c t a b l e norm a l i z a t i o n c o ns t a nt s ( t h e p a r t i t i o n f u nc t i o n) : t he partition f unction is dis c us s e d mos t ly in c hapter . N ormalizing c ons t ants of proba- 18 bilit y f unctions c ome up in inference ( ab o v e ) as well as in learning. Man y probabilis t ic mo dels in v olve s uc h a normalizing c ons t ant. U nfortun ately , learning s uc h a mo del often r e q uires c omputing t he gradient of t he loga- r ithm of t he partition f unction with r e s p e c t t o t he mo del parameters . That c omputation is generally as intractable as c omputing t he partition f unction its e lf. Mon t e Carlo Mark ov c hain ( MCMC) metho ds ( c hapter ) are of- 17 t e n us e d t o deal with t he partition f unction ( c omputing it or its gradient). U nfortun ately , MCMC metho ds s uÔ¨Äer when t he mo des of t he mo del dis t r ibu- t ion are n umerous and well-s e par ated, e s p e c ially in high-dimens ional s paces ( s e c t ion ) . 17.5 One wa y t o c onfront t hes e intractable c omputations is t o approximate t hem, and many approaches hav e b e e n prop os e d as dis c us s e d in t his t hird part of t he b o ok. A nother interes t in g w ay , als o dis c us s e d here, w ould b e t o av oid t hes e in t r actable c omputations altogether by des ign, and metho ds t hat do not r e q uire s uc h c omputations are t hus v e r y app e aling. Several generativ e mo dels ha v e b e e n prop os e d in r e c e nt y e ars , with t hat motiv ation. A wide v ariety of c ontemporary approac hes t o generativ e mo deling are dis c us s e d in c hapter . 20 P art is t he mos t imp ortant f or a r e s e arc her‚Äîs om e one</div>
        </div>
    </div>

    <div class="question-card" id="q94">
        <div class="question-header">
            <span class="question-number">Question 94</span>
            <span class="question-type">multiple-choice</span>
        </div>

        <div class="question-text">Neural networks have evolved to address challenges in processing sequences and hierarchical data structures, particularly in the modeling of long-term dependencies. Specialized architectures such as recursive neural networks, recurrent neural networks, and reservoir computing approaches each employ unique strategies for information propagation and memory retention.

Which neural network architecture achieves reduced computational depth proportional to the logarithm of the input length, allowing for improved information propagation across hierarchical structures and mitigation of vanishing gradients?

1) Classic feedforward neural networks   
2) Gated Recurrent Units (GRUs)   
3) Echo State Networks (ESNs)   
4) Recursive neural networks   
5) Transformer networks   
6) Liquid State Machines (LSMs)   
7) Convolutional neural networks (CNNs)</div>

        <div class="answer-section">
            <div class="answer-label">‚úì Correct Answer:</div>
            <div class="answer-text">The correct answer is 4) Recursive neural networks.</div>
        </div>

        <div class="reference-section">
            <div class="reference-label">üìö Reference Text:</div>
            <button class="toggle-button" onclick="toggleReference(94)">
                Show/Hide Reference
            </button>
            <div id="ref94" class="reference-text hidden">rs i v e n e u ra l n e t w o rk ‚Äù a s ‚Äú R NN‚Äù t o a v o i d c o n f u s i o n with ‚Äú re c u rre n t n e u ra l n e t w o rk . ‚Äù 4 0 0 CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS networkswereintroducedbyPollack1990()andtheirpotentialuseforlearningto reasonwasdescribedby().Recursivenetworkshavebeensuccessfully Bottou2011 appliedtoprocessing d a t a s t r u c t u r e sasinputtoneuralnets(Frasconi1997 e t a l .,, 1998 Socher2011ac2013a ),innaturallanguageprocessing( e t a l .,,,)aswellasin computervision( ,). Socher e t a l .2011b Oneclearadvantageofrecursivenetsoverrecurrentnetsisthatforasequence ofthesamelength œÑ,thedepth(measuredasthenumberofcompositionsof nonlinearoperations)canbedrasticallyreducedfrom œÑto O(log œÑ),whichmight helpdealwithlong-termdependencies.Anopenquestionishowtobeststructure thetree.Oneoptionistohaveatreestructurewhichdoesnotdependonthedata, suchasabalancedbinarytree.Insomeapplicationdomains,externalmethods cansuggesttheappropriatetreestructure.Forexample,whenprocessingnatural languagesentences,thetreestructurefortherecursivenetworkcanbeÔ¨Åxedto thestructureoftheparsetreeofthesentenceprovidedbyanaturallanguage parser( ,,). Ideally,onewouldlikethelearneritselfto Socher e t a l .2011a2013a discoverandinferthetreestructurethatisappropriateforanygiveninput,as suggestedby(). Bottou2011 Manyvariantsoftherecursivenetideaarepossible.Forexample,Frasconi e t a l .()and1997Frasconi1998 e t a l .()associatethedatawithatreestructure, andassociatethe inputsandtargetswith individualnodesofthe tree.The computationperformedbyeachnodedoesnothavetobethetraditionalartiÔ¨Åcial neuroncomputation(aÔ¨Énetransformationofallinputsfollowedbyamonotone nonlinearity).Forexample, ()proposeusingtensoroperations Socher e t a l .2013a andbilinearforms,whichhavepreviouslybeenfoundusefultomodelrelationships betweenconcepts(Weston2010Bordes2012 e t a l .,; e t a l .,)whentheconceptsare representedbycontinuousvectors(embeddings). 10.7TheChallengeofLong-TermDependencies Themathematical challengeoflearninglong-termdependenciesinrecurrentnet- workswasintroducedinsection.Thebasicproblemisthatgradientsprop- 8.2.5 agatedovermanystagestendtoeithervanish(mostofthetime)orexplode (rarely,butwithmuchdamagetotheoptimization). Evenifweassumethatthe parametersaresuchthattherecurrentnetworkisstable(canstorememories, withgradientsnotexploding),thediÔ¨Écultywithlong-termdependenciesarises fromtheexponentiallysmallerweightsgiventolong-terminteractions(involving themultiplicationofmanyJacobians)comparedtoshort-termones.Manyother sourcesprovideadeepertreatment(,; Hochreiter1991Doya1993Bengio,; e t a l ., 4 0 1 CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS ‚àí ‚àí ‚àí 6 0 4 0 2 0 0 2 0 4 0 6 0 I nput c o o r di na t e‚àí 4‚àí 3‚àí 2‚àí 101234P r o j e c t i o n o f o utput0 1 2 3 4 5 Figure10.15:Whencomposingmanynonlinearfunctions(likethelinear-tanhlayershown here),theresultishighlynonlinear,typicallywithmostofthevaluesassociatedwithatiny derivative,somevalueswithalargederivative,andmanyalternationsbetweenincreasing anddecreasing.Inthisplot,weplotalinearprojectionofa100-dimensionalhiddenstate downtoasingledimension,plottedonthe y-axis. The x-axisisthecoordinateofthe initialstatealongarandomdirectioninthe100-dimensionalspace.Wecanthusviewthis plotasalinearcross-sectionofahigh-dimensionalfunction.Theplotsshowthefunction aftereachtimestep,orequivalently,aftereachnumberoftimesthetransitionfunction hasbeencomposed. 1994Pascanu2013 ; e t a l .,).Inthissection,wedescribetheprobleminmore detail.Theremainingsectionsdescribeapproachestoovercomingtheproblem. Recurrentnetworksinvolvethecompositionofthesamefunctionmultiple times,oncepertimestep.Thesecompositionscanresultinextremelynonlinear behavior,asillustratedinÔ¨Ågure.10.15 Inparticular,thefunctioncompositionemployedbyrecurrentneuralnetworks somewhatresemblesmatrixmultiplication. Wecanthinkoftherecurrencerelation h( ) t= WÓÄæh( 1 ) t ‚àí(10.36) asaverysimplerecurrentneuralnetworklackinganonlinearactivationfunction, andlackinginputsx.As described insection , thisrecurrencerelation 8.2.5 essentiallydescribesthepowermethod.ItmaybesimpliÔ¨Åedto h( ) t=ÓÄÄ WtÓÄÅÓÄæh( 0 ), (10.37) andifadmitsaneigendecompositionoftheform W WQQ = ŒõÓÄæ, (10.38) 4 0 2 CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS withorthogonal ,therecurrencemaybesimpliÔ¨Åedfurtherto Q h( ) t= QÓÄæŒõtQh( 0 ). (10.39) Theeigenvaluesareraisedtothepowerof tcausingeigenvalueswithmagnitude lessthanonetodecaytozeroandeigenvalueswithmagnitudegreaterthanoneto explode.Anycomponentofh( 0 )thatisnotalignedwiththelargesteigenvector willeventuallybediscarded. Thisproblemisparticulartorecurrentnetworks.Inthescalarcase,imagine multiplyingaweight wbyitselfmanytimes.Theproduct wtwilleithervanishor explodedependingonthemagnitudeof w.However,ifwemakeanon-recurrent networkthathasadiÔ¨Äerentweight w( ) tateachtimestep,thesituationisdiÔ¨Äerent. Iftheinitialstateisgivenby,thenthestateattime 1 tisgivenbyÓÅë t w( ) t.Suppose thatthe w( ) tvaluesaregeneratedrandomly,independentlyfromoneanother,with zeromeanandvariance v.Thevarianceoftheproductis O( vn).Toobtainsome desiredvariance v‚àówemaychoosetheindividualweightswithvariance v=n‚àö v‚àó. Verydeepfeedforwardnetworkswithcarefullychosenscalingcanthusavoidthe vanishingandexplodinggradientproblem,asarguedby(). Sussillo2014 ThevanishingandexplodinggradientproblemforRNNswasindependently discoveredbyseparateresearchers(,; ,,). Hochreiter1991Bengio e t a l .19931994 Onemayhopethattheproblemcanbeavoidedsimplybystayinginaregionof parameterspacewherethegradientsdonotvanishorexplode.Unfortunately,in ordertostorememoriesinawaythatisrobusttosmallperturbations,theRNN mustenteraregionofparameterspacewheregradientsvanish( ,, Bengio e t a l .1993 1994).SpeciÔ¨Åcally,wheneverthemodelisabletorepresentlongtermdependencies, thegradientofalongterminteractionhasexponentiallysmallermagnitudethan thegradientofashortterminteraction. It doesnotmeanthatitisimpossible tolearn,butthatitmighttakeaverylongtimetolearnlong-termdependencies, becausethesignalaboutthesedependencieswilltendtobehiddenbythesmallest Ô¨Çuctuationsarisingfromshort-termdependencies.Inpractice,theexperiments in ()showthatasweincreasethespanofthedependenciesthat Bengio e t a l .1994 needtobecaptured,gradient-basedoptimization becomesincreasinglydiÔ¨Écult, withtheprobabilityofsuccessfultrainingofatraditionalRNNviaSGDrapidly reaching0forsequencesofonlylength10or20. Foradeepertreatmentofrecurrentnetworksasdynamicalsystems,seeDoya (), ()and (),withareview 1993Bengio e t a l .1994SiegelmannandSontag1995 inPascanu2013 e t a l .().Theremainingsectionsofthischapterdiscussvarious approachesthathavebeenproposedtoreducethediÔ¨Écultyoflearninglong- termdependencies(insomecasesallowinganRNNtolearndependenciesacross 4 0 3 CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS hundredsofsteps),buttheproblemoflearninglong-termdependenciesremains oneofthemainchallengesindeeplearning. 10.8EchoStateNetworks Therecurrentweightsmappingfromh( 1 ) t ‚àítoh( ) tandtheinputweightsmapping fromx( ) ttoh( ) taresomeofthemostdiÔ¨Écultparameterstolearninarecurrent network.Oneproposed(,; ,; ,; Jaeger2003Maass e t a l .2002JaegerandHaas2004 Jaeger2007b,)approachtoavoidingthisdiÔ¨Écultyistosettherecurrentweights suchthattherecurrenthiddenunitsdoagoodjobofcapturingthehistoryofpast inputs,and l e a r n o nl y t h e o u t p u t w e i g h t s.Thisistheideathatwasindependently proposedforechostatenetworksorESNs( ,;,) JaegerandHaas2004Jaeger2007b andliquidstatemachines(,).Thelatterissimilar,except Maass e t a l .2002 thatitusesspikingneurons(withbinaryoutputs)insteadofthecontinuous-valued hiddenunitsusedforESNs.BothESNsandliquidstatemachinesaretermed reservoircomputing(Luko≈°eviƒçiusandJaeger2009,)todenotethefactthat thehiddenunitsformofreservoiroftemporalfeatureswhichmaycapturediÔ¨Äerent aspectsofthehistoryofinputs. Onewaytothinkaboutthesereservoircomputingrecurrentnetworksisthat theyaresimilartokernelmachines:theymapanarbitrarylengthsequence(the historyofinputsuptotime t)intoaÔ¨Åxed-lengthvector(therecurrentstateh( ) t), onwhichalinearpredictor(typicallyalinearregression)canbeappliedtosolve theproblemofinterest.Thetrainingcriterionmaythenbeeasilydesignedtobe convexasafunctionoftheoutputweights.Forexample,iftheoutputconsists oflinearregressionfromthehiddenunitstotheoutputtargets,andthetraining criterionismeansquarederror,thenitisconvexandmaybesolvedreliablywith simplelearningalgorithms(,). Jaeger2003 Theimportantquestionistherefore:howdowesettheinputandrecurrent weightssothatarichsetofhistoriescanberepresentedintherecurrentneural networkstate? Theanswerproposedinthereservoircomputingliteratureisto viewtherecurrentnetasadynamicalsystem,andsettheinputandrecurrent weightssuchthatthedynamicalsystemisneartheedgeofstability. TheoriginalideawastomaketheeigenvaluesoftheJacobianofthestate-to- statetransitionfunctionbecloseto.Asexplainedinsection,animportant 1 8.2.5 characteristicofarecurrentnetworkistheeigenvaluespectrumoftheJacobians J( ) t=‚àÇ s( )</div>
        </div>
    </div>

    <div class="question-card" id="q95">
        <div class="question-header">
            <span class="question-number">Question 95</span>
            <span class="question-type">multiple-choice</span>
        </div>

        <div class="question-text">In probabilistic modeling, Markov Chain Monte Carlo (MCMC) methods are widely used to sample from complex distributions, but their efficiency often depends on how quickly the Markov chain reaches equilibrium. Tempering strategies and block sampling are among several techniques developed to improve mixing in such high-dimensional models.

Which method specifically aims to overcome slow mixing in multi-modal distributions by running multiple Markov chains at different temperatures and allowing their states to be swapped?

1) Parallel tempering   
2) Hybrid Monte Carlo   
3) Block Gibbs sampling   
4) Variational inference   
5) Metropolis-Hastings   
6) Simulated annealing   
7) Contrastive divergence</div>

        <div class="answer-section">
            <div class="answer-label">‚úì Correct Answer:</div>
            <div class="answer-text">The correct answer is 1) Parallel tempering.</div>
        </div>

        <div class="reference-section">
            <div class="reference-label">üìö Reference Text:</div>
            <button class="toggle-button" onclick="toggleReference(95)">
                Show/Hide Reference
            </button>
            <div id="ref95" class="reference-text hidden">v,thenweknowthatthechainmixeswhen At haseÔ¨Äectivelylostalloftheeigenvaluesfrom Abesidestheuniqueeigenvalueof.1 Thismeansthatthemagnitudeofthesecondlargesteigenvaluewilldeterminethe mixingtime.However,inpractice,wecannotactuallyrepresentourMarkovchain intermsofamatrix.Thenumberofstatesthatourprobabilisticmodelcanvisit isexponentiallylargeinthenumberofvariables,soitisinfeasibletorepresent v, A,ortheeigenvaluesof A. Duetotheseandotherobstacles,weusuallydo notknowwhetheraMarkovchainhasmixed.Instead,wesimplyruntheMarkov chainforanamountoftimethatweroughlyestimatetobesuÔ¨Écient,anduse heuristicmethodstodeterminewhetherthechainhasmixed.Theseheuristic methodsincludemanuallyinspectingsamplesormeasuringcorrelationsbetween 5 9 8 CHAPTER17.MONTECARLOMETHODS successivesamples. 17.4GibbsSampling Sofarwehavedescribedhowtodrawsamplesfromadistribution q( x)byrepeatedly updating x x‚ÜêÓÄ∞‚àº T( xÓÄ∞| x).However,wehavenotdescribedhowtoensurethat q( x)isausefuldistribution.Twobasicapproachesareconsideredinthisbook. TheÔ¨Årstoneistoderive Tfromagivenlearned p m o de l,describedbelowwiththe caseofsamplingfromEBMs.Thesecondoneistodirectlyparametrize Tand learnit,sothatitsstationarydistributionimplicitlydeÔ¨Ånesthe p m o de lofinterest. Examplesofthissecondapproacharediscussedinsectionsand. 20.1220.13 Inthecontextofdeeplearning,wecommonlyuseMarkovchainstodraw samplesfromanenergy-basedmodeldeÔ¨Åningadistribution p m o de l( x).Inthiscase, wewantthe q( x)fortheMarkovchaintobe p m o de l( x).Toobtainthedesired q() x,wemustchooseanappropriate T( xÓÄ∞| x). AconceptuallysimpleandeÔ¨ÄectiveapproachtobuildingaMarkovchain thatsamplesfrom p m o de l( x)istouseGibbssampling,inwhichsamplingfrom T( xÓÄ∞| x)isaccomplishedbyselectingonevariablex iandsamplingitfrom p m o de l conditionedonitsneighborsintheundirectedgraph GdeÔ¨Åningthestructureof theenergy-basedmodel.Itisalsopossibletosampleseveralvariablesatthesame timesolongastheyareconditionallyindependentgivenalloftheirneighbors. AsshownintheRBMexampleinsection,allofthehiddenunitsofan 16.7.1 RBMmaybesampledsimultaneouslybecausetheyareconditionallyindependent fromeachothergivenallofthevisibleunits.Likewise,allofthevisibleunitsmay besampledsimultaneouslybecausetheyareconditionallyindependentfromeach othergivenallofthehiddenunits.Gibbssamplingapproachesthatupdatemany variablessimultaneouslyinthiswayarecalledblockGibbssampling. AlternateapproachestodesigningMarkovchainstosamplefrom p m o de lare possible.Forexample,theMetropolis-Hastingsalgorithmiswidelyusedinother disciplines.Inthecontextofthedeeplearningapproachtoundirectedmodeling, itisraretouseanyapproachotherthanGibbssampling.Improvedsampling techniquesareonepossibleresearchfrontier. 17.5TheChallengeofMixingbetweenSeparatedModes TheprimarydiÔ¨ÉcultyinvolvedwithMCMCmethodsisthattheyhaveatendency tomixpoorly.Ideally,successivesamplesfromaMarkovchaindesignedtosample 5 9 9 CHAPTER17.MONTECARLOMETHODS from p( x)wouldbecompletelyindependentfromeachotherandwouldvisitmany diÔ¨Äerentregionsin xspaceproportionaltotheirprobability.Instead,especially inhighdimensionalcases,MCMCsamplesbecomeverycorrelated.Werefer tosuchbehaviorasslowmixingorevenfailuretomix.MCMCmethodswith slowmixingcanbeseenasinadvertentlyperformingsomethingresemblingnoisy gradientdescentontheenergyfunction,orequivalentlynoisyhillclimbingonthe probability,withrespecttothestateofthechain(therandomvariablesbeing sampled). Thechaintendstotakesmallsteps(inthespaceofthestateofthe Markovchain),fromaconÔ¨Åguration x( 1 ) t‚àítoaconÔ¨Åguration x( ) t,withtheenergy E( x( ) t)generallylowerorapproximately equaltotheenergy E( x( 1 ) t‚àí),witha preferenceformovesthatyieldlowerenergyconÔ¨Ågurations. Whenstartingfroma ratherimprobableconÔ¨Åguration(higherenergythanthetypicalonesfrom p( x)), thechaintendstograduallyreducetheenergyofthestateandonlyoccasionally movetoanothermode.Oncethechainhasfoundaregionoflowenergy(for example,ifthevariablesarepixelsinanimage,aregionoflowenergymightbe aconnectedmanifoldofimagesofthesameobject),whichwecallamode,the chainwilltendtowalkaroundthatmode(followingakindofrandomwalk).Once inawhileitwillstepoutofthatmodeandgenerallyreturntoitor(ifitÔ¨Ånds anescaperoute)movetowardsanothermode.Theproblemisthatsuccessful escaperoutesarerareformanyinterestingdistributions,sotheMarkovchainwill continuetosamplethesamemodelongerthanitshould. ThisisveryclearwhenweconsidertheGibbssamplingalgorithm(section).17.4 Inthiscontext,considertheprobabilityofgoingfromonemodetoanearbymode withinagivennumberofsteps.Whatwilldeterminethatprobabilityistheshape ofthe‚Äúenergybarrier‚Äù betweenthesemodes.Transitionsbetweentwomodes thatareseparatedbyahighenergybarrier(aregionoflowprobability)are exponentiallylesslikely(intermsoftheheightoftheenergybarrier).Thisis illustratedinÔ¨Ågure.Theproblemariseswhentherearemultiplemodeswith 17.1 highprobabilitythatareseparatedbyregionsoflowprobability,especiallywhen eachGibbssamplingstepmustupdateonlyasmallsubsetofvariableswhose valuesarelargelydeterminedbytheothervariables. Asasimpleexample,consideranenergy-basedmodelovertwovariablesaand b,whicharebothbinarywithasign,takingonvalues ‚àí1 1and.If E(ab ,) =‚àí wab forsomelargepositivenumber w,thenthemodelexpressesastrongbeliefthata andbhavethesamesign.ConsiderupdatingbusingaGibbssamplingstepwith a= 1. Theconditionaldistributionoverbisgivenby P(b= 1|a= 1)= œÉ( w). If wislarge,thesigmoidsaturates,andtheprobabilityofalsoassigningbtobe 1iscloseto1.Likewise,ifa=‚àí1,theprobabilityofassigningbtobe‚àí1is closeto1.Accordingto P m o de l(ab ,),bothsignsofbothvariablesareequallylikely. 6 0 0 CHAPTER17.MONTECARLOMETHODS Figure17.1:PathsfollowedbyGibbssamplingforthreedistributions,withtheMarkov chaininitializedatthemodeinbothcases. ( L e f t )Amultivariatenormaldistribution withtwoindependentvariables.Gibbssamplingmixeswellbecausethevariablesare independent.Amultivariatenormaldistributionwithhighlycorrelatedvariables. ( C e n t e r ) ThecorrelationbetweenvariablesmakesitdiÔ¨ÉcultfortheMarkovchaintomix.Because theupdateforeachvariablemustbeconditionedontheothervariable,thecorrelation reducestherateatwhichtheMarkovchaincanmoveawayfromthestartingpoint. ( R i g h t )AmixtureofGaussianswithwidelyseparatedmodesthatarenotaxis-aligned. GibbssamplingmixesveryslowlybecauseitisdiÔ¨Éculttochangemodeswhilealtering onlyonevariableatatime. Accordingto P m o de l(ab|),bothvariablesshouldhavethesamesign.Thismeans thatGibbssamplingwillonlyveryrarelyÔ¨Çipthesignsofthesevariables. Inmorepracticalscenarios,thechallengeisevengreaterbecausewecarenot onlyaboutmakingtransitionsbetweentwomodesbutmoregenerallybetween allthemanymodesthatarealmodelmightcontain.Ifseveralsuchtransitions arediÔ¨ÉcultbecauseofthediÔ¨Écultyofmixingbetweenmodes,thenitbecomes veryexpensivetoobtainareliablesetofsamplescoveringmostofthemodes,and convergenceofthechaintoitsstationarydistributionisveryslow. SometimesthisproblemcanberesolvedbyÔ¨Åndinggroupsofhighlydependent unitsandupdatingallofthemsimultaneouslyinablock. Unfortunately,when thedependenciesarecomplicated,itcanbecomputationally intractabletodrawa samplefromthegroup.Afterall,theproblemthattheMarkovchainwasoriginally introducedtosolveisthisproblemofsamplingfromalargegroupofvariables. Inthecontextofmodelswithlatentvariables,whichdeÔ¨Åneajointdistribution p m o de l( x h ,),weoftendrawsamplesof xbyalternatingbetweensamplingfrom p m o de l( x h|)andsamplingfrom p m o de l( h x|).Fromthepointofviewofmixing 6 0 1 CHAPTER17.MONTECARLOMETHODS Figure17.2:Anillustrationoftheslowmixingproblemindeepprobabilisticmodels. Eachpanelshouldbereadlefttoright,toptobottom. ( L e f t )Consecutivesamplesfrom GibbssamplingappliedtoadeepBoltzmannmachinetrainedontheMNISTdataset. Consecutivesamplesaresimilartoeachother.BecausetheGibbssamplingisperformed inadeepgraphicalmodel,thissimilarityisbasedmoreonsemanticratherthanrawvisual features,butitisstilldiÔ¨ÉcultfortheGibbschaintotransitionfromonemodeofthe distributiontoanother,forexamplebychangingthedigitidentity.Consecutive ( R i g h t ) ancestralsamplesfromagenerativeadversarialnetwork.Becauseancestralsampling generateseachsampleindependentlyfromtheothers,thereisnomixingproblem. rapidly,wewouldlike p m o de l( h x|)tohaveveryhighentropy.However,fromthe pointofviewoflearningausefulrepresentationof h,wewouldlike htoencode enoughinformationabout xtoreconstructitwell,whichimpliesthat hand x shouldhaveveryhighmutualinformation. Thesetwogoalsareatoddswitheach other.Weoftenlearngenerativemodelsthatverypreciselyencode xinto hbut arenotabletomixverywell.ThissituationarisesfrequentlywithBoltzmann machines‚ÄîthesharperthedistributionaBoltzmannmachinelearns,theharder itisforaMarkovchainsamplingfromthemodeldistributiontomixwell.This problemisillustratedinÔ¨Ågure.17.2 AllthiscouldmakeMCMCmethodslessusefulwhenthedistributionofinterest hasamanifoldstructurewithaseparatemanifoldforeachclass:thedistribution isconcentratedaroundmanymodesandthesemodesareseparatedbyvastregions ofhighenergy.ThistypeofdistributioniswhatweexpectinmanyclassiÔ¨Åcation problemsandwouldmakeMCMCmethodsconvergeveryslowlybecauseofpoor mixingbetweenmodes. 6 0 2 CHAPTER17.MONTECARLOMETHODS 1 7 . 5 . 1 T em p eri n g t o Mi x b et w een Mo d es Whenadistributionhassharppeaksofhighprobabilitysurroundedbyregionsof lowprobability,itisdiÔ¨ÉculttomixbetweenthediÔ¨Äerentmodesofthedistribution. Severaltechniquesforfastermixingarebasedonconstructingalternativeversions ofthetargetdistributioninwhichthepeaksarenotashighandthesurrounding valleysarenotaslow.Energy-basedmodelsprovideaparticularlysimplewayto doso.Sofar,wehavedescribedanenergy-basedmodelasdeÔ¨Åningaprobability distribution p E . () exp( x‚àù ‚àí()) x (17.25) Energy-basedmodelsmaybeaugmentedwithanextraparameter Œ≤controlling howsharplypeakedthedistributionis: p Œ≤() exp( ()) x‚àù ‚àí Œ≤ E x . (17.26) The Œ≤parameterisoftendescribedasbeingthereciprocalofthetemperature, reÔ¨Çectingtheoriginofenergy-basedmodelsinstatisticalphysics.Whenthe temperaturefallstozeroandrisestoinÔ¨Ånity,theenergy-basedmodelbecomes Œ≤ deterministic.WhenthetemperaturerisestoinÔ¨Ånityand Œ≤fallstozero,the distribution(fordiscrete)becomesuniform. x Typically,amodelistrainedtobeevaluatedat Œ≤= 1.However,wecanmake useofothertemperatures,particularlythosewhere Œ≤ <1.Temperingisageneral strategyofmixingbetweenmodesof p 1rapidlybydrawingsampleswith. Œ≤ <1 Markovchainsbasedontemperedtransitions(,)temporarily Neal1994 samplefromhigher-temperaturedistributionsinordertomixtodiÔ¨Äerentmodes, thenresumesamplingfromtheunittemperaturedistribution.Thesetechniques havebeenappliedtomodelssuchasRBMs (Salakhutdinov2010,).Another approachistouseparalleltempering(,),inwhichtheMarkovchain Iba2001 simulatesmanydiÔ¨Äerentstatesinparallel,atdiÔ¨Äerenttemperatures.Thehighest temperaturestatesmixslowly,whilethelowesttemperaturestates,attemperature 1,provideaccuratesamplesfromthemodel.Thetransitionoperatorincludes stochasticallyswappingstatesbetweentwodiÔ¨Äerenttemperaturelevels,sothata suÔ¨Écientlyhigh-probabilit ysamplefromahigh-temperatureslotcanjumpintoa lowertemperatureslot.ThisapproachhasalsobeenappliedtoRBMs(Desjardins e t a l . e t a l . ,;2010Cho,). Althoughtemperingisapromisingapproach,at 2010 thispointithasnotallowedresearcherstomakeastrongadvanceinsolvingthe challengeofsamplingfromcomplexEBMs.Onepossiblereasonisthatthere arecriticaltemperaturesaroundwhichthetemperaturetransitionmustbe veryslow(asthetemperatureisgraduallyreduced)inorderfortemperingtobe eÔ¨Äective. 6 0 3 CHAPTER17.MONTECARLOMETHODS 1 7 . 5 . 2 D ep t h Ma y Hel p Mi xi n g Whendrawingsamplesfromalatentvariablemodel p( h x ,),wehaveseenthatif p( h x|)encodes xtoowell,thensamplingfrom p( x h|)willnotchange xvery muchandmixingwillbepoor.Onewaytoresolvethisproblemistomake hbea deeprepresentation,thatencodesintoinsuchawaythataMarkovchainin x h thespaceof hcanmixmoreeasily.Manyrepresentationlearningalgorithms,such asautoencodersandRBMs,tendtoyieldamarginaldistributionover hthatis moreuniformandmoreunimodalthantheoriginaldatadistributionover x.Itcan bearguedthatthisarisesfromtryingtominimizereconstructionerrorwhileusing alloftheavailablerepresentationspace,becauseminimizingreconstructionerror overthetrainingexampleswillbebetterachievedwhendiÔ¨Äerenttrainingexamples areeasilydistinguishablefromeachotherin h-space,andthuswellseparated. Bengio2013a e t</div>
        </div>
    </div>

    <div class="question-card" id="q96">
        <div class="question-header">
            <span class="question-number">Question 96</span>
            <span class="question-type">multiple-choice</span>
        </div>

        <div class="question-text">In probabilistic models for unsupervised learning, mean field inference is a common variational technique for approximating complex posterior distributions, particularly in binary sparse coding. Understanding the limitations and dynamics of mean field updates is important for designing effective inference algorithms.

Which limitation of mean field inference in binary sparse coding models prevents the representation of dependencies between hidden units during posterior approximation?

1) It enforces simultaneous activation of all hidden units   
2) It assumes a fully factorized posterior distribution   
3) It utilizes non-iterative update equations   
4) It allows unrestricted block updates of units   
5) It employs deterministic rather than probabilistic encoding   
6) It requires all units to encode the same residual error   
7) It incorporates direct optimization of functionals over vectors</div>

        <div class="answer-section">
            <div class="answer-label">‚úì Correct Answer:</div>
            <div class="answer-text">The correct answer is 2) It assumes a fully factorized posterior distribution.</div>
        </div>

        <div class="reference-section">
            <div class="reference-label">üìö Reference Text:</div>
            <button class="toggle-button" onclick="toggleReference(96)">
                Show/Hide Reference
            </button>
            <div id="ref96" class="reference-text hidden">,ÀÜ h) (19.38) =‚àÇ ‚àÇÀÜh iÔ£Æ Ô£∞mÓÅò j = 1ÓÅ® ÀÜ h j(log( œÉ b j)log‚àí ÀÜ h j)+(1‚àíÀÜ h j)(log( œÉ‚àí b j)log(1 ‚àí ‚àíÀÜ h j))ÓÅ© (19.39) +1 2nÓÅò j = 1Ô£Æ Ô£∞logŒ≤ j 2 œÄ‚àí Œ≤ jÔ£´ Ô£≠ v2 j‚àí2 v j W j , :ÀÜh+ÓÅò kÔ£Æ Ô£∞ W2 j , kÀÜh k+ÓÅò l kÓÄ∂=W j , k W j , lÀÜh kÀÜh lÔ£π Ô£ªÔ£∂ Ô£∏Ô£π Ô£ªÔ£π Ô£ª (19.40) =log( œÉ b i)log‚àí ÀÜh i‚àí ‚àí 1+log(1ÀÜh i)+1log( ‚àí œÉ‚àí b i) (19.41) +nÓÅò j = 1Ô£Æ Ô£∞ Œ≤ jÔ£´ Ô£≠ v j W j , i‚àí1 2W2 j , i‚àíÓÅò k iÓÄ∂=W j , k W j , iÀÜh kÔ£∂ Ô£∏Ô£π Ô£ª (19.42) 6 4 3 CHAPTER19.APPROXIMATEINFERENCE = b i‚àílogÀÜ h i+log(1‚àíÀÜh i)+ vÓÄæŒ≤ W : , i‚àí1 2WÓÄæ : , i Œ≤ W : , i‚àíÓÅò j iÓÄ∂=WÓÄæ : , j Œ≤ W : , iÀÜh j .(19.43) ToapplytheÔ¨Åxedpointupdateinferencerule,wesolvefortheÀÜ h ithatsets equationto0:19.43 ÀÜh i= œÉÔ£´ Ô£≠ b i+ vÓÄæŒ≤ W : , i‚àí1 2WÓÄæ : , i Œ≤ W : , i‚àíÓÅò j iÓÄ∂=WÓÄæ : , j Œ≤ W : , iÀÜh jÔ£∂ Ô£∏ .(19.44) Atthispoint,wecanseethatthereisacloseconnectionbetweenrecurrent neuralnetworksandinferenceingraphicalmodels.SpeciÔ¨Åcally,themeanÔ¨Åeld Ô¨ÅxedpointequationsdeÔ¨Ånedarecurrentneuralnetwork.Thetaskofthisnetwork istoperforminference.Wehavedescribedhowtoderivethisnetworkfroma modeldescription,butitisalsopossibletotraintheinferencenetworkdirectly. Severalideasbasedonthisthemearedescribedinchapter.20 Inthecaseofbinarysparsecoding,wecanseethattherecurrentnetwork connectionspeciÔ¨Åedbyequationconsistsofrepeatedlyupdatingthehidden 19.44 unitsbasedonthechangingvaluesoftheneighboringhiddenunits.Theinput alwayssendsaÔ¨Åxedmessageof vÓÄæŒ≤ Wtothehiddenunits,butthehiddenunits constantlyupdatethemessagetheysendtoeachother.SpeciÔ¨Åcally,twounits ÀÜh i andÀÜ h jinhibiteachotherwhentheirweightvectorsarealigned.Thisisaformof competition‚Äîbetweentwohiddenunitsthatbothexplaintheinput,onlytheone thatexplainstheinputbestwillbeallowedtoremainactive.Thiscompetitionis themeanÔ¨Åeldapproximation‚Äôsattempttocapturetheexplainingawayinteractions inthebinarysparsecodingposterior.TheexplainingawayeÔ¨Äectactuallyshould causeamulti-modalposterior,sothatifwedrawsamplesfromtheposterior, somesampleswillhaveoneunitactive,othersampleswillhavetheotherunit active,butveryfewsampleshavebothactive.Unfortunately,explainingaway interactionscannotbemodeledbythefactorial qusedformeanÔ¨Åeld,sothemean Ô¨Åeldapproximation isforcedtochooseonemodetomodel.Thisisaninstanceof thebehaviorillustratedinÔ¨Ågure.3.6 Wecanrewriteequationintoanequivalentformthatrevealssomefurther 19.44 insights: ÀÜh i= œÉÔ£´ Ô£¨Ô£≠ b i+Ô£´ Ô£≠ v‚àíÓÅò j iÓÄ∂=W : , jÀÜh jÔ£∂ Ô£∏ÓÄæ Œ≤ W : , i‚àí1 2WÓÄæ : , i Œ≤ W : , iÔ£∂ Ô£∑Ô£∏ .(19.45) Inthisreformulation,weseetheinputateachstepasconsistingof v‚àíÓÅê j iÓÄ∂=W : , jÀÜh j ratherthan v.Wecanthusthinkofunit iasattemptingtoencodetheresidual 6 4 4 CHAPTER19.APPROXIMATEINFERENCE errorin vgiventhecodeoftheotherunits.Wecanthusthinkofsparsecodingas aniterativeautoencoder,thatrepeatedlyencodesanddecodesitsinput,attempting toÔ¨Åxmistakesinthereconstructionaftereachiteration. Inthisexample,wehavederivedanupdaterulethatupdatesasingleunitat atime.Itwouldbeadvantageoustobeabletoupdatemoreunitssimultaneously. Somegraphicalmodels,suchasdeepBoltzmannmachines,arestructuredinsucha waythatwecansolveformanyentriesofÀÜhsimultaneously.Unfortunately,binary sparsecodingdoesnotadmitsuchblockupdates.Instead,wecanuseaheuristic techniquecalleddampingtoperformblockupdates.Inthedampingapproach, wesolvefortheindividuallyoptimalvaluesofeveryelementofÀÜh,thenmoveallof thevaluesinasmallstepinthatdirection.Thisapproachisnolongerguaranteed toincreaseLateachstep,butworkswellinpracticeformanymodels.SeeKoller andFriedman2009()formoreinformationaboutchoosingthedegreeofsynchrony anddampingstrategiesinmessagepassingalgorithms. 19.4.2CalculusofVariations Beforecontinuingwithourpresentationofvariationallearning,wemustbrieÔ¨Çy introduceanimportantsetofmathematical toolsusedinvariationallearning: calculusofvariations. Manymachinelearningtechniquesarebasedonminimizingafunction J( Œ∏)by Ô¨Åndingtheinputvector Œ∏‚àà Rnforwhichittakesonitsminimalvalue.Thiscan beaccomplishedwithmultivariatecalculusandlinearalgebra,bysolvingforthe criticalpointswhere ‚àá Œ∏ J( Œ∏) = 0.Insomecases,weactuallywanttosolvefora function f( x),suchaswhenwewanttoÔ¨Åndtheprobabilitydensityfunctionover somerandomvariable.Thisiswhatcalculusofvariationsenablesustodo. Afunction ofa function fisknown asafunctional J[ f].Muchas we cantakepartialderivativesofafunctionwithrespecttoelementsofitsvector- valuedargument,wecantakefunctionalderivatives,alsoknownasvariational derivatives,ofafunctional J[ f]withrespecttoindividualvaluesofthefunction f( x)atanyspeciÔ¨Åcvalueof x.Thefunctionalderivativeofthefunctional Jwith respecttothevalueofthefunctionatpointisdenoted f xŒ¥ Œ¥ f x ( )J. Acompleteformaldevelopmentoffunctionalderivativesisbeyondthescopeof thisbook.Forourpurposes,itissuÔ¨ÉcienttostatethatfordiÔ¨Äerentiablefunctions f g y , () xanddiÔ¨Äerentiable functions ( x)withcontinuousderivatives,that Œ¥ Œ¥ f() xÓÅö g f , d (() x x) x=‚àÇ ‚àÇ yg f , . (() x x) (19.46) 6 4 5</div>
        </div>
    </div>

    <div class="question-card" id="q97">
        <div class="question-header">
            <span class="question-number">Question 97</span>
            <span class="question-type">multiple-choice</span>
        </div>

        <div class="question-text">Deep Belief Networks (DBNs) are a class of deep generative models that played a central role in the development of modern deep learning. Their architecture combines both undirected and directed graphical model components, enabling hierarchical representation learning.

Which of the following statements correctly describes a structural property unique to Deep Belief Networks compared to other deep neural architectures?

1) All layers in a Deep Belief Network have undirected connections between units.   
2) Deep Belief Networks use only real-valued latent variables in every layer.   
3) Every layer in a Deep Belief Network is trained jointly via maximum likelihood estimation.   
4) Deep Belief Networks feature undirected connections at the top layer and directed connections in lower layers.   
5) The acronym "DBN" is exclusively used to denote dynamic Bayesian networks in machine learning.   
6) Deep Belief Networks always require generative fine-tuning for effective classification.   
7) Deep Belief Networks achieve exact inference across all layers due to their factorial structure.</div>

        <div class="answer-section">
            <div class="answer-label">‚úì Correct Answer:</div>
            <div class="answer-text">The correct answer is 4) Deep Belief Networks feature undirected connections at the top layer and directed connections in lower layers..</div>
        </div>

        <div class="reference-section">
            <div class="reference-label">üìö Reference Text:</div>
            <button class="toggle-button" onclick="toggleReference(97)">
                Show/Hide Reference
            </button>
            <div id="ref97" class="reference-text hidden">j+ vÓÄæW : , j h jÓÅØ (20.11) Sinceweareconditioningonthevisibleunits v,wecantreattheseasconstant withrespecttothedistribution P(hv|).Thefactorialnatureoftheconditional P(hv|)followsimmediately fromourabilitytowritethejointprobabilityover thevector hastheproductof(unnormalized) distributionsovertheindividual elements, h j.Itisnowasimplematterofnormalizingthedistributionsoverthe individualbinary h j. P h( j= 1 ) =| vÀú P h( j= 1 )| v Àú P h( j= 0 )+| v Àú P h( j= 1 )| v(20.12) =expÓÄà c j+ vÓÄæW : , jÓÄâ exp0+exp {} { c j+ vÓÄæ W : , j}(20.13) = œÉÓÄê c j+ vÓÄæW : , jÓÄë . (20.14) Wecannowexpressthefullconditionaloverthehiddenlayerasthefactorial distribution: P( ) = h v|n hÓÅô j = 1œÉÓÄê (2 1) (+ h‚àí ÓÄå c WÓÄæv)ÓÄë j.(20.15) Asimilarderivationwillshowthattheotherconditionofinteresttous, P( v h|), isalsoafactorialdistribution: P( ) = v h|n vÓÅô i = 1œÉ((2 1) (+ )) v‚àí ÓÄå b W hi . (20.16) 20.2.2TrainingRestrictedBoltzmannMachines BecausetheRBMadmitseÔ¨ÉcientevaluationanddiÔ¨Äerentiation ofÀú P( v)and eÔ¨ÉcientMCMCsamplingintheformofblockGibbssampling,itcanreadilybe trainedwithanyofthetechniquesdescribedinchapterfortrainingmodels 18 thathaveintractablepartitionfunctions. ThisincludesCD,SML(PCD),ratio matchingandsoon.Comparedtootherundirectedmodelsusedindeeplearning, theRBMisrelativelystraightforwardtotrainbecausewecancompute P(h| v) 6 5 9 CHAPTER20.DEEPGENERATIVEMODELS exactlyinclosedform.Someotherdeepmodels,suchasthedeepBoltzmann machine,combineboththediÔ¨Écultyofanintractablepartitionfunctionandthe diÔ¨Écultyofintractableinference. 20.3DeepBeliefNetworks Deepbeliefnetworks(DBNs)wereoneoftheÔ¨Årstnon-convolutionalmodels tosuccessfullyadmittrainingofdeeparchitectures(Hinton2006Hinton e t a l .,;, 2007b).Theintroduction ofdeepbeliefnetworksin2006beganthecurrentdeep learningrenaissance.Priortotheintroductionofdeepbeliefnetworks,deepmodels wereconsideredtoodiÔ¨Éculttooptimize.Kernelmachineswithconvexobjective functionsdominatedtheresearchlandscape.Deepbeliefnetworksdemonstrated thatdeeparchitecturescanbesuccessful,byoutperformingkernelizedsupport vectormachinesontheMNISTdataset( ,).Today,deepbelief Hinton e t a l .2006 networkshavemostlyfallenoutoffavorandarerarelyused,evencomparedto otherunsupervisedorgenerativelearningalgorithms,buttheyarestilldeservedly recognizedfortheirimportantroleindeeplearninghistory. Deepbeliefnetworksaregenerativemodelswithseverallayersoflatentvariables. Thelatentvariablesaretypicallybinary,whilethevisibleunitsmaybebinary orreal.Therearenointralayerconnections.Usually,everyunitineachlayeris connectedtoeveryunitineachneighboringlayer,thoughitispossibletoconstruct moresparselyconnectedDBNs.Theconnectionsbetweenthetoptwolayersare undirected.Theconnectionsbetweenallotherlayersaredirected,withthearrows pointedtowardthelayerthatisclosesttothedata.SeeÔ¨Ågurebforanexample. 20.1 ADBNwith lhiddenlayerscontains lweightmatrices: W( 1 ), . . . , W( ) l.It alsocontains l+1biasvectors: b( 0 ), . . . , b( ) l,with b( 0 )providingthebiasesforthe visiblelayer.TheprobabilitydistributionrepresentedbytheDBNisgivenby P( h( ) l, h( 1 ) l‚àí) exp‚àùÓÄê b( ) lÓÄæh( ) l+ b( 1 ) l‚àíÓÄæh( 1 ) l‚àí+ h( 1 ) l‚àíÓÄæW( ) lh( ) lÓÄë ,(20.17) P h(( ) k i= 1 | h( + 1 ) k) = œÉÓÄê b( ) k i+ W( + 1 ) k ÓÄæ : , i h( + 1 ) kÓÄë ‚àÄ‚àÄ‚àà ‚àí i , k1 , . . . , l2 ,(20.18) P v( i= 1 | h( 1 )) = œÉÓÄê b( 0 ) i+ W( 1 )ÓÄæ : , i h( 1 )ÓÄë ‚àÄ i .(20.19) Inthecaseofreal-valuedvisibleunits,substitute v‚àºNÓÄê v b;( 0 )+ W( 1 )ÓÄæh( 1 ), Œ≤‚àí 1ÓÄë (20.20) 6 6 0 CHAPTER20.DEEPGENERATIVEMODELS with Œ≤diagonalfortractability.Generalizations tootherexponentialfamilyvisible unitsarestraightforward,atleastintheory.ADBNwithonlyonehiddenlayeris justanRBM. TogenerateasamplefromaDBN,weÔ¨ÅrstrunseveralstepsofGibbssampling onthetoptwohiddenlayers.Thisstageisessentiallydrawingasamplefrom theRBMdeÔ¨Ånedbythetoptwohiddenlayers.Wecanthenuseasinglepassof ancestralsamplingthroughtherestofthemodeltodrawasamplefromthevisible units. Deepbeliefnetworksincurmanyoftheproblemsassociatedwithbothdirected modelsandundirectedmodels. Inferenceinadeepbeliefnetworkisintractableduetotheexplainingaway eÔ¨Äectwithineachdirectedlayer,andduetotheinteractionbetweenthetwohidden layersthathaveundirectedconnections.Evaluatingormaximizingthestandard evidencelowerboundonthelog-likelihoodisalsointractable,becausetheevidence lowerboundtakestheexpectationofcliqueswhosesizeisequaltothenetwork width. Evaluatingormaximizingthelog-likelihoodrequiresnotjustconfrontingthe problemofintractableinferencetomarginalizeoutthelatentvariables,butalso theproblemofanintractablepartitionfunctionwithintheundirectedmodelof thetoptwolayers. Totrainadeepbeliefnetwork,onebeginsbytraininganRBMtomaximize E v‚àº pdatalog p( v)usingcontrastivedivergenceorstochasticmaximumlikelihood. TheparametersoftheRBMthendeÔ¨ÅnetheparametersoftheÔ¨Årstlayerofthe DBN.Next,asecondRBMistrainedtoapproximatelymaximize E v‚àº pdata Eh( 1 )‚àº p( 1 ) ( h( 1 )| v )log p( 2 )( h( 1 )) (20.21) where p( 1 )istheprobabilitydistributionrepresentedbytheÔ¨ÅrstRBMand p( 2 ) istheprobabilitydistributionrepresentedbythesecondRBM.Inotherwords, thesecondRBMistrainedtomodelthedistributiondeÔ¨Ånedbysamplingthe hiddenunitsoftheÔ¨ÅrstRBM,whentheÔ¨ÅrstRBMisdrivenbythedata. This procedurecanberepeatedindeÔ¨Ånitely,toaddasmanylayerstotheDBNas desired,witheachnewRBMmodelingthesamplesofthepreviousone.EachRBM deÔ¨ÅnesanotherlayeroftheDBN.ThisprocedurecanbejustiÔ¨Åedasincreasinga variationallowerboundonthelog-likelihoodofthedataundertheDBN(Hinton e t a l .,).2006 Inmostapplications,noeÔ¨ÄortismadetojointlytraintheDBNafterthegreedy layer-wiseprocedureiscomplete.However,itispossibletoperformgenerative Ô¨Åne-tuningusingthewake-sleepalgorithm. 6 6 1 CHAPTER20.DEEPGENERATIVEMODELS ThetrainedDBNmaybeuseddirectlyasagenerativemodel,butmostofthe interestinDBNsarosefromtheirabilitytoimproveclassiÔ¨Åcationmodels.Wecan taketheweightsfromtheDBNandusethemtodeÔ¨ÅneanMLP: h( 1 )= œÉÓÄê b( 1 )+ vÓÄæW( 1 )ÓÄë . (20.22) h( ) l= œÉÓÄê b( ) l i+ h( 1 ) l‚àíÓÄæW( ) lÓÄë ‚àÄ‚àà l2 , . . . , m ,(20.23) AfterinitializingthisMLPwiththeweightsandbiaseslearnedviagenerative trainingoftheDBN,wemaytraintheMLPtoperformaclassiÔ¨Åcationtask.This additionaltrainingoftheMLPisanexampleofdiscriminativeÔ¨Åne-tuning. ThisspeciÔ¨ÅcchoiceofMLPissomewhatarbitrary,comparedtomanyofthe inferenceequationsinchapterthatarederivedfromÔ¨Årstprinciples.ThisMLP 19 isaheuristicchoicethatseemstoworkwellinpracticeandisusedconsistently intheliterature.Manyapproximate inferencetechniquesaremotivatedbytheir abilitytoÔ¨Åndamaximally variationallowerboundonthelog-likelihood t i g h t undersomesetofconstraints.Onecanconstructavariationallowerboundonthe log-likelihoodusingthehiddenunitexpectationsdeÔ¨ÅnedbytheDBN‚ÄôsMLP,but thisistrueofprobabilitydistributionoverthehiddenunits,andthereisno a ny reasontobelievethatthisMLPprovidesaparticularlytightbound. Inparticular, theMLPignoresmanyimportantinteractionsintheDBNgraphicalmodel.The MLPpropagatesinformationupwardfromthevisibleunitstothedeepesthidden units,butdoesnotpropagateanyinformationdownwardorsideways.TheDBN graphicalmodelhasexplainingawayinteractionsbetweenallofthehiddenunits withinthesamelayeraswellastop-downinteractionsbetweenlayers. Whilethelog-likelihoodofaDBNisintractable,itmaybeapproximatedwith AIS(SalakhutdinovandMurray2008,).Thispermitsevaluatingitsqualityasa generativemodel. Theterm‚Äúdeepbeliefnetwork‚Äùiscommonlyusedincorrectlytorefertoany kindofdeepneuralnetwork,evennetworkswithoutlatentvariablesemantics. Theterm‚Äúdeepbeliefnetwork‚ÄùshouldreferspeciÔ¨Åcallytomodelswithundirected connectionsinthedeepestlayeranddirectedconnectionspointingdownward betweenallotherpairsofconsecutivelayers. Theterm‚Äúdeepbeliefnetwork‚Äùmayalsocausesomeconfusionbecausethe term‚Äúbeliefnetwork‚Äùissometimesusedtorefertopurelydirectedmodels,while deepbeliefnetworkscontainanundirectedlayer.Deepbeliefnetworksalsoshare theacronymDBNwithdynamicBayesiannetworks(DeanandKanazawa1989,), whichareBayesiannetworksforrepresentingMarkovchains. 6 6 2 CHAPTER20.DEEPGENERATIVEMODELS h( 1 ) 1 h( 1 ) 1 h( 1 ) 2 h( 1 ) 2 h(</div>
        </div>
    </div>

    <div class="question-card" id="q98">
        <div class="question-header">
            <span class="question-number">Question 98</span>
            <span class="question-type">multiple-choice</span>
        </div>

        <div class="question-text">Generative models are central to advances in artificial intelligence, but evaluating their performance remains challenging due to limitations in commonly used metrics. Understanding the strengths and weaknesses of these evaluation methods is crucial for assessing model quality and guiding future research.

Which metric, when applied to real-valued datasets like MNIST, can be artificially inflated if generative models assign extremely low variance to constant features, resulting in high scores that do not reflect meaningful learning?

1) Inception Score   
2) Fr√©chet Inception Distance (FID)   
3) Precision/Recall for distributions   
4) Log-likelihood   
5) Structural Similarity Index (SSIM)   
6) Mode Score   
7) Adversarial Accuracy</div>

        <div class="answer-section">
            <div class="answer-label">‚úì Correct Answer:</div>
            <div class="answer-text">The correct answer is 4) Log-likelihood.</div>
        </div>

        <div class="reference-section">
            <div class="reference-label">üìö Reference Text:</div>
            <button class="toggle-button" onclick="toggleReference(98)">
                Show/Hide Reference
            </button>
            <div id="ref98" class="reference-text hidden">quality ofsamples is not areliable guide, we often also evaluatethelog-likelihoodthatthemodelassignstothetestdata,whenthisis computationally feasible.Unfortunately,insomecasesthelikelihoodseemsnot tomeasureanyattributeofthemodelthatwereallycareabout.Forexample, real-valuedmodelsofMNISTcanobtainarbitrarilyhighlikelihoodbyassigning arbitrarilylowvariancetobackgroundpixelsthatneverchange.Modelsand algorithmsthatdetecttheseconstantfeaturescanreapunlimitedrewards,even thoughthisisnotaveryusefulthingtodo.Thepotentialtoachieveacost approaching negativeinÔ¨Ånityis present foranykind of maximum likelihood problemwithrealvalues,butitisespeciallyproblematicforgenerativemodelsof MNISTbecausesomanyoftheoutputvaluesaretrivialtopredict.Thisstrongly suggestsaneedfordevelopingotherwaysofevaluatinggenerativemodels. Theis2015 e t a l .()reviewmanyoftheissuesinvolvedinevaluatinggenerative 7 1 9 CHAPTER20.DEEPGENERATIVEMODELS models,includingmanyoftheideasdescribedabove.Theyhighlightthefact thattherearemanydiÔ¨Äerentusesofgenerativemodelsandthatthechoiceof metricmustmatchtheintendeduseofthemodel.Forexample,somegenerative modelsarebetteratassigninghighprobabilitytomostrealisticpointswhileother generativemodelsarebetteratrarelyassigninghighprobabilitytounrealistic points.ThesediÔ¨Äerencescanresultfromwhetheragenerativemodelisdesigned tominimize D K L( p da t aÓÅ´ p m o de l)or D K L( p m o de lÓÅ´ p da t a),asillustratedinÔ¨Ågure.3.6 Unfortunately,evenwhenwerestricttheuseofeachmetrictothetaskitismost suitedfor,allofthemetricscurrentlyinusecontinuetohaveseriousweaknesses. Oneofthemostimportantresearchtopicsingenerativemodelingisthereforenot justhowtoimprovegenerativemodels,butinfact,designingnewtechniquesto measureourprogress. 20.15Conclusion Traininggenerativemodelswithhiddenunitsisapowerfulwaytomakemodels understandtheworldrepresentedinthegiventrainingdata.Bylearningamodel p m o de l( x)andarepresentation p m o de l( h x|),agenerativemodelcanprovide answerstomanyinferenceproblemsabouttherelationshipsbetweeninputvariables in xandcanprovidemanydiÔ¨Äerentwaysofrepresenting xbytakingexpectations of hatdiÔ¨Äerentlayersofthehierarchy. Generativemodelsholdthepromiseto provideAIsystemswithaframeworkforallofthemanydiÔ¨Äerentintuitiveconcepts theyneedtounderstand,andtheabilitytoreasonabouttheseconceptsinthe faceofuncertainty.WehopethatourreaderswillÔ¨Åndnewwaystomakethese approachesmorepowerfulandcontinuethejourneytounderstandingtheprinciples thatunderlielearningandintelligence. 7 2 0 Bibliograp hy Abadi,M.,Agarwal,A.,Barham,P.,Brevdo,E.,Chen,Z.,Citro,C.,Corrado,G.S.,Davis, A.,Dean,J.,Devin,M.,Ghemawat,S.,Goodfellow,I.,Harp,A.,Irving,G.,Isard,M., Jia,Y.,Jozefowicz,R.,Kaiser,L.,Kudlur,M.,Levenberg,J.,Man√©,D.,Monga,R., Moore,S.,Murray,D.,Olah,C.,Schuster,M.,Shlens,J.,Steiner,B.,Sutskever,I., Talwar,K.,Tucker,P.,Vanhoucke,V.,Vasudevan,V.,Vi√©gas,F.,Vinyals,O.,Warden, P.,Wattenberg,M.,Wicke,M.,Yu,Y.,andZheng,X.(2015).TensorFlow:Large-scale machinelearningonheterogeneoussystems.SoftwareavailablefromtensorÔ¨Çow.org.,25 214446, Ackley,D.H.,Hinton,G.E.,andSejnowski,T.J.(1985).Alearningalgorithmfor Boltzmannmachines.CognitiveScience,,147‚Äì169. , 9 570654 Alain,G.andBengio,Y.(2013). Whatregularizedauto-encoderslearnfromthedata generatingdistribution.In .,,, ICLR‚Äô2013,arXiv:1211.4246507513514521 Alain,G.,Bengio,Y.,Yao,L.,√âricThibodeau-Laufer,Yosinski,J.,andVincent,P.(2015). GSNs:Generativestochasticnetworks.arXiv:1503.05571.,510713 Anderson,E.(1935).TheIrisesoftheGasp√©Peninsula.BulletinoftheAmericanIris Society,,2‚Äì5. 5 921 Ba,J.,Mnih,V.,andKavukcuoglu,K.(2014).Multipleobjectrecognitionwithvisual attention. . arXiv:1412.7755691 Bachman,P.andPrecup,D.(2015).Variationalgenerativestochasticnetworkswith collaborativeshaping.InProceedingsofthe32ndInternationalConferenceonMachine Learning,ICML2015,Lille,France,6-11July2015,pages1964‚Äì1972. 717 Bacon,P.-L.,Bengio,E.,Pineau,J.,andPrecup,D.(2015).Conditionalcomputationin neuralnetworksusingadecision-theoreticapproach.In2ndMultidisciplinaryConference onReinforcementLearningandDecisionMaking(RLDM2015).450 Bagnell,J.A.andBradley,D.M.(2009).DiÔ¨Äerentiablesparsecoding.InD.Koller, D.Schuurmans,Y.Bengio,andL.Bottou,editors,AdvancesinNeuralInformation ProcessingSystems21(NIPS‚Äô08),pages113‚Äì120.498 721 BIBLIOGRAPHY Bahdanau,D.,Cho,K.,andBengio,Y.(2015).Neuralmachinetranslationbyjointly learningtoalignandtranslate.In .,,,,, ICLR‚Äô2015,arXiv:1409.047325101397418420 465475476,, Bahl,L.R.,Brown,P.,deSouza,P.V.,andMercer,R.L.(1987). Speechrecognition withcontinuous-parameterhiddenMarkovmodels.Computer,SpeechandLanguage, 2, 219‚Äì234.458 Baldi,P.andHornik,K.(1989).Neuralnetworksandprincipalcomponentanalysis: Learningfromexampleswithoutlocalminima.NeuralNetworks,,53‚Äì58. 2286 Baldi,P.,Brunak,S.,Frasconi,P.,Soda,G.,andPollastri,G.(1999).Exploitingthe pastandthefutureinproteinsecondarystructureprediction. , Bioinformatics 1 5(11), 937‚Äì946.395 Baldi, P., Sadowski, P., andWhiteson, D.(2014).Searchingforexoticparticlesin high-energyphysicswithdeeplearning.Naturecommunications,. 526 Ballard,D.H.,Hinton,G.E.,andSejnowski,T.J.(1983).Parallelvisioncomputation. Nature.452 Barlow,H.B.(1989).Unsupervisedlearning.NeuralComputation,,295‚Äì311. 1 147 Barron,A.E.(1993).Universalapproximationboundsforsuperpositionsofasigmoidal function.IEEETrans.onInformationTheory,,930‚Äì945. 3 9 199 Bartholomew,D.J.(1987).Latentvariablemodelsandfactoranalysis.OxfordUniversity Press.490 Basilevsky,A.(1994).StatisticalFactorAnalysisandRelatedMethods:Theoryand Applications.Wiley.490 Bastien,F.,Lamblin,P., Pascanu,R.,Bergstra,J., Goodfellow,I.J.,Bergeron,A., Bouchard,N.,andBengio,Y.(2012).Theano:newfeaturesandspeedimprovements. DeepLearningandUnsupervisedFeatureLearningNIPS2012Workshop.,,,2582214 222446, Basu,S.andChristensen,J.(2013). TeachingclassiÔ¨Åcationboundariestohumans. In AAAI‚Äô2013.329 Baxter,J.(1995).Learninginternalrepresentations.InProceedingsofthe8thInternational ConferenceonComputationalLearningTheory(COLT‚Äô95),pages311‚Äì320,SantaCruz, California.ACMPress.245 Bayer,J.andOsendorfer,C.(2014).Learningstochasticrecurrentnetworks.ArXiv e-prints.265 Becker,S.andHinton,G.(1992).Aself-organizingneuralnetworkthatdiscoverssurfaces inrandom-dotstereograms.Nature,,161‚Äì163. 3 5 5 541 7 2 2 BIBLIOGRAPHY Behnke,S.(2001).Learningiterativeimagereconstructionintheneuralabstraction pyramid.Int.J.ComputationalIntelligenceandApplications,(4),427‚Äì438. 1 515 Beiu,V.,Quintana,J.M.,andAvedillo,M.J.(2003).VLSIimplementationsofthreshold logic-acomprehensivesurvey.NeuralNetworks,IEEETransactionson, 1 4(5),1217‚Äì 1243.451 Belkin, M.and Niyogi, P.(2002).Laplacianeigenmapsandspectraltechniquesfor embeddingandclustering. InT.Dietterich,S.Becker,andZ.Ghahramani,editors, AdvancesinNeuralInformationProcessingSystems14(NIPS‚Äô01),Cambridge,MA. MITPress.244 Belkin,M.andNiyogi,P.(2003).Laplacianeigenmapsfordimensionalityreductionand datarepresentation.NeuralComputation,(6),1373‚Äì1396. , 1 5 164518 Bengio,E.,Bacon,P.-L.,Pineau,J.,andPrecup,D.(2015a).Conditionalcomputationin neuralnetworksforfastermodels.arXiv:1511.06297.450 Bengio, S. andBengio, Y. (2000a).Taking onthecurseofdimensionalityinjoint distributionsusingneuralnetworks.IEEETransactionsonNeuralNetworks,special issueonDataMiningandKnowledgeDiscovery,(3),550‚Äì557. 1 1 707 Bengio,S.,Vinyals,O.,Jaitly,N.,andShazeer,N.(2015b).Scheduledsamplingfor sequencepredictionwithrecurrentneuralnetworks.Technicalreport,arXiv:1506.03099. 384 Bengio,Y.(1991).ArtiÔ¨ÅcialNeuralNetworksandtheirApplicationtoSequenceRecognition. Ph.D.thesis,McGillUniversity,(ComputerScience),Montreal,Canada.407 Bengio,Y.(2000).Gradient-basedoptimizationofhyperparameters.NeuralComputation, 1 2(8),1889‚Äì1900. 435 Bengio,Y.(2002).Newdistributedprobabilisticlanguagemodels.TechnicalReport1215, Dept.IRO,Universit√©deMontr√©al.467 Bengio,Y.(2009).LearningdeeparchitecturesforAI.NowPublishers.,201622 Bengio,Y.(2013).Deeplearningofrepresentations:lookingforward.InStatistical LanguageandSpeechProcessing,volume7978ofLectureNotesinComputerScience, pages1‚Äì37.Springer,alsoinarXivathttp://arxiv.org/abs/1305.0445.448 Bengio,Y.(2015).Earlyinferenceinenergy-basedmodelsapproximatesback-propagation. TechnicalReportarXiv:1510.02777,UniversitedeMontreal.656 Bengio,Y.andBengio,S.(2000b).Modelinghigh-dimensionaldiscretedatawithmulti- layerneuralnetworks.In,pages400‚Äì406.MITPress.,,, NIPS12 705707708710 Bengio,Y.andDelalleau,O.(2009).Justifyingandgeneralizingcontrastivedivergence. NeuralComputation,(6),1601‚Äì1621. , 2 1 513611 7 2 3 BIBLIOGRAPHY Bengio,Y.andGrandvalet,Y.(2004).Nounbiasedestimatorofthevarianceofk-fold cross-validation.InS.Thrun,L.Saul,andB.Sch√∂lkopf,editors,AdvancesinNeural InformationProcessingSystems16(NIPS‚Äô03),Cambridge,MA.MITPress,Cambridge. 122 Bengio,Y.andLeCun,Y.(2007).ScalinglearningalgorithmstowardsAI.InLargeScale KernelMachines.19 Bengio,Y.andMonperrus,M.(2005).Non-localmanifoldtangentlearning.InL.Saul, Y.Weiss,andL.Bottou,editors,AdvancesinNeuralInformationProcessingSystems 17(NIPS‚Äô04),pages129‚Äì136.MITPress.,160519 Bengio,Y.andS√©n√©cal,J.-S.(2003).Quicktrainingofprobabilisticneuralnetsby importancesampling.InProceedingsofAISTATS2003.470 Bengio,Y.andS√©n√©cal,J.-S.(2008).Adaptiveimportancesamplingtoacceleratetraining ofaneuralprobabilisticlanguagemodel.IEEETrans.NeuralNetworks, 1 9(4),713‚Äì722. 470 Bengio,Y.,DeMori,R.,Flammia,G.,andKompe,R.(1991).Phoneticallymotivated acousticparametersforcontinuousspeechrecognitionusingartiÔ¨Åcialneuralnetworks. InProceedingsofEuroSpeech‚Äô91.,27459 Bengio,Y.,DeMori,R.,Flammia,G.,andKompe,R.(1992).Neuralnetwork-Gaussian mixturehybridforspeechrecognitionordensityestimation.In,pages175‚Äì182. NIPS4 MorganKaufmann.459 Bengio,Y.,Frasconi,P.,andSimard,P.(1993).Theproblemoflearninglong-term dependenciesinrecurrentnetworks.InIEEEInternationalConferenceonNeural Networks,pages1183‚Äì1195, SanFrancisco.IEEEPress.(invitedpaper).403 Bengio,Y.,Simard,P.,andFrasconi,P.(1994).Learninglong-termdependencieswith gradientdescentisdiÔ¨Écult.IEEETr.NeuralNets.,,, 18401403411 Bengio,Y.,Latendresse,S.,andDugas,C.(1999).Gradient-basedlearningofhyper- parameters.LearningConference,Snowbird.435 Bengio,Y.,Ducharme,R.,andVincent,P.(2001).Aneuralprobabilisticlanguagemodel. InT.K.Leen,T.G.Dietterich,andV.Tresp,editors, ,pages932‚Äì938.MIT NIPS‚Äô2000 Press.,,,,,, 18447464466472477482 Bengio,Y.,Ducharme,R.,Vincent,P.,andJauvin,C.(2003).Aneuralprobabilistic languagemodel.,,1137‚Äì1155. , JMLR 3 466472 Bengio,Y.,LeRoux,N.,Vincent,P.,Delalleau,O.,andMarcotte,P.(2006a).Convex neuralnetworks.In ,pages123‚Äì130. NIPS‚Äô2005 258 Bengio,Y.,Delalleau,O.,andLeRoux,N.(2006b).Thecurseofhighlyvariablefunctions forlocalkernelmachines.In . NIPS‚Äô2005158 7 2 4 BIBLIOGRAPHY Bengio,Y.,Larochelle,H.,andVincent,P.(2006c).Non-localmanifoldParzenwindows. In .MITPress., NIPS‚Äô2005 160520 Bengio,Y.,Lamblin,P.,Popovici,D.,andLarochelle,H.(2007).Greedylayer-wise trainingofdeepnetworks.In .,,,,,, NIPS‚Äô20061419201323324528530 Bengio,Y.,Louradour,J.,Collobert,R.,andWeston,J.(2009).Curriculumlearning.In ICML‚Äô09.328 Bengio,Y.,Mesnil,G.,Dauphin,Y.,andRifai,S.(2013a).Bettermixingviadeep representations.In . ICML‚Äô2013604 Bengio,Y.,L√©onard,N.,andCourville,A.(2013b).Estimatingorpropagatinggradients throughstochasticneuronsforconditionalcomputation. arXiv:1308.3432.,,448450 689691, Bengio,Y.,Yao,L.,Alain,G.,andVincent,P.(2013c).Generalizeddenoisingauto- encodersasgenerativemodels.In .,, NIPS‚Äô2013507711714 Bengio,Y.,Courville,A.,andVincent,P.(2013d).Representationlearning:Areviewand newperspectives.IEEETrans.PatternAnalysisandMachineIntelligence(PAMI), 3 5(8),1798‚Äì1828. 555 Bengio,Y.,Thibodeau-Laufer,E.,Alain,G.,andYosinski,J.(2014). Deepgenerative stochasticnetworkstrainablebybackprop.In .,,,, ICML‚Äô2014711712713714715 Bennett,C.(1976).EÔ¨ÉcientestimationoffreeenergydiÔ¨ÄerencesfromMonteCarlodata. JournalofComputationalPhysics,(2),245‚Äì268. 2 2 628 Bennett,J.andLanning,S.(2007).TheNetÔ¨Çixprize.479 Berger,A.L.,DellaPietra,V.J.,andDellaPietra,S.A.(1996).Amaximumentropy approachtonaturallanguageprocessing. ,,39‚Äì71. ComputationalLinguistics 2 2473 Berglund,M.andRaiko,T.(2013).Stochasticgradientestimatevarianceincontrastive divergenceandpersistentcontrastivedivergence., . CoRR a b s/ 1 3 1 2 .6 0 0 2614 Bergstra, J.(2011).IncorporatingComplexCellsintoNeuralNetworksfor Pattern ClassiÔ¨Åcation.Ph.D.thesis,Universit√©deMontr√©al.255 Bergstra,J.andBengio,Y.(2009).Slow,decorrelatedfeaturesforpretrainingcomplex cell-likenetworks.In . NIPS‚Äô2009494 Bergstra,J.andBengio,Y.(2012).Randomsearchforhyper-parameteroptimization.J. MachineLearningRes.,,281‚Äì305. ,, 1 3 433434435 Bergstra,J.,Breuleux,O.,Bastien,F.,Lamblin,P.,Pascanu,R.,Desjardins,G.,Turian, J.,Warde-Farley,D.,andBengio,Y.(2010).Theano:aCPUandGPUmathexpression compiler.InProc.SciPy.,,,, 2582214222446 7 2 5</div>
        </div>
    </div>

    <div class="question-card" id="q99">
        <div class="question-header">
            <span class="question-number">Question 99</span>
            <span class="question-type">multiple-choice</span>
        </div>

        <div class="question-text">Supervised learning involves training models to make predictions based on labeled datasets, with various algorithms offering different approaches to regularization, optimization, and generalization. Understanding the mathematical relationship between prior distributions, regularization methods, and inference techniques is essential for mastering these algorithms.

Which of the following regularization techniques is mathematically equivalent to Maximum a Posteriori (MAP) estimation with a Gaussian prior on the weights in linear regression?

1) Dropout regularization   
2) L1 regularization (lasso)   
3) Elastic Net regularization   
4) Early stopping   
5) Data augmentation   
6) Batch normalization   
7) L2 regularization (weight decay) </div>

        <div class="answer-section">
            <div class="answer-label">‚úì Correct Answer:</div>
            <div class="answer-text">The correct answer is 7) L2 regularization (weight decay).</div>
        </div>

        <div class="reference-section">
            <div class="reference-label">üìö Reference Text:</div>
            <button class="toggle-button" onclick="toggleReference(99)">
                Show/Hide Reference
            </button>
            <div id="ref99" class="reference-text hidden">a l c o v a ria n c e m a t rix Œõ0= diag( Œª0) . 1 3 8 CHAPTER5.MACHINELEARNINGBASICS singlepointestimate. Onecommonreasonfordesiringapointestimateisthat mostoperationsinvolvingtheBayesianposteriorformostinterestingmodelsare intractable,andapointestimateoÔ¨Äersatractableapproximation.Ratherthan simplyreturningtothemaximumlikelihoodestimate,wecanstillgainsomeof thebeneÔ¨ÅtoftheBayesianapproachbyallowingthepriortoinÔ¨Çuencethechoice ofthepointestimate.Onerationalwaytodothisistochoosethemaximum aposteriori(MAP)pointestimate.TheMAPestimatechoosesthepointof maximalposteriorprobability(ormaximalprobabilitydensityinthemorecommon caseofcontinuous):Œ∏ Œ∏MAP= argmax Œ∏p( ) = argmaxŒ∏x| Œ∏log( )+log() pxŒ∏|pŒ∏.(5.79) Werecognize,aboveontherighthandside,logp(xŒ∏|),i.e.thestandardlog- likelihoodterm,and,correspondingtothepriordistribution. log()pŒ∏ Asanexample,consideralinearregressionmodelwithaGaussianprioron theweightsw.IfthispriorisgivenbyN(w; 0,1 ŒªI2),thenthelog-priortermin equationisproportional tothefamiliar 5.79 ŒªwÓÄæwweightdecaypenalty,plusa termthatdoesnotdependonwanddoesnotaÔ¨Äectthelearningprocess.MAP BayesianinferencewithaGaussianpriorontheweightsthuscorrespondstoweight decay. AswithfullBayesianinference,MAPBayesianinferencehastheadvantageof leveraginginformationthatisbroughtbythepriorandcannotbefoundinthe trainingdata.Thisadditionalinformationhelpstoreducethevarianceinthe MAPpointestimate(incomparisontotheMLestimate).However,itdoessoat thepriceofincreasedbias. Manyregularizedestimationstrategies,suchasmaximumlikelihoodlearning regularizedwithweightdecay,canbeinterpretedasmakingtheMAPapproxima- tiontoBayesianinference.Thisviewapplieswhentheregularizationconsistsof addinganextratermtotheobjectivefunctionthatcorrespondstologp(Œ∏).Not allregularizationpenaltiescorrespondtoMAPBayesianinference.Forexample, someregularizertermsmaynotbethelogarithmofaprobabilitydistribution. Otherregularizationtermsdependonthedata,whichofcourseapriorprobability distributionisnotallowedtodo. MAPBayesianinferenceprovidesastraightforwardwaytodesigncomplicated yetinterpretableregularizationterms.Forexample,amorecomplicatedpenalty termcanbederivedbyusingamixtureofGaussians,ratherthanasingleGaussian distribution,astheprior(NowlanandHinton1992,). 1 3 9 CHAPTER5.MACHINELEARNINGBASICS 5.7SupervisedLearningAlgorithms Recallfromsectionthatsupervisedlearningalgorithmsare,roughlyspeaking, 5.1.3 learningalgorithmsthatlearntoassociatesomeinputwithsomeoutput,givena trainingsetofexamplesofinputsxandoutputsy. Inmanycasestheoutputs ymaybediÔ¨Éculttocollectautomatically andmustbeprovidedbyahuman ‚Äúsupervisor,‚Äùbutthetermstillappliesevenwhenthetrainingsettargetswere collectedautomatically . 5.7.1ProbabilisticSupervisedLearning Most supervised learning algorithms inthis book are based on estimating a probabilitydistributionp(y|x).Wecandothissimplybyusingmaximum likelihoodestimationtoÔ¨ÅndthebestparametervectorŒ∏foraparametricfamily ofdistributions .py(|xŒ∏;) Wehavealreadyseenthatlinearregressioncorrespondstothefamily pyy (| NxŒ∏;) = (;Œ∏ÓÄæxI,). (5.80) WecangeneralizelinearregressiontotheclassiÔ¨ÅcationscenariobydeÔ¨Åninga diÔ¨Äerentfamilyofprobabilitydistributions.Ifwehavetwoclasses,class0and class1,thenweneedonlyspecifytheprobabilityofoneoftheseclasses.The probabilityofclass1determinestheprobabilityofclass0,becausethesetwovalues mustaddupto1. Thenormaldistributionoverreal-valuednumbersthatweusedforlinear regressionisparametrized intermsofamean.Anyvaluewesupplyforthismean isvalid.Adistributionoverabinaryvariableisslightlymorecomplicated,because itsmeanmustalwaysbebetween0and1.Onewaytosolvethisproblemistouse thelogisticsigmoidfunctiontosquashtheoutputofthelinearfunctionintothe interval(0,1)andinterpretthatvalueasaprobability: py œÉ (= 1 ;) = |xŒ∏ (Œ∏ÓÄæx). (5.81) Thisapproachisknownaslogisticregression(asomewhatstrangenamesince weusethemodelforclassiÔ¨Åcationratherthanregression). Inthecaseoflinearregression,wewereabletoÔ¨Åndtheoptimalweightsby solvingthenormalequations.LogisticregressionissomewhatmorediÔ¨Écult.There isnoclosed-formsolutionforitsoptimalweights.Instead,wemustsearchfor thembymaximizingthelog-likelihood.Wecandothisbyminimizingthenegative log-likelihood(NLL)usinggradientdescent. 1 4 0 CHAPTER5.MACHINELEARNINGBASICS Thissamestrategycanbeappliedtoessentiallyanysupervisedlearningproblem, bywritingdownaparametricfamilyofconditionalprobabilitydistributionsover therightkindofinputandoutputvariables. 5.7.2SupportVectorMachines OneofthemostinÔ¨Çuentialapproachestosupervisedlearningisthesupportvector machine(,; Boseretal.1992CortesandVapnik1995,).Thismodelissimilarto logisticregressioninthatitisdrivenbyalinearfunctionwÓÄæx+b.Unlikelogistic regression,thesupportvectormachinedoesnotprovideprobabilities, butonly outputsaclassidentity.TheSVMpredictsthatthepositiveclassispresentwhen wÓÄæx+bispositive.Likewise,itpredictsthatthenegativeclassispresentwhen wÓÄæx+bisnegative. Onekeyinnovationassociatedwithsupportvectormachinesisthekernel trick.Thekerneltrickconsistsofobservingthatmanymachinelearningalgorithms canbewrittenexclusivelyintermsofdotproductsbetweenexamples.Forexample, itcanbeshownthatthelinearfunctionusedbythesupportvectormachinecan bere-writtenas wÓÄæx+= +bbmÓÅò i=1Œ± ixÓÄæx() i(5.82) wherex() iisatrainingexampleandŒ±isavectorofcoeÔ¨Écients.Rewritingthe learningalgorithmthiswayallowsustoreplacexbytheoutputofagivenfeature functionœÜ(x) andthedotproductwithafunctionk(xx,() i) =œÜ(x)¬∑œÜ(x() i) called akernel.The ¬∑operatorrepresentsaninnerproductanalogoustoœÜ(x)ÓÄæœÜ(x() i). Forsomefeaturespaces,wemaynotuseliterallythevectorinnerproduct.In someinÔ¨Ånitedimensionalspaces,weneedtouseotherkindsofinnerproducts,for example,innerproductsbasedonintegrationratherthansummation.Acomplete developmentofthesekindsofinnerproductsisbeyondthescopeofthisbook. Afterreplacingdotproductswithkernelevaluations,wecanmakepredictions usingthefunction fb () = x +ÓÅò iŒ± ik,(xx() i). (5.83) Thisfunctionisnonlinearwithrespecttox,buttherelationshipbetweenœÜ(x) andf(x)islinear.Also,therelationshipbetweenŒ±andf(x)islinear.The kernel-basedfunctionisexactlyequivalenttopreprocessingthedatabyapplying œÜ()xtoallinputs,thenlearningalinearmodelinthenewtransformedspace. Thekerneltrickispowerfulfortworeasons.First,itallowsustolearnmodels thatarenonlinearasafunctionofxusingconvexoptimization techniquesthatare 1 4 1 CHAPTER5.MACHINELEARNINGBASICS guaranteedtoconvergeeÔ¨Éciently.ThisispossiblebecauseweconsiderœÜÔ¨Åxedand optimizeonlyŒ±,i.e.,theoptimization algorithmcanviewthedecisionfunction asbeinglinearinadiÔ¨Äerentspace.Second,thekernelfunctionkoftenadmits animplementationthatissigniÔ¨Åcantlymorecomputational eÔ¨Écientthannaively constructingtwovectorsandexplicitlytakingtheirdotproduct. œÜ()x Insomecases,œÜ(x)canevenbeinÔ¨Ånitedimensional,whichwouldresultin aninÔ¨Ånitecomputational costforthenaive,explicitapproach.Inmanycases, k(xx,ÓÄ∞)isanonlinear,tractablefunctionofxevenwhenœÜ(x)isintractable.As anexampleofaninÔ¨Ånite-dimens ionalfeaturespacewithatractablekernel,we constructafeaturemappingœÜ(x)overthenon-negativeintegersx.Supposethat thismappingreturnsavectorcontainingxonesfollowedbyinÔ¨Ånitelymanyzeros. Wecanwriteakernelfunctionk(x,x() i) =min(x,x() i)thatisexactlyequivalent tothecorrespondinginÔ¨Ånite-dimens ionaldotproduct. ThemostcommonlyusedkernelistheGaussiankernel k, ,œÉ (uvuv ) = (N ‚àí;02I) (5.84) where N(x;¬µ, Œ£)isthestandardnormaldensity.Thiskernelisalsoknownas theradialbasisfunction(RBF)kernel,becauseitsvaluedecreasesalonglines invspaceradiatingoutwardfromu.TheGaussiankernelcorrespondstoadot productinaninÔ¨Ånite-dimens ionalspace,butthederivationofthisspaceisless straightforwardthaninourexampleofthekernelovertheintegers. min WecanthinkoftheGaussiankernelasperformingakindoftemplatematch- ing.Atrainingexamplexassociatedwithtraininglabelybecomesatemplate forclassy.WhenatestpointxÓÄ∞isnearxaccordingtoEuclideandistance,the Gaussiankernelhasalargeresponse,indicatingthatxÓÄ∞isverysimilartothex template.Themodelthenputsalargeweightontheassociatedtraininglabely. Overall,thepredictionwillcombinemanysuchtraininglabelsweightedbythe similarityofthecorrespondingtrainingexamples. Supportvectormachinesarenottheonlyalgorithmthatcanbeenhanced usingthekerneltrick.Manyotherlinearmodelscanbeenhancedinthisway.The categoryofalgorithmsthatemploythekerneltrickisknownaskernelmachines orkernelmethods( ,; WilliamsandRasmussen1996Sch√∂lkopf1999etal.,). Amajordrawbacktokernelmachinesisthatthecostofevaluatingthedecision functionislinearinthenumberoftrainingexamples,becausethei-thexample contributesatermŒ± ik(xx,() i)tothedecisionfunction.Supportvectormachines areabletomitigatethisbylearninganŒ±vectorthatcontainsmostlyzeros. Classifyinganewexamplethenrequiresevaluatingthekernelfunctiononlyfor thetrainingexamplesthathavenon-zeroŒ± i.Thesetrainingexamplesareknown 1 4 2 CHAPTER5.MACHINELEARNINGBASICS assupportvectors. KernelmachinesalsosuÔ¨Äerfromahighcomputational costoftrainingwhen thedatasetislarge.Wewillrevisitthisideainsection.Kernelmachineswith 5.9 generickernelsstruggletogeneralizewell.Wewillexplainwhyinsection.The5.11 modernincarnationofdeeplearningwasdesignedtoovercometheselimitationsof kernelmachines.ThecurrentdeeplearningrenaissancebeganwhenHintonetal. ()demonstratedthataneuralnetworkcouldoutperformtheRBFkernelSVM 2006 ontheMNISTbenchmark. 5.7.3OtherSimpleSupervisedLearningAlgorithms WehavealreadybrieÔ¨Çyencounteredanothernon-probabilis ticsupervisedlearning algorithm,nearestneighborregression.Moregenerally,k-nearestneighborsis afamilyoftechniquesthatcanbeusedforclassiÔ¨Åcationorregression.Asa non-parametric learningalgorithm,k-nearestneighborsisnotrestrictedtoaÔ¨Åxed numberofparameters.Weusuallythinkofthek-nearestneighborsalgorithm asnothavinganyparameters,butratherimplementingasimplefunctionofthe trainingdata.Infact,thereisnotevenreallyatrainingstageorlearningprocess. Instead,attesttime,whenwewanttoproduceanoutputyforanewtestinputx, weÔ¨Åndthek-nearestneighborstoxinthetrainingdataX.Wethenreturnthe averageofthecorrespondingyvaluesinthetrainingset.Thisworksforessentially anykindofsupervisedlearningwherewecandeÔ¨Åneanaverageoveryvalues.In thecaseofclassiÔ¨Åcation,wecanaverageoverone-hotcodevectorscwithc y= 1 andc i= 0forallothervaluesofi.Wecantheninterprettheaverageoverthese one-hotcodesasgivingaprobabilitydistributionoverclasses.Asanon-parametric learningalgorithm,k-nearestneighborcanachieveveryhighcapacity.Forexample, supposewehaveamulticlassclassiÔ¨Åcationtaskandmeasureperformancewith0-1 loss.Inthissetting,-nearestneighborconvergestodoubletheBayeserrorasthe 1 numberoftrainingexamplesapproachesinÔ¨Ånity.TheerrorinexcessoftheBayes errorresultsfromchoosingasingleneighborbybreakingtiesbetweenequally distantneighborsrandomly.WhenthereisinÔ¨Ånitetrainingdata,alltestpointsx willhaveinÔ¨Ånitelymanytrainingsetneighborsatdistancezero.Ifweallowthe algorithmtousealloftheseneighborstovote,ratherthanrandomlychoosingone ofthem,theprocedureconvergestotheBayeserrorrate. Thehighcapacityof k-nearestneighborsallowsittoobtainhighaccuracygivenalargetrainingset. However,itdoessoathighcomputational cost,anditmaygeneralizeverybadly givenasmall,Ô¨Ånitetrainingset.Oneweaknessofk-nearestneighborsisthatit cannotlearnthatonefeatureismorediscriminativethananother.Forexample, imaginewehavearegressiontaskwithx‚àà R100drawnfromanisotropicGaussian 1 4 3 CHAPTER5.MACHINELEARNINGBASICS distribution,butonlyasinglevariablex1isrelevanttotheoutput.Suppose furtherthatthisfeaturesimplyencodestheoutputdirectly,i.e.thaty=x1inall cases.Nearestneighborregressionwillnotbeabletodetectthissimplepattern. Thenearestneighborofmostpointsxwillbedeterminedbythelargenumberof featuresx2throughx100,notbythelonefeaturex1. Thustheoutputonsmall trainingsetswillessentiallyberandom. 1 4 4 CHAPTER5.MACHINELEARNINGBASICS 0 101 1110 1 011 1111 1110110 10010 001110 11111101001 00 010 01111 111 11 Figure5.7:Diagramsdescribinghowadecisiontreeworks. ( T o p )Eachnodeofthetree choosestosendtheinputexampletothechildnodeontheleft(0)ororthechildnodeon theright(1).Internalnodesaredrawnascirclesandleafnodesassquares.Eachnodeis displayedwithabinarystringidentiÔ¨Åercorrespondingtoitspositioninthetree,obtained byappendingabittoitsparentidentiÔ¨Åer(0=chooseleftortop,1=chooserightorbottom). ( Bottom )Thetreedividesspaceintoregions.The2Dplaneshowshowadecisiontree mightdivide R2.Thenodesofthetreeareplottedinthisplane,witheachinternalnode drawnalongthedividinglineitusestocategorizeexamples,andleafnodesdrawninthe centeroftheregionofexamplestheyreceive.Theresultisapiecewise-constantfunction, withonepieceperleaf.EachleafrequiresatleastonetrainingexampletodeÔ¨Åne,soitis notpossibleforthedecisiontreetolearnafunctionthathasmorelocalmaximathanthe numberoftrainingexamples. 1 4 5</div>
        </div>
    </div>

    <div class="question-card" id="q100">
        <div class="question-header">
            <span class="question-number">Question 100</span>
            <span class="question-type">multiple-choice</span>
        </div>

        <div class="question-text">Probabilistic graphical models are powerful tools in machine learning for representing and reasoning about dependencies among random variables. Understanding how different graphical structures encode conditional independence is crucial for effective model selection and inference.

Which scenario best exemplifies a "V-structure" (collider) in a directed graphical model, and describes when two parent variables become dependent?

1) Two variables are conditionally independent if their shared ancestor is observed.   
2) A chain of three variables where observing the intermediate variable blocks the dependency between the first and third.   
3) Two sibling variables that are independent unless both are observed simultaneously.   
4) Two parent variables pointing to a common child, becoming dependent if the child or its descendant is observed.   
5) Three variables forming a complete graph, implying no independences.   
6) Two variables sharing a parent, and are always dependent regardless of observations.   
7) A bipartite graph where every variable node is connected only to factor nodes.</div>

        <div class="answer-section">
            <div class="answer-label">‚úì Correct Answer:</div>
            <div class="answer-text">The correct answer is 4) Two parent variables pointing to a common child, becoming dependent if the child or its descendant is observed..</div>
        </div>

        <div class="reference-section">
            <div class="reference-label">üìö Reference Text:</div>
            <button class="toggle-button" onclick="toggleReference(100)">
                Show/Hide Reference
            </button>
            <div id="ref100" class="reference-text hidden">e g r a p h .Thereisno requirementthatthegraphimplyallindependencesthatarepresent.Inparticular, itisalwayslegitimatetousethecompletegraph(thegraphwithallpossibleedges) torepresentanydistribution.Infact,somedistributionscontainindependences thatarenotpossibletorepresentwithexistinggraphicalnotation.Context- speciÔ¨Åcindependencesareindependencesthatarepresentdependentonthe valueofsomevariablesinthenetwork. Forexample,consideramodelofthree binaryvariables:a,bandc.Supposethatwhenais0,bandcareindependent, butwhenais1,bisdeterministicallyequaltoc. Encodingthebehaviorwhen a= 1requiresanedgeconnectingbandc.Thegraphthenfailstoindicatethatb andcareindependentwhena.= 0 Ingeneral,agraphwillneverimplythatanindependenceexistswhenitdoes not.However,agraphmayfailtoencodeanindependence. 5 7 3 CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING a s b a s b a sb a s ba s b c( a ) ( b ) ( c ) ( d ) Figure16.8:Allofthekindsofactivepathsoflengthtwothatcanexistbetweenrandom variablesaandb.Anypathwitharrowsproceedingdirectlyfrom ( a ) atoborviceversa. Thiskindofpathbecomesblockedifsisobserved. Wehavealreadyseenthiskindof pathintherelayraceexample. ( b )aandbareconnectedbya c o m m o n c a u s es.For example,supposesisavariableindicatingwhetherornotthereisahurricaneandaand bmeasurethewindspeedattwodiÔ¨Äerentnearbyweathermonitoringoutposts.Ifwe observeveryhighwindsatstationa,wemightexpecttoalsoseehighwindsatb.This kindofpathcanbeblockedbyobservings.Ifwealreadyknowthereisahurricane,we expecttoseehighwindsatb,regardlessofwhatisobservedata.Alowerthanexpected windata(forahurricane)wouldnotchangeourexpectationofwindsatb(knowing thereisahurricane).However,ifsisnotobserved,thenaandbaredependent,i.e.,the pathisactive. ( c )aandbarebothparentsofs.ThisiscalledaV-structureorthe collidercase. TheV-structurecausesaandbtoberelatedbytheexplainingaway eÔ¨Äect.Inthiscase,thepathisactuallyactivewhensisobserved.Forexample,suppose sisavariableindicatingthatyourcolleagueisnotatwork. Thevariablearepresents herbeingsick,whilebrepresentsherbeingonvacation. Ifyouobservethatsheisnot atwork,youcanpresumesheisprobablysickoronvacation,butitisnotespecially likelythatbothhavehappenedatthesametime.IfyouÔ¨Åndoutthatsheisonvacation, thisfactissuÔ¨Écienttoherabsence.Youcaninferthatsheisprobablynotalso e x p l a i n sick.TheexplainingawayeÔ¨Äecthappensevenifanydescendantof ( d ) sisobserved!For example,supposethatcisavariablerepresentingwhetheryouhavereceivedareport fromyourcolleague.Ifyounoticethatyouhavenotreceivedthereport,thisincreases yourestimateoftheprobabilitythatsheisnotatworktoday,whichinturnmakesit morelikelythatsheiseithersickoronvacation.Theonlywaytoblockapaththrougha V-structureistoobservenoneofthedescendantsofthesharedchild. 5 7 4 CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING a b c d e Figure16.9:Fromthisgraph,wecanreadoutseverald-separationproperties.Examples include: ‚Ä¢aandbared-separatedgiventheemptyset. ‚Ä¢aandeared-separatedgivenc. ‚Ä¢dandeared-separatedgivenc. Wecanalsoseethatsomevariablesarenolongerd-separatedwhenweobservesome variables: ‚Ä¢aandbarenotd-separatedgivenc. ‚Ä¢aandbarenotd-separatedgivend. 5 7 5 CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING 1 6 . 2 . 6 Co n vert i n g b et ween Un d i rec t ed a n d D i rect ed G ra p h s WeoftenrefertoaspeciÔ¨Åcmachinelearningmodelasbeingundirectedordirected. Forexample,wetypicallyrefertoRBMsasundirectedandsparsecodingasdirected. Thischoiceofwordingcanbesomewhatmisleading,becausenoprobabilisticmodel isinherentlydirectedorundirected.Instead,somemodelsaremosteasily d e s c r i b e d usingadirectedgraph,ormosteasilydescribedusinganundirectedgraph. Directedmodelsandundirectedmodelsbothhavetheiradvantagesanddisad- vantages.Neitherapproachisclearlysuperioranduniversallypreferred.Instead, weshouldchoosewhichlanguagetouseforeachtask.Thischoicewillpartially dependonwhichprobabilitydistributionwewishtodescribe.Wemaychooseto useeitherdirectedmodelingorundirectedmodelingbasedonwhichapproachcan capturethemostindependencesintheprobabilitydistributionorwhichapproach usesthefewestedgestodescribethedistribution.Thereareotherfactorsthat canaÔ¨Äectthedecisionofwhichlanguagetouse.Evenwhileworkingwithasingle probabilitydistribution,wemaysometimesswitchbetweendiÔ¨Äerentmodeling languages.SometimesadiÔ¨Äerentlanguagebecomesmoreappropriateifweobserve acertainsubsetofvariables,orifwewishtoperformadiÔ¨Äerentcomputational task.Forexample,thedirectedmodeldescriptionoftenprovidesastraightforward approachtoeÔ¨Écientlydrawsamplesfromthemodel(describedinsection)16.3 whiletheundirectedmodelformulationisoftenusefulforderivingapproximate inferenceprocedures(aswewillseeinchapter,wheretheroleofundirected 19 modelsishighlightedinequation).19.56 Everyprobabilitydistributioncanberepresentedbyeitheradirectedmodel orbyanundirectedmodel.Intheworstcase,onecanalwaysrepresentany distributionbyusinga‚Äúcompletegraph.‚ÄùInthecaseofadirectedmodel,the completegraphisanydirectedacyclicgraphwhereweimposesomeorderingon therandomvariables,andeachvariablehasallothervariablesthatprecedeitin theorderingasitsancestorsinthegraph.Foranundirectedmodel,thecomplete graphissimplyagraphcontainingasinglecliqueencompassingallofthevariables. SeeÔ¨Ågureforanexample. 16.10 Ofcourse,theutilityofagraphicalmodelisthatthegraphimpliesthatsome variablesdonotinteractdirectly.Thecompletegraphisnotveryusefulbecauseit doesnotimplyanyindependences. Whenwerepresentaprobabilitydistributionwithagraph,wewanttochoose agraphthatimpliesasmanyindependencesaspossible,withoutimplyingany independencesthatdonotactuallyexist. Fromthispointofview,somedistributionscanberepresentedmoreeÔ¨Éciently 5 7 6 CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING Figure16.10:Examplesofcompletegraphs,whichcandescribeanyprobabilitydistribution. Hereweshowexampleswithfourrandomvariables. ( L e f t )Thecompleteundirectedgraph. Intheundirectedcase,thecompletegraphisunique.Acompletedirectedgraph. ( R i g h t ) Inthedirectedcase,thereisnotauniquecompletegraph.Wechooseanorderingofthe variablesanddrawanarcfromeachvariabletoeveryvariablethatcomesafteritinthe ordering.Therearethusafactorialnumberofcompletegraphsforeverysetofrandom variables.Inthisexampleweorderthevariablesfromlefttoright,toptobottom. usingdirectedmodels,whileotherdistributionscanberepresentedmoreeÔ¨Éciently using undirectedmodels.In other words,directed models canencode some independencesthatundirectedmodelscannotencode,andviceversa. DirectedmodelsareabletouseonespeciÔ¨Åckindofsubstructurethatundirected modelscannotrepresentperfectly.Thissubstructureiscalledanimmorality. Thestructureoccurswhentworandomvariablesaandbarebothparentsofa thirdrandomvariablec,andthereisnoedgedirectlyconnectingaandbineither direction.(Thename‚Äúimmorality‚Äùmayseemstrange;itwascoinedinthegraphical modelsliteratureasajokeaboutunmarriedparents.)Toconvertadirectedmodel withgraph Dintoanundirectedmodel,weneedtocreateanewgraph U. For everypairofvariablesxandy,weaddanundirectededgeconnectingxandyto Uifthereisadirectededge(ineitherdirection)connectingxandyinDorifx andyarebothparentsinDofathirdvariablez.Theresulting Uisknownasa moralizedgraph.SeeÔ¨Ågureforexamplesofconvertingdirectedmodelsto 16.11 undirectedmodelsviamoralization. Likewise,undirectedmodelscanincludesubstructuresthatnodirectedmodel canrepresentperfectly.SpeciÔ¨Åcally,adirectedgraphcannotcaptureallofthe D conditionalindependencesimpliedbyanundirectedgraph UifUcontainsaloop oflengthgreaterthanthree,unlessthatloopalsocontainsachord.Aloopis asequenceofvariablesconnectedbyundirectededges,withthelastvariablein thesequenceconnectedbacktotheÔ¨Årstvariableinthesequence. Achordisa connectionbetweenanytwonon-consecutivevariablesinthesequencedeÔ¨Åninga loop.IfUhasloopsoflengthfourorgreateranddoesnothavechordsforthese loops,wemustaddthechordsbeforewecanconvertittoadirectedmodel.Adding 5 7 7 CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING h 1 h 1 h 2 h 2 h 3 h 3 v 1 v 1 v 2 v 2 v 3 v 3a b ca cb h 1 h 1 h 2 h 2 h 3 h 3 v 1 v 1 v 2 v 2 v 3 v 3a b ca cb Figure16.11: Exam plesofconvertingdirectedmodels(toprow)toundirectedmodels (bottomrow)byconstructingmoralizedgraphs. ( L e f t )Thissimplechaincanbeconverted toamoralizedgraphmerelybyreplacingitsdirectededgeswithundirectededges.The resultingundirectedmodelimpliesexactlythesamesetofindependencesandconditional independences.Thisgraphisthesimplestdirectedmodelthatcannotbeconverted ( C e n t e r ) toanundirectedmodelwithoutlosingsomeindependences.Thisgraphconsistsentirely ofasingleimmorality.Becauseaandbareparentsofc,theyareconnectedbyanactive pathwhencisobserved.Tocapturethisdependence,theundirectedmodelmustinclude acliqueencompassingallthreevariables.Thiscliquefailstoencodethefactthatab‚ä•. ( R i g h t )Ingeneral,moralizationmayaddmanyedgestothegraph,thuslosingmany impliedindependences.Forexample,thissparsecodinggraphrequiresaddingmoralizing edgesbetweeneverypairofhiddenunits,thusintroducingaquadraticnumberofnew directdependences. 5 7 8 CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING a b d ca b d ca b d c Figure16.12:Convertinganundirectedmodeltoadirectedmodel. ( L e f t )Thisundirected modelcannotbeconverteddirectedtoadirectedmodelbecauseithasaloopoflengthfour withnochords.SpeciÔ¨Åcally,theundirectedmodelencodestwodiÔ¨Äerentindependencesthat nodirectedmodelcancapturesimultaneously:acbd ‚ä•|{ ,}andbdac ‚ä•|{ ,}.To ( C e n t e r ) converttheundirectedmodeltoadirectedmodel,wemusttriangulatethegraph,by ensuringthatallloopsofgreaterthanlengththreehaveachord.Todoso,wecaneither addanedgeconnectingaandcorwecanaddanedgeconnectingbandd.Inthis example,wechoosetoaddtheedgeconnectingaandc.ToÔ¨Ånishtheconversion ( R i g h t ) process,wemustassignadirectiontoeachedge.Whendoingso,wemustnotcreateany directedcycles.Onewaytoavoiddirectedcyclesistoimposeanorderingoverthenodes, andalwayspointeachedgefromthenodethatcomesearlierintheorderingtothenode thatcomeslaterintheordering.Inthisexample,weusethevariablenamestoimpose alphabeticalorder. thesechordsdiscardssomeoftheindependenceinformationthatwasencodedinU. ThegraphformedbyaddingchordstoUisknownasachordalortriangulated graph,becausealltheloopscannowbedescribedintermsofsmaller,triangular loops.Tobuildadirectedgraph Dfromthechordalgraph,weneedtoalsoassign directionstotheedges.Whendoingso,wemustnotcreateadirectedcyclein D,ortheresultdoesnotdeÔ¨Åneavaliddirectedprobabilisticmodel.Oneway toassigndirectionstotheedgesinDistoimposeanorderingontherandom variables,thenpointeachedgefromthenodethatcomesearlierintheorderingto thenodethatcomeslaterintheordering.SeeÔ¨Ågureforademonstration. 16.12 1 6 . 2 . 7 F a ct o r G ra p h s Factorgraphsareanotherwayofdrawingundirectedmodelsthatresolvean ambiguityinthegraphicalrepresentationofstandardundirectedmodelsyntax.In anundirectedmodel,thescopeofevery œÜfunctionmustbeaofsomeclique s u b s e t inthegraph.Ambiguityarisesbecauseitisnotclearifeachcliqueactuallyhas acorrespondingfactorwhosescopeencompassestheentireclique‚Äîforexample, acliquecontainingthreenodesmaycorrespondtoafactoroverallthreenodes, ormaycorrespondtothreefactorsthateachcontainonlyapairofthenodes. 5 7 9 CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING Factorgraphsresolvethisambiguitybyexplicitlyrepresentingthescopeofeach œÜ function.SpeciÔ¨Åcally,afactorgraphisagraphicalrepresentationofanundirected modelthatconsistsofabipartiteundirectedgraph.Someofthenodesaredrawn ascircles.Thesenodescorrespondtorandomvariablesasinastandardundirected model. Therestofthenodesaredrawnassquares. Thesenodescorrespondto thefactors œÜoftheunnormalized probabilitydistribution.Variablesandfactors maybeconnectedwithundirectededges.Avariableandafactorareconnected inthegraphifandonlyifthevariableisoneoftheargumentstothefactorin theunnormalized probabilitydistribution.Nofactormaybeconnectedtoanother factorinthegraph,norcanavariablebeconnectedtoavariable.SeeÔ¨Ågure16.13 foranexampleofhowfactorgraphscanresolveambiguityintheinterpretation of undirectednetworks. a b ca b cf 1 f 1a b cf 1 f 1f 2</div>
        </div>
    </div>

    <div class="question-card" id="q101">
        <div class="question-header">
            <span class="question-number">Question 101</span>
            <span class="question-type">multiple-choice</span>
        </div>

        <div class="question-text">Representation learning and transfer learning are foundational techniques in modern machine learning, enabling models to leverage knowledge across tasks and domains. Unsupervised pretraining historically played a crucial role in improving generalization, particularly when labeled data is scarce.

Which of the following statements most accurately explains why unsupervised pretraining is particularly helpful for deep neural networks when labeled data is limited?

1) It reduces the number of layers required for the network to function effectively.   
2) It eliminates the need for any supervised training phase in the learning process.   
3) It ensures that learned features are always interpretable and human-readable.   
4) It simplifies hyperparameter tuning by merging unsupervised and supervised phases.   
5) It primarily acts as a regularizer by penalizing complex functions during training.   
6) It prevents concept drift by dynamically adjusting the model to new distributions without retraining.   
7) It guides networks to discover useful feature representations, initializing parameters in advantageous regions of function space and reducing overfitting when labeled data is scarce. </div>

        <div class="answer-section">
            <div class="answer-label">‚úì Correct Answer:</div>
            <div class="answer-text">The correct answer is 7) It guides networks to discover useful feature representations, initializing parameters in advantageous regions of function space and reducing overfitting when labeled data is scarce..</div>
        </div>

        <div class="reference-section">
            <div class="reference-label">üìö Reference Text:</div>
            <button class="toggle-button" onclick="toggleReference(101)">
                Show/Hide Reference
            </button>
            <div id="ref101" class="reference-text hidden">advantageofsemi-supervisedlearningviaunsupervisedpretrainingwithmany unlabeledexamplesandfewlabeledexampleswasmadeparticularlyclearin 2011withunsupervisedpretrainingwinningtwointernationaltransferlearning competitions( ,; ,),insettingswherethe Mesniletal.2011Goodfellowetal.2011 numberoflabeledexamplesinthetargettaskwassmall(fromahandfultodozens ofexamplesperclass).TheseeÔ¨Äectswerealsodocumentedincarefullycontrolled experimentsbyPaine2014etal.(). Otherfactorsarelikelytobeinvolved.Forexample,unsupervisedpretraining islikelytobemostusefulwhenthefunctiontobelearnedisextremelycomplicated. UnsupervisedlearningdiÔ¨Äersfromregularizerslikeweightdecaybecauseitdoesnot biasthelearnertowarddiscoveringasimplefunctionbutrathertowarddiscovering featurefunctionsthatareusefulfortheunsupervisedlearningtask. Ifthetrue underlyingfunctionsarecomplicatedandshapedbyregularitiesoftheinput distribution,unsupervisedlearningcanbeamoreappropriateregularizer. Thesecaveatsaside,wenowanalyzesomesuccesscaseswhereunsupervised pretrainingisknowntocauseanimprovement,andexplainwhatisknownabout whythisimprovementoccurs.Unsupervisedpretraininghasusuallybeenused toimproveclassiÔ¨Åers,andisusuallymostinterestingfromthepointofviewof 5 3 2 CHAPTER15.REPRESENTATIONLEARNING  ÓÄ¥ ÓÄ∞ ÓÄ∞ ÓÄ∞  ÓÄ≥ ÓÄ∞ ÓÄ∞ ÓÄ∞  ÓÄ≤ ÓÄ∞ ÓÄ∞ ÓÄ∞  ÓÄ± ÓÄ∞ ÓÄ∞ ÓÄ∞ ÓÄ∞ ÓÄ± ÓÄ∞ ÓÄ∞ ÓÄ∞ ÓÄ≤ ÓÄ∞ ÓÄ∞ ÓÄ∞ ÓÄ≥ ÓÄ∞ ÓÄ∞ ÓÄ∞ ÓÄ¥ ÓÄ∞ ÓÄ∞ ÓÄ∞ ÓÄ± ÓÄµ ÓÄ∞ ÓÄ∞ ÓÄ± ÓÄ∞ ÓÄ∞ ÓÄ∞ ÓÄµ ÓÄ∞ ÓÄ∞ÓÄ∞ÓÄµ ÓÄ∞ ÓÄ∞ÓÄ± ÓÄ∞ ÓÄ∞ ÓÄ∞ÓÄ± ÓÄµ ÓÄ∞ ÓÄ∞ ÓÅó ÓÅ© ÓÅ¥ ÓÅ® ÓÄ† ÓÅ∞ ÓÅ≤ ÓÅ• ÓÅ¥ ÓÅ≤ ÓÅ° ÓÅ© ÓÅÆ ÓÅ© ÓÅÆ ÓÅß ÓÅó ÓÅ© ÓÅ¥ ÓÅ® ÓÅØ ÓÅµ ÓÅ¥ ÓÄ† ÓÅ∞ ÓÅ≤ ÓÅ• ÓÅ¥ ÓÅ≤ ÓÅ° ÓÅ© ÓÅÆ ÓÅ© ÓÅÆ ÓÅß Figure15.1:VisualizationvianonlinearprojectionofthelearningtrajectoriesofdiÔ¨Äerent neuralnetworksin f u n c t i o n s p a c e(notparameterspace,toavoidtheissueofmany-to-one mappingsfromparametervectorstofunctions),withdiÔ¨Äerentrandominitializations andwithorwithoutunsupervisedpretraining.EachpointcorrespondstoadiÔ¨Äerent neuralnetwork,ataparticulartimeduringitstrainingprocess.ThisÔ¨Ågureisadapted withpermissionfrom ().AcoordinateinfunctionspaceisaninÔ¨Ånite- Erhan e t a l .2010 dimensionalvectorassociatingeveryinputxwithanoutputy. ()made Erhan e t a l .2010 alinearprojectiontohigh-dimensionalspacebyconcatenatingtheyformanyspeciÔ¨Åcx points.Theythenmadeafurthernonlinearprojectionto2-DbyIsomap(Tenenbaum e t a l .,).Colorindicatestime.Allnetworksareinitializednearthecenteroftheplot 2000 (correspondingtotheregionoffunctionsthatproduceapproximatelyuniformdistributions overtheclassyformostinputs).Overtime,learningmovesthefunctionoutward,to pointsthatmakestrongpredictions.Trainingconsistentlyterminatesinoneregionwhen usingpretrainingandinanother,non-overlappingregionwhennotusingpretraining. Isomaptriestopreserveglobalrelativedistances(andhencevolumes)sothesmallregion correspondingtopretrainedmodelsmayindicatethatthepretraining-basedestimator hasreducedvariance. 5 3 3 CHAPTER15.REPRESENTATIONLEARNING reducingtestseterror.However,unsupervisedpretrainingcanhelptasksother thanclassiÔ¨Åcation,andcanacttoimproveoptimization ratherthanbeingmerely aregularizer.Forexample,itcanimprovebothtrainandtestreconstructionerror fordeepautoencoders(HintonandSalakhutdinov2006,). Erhan2010etal.()performedmanyexperimentstoexplainseveralsuccessesof unsupervisedpretraining.Bothimprovementstotrainingerrorandimprovements totesterrormaybeexplainedintermsofunsupervisedpretrainingtakingthe parametersintoaregionthatwouldotherwisebeinaccessible.Neuralnetwork trainingisnon-determinis tic,andconvergestoadiÔ¨Äerentfunctioneverytimeit isrun. Trainingmayhaltatapointwherethegradientbecomessmall,apoint whereearlystoppingendstrainingtopreventoverÔ¨Åtting,oratapointwherethe gradientislargebutitisdiÔ¨ÉculttoÔ¨Åndadownhillstepduetoproblemssuchas stochasticityorpoorconditioningoftheHessian. Neuralnetworksthatreceive unsupervisedpretrainingconsistentlyhaltinthesameregionoffunctionspace, whileneuralnetworkswithoutpretrainingconsistentlyhaltinanotherregion.See Ô¨Ågureforavisualizationofthisphenomenon. Theregionwherepretrained 15.1 networksarriveissmaller,suggestingthatpretrainingreducesthevarianceofthe estimationprocess,whichcaninturnreducetheriskofsevereover-Ô¨Åtting.In otherwords,unsupervisedpretraininginitializesneuralnetworkparametersinto aregionthattheydonotescape,andtheresultsfollowingthisinitialization are moreconsistentandlesslikelytobeverybadthanwithoutthisinitialization. Erhan2010etal.()alsoprovidesomeanswersastopretrainingworks when best‚Äîthemeanandvarianceofthetesterrorweremostreducedbypretrainingfor deepernetworks.Keepinmindthattheseexperimentswereperformedbeforethe inventionandpopularization ofmoderntechniquesfortrainingverydeepnetworks (rectiÔ¨Åedlinearunits,dropoutandbatchnormalization) solessisknownaboutthe eÔ¨Äectofunsupervisedpretraininginconjunctionwithcontemporaryapproaches. Animportantquestionishowunsupervisedpretrainingcanactasaregularizer. Onehypothesisisthatpretrainingencouragesthelearningalgorithmtodiscover featuresthatrelatetotheunderlyingcausesthatgeneratetheobserveddata. Thisisanimportantideamotivatingmanyotheralgorithmsbesidesunsupervised pretraining,andisdescribedfurtherinsection.15.3 Comparedtootherformsofunsupervisedlearning,unsupervisedpretraining hasthedisadvantagethatitoperateswithtwoseparatetrainingphases. Many regularizationstrategieshavetheadvantageofallowingtheusertocontrolthe strengthoftheregularizationbyadjustingthevalueofasinglehyperparameter. UnsupervisedpretrainingdoesnotoÔ¨Äeraclearwaytoadjustthethestrength oftheregularization arisi ngfromtheunsupervised stage.Instead, thereare 5 3 4 CHAPTER15.REPRESENTATIONLEARNING verymanyhyperparameters ,whoseeÔ¨Äectmaybemeasuredafterthefactbut isoftendiÔ¨Éculttopredictaheadoftime.Whenweperformunsupervisedand supervisedlearningsimultaneously,insteadofusingthepretrainingstrategy,there isasinglehyperparameter,usuallyacoeÔ¨Écientattachedtotheunsupervised cost, thatdetermineshowstronglytheunsupervisedobjectivewillregularize thesupervisedmodel.Onecanalwayspredictablyobtainlessregularizationby decreasingthiscoeÔ¨Écient.Inthecaseofunsupervisedpretraining,thereisnota wayofÔ¨Çexiblyadaptingthestrengthoftheregularization‚Äîeither thesupervised modelisinitializedtopretrainedparameters,oritisnot. Anotherdisadvantageofhavingtwoseparatetrainingphasesisthateachphase hasitsownhyperparameters.Theperformanceofthesecondphaseusuallycannot bepredictedduringtheÔ¨Årstphase,sothereisalongdelaybetweenproposing hyperparametersfortheÔ¨Årstphaseandbeingabletoupdatethemusingfeedback fromthesecondphase.Themostprincipledapproachistousevalidationseterror inthesupervisedphaseinordertoselectthehyperparameters ofthepretraining phase,asdiscussedin ().Inpractice,somehyperparameters, Larochelleetal.2009 likethenumberofpretrainingiterations,aremoreconvenientlysetduringthe pretrainingphase,usingearlystoppingontheunsupervisedobjective,whichis notidealbutcomputationally muchcheaperthanusingthesupervisedobjective. Today,unsupervisedpretraininghasbeenlargelyabandoned,exceptinthe Ô¨Åeldofnaturallanguageprocessing,wherethenaturalrepresentationofwordsas one-hotvectorsconveysnosimilarityinformationandwhereverylargeunlabeled setsareavailable.Inthatcase,theadvantageofpretrainingisthatonecanpretrain onceonahugeunlabeledset(forexamplewithacorpuscontainingbillionsof words),learnagoodrepresentation(typicallyofwords,butalsoofsentences),and thenusethisrepresentationorÔ¨Åne-tuneitforasupervisedtaskforwhichthe trainingsetcontainssubstantiallyfewerexamples.Thisapproachwaspioneered bybyCollobertandWeston2008bTurian2010Collobert (), etal.(),and etal. ()andremainsincommonusetoday. 2011a Deeplearningtechniquesbasedonsupervisedlearning,regularizedwithdropout orbatchnormalization, areabletoachievehuman-levelperformanceonverymany tasks,butonlywithextremelylargelabeleddatasets.Thesesametechniquesout- performunsupervisedpretrainingonmedium-sizeddatasetssuchasCIFAR-10and MNIST,whichhaveroughly5,000labeledexamplesperclass.Onextremelysmall datasets,suchasthealternativesplicingdataset,Bayesianmethodsoutperform methodsbasedonunsupervisedpretraining(Srivastava2013,).Forthesereasons, thepopularityofunsupervisedpretraininghasdeclined.Nevertheless,unsupervised pretrainingremainsanimportantmilestoneinthehistoryofdeeplearningresearch 5 3 5 CHAPTER15.REPRESENTATIONLEARNING andcontinuestoinÔ¨Çuencecontemporaryapproaches.Theideaofpretraininghas beengeneralizedto sup e r v i se d pr e t r ai ni n gdiscussedinsection,asavery 8.7.4 commonapproachfortransferlearning.Supervisedpretrainingfortransferlearning ispopular( ,; Oquabetal.2014Yosinski2014etal.,)forusewithconvolutional networkspretrainedontheImageNetdataset.Practitionerspublishtheparameters ofthesetrainednetworksforthispurpose,justlikepretrainedwordvectorsare publishedfornaturallanguagetasks( ,; Collobertetal.2011aMikolov2013aetal.,). 15. 2 T ransfer L earni n g an d D om ai n A d ap t at i o n Transferlearninganddomainadaptationrefertothesituationwherewhathasbeen learnedinonesetting(i.e.,distributionP 1)isexploitedtoimprovegeneralization inanothersetting(saydistributionP 2).Thisgeneralizestheideapresentedinthe previoussection,wherewetransferredrepresentationsbetweenanunsupervised learningtaskandasupervisedlearningtask. In t r ansf e r l e ar ni ng,thelearnermustperformtwoormorediÔ¨Äerenttasks, butweassumethatmanyofthefactorsthatexplainthevariationsinP 1are relevanttothevariationsthatneedtobecapturedforlearningP 2.Thisistypically understoodinasupervisedlearningcontext,wheretheinputisthesamebutthe targetmaybeofadiÔ¨Äerentnature.Forexample,wemaylearnaboutonesetof visualcategories,suchascatsanddogs,intheÔ¨Årstsetting,thenlearnabouta diÔ¨Äerentsetofvisualcategories,suchasantsandwasps,inthesecondsetting.If thereissigniÔ¨ÅcantlymoredataintheÔ¨Årstsetting(sampledfromP 1),thenthat mayhelptolearnrepresentationsthatareusefultoquicklygeneralizefromonly veryfewexamplesdrawnfromP 2.Manyvisualcategories sharelow-levelnotions ofedgesandvisualshapes,theeÔ¨Äectsofgeometricchanges,changesinlighting, etc. Ingeneral,transferlearning,multi-tasklearning(section),anddomain7.7 adaptationcanbeachievedviarepresentationlearningwhenthereexistfeatures thatareusefulforthediÔ¨Äerentsettingsortasks,correspondingtounderlying factorsthatappearinmorethanonesetting.ThisisillustratedinÔ¨Ågure,with7.2 sharedlowerlayersandtask-dependentupperlayers. However, sometimes, whatisshared amongthe diÔ¨Äerent tasksisnotthe semanticsoftheinputbutthesemanticsoftheoutput.Forexample,aspeech recognitionsystemneedstoproducevalidsentencesattheoutputlayer,but theearlierlayersneartheinputmayneedtorecognizeverydiÔ¨Äerentversionsof thesamephonemesorsub-phonemicvocalizationsdependingonwhichperson isspeaking.Incaseslikethese,itmakesmoresensetosharetheupperlayers (neartheoutput)oftheneuralnetwork,andhaveatask-speciÔ¨Åcpreprocessing,as 5 3 6 CHAPTER15.REPRESENTATIONLEARNING illustratedinÔ¨Ågure.15.2 Se l e c t i on sw i t c h h(1)h(1)h(2)h(2)h(3)h(3)yy h(shared)h(shared) x(1)x(1)x( 2 )x( 2 )x( 3 )x( 3 ) Figure15.2: Example architectureformulti-taskortransferlearningwhentheoutput variablehasthesamesemanticsforalltaskswhiletheinputvariablehasadiÔ¨Äerent y x meaning(andpossiblyevenadiÔ¨Äerentdimension)foreachtask(or,forexample,each user),called x( 1 ),x( 2 )andx( 3 )forthreetasks.Thelowerlevels(uptotheselection switch)aretask-speciÔ¨Åc,whiletheupperlevelsareshared.Thelowerlevelslearnto translatetheirtask-speciÔ¨Åcinputintoagenericsetoffeatures. Intherelatedcaseof domain adapt at i o n,thetask(andtheoptimalinput-to- outputmapping)remainsthesamebetweeneachsetting,buttheinputdistribution isslightlydiÔ¨Äerent.Forexample,considerthetaskofsentimentanalysis,which consistsofdeterminingwhetheracommentexpressespositiveornegativesentiment. Commentspostedonthewebcomefrommanycategories.Adomainadaptation scenariocanarisewhenasentimentpredictortrainedoncustomerreviewsof mediacontentsuchasbooks,videosandmusicislaterusedtoanalyzecomments aboutconsumerelectronicssuchastelevisionsorsmartphones.Onecanimagine thatthereisanunderlyingfunctionthattellswhetheranystatementispositive, neutralornegative,butofcoursethevocabularyandstylemayvaryfromone domaintoanother,makingitmorediÔ¨Éculttogeneralizeacrossdomains.Simple unsupervisedpretraining(withdenoisingautoencoders)hasbeenfoundtobevery successfulforsentimentanalysiswithdomainadaptation( ,). Glorotetal.2011b Arelatedproblemisthatof c o nc e pt dr i f t,whichwecanviewasaform oftransferlearningduetogradualchangesinthedatadistributionovertime. Bothconceptdriftandtransferlearningcanbeviewedasparticularformsof 5 3 7 CHAPTER15.REPRESENTATIONLEARNING multi-tasklearning.Whilethephrase‚Äúmulti-tasklearning‚Äù typicallyrefersto supervisedlearningtasks,themoregeneralnotionoftransferlearningisapplicable tounsupervisedlearningandreinforcementlearningaswell. Inallofthesecases,theobjectiveistotakeadvantageofdatafromtheÔ¨Årst settingtoextractinformationthatmaybeusefulwhenlearningorevenwhen directlymakingpredictionsinthesecondsetting.Thecoreideaofrepresentation learningisthatthesamerepresentationmaybeusefulinbothsettings.Usingthe samerepresentationinbothsettingsallowstherepresentationtobeneÔ¨Åtfromthe trainingdatathatisavailableforbothtasks. Asmentionedbefore,unsuperviseddeeplearningfortransferlearninghasfound successinsomemachinelearningcompetitions( ,; Mesniletal.2011Goodfellow etal.,).IntheÔ¨Årstofthesecompetitions,theexperimentalsetupisthe 2011 following.EachparticipantisÔ¨ÅrstgivenadatasetfromtheÔ¨Årstsetting(from distributionP 1),illustratingexamplesofsomesetofcategories.Theparticipants mustusethistolearnagoodfeaturespace(mappingtherawinputtosome representation),suchthatwhenweapplythislearnedtransformationtoinputs fromthetransfersetting(distributionP 2),alinearclassiÔ¨Åercanbetrainedand generalizewellfromveryfewlabeledexamples.Oneofthemoststrikingresults foundinthiscompetitionisthatasanarchitecturemakesuseofdeeperand deeperrepresentations(learnedinapurelyunsupervisedwayfromdatacollected intheÔ¨Årstsetting,P 1),thelearningcurveonthenewcategoriesofthesecond (transfer)settingP 2becomesmuchbetter.Fordeeprepresentations,fewerlabeled examplesofthetransfertasksarenecessarytoachievetheapparentlyasymptotic generalization performance. Twoextremeformsoftransferlearningare o ne-shot l e ar ni ngand z e r</div>
        </div>
    </div>

    <div class="question-card" id="q102">
        <div class="question-header">
            <span class="question-number">Question 102</span>
            <span class="question-type">multiple-choice</span>
        </div>

        <div class="question-text">Machine learning recommendation systems frequently leverage deep learning and reinforcement learning frameworks to personalize user experiences. Such systems must address challenges in data collection, policy evaluation, and the inherent exploration-exploitation trade-off.

Which of the following best explains why policy evaluation in reinforcement learning-based recommendation systems is challenging?

1) The system always has access to complete and unbiased feedback for all possible actions.   
2) The user-item interaction data is fully representative of unseen recommendations.   
3) The learning algorithm is unaffected by the actions it takes during training.   
4) Negative examples in the dataset are naturally abundant and do not require generation.   
5) Supervised learning techniques can directly evaluate policies without complications from feedback loops.   
6) The actions chosen by the system influence future data collection, creating a feedback loop that complicates accurate performance measurement.   
7) Knowledge bases consistently provide explicit negative facts to facilitate policy evaluation.</div>

        <div class="answer-section">
            <div class="answer-label">‚úì Correct Answer:</div>
            <div class="answer-text">The correct answer is 6) The actions chosen by the system influence future data collection, creating a feedback loop that complicates accurate performance measurement..</div>
        </div>

        <div class="reference-section">
            <div class="reference-label">üìö Reference Text:</div>
            <button class="toggle-button" onclick="toggleReference(102)">
                Show/Hide Reference
            </button>
            <div id="ref102" class="reference-text hidden">deeplearningarchitecture( ,; Huang e t a l .2013Elkahky2015 e t a l .,). Specializeddeeplearningarchitecturessuchasconvolutionalnetworkshavealso beenappliedtolearntoextractfeaturesfromrichcontentsuchasfrommusical audiotracks,formusicrecommendation (vandenO√∂rd2013 e t a l .,).Inthatwork, theconvolutionalnettakesacousticfeaturesasinputandcomputesanembedding fortheassociatedsong.Thedotproductbetweenthissongembeddingandthe embeddingforauseristhenusedtopredictwhetherauserwilllistentothesong. 12.5.1.1ExplorationVersusExploitation Whenmakingrecommendations tousers,anissuearisesthatgoesbeyondordinary supervisedlearningandintotherealmofreinforcementlearning.Manyrecom- mendationproblemsaremostaccuratelydescribedtheoreticallyascontextual bandits( ,;,).Theissueisthatwhenwe LangfordandZhang2008Lu e t a l .2010 usetherecommendation systemtocollectdata,wegetabiasedandincomplete viewofthepreferencesofusers:weonlyseetheresponsesofuserstotheitems theywererecommendedandnottotheotheritems. Inaddition,insomecases wemaynotgetanyinformationonusersforwhomnorecommendation hasbeen made(forexample,withadauctions,itmaybethatthepriceproposedforan 4 8 0 CHAPTER12.APPLICATIONS adwasbelowaminimumpricethreshold,ordoesnotwintheauction,sothe adisnotshownatall).Moreimportantly,wegetnoinformationaboutwhat outcomewouldhaveresultedfromrecommendinganyoftheotheritems.This wouldbeliketrainingaclassiÔ¨ÅerbypickingoneclassÀÜ yforeachtrainingexample x(typicallytheclasswiththehighestprobabilityaccordingtothemodel)and thenonlygettingasfeedbackwhetherthiswasthecorrectclassornot.Clearly, eachexampleconveyslessinformationthaninthesupervisedcasewherethetrue label yisdirectlyaccessible,somoreexamplesarenecessary.Worse,ifwearenot careful,wecouldendupwithasystemthatcontinuespickingthewrongdecisions evenasmoreandmoredataiscollected,becausethecorrectdecisioninitiallyhada verylowprobability:untilthelearnerpicksthatcorrectdecision,itdoesnotlearn aboutthecorrectdecision.Thisissimilartothesituationinreinforcementlearning whereonlytherewardfortheselectedactionisobserved.Ingeneral,reinforcement learningcaninvolveasequenceofmanyactionsandmanyrewards.Thebandits scenarioisaspecialcaseofreinforcementlearning,inwhichthelearnertakesonly asingleactionandreceivesasinglereward.Thebanditproblemiseasierinthe sensethatthelearnerknowswhichrewardisassociatedwithwhichaction.In thegeneralreinforcementlearningscenario,ahighrewardoralowrewardmight havebeencausedbyarecentactionorbyanactioninthedistantpast.Theterm contextualbanditsreferstothecasewheretheactionistakeninthecontextof someinputvariablethatcaninformthedecision.Forexample,weatleastknow theuseridentity,andwewanttopickanitem.Themappingfromcontextto actionisalsocalledapolicy.Thefeedbackloopbetweenthelearnerandthedata distribution(whichnowdependsontheactionsofthelearner)isacentralresearch issueinthereinforcementlearningandbanditsliterature. ReinforcementlearningrequireschoosingatradeoÔ¨Äbetweenexplorationand exploitation.Exploitationreferstotakingactionsthatcomefromthecurrent, bestversionofthelearnedpolicy‚Äîactionsthatweknowwillachieveahighreward. ExplorationreferstotakingactionsspeciÔ¨Åcallyinordertoobtainmoretraining data.Ifweknowthatgivencontextx,action agivesusarewardof1,wedonot knowwhetherthatisthebestpossiblereward.Wemaywanttoexploitourcurrent policyandcontinuetakingaction ainordertoberelativelysureofobtaininga rewardof1.However,wemayalsowanttoexplorebytryingaction aÓÄ∞.Wedonot knowwhatwillhappenifwetryaction aÓÄ∞.Wehopetogetarewardof,butwe 2 runtheriskofgettingarewardof.Eitherway,weatleastgainsomeknowledge. 0 Explorationcanbeimplementedinmanyways,rangingfromoccasionally takingrandomactionsintendedtocovertheentirespaceofpossibleactions,to model-basedapproachesthatcomputeachoiceofactionbasedonitsexpected rewardandthemodel‚Äôsamountofuncertaintyaboutthatreward. 4 8 1 CHAPTER12.APPLICATIONS Manyfactorsdeterminetheextenttowhichwepreferexplorationorexploitation. Oneofthemostprominentfactorsisthetimescaleweareinterestedin. Ifthe agenthasonlyashortamountoftimetoaccruereward,thenweprefermore exploitation.Iftheagenthasalongtimetoaccruereward,thenwebeginwith moreexplorationsothatfutureactionscanbeplannedmoreeÔ¨Äectivelywithmore knowledge.Astimeprogressesandourlearnedpolicyimproves,wemovetoward moreexploitation. Supervised learninghas notradeoÔ¨Ä between explorationand exploitation becausethesupervisionsignalalwaysspeciÔ¨Åeswhichoutputiscorrectforeach input.ThereisnoneedtotryoutdiÔ¨Äerentoutputstodetermineifoneisbetter thanthemodel‚Äôscurrentoutput‚Äîwealwaysknowthatthelabelisthebestoutput. AnotherdiÔ¨Écultyarisinginthecontextofreinforcementlearning,besidesthe exploration-exploitationtrade-oÔ¨Ä,isthediÔ¨Écultyofevaluatingandcomparing diÔ¨Äerentpolicies.Reinforcementlearninginvolvesinteractionbetweenthelearner andtheenvironment.Thisfeedbackloopmeansthatitisnotstraightforwardto evaluatethelearner‚ÄôsperformanceusingaÔ¨Åxedsetoftestsetinputvalues.The policyitselfdetermineswhichinputswillbeseen. ()present Dudik e t a l .2011 techniquesforevaluatingcontextualbandits. 12.5.2KnowledgeRepresentation,ReasoningandQuestionAn- swering Deeplearningapproacheshavebeenverysuccessfulinlanguagemodeling,machine translationandnaturallanguageprocessingduetotheuseofembeddingsfor symbols( ,)andwords( Rumelhart e t a l .1986a Deerwester1990Bengio e t a l .,; e t a l ., 2001).Theseembeddingsrepresentsemanticknowledgeaboutindividualwords andconcepts.Aresearchfrontieristodevelopembeddingsforphrasesandfor relationsbetweenwordsandfacts.Searchenginesalreadyusemachinelearningfor thispurposebutmuchmoreremainstobedonetoimprovethesemoreadvanced representations. 12.5.2.1Knowledge,RelationsandQuestionAnswering Oneinterestingresearchdirectionisdetermininghowdistributedrepresentations canbetrainedtocapturetherelationsbetweentwoentities.Theserelations allowustoformalizefactsaboutobjectsandhowobjectsinteractwitheachother. Inmathematics,abinaryrelationisasetoforderedpairsofobjects.Pairs thatareinthesetaresaidtohavetherelationwhilethosewhoarenotintheset 4 8 2 CHAPTER12.APPLICATIONS donot.Forexample,wecandeÔ¨Ånetherelation‚Äúislessthan‚Äùonthesetofentities {1 ,2 ,3}bydeÔ¨Åningthesetoforderedpairs S={(1 ,2) ,(1 ,3) ,(2 ,3)}.Oncethis relationisdeÔ¨Åned,wecanuseitlikeaverb.Because(1 ,2)‚àà S,wesaythat1is lessthan2.Because(2 ,1)ÓÄ∂‚àà S,wecannotsaythat2islessthan1.Ofcourse,the entitiesthatarerelatedtooneanotherneednotbenumbers.WecoulddeÔ¨Ånea relation containingtupleslike(,). is_a_type_of dogmammal InthecontextofAI,wethinkofarelationasasentenceinasyntactically simpleandhighlystructuredlanguage.Therelationplaystheroleofaverb, whiletwoargumentstotherelationplaytheroleofitssubjectandobject.These sentencestaketheformofatripletoftokens (subjectverbobject) , , (12.21) withvalues (entityi ,relation j ,entityk) . (12.22) WecanalsodeÔ¨Åneanattribute,aconceptanalogoustoarelation,buttaking onlyoneargument: (entity i ,attribute j) . (12.23) Forexample,wecoulddeÔ¨Ånethehas_furattribute,andapplyittoentitieslike dog. Manyapplicationsrequirerepresentingrelationsandreasoningaboutthem. Howshouldwebestdothiswithinthecontextofneuralnetworks? Machinelearningmodelsofcourserequiretrainingdata.Wecaninferrelations betweenentitiesfromtrainingdatasetsconsistingofunstructurednaturallanguage. Therearealsostructureddatabasesthatidentifyrelationsexplicitly.Acommon structureforthesedatabasesistherelationaldatabase,whichstoresthissame kindofinformation, alb eit notformattedasthreetokensentences.Whena databaseisintendedtoconveycommonsense knowledgeabouteverydaylifeor expertknowledgeaboutanapplicationareatoanartiÔ¨Åcialintelligencesystem, wecallthedatabaseaknowledgebase.Knowledgebasesrangefromgeneral oneslikeFreebase,OpenCyc,WordNet,orWikibase,1etc.tomorespecialized knowledgebases,likeGeneOntology.2Representationsforentitiesandrelations canbelearnedbyconsideringeachtripletinaknowledgebaseasatrainingexample andmaximizingatrainingobjectivethatcapturestheirjointdistribution(Bordes e t a l .,).2013a 1R e s p e c t i v e l y a v a i l a b l e f ro m t h e s e w e b s i t e s : f r e e b a s e . c o m , c y c . c o m / o p e n c y c , w o r d n e t . p r i n c e t o n . e d u w i k i b a . s e , 2g e n e o n t o l o g y . o r g 4 8 3 CHAPTER12.APPLICATIONS Inadditiontotrainingdata,wealsoneedtodeÔ¨Åneamodelfamilytotrain. Acommonapproachistoextendneurallanguagemodelstomodelentitiesand relations.Neurallanguagemodelslearnavectorthatprovidesadistributed representationofeachword.Theyalsolearnaboutinteractionsbetweenwords, suchaswhichwordislikelytocomeafterasequenceofwords,bylearningfunctions ofthesevectors.Wecanextendthisapproachtoentitiesandrelationsbylearning anembeddingvectorforeachrelation.Infact,theparallelbetweenmodeling languageandmodelingknowledgeencodedasrelationsissoclosethatresearchers havetrainedrepresentationsofsuchentitiesbyusing b o t h a nd knowledgebases naturallanguagesentences( ,,; Bordes e t a l .20112012Wang2014a e t a l .,)or combiningdatafrommultiplerelationaldatabases( ,).Many Bordes e t a l .2013b possibilitiesexistfortheparticularparametrization associatedwithsuchamodel. Earlyworkonlearningaboutrelationsbetweenentities( , PaccanaroandHinton 2000)positedhighlyconstrainedparametricforms(‚Äúlinearrelationalembeddings‚Äù), oftenusingadiÔ¨Äerentformofrepresentationfortherelationthanfortheentities. Forexample,PaccanaroandHinton2000Bordes2011 ()and e t a l .()usedvectorsfor entitiesandmatricesforrelations,withtheideathatarelationactslikeanoperator onentities.Alternatively,relationscanbeconsideredasanyotherentity(Bordes e t a l .,),allowingustomakestatementsaboutrelations,butmoreÔ¨Çexibilityis 2012 putinthemachinerythatcombinestheminordertomodeltheirjointdistribution. Apracticalshort-termapplicationofsuchmodelsislinkprediction:predict- ingmissingarcsintheknowledgegraph.Thisisaformofgeneralization tonew facts,basedonoldfacts.Mostoftheknowledgebasesthatcurrentlyexisthave beenconstructedthroughmanuallabor,whichtendstoleavemanyandprobably themajorityoftruerelationsabsentfromtheknowledgebase.SeeWang e t a l . (),()and ()forexamplesofsuchan 2014bLin e t a l .2015Garcia-Duran e t a l .2015 application. EvaluatingtheperformanceofamodelonalinkpredictiontaskisdiÔ¨Écult becausewehaveonlyadatasetofpositiveexamples(factsthatareknownto betrue). Ifthemodelproposesafactthatisnotinthedataset,weareunsure whetherthemodelhasmadeamistakeordiscoveredanew,previouslyunknown fact.Themetricsarethussomewhatimpreciseandarebasedontestinghowthe modelranksaheld-outofsetofknowntruepositivefactscomparedtootherfacts thatarelesslikelytobetrue.Acommonwaytoconstructinterestingexamples thatareprobablynegative(factsthatareprobablyfalse)istobeginwithatrue factandcreatecorruptedversionsofthatfact,forexamplebyreplacingoneentity intherelationwithadiÔ¨Äerententityselectedatrandom.Thepopularprecisionat 10%metriccountshowmanytimesthemodelranksa‚Äúcorrect‚Äùfactamongthe top10%ofallcorruptedversionsofthatfact. 4 8 4 CHAPTER12.APPLICATIONS Anotherapplicationofknowledgebasesanddistributedrepresentationsfor themisword-sensedisambiguation(NavigliandVelardi2005Bordes,; e t a l ., 2012),whichisthetaskofdecidingwhichofthesensesofawordistheappropriate one,insomecontext. Eventually,knowledgeofrelationscombinedwithareasoningprocessand understandingofnaturallanguagecouldallowustobuildageneralquestion answeringsystem.Ageneralquestionansweringsystemmustbeabletoprocess inputinformationandrememberimportantfacts,organizedinawaythatenables ittoretrieveandreasonaboutthemlater.ThisremainsadiÔ¨Écultopenproblem whichcanonlybesolvedinrestricted‚Äútoy‚Äùenvironments.Currently,thebest approachtorememberingandretrievingspeciÔ¨Åcdeclarativefactsistousean explicitmemorymechanism,asdescribedinsection.Memorynetworkswere 10.12 Ô¨Årstproposedtosolveatoyquestionansweringtask(Weston2014Kumar e t a l .,). e t a l .()haveproposedanextensionthatusesGRUrecurrentnetstoread 2015 theinputintothememoryandtoproducetheanswergiventhecontentsofthe memory. Deeplearninghasbeenappliedtomanyotherapplicationsbesidestheones describedhere,andwillsurelybeappliedtoevenmoreafterthiswriting.Itwould beimpossibletodescribeanythingremotelyresemblingacomprehensivecoverage ofsuchatopic.Thissurveyprovidesarepresentativesampleofwhatispossible asofthiswriting. Thisconcludespart,whichhasdescribedmodernpracticesinvolvingdeep II networks,comprisingallofthemostsuccessfulmethods.Generallyspeaking,these methodsinvolveusingthegradientofacostfunctiontoÔ¨Åndtheparametersofa</div>
        </div>
    </div>

    <div class="question-card" id="q103">
        <div class="question-header">
            <span class="question-number">Question 103</span>
            <span class="question-type">multiple-choice</span>
        </div>

        <div class="question-text">Neural networks are widely used in machine learning for tasks such as regression and classification. The choice of output unit and cost function influences which statistic of the target distribution is estimated and impacts the effectiveness of gradient-based optimization.

Which combination of output unit and cost function is most suitable for robust gradient-based optimization in multiclass classification tasks, ensuring strong gradients for incorrect predictions and a valid probability distribution over classes?

1) Softmax output unit with cross-entropy loss   
2) Linear output unit with mean absolute error   
3) Sigmoid output unit with mean squared error   
4) Linear output unit with cross-entropy loss   
5) Softmax output unit with mean squared error   
6) Sigmoid output unit with mean absolute error   
7) Linear output unit with mean squared error</div>

        <div class="answer-section">
            <div class="answer-label">‚úì Correct Answer:</div>
            <div class="answer-text">The correct answer is 1) Softmax output unit with cross-entropy loss.</div>
        </div>

        <div class="reference-section">
            <div class="reference-label">üìö Reference Text:</div>
            <button class="toggle-button" onclick="toggleReference(103)">
                Show/Hide Reference
            </button>
            <div id="ref103" class="reference-text hidden">p(yx|;Œ∏)weoftenwanttolearn justoneconditionalstatisticofgiven.yx Forexample,wemayhaveapredictor f(x;Œ∏) thatwewishtopredictthemean of.y IfweuseasuÔ¨Écientlypowerfulneuralnetwork,wecanthinkoftheneural networkasbeingabletorepresentanyfunction ffromawideclassoffunctions, withthisclassbeinglimitedonlybyfeaturessuchascontinuityandboundedness ratherthanbyhavingaspeciÔ¨Åcparametricform.Fromthispointofview,we canviewthecostfunctionasbeingafunctionalratherthanjustafunction.A functionalisamappingfromfunctionstorealnumbers.Wecanthusthinkof learningaschoosingafunctionratherthanmerelychoosingasetofparameters. WecandesignourcostfunctionaltohaveitsminimumoccuratsomespeciÔ¨Åc functionwedesire.Forexample,wecandesignthecostfunctionaltohaveits minimumlieonthefunctionthatmapsxtotheexpectedvalueofygivenx. Solvinganoptimizationproblemwithrespecttoafunctionrequiresamathematical toolcalledcalculusofvariations,describedinsection.Itisnotnecessary 19.4.2 tounderstandcalculusofvariationstounderstandthecontentofthischapter.At themoment,itisonlynecessarytounderstandthatcalculusofvariationsmaybe usedtoderivethefollowingtworesults. OurÔ¨Årstresultderivedusingcalculusofvariationsisthatsolvingtheoptimiza- tionproblem f‚àó= argmin fE x y ,‚àº pdata||‚àí ||y f()x2(6.14) yields f‚àó() = x E y‚àº pdata ( ) y x|[]y , (6.15) solongasthisfunctionlieswithintheclassweoptimizeover.Inotherwords,ifwe couldtrainoninÔ¨Ånitelymanysamplesfromthetruedatageneratingdistribution, minimizingthemeansquarederrorcostfunctiongivesafunctionthatpredictsthe meanofforeachvalueof. y x 1 8 0 CHAPTER6.DEEPFEEDFORWARDNETWORKS DiÔ¨ÄerentcostfunctionsgivediÔ¨Äerentstatistics.Asecondresultderivedusing calculusofvariationsisthat f‚àó= argmin fE x y ,‚àº pdata||‚àí ||y f()x 1 (6.16) yieldsafunctionthatpredictsthe m e d i a nvalueofyforeachx,solongassucha functionmaybedescribedbythefamilyoffunctionsweoptimizeover.Thiscost functioniscommonlycalled . meanabsoluteerror Unfortunately,meansquarederrorandmeanabsoluteerroroftenleadtopoor resultswhenusedwithgradient-basedoptimization. Someoutputunitsthat saturateproduceverysmallgradientswhencombinedwiththesecostfunctions. Thisisonereasonthatthecross-entropycostfunctionismorepopularthanmean squarederrorormeanabsoluteerror,evenwhenitisnotnecessarytoestimatean entiredistribution. p( )yx| 6.2.2OutputUnits Thechoiceofcostfunctionistightlycoupledwiththechoiceofoutputunit.Most ofthetime,wesimplyusethecross-entropybetweenthedatadistributionandthe modeldistribution. Thechoiceofhowtorepresenttheoutputthendetermines theformofthecross-entropyfunction. Anykindofneuralnetworkunitthatmaybeusedasanoutputcanalsobe usedasahiddenunit.Here,wefocusontheuseoftheseunitsasoutputsofthe model,butinprincipletheycanbeusedinternallyaswell.Werevisittheseunits withadditionaldetailabouttheiruseashiddenunitsinsection.6.3 Throughoutthissection,wesupposethatthefeedforwardnetworkprovidesa setofhiddenfeaturesdeÔ¨Ånedbyh= f(x;Œ∏).Theroleoftheoutputlayeristhen toprovidesomeadditionaltransformationfromthefeaturestocompletethetask thatthenetworkmustperform. 6.2.2.1LinearUnitsforGaussianOutputDistributions OnesimplekindofoutputunitisanoutputunitbasedonanaÔ¨Énetransformation withnononlinearity.Theseareoftenjustcalledlinearunits. Givenfeaturesh,alayeroflinearoutputunitsproducesavectorÀÜy=WÓÄæh+b. Linearoutputlayersareoftenusedtoproducethemeanofaconditional Gaussiandistribution: p( ) = (;yx| NyÀÜyI ,) . (6.17) 1 8 1 CHAPTER6.DEEPFEEDFORWARDNETWORKS Maximizingthelog-likelihoodisthenequivalenttominimizingthemeansquared error. Themaximumlikelihoodframeworkmakesitstraightforwardtolearnthe covarianceoftheGaussiantoo,ortomakethecovarianceoftheGaussianbea functionoftheinput.However,thecovariancemustbeconstrainedtobeapositive deÔ¨Ånitematrixforallinputs.ItisdiÔ¨Éculttosatisfysuchconstraintswithalinear outputlayer,sotypicallyotheroutputunitsareusedtoparametrizethecovariance. Approachestomodelingthecovariancearedescribedshortly,insection.6.2.2.4 Becauselinearunitsdonotsaturate,theyposelittlediÔ¨Écultyforgradient- basedoptimizationalgorithmsandmaybeusedwithawidevarietyofoptimization algorithms. 6.2.2.2SigmoidUnitsforBernoulliOutputDistributions Manytasksrequirepredictingthevalueofabinaryvariable y.ClassiÔ¨Åcation problemswithtwoclassescanbecastinthisform. Themaximum-likelihoodapproachistodeÔ¨ÅneaBernoullidistributionover y conditionedon.x ABernoullidistributionisdeÔ¨Ånedbyjustasinglenumber.Theneuralnet needstopredictonly P( y= 1|x).Forthisnumbertobeavalidprobability,it mustlieintheinterval[0,1]. SatisfyingthisconstraintrequiressomecarefuldesigneÔ¨Äort.Supposewewere tousealinearunit,andthresholditsvaluetoobtainavalidprobability: P y(= 1 ) = max |xÓÅÆ 0min ,ÓÅÆ 1 ,wÓÄæh+ bÓÅØÓÅØ .(6.18) ThiswouldindeeddeÔ¨Åneavalidconditionaldistribution,butwewouldnotbeable totrainitveryeÔ¨Äectivelywithgradientdescent.AnytimethatwÓÄæh+ bstrayed outsidetheunitinterval,thegradientoftheoutputofthemodelwithrespectto itsparameterswouldbe 0.Agradientof 0istypicallyproblematicbecausethe learningalgorithmnolongerhasaguideforhowtoimprovethecorresponding parameters. Instead,itisbettertouseadiÔ¨Äerentapproachthatensuresthereisalwaysa stronggradientwheneverthemodelhasthewronganswer.Thisapproachisbased onusingsigmoidoutputunitscombinedwithmaximumlikelihood. AsigmoidoutputunitisdeÔ¨Ånedby ÀÜ y œÉ= ÓÄê wÓÄæh+ bÓÄë (6.19) 1 8 2 CHAPTER6.DEEPFEEDFORWARDNETWORKS whereisthelogisticsigmoidfunctiondescribedinsection. œÉ 3.10 Wecanthinkofthesigmoidoutputunitashavingtwocomponents.First,it usesalinearlayertocompute z=wÓÄæh+ b.Next,itusesthesigmoidactivation functiontoconvertintoaprobability. z WeomitthedependenceonxforthemomenttodiscusshowtodeÔ¨Ånea probabilitydistributionover yusingthevalue z.Thesigmoidcanbemotivated byconstructinganunnormalized probabilitydistributionÀú P( y),whichdoesnot sumto1.Wecanthendividebyanappropriateconstanttoobtainavalid probabilitydistribution.Ifwebeginwiththeassumptionthattheunnormalized log probabilitiesarelinearin yand z,wecanexponentiatetoobtaintheunnormalized probabilities. WethennormalizetoseethatthisyieldsaBernoullidistribution controlledbyasigmoidaltransformationof: z logÀú P y y z () = (6.20) Àú P y y z () = exp() (6.21) P y() =exp() y zÓÅê1 yÓÄ∞= 0exp( yÓÄ∞z)(6.22) P y œÉ y z . () = ((2‚àí1)) (6.23) Probabilitydistributionsbasedonexponentiationandnormalization arecommon throughoutthestatisticalmodelingliterature.The zvariabledeÔ¨Åningsucha distributionoverbinaryvariablesiscalleda.logit Thisapproachtopredictingtheprobabilities inlog-spaceisnaturaltouse withmaximumlikelihoodlearning.Becausethecostfunctionusedwithmaximum likelihoodis‚àílog P( y|x),theloginthecostfunctionundoestheexpofthe sigmoid.WithoutthiseÔ¨Äect,thesaturationofthesigmoidcouldpreventgradient- based learningfrom makinggoodprogress.Theloss functionfor maximum likelihoodlearningofaBernoulliparametrized byasigmoidis J P y () = logŒ∏ ‚àí (|x) (6.24) = log((2 1)) ‚àí œÉ y‚àí z (6.25) = ((12)) Œ∂ ‚àí y z . (6.26) Thisderivationmakesuseofsomepropertiesfromsection.Byrewriting3.10 thelossintermsofthesoftplusfunction,wecanseethatitsaturatesonlywhen (1‚àí2 y) zisverynegative.Saturationthusoccursonlywhenthemodelalready hastherightanswer‚Äîwhen y= 1and zisverypositive,or y= 0and zisvery negative.When zhasthewrongsign,theargumenttothesoftplusfunction, 1 8 3 CHAPTER6.DEEPFEEDFORWARDNETWORKS (1‚àí2 y) z,maybesimpliÔ¨Åedto|| z.As|| zbecomeslargewhile zhasthewrongsign, thesoftplusfunctionasymptotestowardsimplyreturningitsargument || z.The derivativewithrespectto zasymptotestosign( z),so,inthelimitofextremely incorrect z,thesoftplusfunctiondoesnotshrinkthegradientatall.Thisproperty isveryusefulbecauseitmeansthatgradient-basedlearningcanacttoquickly correctamistaken. z Whenweuseotherlossfunctions,suchasmeansquarederror,thelosscan saturateanytime œÉ( z)saturates.Thesigmoidactivationfunctionsaturatesto0 when zbecomesverynegativeandsaturatestowhen1 zbecomesverypositive. Thegradientcanshrinktoosmalltobeusefulforlearningwheneverthishappens, whetherthemodelhasthecorrectanswerortheincorrectanswer.Forthisreason, maximumlikelihoodisalmostalwaysthepreferredapproachtotrainingsigmoid outputunits. Analytically,thelogarithmofthesigmoidisalwaysdeÔ¨ÅnedandÔ¨Ånite,because thesigmoidreturnsvaluesrestrictedtotheopeninterval(0 ,1),ratherthanusing theentireclosedintervalofvalidprobabilities [0 ,1].Insoftwareimplementations, toavoidnumericalproblems,itisbesttowritethenegativelog-likelihoodasa functionof z,ratherthanasafunctionofÀÜ y= œÉ( z).Ifthesigmoidfunction underÔ¨Çowstozero,thentakingthelogarithmofÀÜ yyieldsnegativeinÔ¨Ånity. 6.2.2.3SoftmaxUnitsforMultinoulliOutputDistributions Anytimewewishtorepresentaprobabilitydistributionoveradiscretevariable with npossiblevalues,wemayusethesoftmaxfunction.Thiscanbeseenasa generalization ofthesigmoidfunctionwhichwasusedtorepresentaprobability distributionoverabinaryvariable. SoftmaxfunctionsaremostoftenusedastheoutputofaclassiÔ¨Åer,torepresent theprobabilitydistributionover ndiÔ¨Äerentclasses.Morerarely,softmaxfunctions canbeusedinsidethemodelitself,ifwewishthemodeltochoosebetweenoneof ndiÔ¨Äerentoptionsforsomeinternalvariable. Inthecaseofbinaryvariables,wewishedtoproduceasinglenumber ÀÜ y P y . = (= 1 )|x (6.27) Becausethisnumberneededtoliebetweenand,andbecausewewantedthe 0 1 logarithmofthenumbertobewell-behavedforgradient-basedoptimization of thelog-likelihood,wechosetoinsteadpredictanumber z=logÀú P( y=1|x). ExponentiatingandnormalizinggaveusaBernoullidistributioncontrolledbythe sigmoidfunction. 1 8 4 CHAPTER6.DEEPFEEDFORWARDNETWORKS Togeneralizetothecaseofadiscretevariablewith nvalues,wenowneed toproduceavectorÀÜy,with ÀÜ y i= P( y= i|x).Werequirenotonlythateach elementofÀÜ y ibebetweenand,butalsothattheentirevectorsumstosothat 0 1 1 itrepresentsavalidprobabilitydistribution.Thesameapproachthatworkedfor theBernoullidistributiongeneralizestothemultinoullidistribution.First,alinear layerpredictsunnormalized logprobabilities: zW= ÓÄæhb+ , (6.28) where z i=logÀú P( y= i|x) .Thesoftmaxfunctioncanthenexponentiateand normalizetoobtainthedesired z ÀÜy.Formally,thesoftmaxfunctionisgivenby softmax()z i=exp( z i)ÓÅê jexp( z j). (6.29) Aswiththelogisticsigmoid,theuseoftheexpfunctionworksverywellwhen trainingthesoftmaxtooutputatargetvalueyusingmaximumlog-likelihood.In thiscase,wewishtomaximize log P(y= i;z)=logsoftmax(z) i.DeÔ¨Åningthe softmaxintermsofexpisnaturalbecausetheloginthelog-likelihoodcanundo theofthesoftmax: exp logsoftmax()z i= z i‚àílogÓÅò jexp( z j) . (6.30) TheÔ¨Årsttermofequationshowsthattheinput 6.30 z ialwayshasadirect contributiontothecostfunction.Becausethistermcannotsaturate,weknow thatlearningcanproceed,evenifthecontributionof z itothesecondtermof equationbecomesverysmall.Whenmaximizingthelog-likelihood,theÔ¨Årst 6.30 termencourages z itobepushedup,whilethesecondtermencouragesallofztobe pusheddown.Togainsomeintuitionforthesecondterm,logÓÅê jexp( z j),observe thatthistermcanberoughlyapproximatedbymax j z j.Thisapproximation is basedontheideathatexp( z k) isinsigniÔ¨Åcantforany z kthatisnoticeablylessthan max j z j.Theintuitionwecangainfromthisapproximation isthatthenegative log-likelihoodcostfunctionalwaysstronglypenalizesthemostactiveincorrect prediction.Ifthecorrectansweralreadyhasthelargestinputtothesoftmax,then the‚àí z itermandthelogÓÅê jexp( z j)‚âàmax j z j= z itermswillroughlycancel. Thisexamplewillthencontributelittletotheoveralltrainingcost,whichwillbe dominatedbyotherexamplesthatarenotyetcorrectlyclassiÔ¨Åed. Sofarwehavediscussedonlyasingleexample.Overall,unregularized maximum</div>
        </div>
    </div>

    <div class="question-card" id="q104">
        <div class="question-header">
            <span class="question-number">Question 104</span>
            <span class="question-type">multiple-choice</span>
        </div>

        <div class="question-text">Convolutional neural networks (CNNs) are widely used in computer vision and time series analysis due to their ability to efficiently extract and summarize local features. These networks use convolution and pooling operations to achieve invariance and equivariance to certain input transformations.

Which property allows convolutional neural networks to produce outputs that shift identically when the input is translated, thereby ensuring consistent feature detection regardless of input position?

1) Equivariance to translation   
2) Invariance to scale   
3) Robustness to rotation   
4) Sensitivity to pooling regions   
5) Parameter sharing over channels   
6) Non-linearity of activations   
7) Downsampling through strided convolutions</div>

        <div class="answer-section">
            <div class="answer-label">‚úì Correct Answer:</div>
            <div class="answer-text">The correct answer is 1) Equivariance to translation.</div>
        </div>

        <div class="reference-section">
            <div class="reference-label">üìö Reference Text:</div>
            <button class="toggle-button" onclick="toggleReference(104)">
                Show/Hide Reference
            </button>
            <div id="ref104" class="reference-text hidden">I( x ‚àí1 , y).Thisshiftseverypixelof Ione unittotheright.Ifweapplythistransformationto I,thenapplyconvolution, theresultwillbethesameasifweappliedconvolutionto IÓÄ∞,thenappliedthe transformation gtotheoutput.Whenprocessingtimeseriesdata,thismeans thatconvolutionproducesasortoftimelinethatshowswhendiÔ¨Äerentfeatures appearintheinput.Ifwemoveaneventlaterintimeintheinput,theexact samerepresentationofitwillappearintheoutput,justlaterintime.Similarly withimages,convolutioncreatesa2-Dmapofwherecertainfeaturesappearin theinput.Ifwemovetheobjectintheinput,itsrepresentationwillmovethe sameamountintheoutput.Thisisusefulforwhenweknowthatsomefunction ofasmallnumberofneighboringpixelsisusefulwhenappliedtomultipleinput locations.Forexample,whenprocessingimages,itisusefultodetectedgesin theÔ¨Årstlayerofaconvolutionalnetwork.Thesameedgesappearmoreorless everywhereintheimage,soitispracticaltoshareparametersacrosstheentire image.Insomecases,wemaynotwishtoshareparametersacrosstheentire image.Forexample,ifweareprocessingimagesthatarecroppedtobecentered onanindividual‚Äôsface,weprobablywanttoextractdiÔ¨ÄerentfeaturesatdiÔ¨Äerent locations‚Äîthepartofthenetworkprocessingthetopofthefaceneedstolookfor eyebrows,whilethepartofthenetworkprocessingthebottomofthefaceneedsto lookforachin. Convolutionisnotnaturallyequivarianttosomeothertransformations,such aschangesinthescaleorrotationofanimage.Othermechanismsarenecessary forhandlingthesekindsoftransformations. Finally,somekindsofdatacannotbeprocessedbyneuralnetworksdeÔ¨Ånedby matrixmultiplication withaÔ¨Åxed-shapematrix.Convolutionenablesprocessing ofsomeofthesekindsofdata.Wediscussthisfurtherinsection.9.7 9.3Pooling Atypicallayerofaconvolutionalnetworkconsistsofthreestages(seeÔ¨Ågure).9.7 IntheÔ¨Årststage,thelayerperformsseveralconvolutionsinparalleltoproducea setoflinearactivations.Inthesecondstage,eachlinearactivationisrunthrough anonlinearactivationfunction,suchastherectiÔ¨Åedlinearactivationfunction. Thisstageissometimescalledthe det e c t o rstage. Inthethirdstage,weusea p o o l i ng f unc t i o ntomodifytheoutputofthelayerfurther. Apoolingfunctionreplacestheoutputofthenetatacertainlocationwitha summarystatisticofthenearbyoutputs.Forexample,the m ax p o o l i ng(Zhou 3 3 9 CHAPTER9.CONVOLUTIONALNETWORKS Figure9.6: E Ô¨É c i e n c y o f e d g e d e t e c t i o n. Theimageontherightwasformedbytaking eachpixelintheoriginalimageandsubtractingthevalueofitsneighboringpixelonthe left. Thisshowsthestrengthofalloftheverticallyorientededgesintheinputimage, whichcanbeausefuloperationforobjectdetection.Bothimagesare280pixelstall. Theinputimageis320pixelswidewhiletheoutputimageis319pixelswide.This transformationcanbedescribedbyaconvolutionkernelcontainingtwoelements,and requires319 √ó280 √ó3=267 ,960Ô¨Çoatingpointoperations(twomultiplicationsand oneadditionperoutputpixel)tocomputeusingconvolution.Todescribethesame transformationwithamatrixmultiplicationwouldtake320 √ó280 √ó319 √ó280,orover eightbillion,entriesinthematrix,makingconvolutionfourbilliontimesmoreeÔ¨Écientfor representingthistransformation.Thestraightforwardmatrixmultiplicationalgorithm performsoversixteenbillionÔ¨Çoatingpointoperations,makingconvolutionroughly60,000 timesmoreeÔ¨Écientcomputationally.Ofcourse,mostoftheentriesofthematrixwouldbe zero.Ifwestoredonlythenonzeroentriesofthematrix,thenbothmatrixmultiplication andconvolutionwouldrequirethesamenumberofÔ¨Çoatingpointoperationstocompute. Thematrixwouldstillneedtocontain2 √ó319 √ó280=178 ,640entries.Convolution isanextremelyeÔ¨Écientwayofdescribingtransformationsthatapplythesamelinear transformationofasmall,localregionacrosstheentireinput.(Photocredit:Paula Goodfellow) 3 4 0 CHAPTER9.CONVOLUTIONALNETWORKS Convolutional Layer Input to layerConvolution stage: Ane transform Ô¨ÉDetector stage: Nonlinearity e.g., rectiÔ¨Åed linearPooling stageNext layer Input to layersConvolution layer: Ane transform Ô¨ÉDetector layer: Nonlinearity e.g., rectiÔ¨Åed linearPooling layerNext layerComplex layer terminology Simple layer terminology Figure9.7:Thecomponentsofatypicalconvolutionalneuralnetworklayer.Therearetwo commonlyusedsetsofterminologyfordescribingtheselayers. ( L e f t )Inthisterminology, theconvolutionalnetisviewedasasmallnumberofrelativelycomplexlayers,with eachlayerhavingmany‚Äústages.‚ÄùInthisterminology,thereisaone-to-onemapping betweenkerneltensorsandnetworklayers.Inthisbookwegenerallyusethisterminology. ( R i g h t )Inthisterminology,theconvolutionalnetisviewedasalargernumberofsimple layers;everystepofprocessingisregardedasalayerinitsownright.Thismeansthat notevery‚Äúlayer‚Äùhasparameters. 3 4 1 CHAPTER9.CONVOLUTIONALNETWORKS andChellappa1988,)operationreportsthemaximumoutputwithinarectangular neighborhood.Otherpopularpoolingfunctionsincludetheaverageofarectangular neighborhood,the L2normofarectangularneighborhood,oraweightedaverage basedonthedistancefromthecentralpixel. Inallcases,poolinghelpstomaketherepresentationbecomeapproximately i n v ar i an ttosmalltranslationsoftheinput.Invariancetotranslationmeansthat ifwetranslatetheinputbyasmallamount,thevaluesofmostofthepooled outputsdonotchange.SeeÔ¨Ågureforanexampleofhowthisworks. 9.8 Invariance tolocaltranslationcanbeaveryusefulpropertyifwecaremoreaboutwhether somefeatureispresentthanexactlywhereitis.Forexample,whendetermining whetheranimagecontainsaface,weneednotknowthelocationoftheeyeswith pixel-perfectaccuracy,wejustneedtoknowthatthereisaneyeontheleftside ofthefaceandaneyeontherightsideoftheface.Inothercontexts,itismore importanttopreservethelocationofafeature.Forexample,ifwewanttoÔ¨Ånda cornerdeÔ¨ÅnedbytwoedgesmeetingataspeciÔ¨Åcorientation,weneedtopreserve thelocationoftheedgeswellenoughtotestwhethertheymeet. TheuseofpoolingcanbeviewedasaddinganinÔ¨Ånitelystrongpriorthat thefunctionthelayerlearnsmustbeinvarianttosmalltranslations.Whenthis assumptioniscorrect,itcangreatlyimprovethestatisticaleÔ¨Éciencyofthenetwork. Poolingoverspatialregionsproducesinvariancetotranslation,butifwepool overtheoutputsofseparatelyparametrized convolutions,thefeaturescanlearn whichtransformationstobecomeinvariantto(seeÔ¨Ågure).9.9 Becausepoolingsummarizestheresponsesoverawholeneighborhood,itis possibletousefewerpoolingunitsthandetectorunits,byreportingsummary statisticsforpoolingregionsspaced kpixelsapartratherthan1pixelapart.See Ô¨Ågureforanexample.Thisimprovesthecomputational eÔ¨Éciencyofthe 9.10 networkbecausethenextlayerhasroughly ktimesfewerinputstoprocess.When thenumberofparametersinthenextlayerisafunctionofitsinputsize(suchas whenthenextlayerisfullyconnectedandbasedonmatrixmultiplication) this reductionintheinputsizecanalsoresultinimprovedstatisticaleÔ¨Éciencyand reducedmemoryrequirementsforstoringtheparameters. Formanytasks,poolingisessentialforhandlinginputsofvaryingsize. For example,ifwewanttoclassifyimagesofvariablesize,theinputtotheclassiÔ¨Åcation layermusthaveaÔ¨Åxedsize.Thisisusuallyaccomplishedbyvaryingthesizeofan oÔ¨ÄsetbetweenpoolingregionssothattheclassiÔ¨Åcationlayeralwaysreceivesthe samenumberofsummarystatisticsregardlessoftheinputsize.Forexample,the Ô¨ÅnalpoolinglayerofthenetworkmaybedeÔ¨Ånedtooutputfoursetsofsummary statistics,oneforeachquadrantofanimage,regardlessoftheimagesize. 3 4 2 CHAPTER9.CONVOLUTIONALNETWORKS 0. 1 1. 0. 21. 1. 1. 0. 10. 2 . . . . . .. . . . . . 0. 3 0. 1 1.1. 0. 3 1. 0. 21. . . . . . .. . . . . .D E T E C T O R S T A GEP O O L I N G ST A GE P O O L I N G ST A GE D E T E C T O R S T A GE Figure9.8:Maxpoolingintroducesinvariance. ( T o p )Aviewofthemiddleoftheoutput ofaconvolutionallayer.Thebottomrowshowsoutputsofthenonlinearity.Thetop rowshowstheoutputsofmaxpooling,withastrideofonepixelbetweenpoolingregions andapoolingregionwidthofthreepixels.Aviewofthesamenetwork,after ( Bottom ) theinputhasbeenshiftedtotherightbyonepixel.Everyvalueinthebottomrowhas changed,butonlyhalfofthevaluesinthetoprowhavechanged,becausethemaxpooling unitsareonlysensitivetothemaximumvalueintheneighborhood,notitsexactlocation. 3 4 3 CHAPTER9.CONVOLUTIONALNETWORKS L ar ge r e s pon s e i n po ol i ng uni tL ar ge r e s pon s e i n po ol i ng uni t L ar ge r e s ponse i n de t e c t or uni t 1L ar ge r e s ponse i n de t e c t or uni t 3 Figure9.9: E x a m p l e o f l e a r n e d i n v a r i a n c e s :Apoolingunitthatpoolsovermultiplefeatures thatarelearnedwithseparateparameterscanlearntobeinvarianttotransformationsof theinput.HereweshowhowasetofthreelearnedÔ¨Åltersandamaxpoolingunitcanlearn tobecomeinvarianttorotation.AllthreeÔ¨Åltersareintendedtodetectahand-written5. EachÔ¨ÅlterattemptstomatchaslightlydiÔ¨Äerentorientationofthe5.Whena5appearsin theinput,thecorrespondingÔ¨Ålterwillmatchitandcausealargeactivationinadetector unit.Themaxpoolingunitthenhasalargeactivationregardlessofwhichdetectorunit wasactivated.WeshowherehowthenetworkprocessestwodiÔ¨Äerentinputs,resulting intwodiÔ¨Äerentdetectorunitsbeingactivated.TheeÔ¨Äectonthepoolingunitisroughly thesameeitherway.Thisprincipleisleveragedbymaxoutnetworks(Goodfellow e t a l ., 2013a)andotherconvolutionalnetworks.Maxpoolingoverspatialpositionsisnaturally invarianttotranslation;thismulti-channelapproachisonlynecessaryforlearningother transformations. 0. 1 1. 0. 21. 0. 2 0. 10. 1 0. 0 0. 1 Figure9.10: P o o l i n g w i t h d o w n s a m p l</div>
        </div>
    </div>

    <div class="question-card" id="q105">
        <div class="question-header">
            <span class="question-number">Question 105</span>
            <span class="question-type">multiple-choice</span>
        </div>

        <div class="question-text">Mixture models are widely used in statistics and machine learning to represent complex data distributions by combining simpler component distributions. The Gaussian mixture model is a fundamental example, utilizing normal distributions and introducing latent variables to express subpopulation membership.

Which statement best describes the role of latent variables in Gaussian mixture models?

1) They determine the covariance matrix for each Gaussian component.   
2) They provide the observed value for each data point in the model.   
3) They assign prior probabilities to each Gaussian component.   
4) They directly specify the mean of each component distribution.   
5) They encode the identity of the component from which each data point is drawn.   
6) They transform the mixture model into a discrete probability distribution.   
7) They ensure that all mixing coefficients are equal across components.</div>

        <div class="answer-section">
            <div class="answer-label">‚úì Correct Answer:</div>
            <div class="answer-text">The correct answer is 5) They encode the identity of the component from which each data point is drawn..</div>
        </div>

        <div class="reference-section">
            <div class="reference-label">üìö Reference Text:</div>
            <button class="toggle-button" onclick="toggleReference(105)">
                Show/Hide Reference
            </button>
            <div id="ref105" class="reference-text hidden">CHAPTER3.PROBABILITYANDINFORMATIONTHEORY mathematical objectcalleda g e ner al i z e d f unc t i o nthatisdeÔ¨Ånedintermsofits propertieswhenintegrated.WecanthinkoftheDiracdeltafunctionasbeingthe limitpointofaseriesoffunctionsthatputlessandlessmassonallpointsother thanzero. BydeÔ¨Åningp(x)tobeŒ¥shiftedby‚àí¬µweobtainaninÔ¨Ånitelynarrowand inÔ¨Ånitelyhighpeakofprobabilitymasswhere.x¬µ= AcommonuseoftheDiracdeltadistributionisasacomponentofan e m pi r i c a l di st r i but i o n, ÀÜp() = x1 mmÓÅò i = 1Œ¥( x x‚àí( ) i) (3.28) whichputsprobabilitymass1 moneachofthempoints x( 1 ),..., x( ) mforminga givendatasetorcollectionofsamples.TheDiracdeltadistributionisonlynecessary todeÔ¨Ånetheempiricaldistributionovercontinuousvariables.Fordiscretevariables, thesituationissimpler:anempiricaldistributioncanbeconceptualized asa multinoullidistribution,withaprobabilityassociatedtoeachpossibleinputvalue thatissimplyequaltothe e m pi r i c a l f r e q uenc yofthatvalueinthetrainingset. Wecanviewtheempiricaldistributionformedfromadatasetoftraining examplesasspecifyingthedistributionthatwesamplefromwhenwetrainamodel onthisdataset. Anotherimportantperspectiveontheempiricaldistributionis thatitistheprobabilitydensitythatmaximizesthelikelihoodofthetrainingdata (seesection).5.5 3.9.6MixturesofDistributions ItisalsocommontodeÔ¨Åneprobabilitydistributionsbycombiningothersimpler probabilitydistributions.Onecommon wayof combining distributionsis to constructa m i x t ur e di st r i but i o n.Amixturedistributionismadeupofseveral componentdistributions.Oneachtrial,thechoiceofwhichcomponentdistribution generatesthesampleisdeterminedbysamplingacomponentidentityfroma multinoullidistribution: P() =xÓÅò iPiPi (= c )( = xc| ) (3.29) wherecisthemultinoullidistributionovercomponentidentities. P() Wehavealreadyseenoneexampleofamixturedistribution:theempirical distributionoverreal-valuedvariablesisamixturedistributionwithoneDirac componentforeachtrainingexample. 66 CHAPTER3.PROBABILITYANDINFORMATIONTHEORY Themixturemodelisonesimplestrategyforcombiningprobabilitydistributions tocreatearicherdistribution.Inchapter,weexploretheartofbuildingcomplex 16 probabilitydistributionsfromsimpleonesinmoredetail. ThemixturemodelallowsustobrieÔ¨Çyglimpseaconceptthatwillbeof paramountimportancelater‚Äîthe l at e n t v ar i abl e.Alatentvariableisarandom variablethatwecannotobservedirectly.Thecomponentidentityvariablecofthe mixturemodelprovidesanexample.Latentvariablesmayberelatedtoxthrough thejointdistribution,inthiscase,P(xc,) =P(xc|)P(c).ThedistributionP(c) overthelatentvariableandthedistributionP(xc|)relatingthelatentvariables tothevisiblevariablesdeterminestheshapeofthedistributionP(x)eventhough itispossibletodescribeP(x)withoutreferencetothelatentvariable.Latent variablesarediscussedfurtherinsection.16.5 Averypowerfulandcommontypeofmixturemodelisthe G aussian m i x t ur e model,inwhichthecomponentsp( x|c=i)areGaussians.Eachcomponenthas aseparatelyparametrized mean ¬µ( ) iandcovariance Œ£( ) i.Somemixturescanhave moreconstraints.Forexample,thecovariancescouldbesharedacrosscomponents viatheconstraint Œ£( ) i= Œ£,i‚àÄ.AswithasingleGaussiandistribution,themixture ofGaussiansmightconstrainthecovariancematrixforeachcomponenttobe diagonalorisotropic. Inadditiontothemeansandcovariances,theparametersofaGaussianmixture specifythe pr i o r pr o babili t yŒ± i=P(c=i) giventoeachcomponenti.Theword ‚Äúprior‚Äùindicatesthatitexpressesthemodel‚Äôsbeliefsaboutc b e f o r eithasobserved x.Bycomparison,P(c| x)isa p o st e r i o r pr o babili t y,becauseitiscomputed a f t e robservationof x.AGaussianmixturemodelisa uni v e r sal appr o x i m a t o r ofdensities,inthesensethatanysmoothdensitycanbeapproximatedwithany speciÔ¨Åc,non-zeroamountoferrorbyaGaussianmixturemodelwithenough components. FigureshowssamplesfromaGaussianmixturemodel. 3.2 3.10UsefulPropertiesofCommonFunctions Certainfunctionsariseoftenwhileworkingwithprobabilitydistributions,especially theprobabilitydistributionsusedindeeplearningmodels. Oneofthesefunctionsisthe : l o g i st i c si g m o i d œÉx() =1 1+exp()‚àíx. (3.30) ThelogisticsigmoidiscommonlyusedtoproducetheœÜparameterofaBernoulli 67 CHAPTER3.PROBABILITYANDINFORMATIONTHEORY x 1x 2 Figure3.2: SamplesfromaGaussianmixturemodel.Inthisexample,therearethree components.Fromlefttoright,theÔ¨Årstcomponenthasanisotropiccovariancematrix, meaningithasthesameamountofvarianceineachdirection.Thesecondhasadiagonal covariancematrix,meaningitcancontrolthevarianceseparatelyalongeachaxis-aligned direction.Thisexamplehasmorevariancealongthex 2axisthanalongthex 1axis.The thirdcomponenthasafull-rankcovariancematrix,allowingittocontrolthevariance separatelyalonganarbitrarybasisofdirections. distributionbecauseitsrangeis(0,1),whichlieswithinthevalidrangeofvalues fortheœÜparameter.SeeÔ¨Ågureforagraphofthesigmoidfunction.The 3.3 sigmoidfunction sat ur at e swhenitsargumentisverypositiveorverynegative, meaningthatthefunctionbecomesveryÔ¨Çatandinsensitivetosmallchangesinits input. Anothercommonlyencounteredfunctionisthe sof t pl usfunction(,Dugas e t a l . 2001): Œ∂x x. () = log(1+exp()) (3.31) ThesoftplusfunctioncanbeusefulforproducingtheŒ≤orœÉparameterofanormal distributionbecauseitsrangeis(0,‚àû).Italsoarisescommonlywhenmanipulating expressionsinvolvingsigmoids.Thenameofthesoftplusfunctioncomesfromthe factthatitisasmoothedor‚Äúsoftened‚Äùversionof x+= max(0),x. (3.32) SeeÔ¨Ågureforagraphofthesoftplusfunction. 3.4 Thefollowingpropertiesareallusefulenoughthatyoumaywishtomemorize them: 68 CHAPTER3.PROBABILITYANDINFORMATIONTHEORY ‚àí ‚àí 1 0 5 0 5 1 0 x0 0 .0 2 .0 4 .0 6 .0 8 .1 0 .œÉ x ( ) Figure3.3:Thelogisticsigmoidfunction. ‚àí ‚àí 1 0 5 0 5 1 0 x024681 0Œ∂ x ( ) Figure3.4:Thesoftplusfunction. 69 CHAPTER3.PROBABILITYANDINFORMATIONTHEORY œÉx() =exp()x exp()+exp(0)x(3.33) d dxœÉxœÉxœÉx () = ()(1‚àí()) (3.34) 1 () = () ‚àíœÉxœÉ‚àíx (3.35) log() = () œÉx ‚àíŒ∂‚àíx (3.36) d dxŒ∂xœÉx () = () (3.37) ‚àÄ‚ààx(01),,œÉ‚àí 1() = logxÓÄíx 1‚àíxÓÄì (3.38) ‚àÄx>,Œ∂0‚àí 1() = log(exp()1) x x‚àí (3.39) Œ∂x() =ÓÅöx ‚àí ‚àûœÉydy() (3.40) Œ∂xŒ∂xx ()‚àí(‚àí) = (3.41) ThefunctionœÉ‚àí 1(x)iscalledthe l o g i tinstatistics,butthistermismorerarely usedinmachinelearning. EquationprovidesextrajustiÔ¨Åcationforthename‚Äúsoftplus.‚ÄùThesoftplus 3.41 functionisintendedasasmoothedversionofthe p o si t i v e par tfunction,x+= max{0,x}.Thepositivepartfunctionisthecounterpartofthe negat i v e par t function,x‚àí=max{0,x‚àí}.Toobtainasmoothfunctionthatisanalogoustothe negativepart,onecanuseŒ∂(‚àíx).Justasxcanberecoveredfromitspositivepart andnegativepartviatheidentityx+‚àíx‚àí=x,itisalsopossibletorecoverx usingthesamerelationshipbetweenand,asshowninequation. Œ∂x()Œ∂x(‚àí) 3.41 3.11Bayes‚ÄôRule WeoftenÔ¨ÅndourselvesinasituationwhereweknowP(yx|)andneedtoknow P(xy|).Fortunately,ifwealsoknowP(x),wecancomputethedesiredquantity using B a y e s‚Äô r ul e: P( ) =xy|PP()x( )yx| P()y. (3.42) NotethatwhileP(y)appearsintheformula,itisusuallyfeasibletocompute P() =yÓÅê xPxPx P (y|)(),sowedonotneedtobeginwithknowledgeof()y. 70 CHAPTER3.PROBABILITYANDINFORMATIONTHEORY Bayes‚Äôruleis straightforwardto derivefrom thedeÔ¨Ånitionofconditional probability,butitisusefultoknowthenameofthisformulasincemanytexts refertoitbyname.ItisnamedaftertheReverendThomasBayes,whoÔ¨Årst discoveredaspecialcaseoftheformula.Thegeneralversionpresentedherewas independentlydiscoveredbyPierre-SimonLaplace. 3.12TechnicalDetailsofContinuousVariables Aproperformalunderstandingofcontinuousrandomvariablesandprobability densityfunctionsrequiresdevelopingprobabilitytheoryintermsofabranchof mathematics knownas m e asur e t heor y.Measuretheoryisbeyondthescopeof thistextbook,butwecanbrieÔ¨Çysketchsomeoftheissuesthatmeasuretheoryis employedtoresolve. Insection,wesawthattheprobabilityofacontinuousvector-valued 3.3.2 x lyinginsomeset Sisgivenbytheintegralofp( x)overtheset S.Somechoices ofset Scanproduceparadoxes.Forexample,itispossibletoconstructtwosets S</div>
        </div>
    </div>

    <div class="question-card" id="q106">
        <div class="question-header">
            <span class="question-number">Question 106</span>
            <span class="question-type">multiple-choice</span>
        </div>

        <div class="question-text">Numerical gradient estimation is crucial for verifying implementations and debugging in scientific computing and machine learning. Various finite difference methods and more advanced techniques offer varying degrees of accuracy and computational efficiency.

Which method allows for highly accurate numerical gradient estimation by avoiding subtractive cancellation errors through the use of complex numbers?

1) Forward finite difference   
2) Backward finite difference   
3) Centered finite difference   
4) Random projection testing   
5) Complex-step differentiation   
6) Automatic differentiation   
7) Symbolic differentiation</div>

        <div class="answer-section">
            <div class="answer-label">‚úì Correct Answer:</div>
            <div class="answer-text">The correct answer is 5) Complex-step differentiation.</div>
        </div>

        <div class="reference-section">
            <div class="reference-label">üìö Reference Text:</div>
            <button class="toggle-button" onclick="toggleReference(106)">
                Show/Hide Reference
            </button>
            <div id="ref106" class="reference-text hidden">x (+)‚àí() ÓÄè, (11.5) wecanapproximate thederivativebyusingasmall,Ô¨Ånite: ÓÄè fÓÄ∞() x‚âàf x ÓÄè f x (+)‚àí() ÓÄè. (11.6) Wecanimprovetheaccuracyoftheapproximation byusingthe c e n t e r e d di Ô¨Ä e r - e nc e: fÓÄ∞() x‚âàf x(+1 2ÓÄè f x )‚àí(‚àí1 2 ÓÄè) ÓÄè. (11.7) Theperturbationsize ÓÄèmustchosentobelargeenoughtoensurethatthepertur- bationisnotroundeddowntoomuchbyÔ¨Ånite-precisionnumericalcomputations. Usually,wewillwanttotestthegradientorJacobianofavector-valuedfunction g: Rm‚Üí Rn.Unfortunately,Ô¨ÅnitediÔ¨Äerencingonlyallowsustotakeasingle derivativeatatime.WecaneitherrunÔ¨ÅnitediÔ¨Äerencing m ntimestoevaluateall ofthepartialderivativesof g,orwecanapplythetesttoanewfunctionthatuses randomprojectionsatboththeinputandoutputof g.Forexample,wecanapply ourtestoftheimplementationofthederivativesto f( x)where f( x) = uTg( v x), where uand varerandomlychosenvectors.Computing fÓÄ∞( x)correctlyrequires beingabletoback-propagatethrough gcorrectly,yetiseÔ¨ÉcienttodowithÔ¨Ånite diÔ¨Äerencesbecause fhasonlyasingleinputandasingleoutput.Itisusually agoodideatorepeatthistestformorethanonevalueof uand vtoreduce thechancethatthetestoverlooksmistakesthatareorthogonaltotherandom projection. Ifonehasaccesstonumericalcomputationoncomplexnumbers,thenthereis averyeÔ¨Écientwaytonumericallyestimatethegradientbyusingcomplexnumbers asinputtothefunction(SquireandTrapp1998,).Themethodisbasedonthe observationthat f x i ÓÄè f x i ÓÄè f (+) = ()+ÓÄ∞()+( x O ÓÄè2) (11.8) real((+)) = ()+( f x i ÓÄè f x O ÓÄè2)imag( ,f x i ÓÄè (+) ÓÄè) = fÓÄ∞()+( x O ÓÄè2) ,(11.9) where i=‚àö ‚àí1.Unlikeinthereal-valuedcaseabove,thereisnocancellationeÔ¨Äect duetotakingthediÔ¨Äerencebetweenthevalueof fatdiÔ¨Äerentpoints.Thisallows theuseoftinyvaluesof ÓÄèlike ÓÄè= 10‚àí150,whichmakethe O( ÓÄè2)errorinsigniÔ¨Åcant forallpracticalpurposes. 4 3 9 CHAPTER11.PRACTICALMETHODOLOGY Monitorhistogramsofactivationsandgradient:Itisoftenusefultovisualize statisticsofneuralnetworkactivationsandgradients,collectedoveralargeamount oftrainingiterations(maybeoneepoch).Thepre-activationvalueofhiddenunits cantellusiftheunitssaturate,orhowoftentheydo.Forexample,forrectiÔ¨Åers, howoftenaretheyoÔ¨Ä?ArethereunitsthatarealwaysoÔ¨Ä?Fortanhunits, theaverageoftheabsolutevalueofthepre-activationstellsushowsaturated theunitis.Inadeepnetworkwherethepropagatedgradientsquicklygrowor quicklyvanish,optimization maybehampered.Finally,itisusefultocomparethe magnitudeofparametergradientstothemagnitudeoftheparametersthemselves. Assuggestedby(),wewouldlikethemagnitudeofparameterupdates Bottou2015 overaminibatchtorepresentsomethinglike1%ofthemagnitudeoftheparameter, not50%or0.001%(whichwouldmaketheparametersmovetooslowly).Itmay bethatsomegroupsofparametersaremovingatagoodpacewhileothersare stalled.Whenthedataissparse(likeinnaturallanguage),someparametersmay beveryrarelyupdated,andthisshouldbekeptinmindwhenmonitoringtheir evolution. Finally,manydeeplearningalgorithmsprovidesomesortofguaranteeabout theresultsproducedateachstep.Forexample,inpart,wewillseesomeapprox- III imateinferencealgorithmsthatworkbyusingalgebraicsolutionstooptimization problems. Typicallythesecanbedebuggedbytestingeachoftheirguarantees. SomeguaranteesthatsomeoptimizationalgorithmsoÔ¨Äerincludethattheobjective functionwillneverincreaseafteronestepofthealgorithm,thatthegradientwith respecttosomesubsetofvariableswillbezeroaftereachstepofthealgorithm, andthatthegradientwithrespecttoallvariableswillbezeroatconvergence. Usuallyduetoroundingerror,theseconditionswillnotholdexactlyinadigital computer,sothedebuggingtestshouldincludesometoleranceparameter. 11.6Example:Multi-DigitNumberRecognition Toprovideanend-to-enddescriptionofhowtoapplyourdesignmethodology inpractice,wepresentabriefaccountoftheStreetViewtranscriptionsystem, fromthepointofviewofdesigningthedeeplearningcomponents.Obviously, manyothercomponentsofthecompletesystem,suchastheStreetViewcars,the databaseinfrastructure,andsoon,wereofparamountimportance. Fromthepointofviewofthemachinelearningtask,theprocessbeganwith datacollection. The carscollectedtherawdataandhumanoperatorsprovided labels.ThetranscriptiontaskwasprecededbyasigniÔ¨Åcantamountofdataset curation,includingusingothermachinelearningtechniquestodetectthehouse 4 4 0 CHAPTER11.PRACTICALMETHODOLOGY numberspriortotranscribingthem. Thetranscriptionprojectbeganwithachoiceofperformancemetricsand desiredvaluesforthesemetrics. Animportantgeneralprincipleistotailorthe choiceofmetrictothebusinessgoalsfortheproject.Becausemapsareonlyuseful iftheyhavehighaccuracy,itwasimportanttosetahighaccuracyrequirement forthisproject. SpeciÔ¨Åcally,thegoalwastoobtainhuman-level,98%accuracy. Thislevelofaccuracymaynotalwaysbefeasibletoobtain.Inordertoreach thislevelofaccuracy,theStreetViewtranscriptionsystemsacriÔ¨Åcescoverage. Coveragethusbecamethemainperformancemetricoptimizedduringtheproject, withaccuracyheldat98%.Astheconvolutionalnetworkimproved,itbecame possibletoreducetheconÔ¨Ådencethresholdbelowwhichthenetworkrefusesto transcribetheinput,eventuallyexceedingthegoalof95%coverage. Afterchoosingquantitativegoals,thenextstepinourrecommendedmethodol- ogyistorapidlyestablishasensiblebaselinesystem.Forvisiontasks,thismeansa convolutionalnetworkwithrectiÔ¨Åedlinearunits.Thetranscriptionprojectbegan withsuchamodel.Atthetime,itwasnotcommonforaconvolutionalnetwork tooutputasequenceofpredictions.Inordertobeginwiththesimplestpossible baseline,theÔ¨Årstimplementation oftheoutputlayerofthemodelconsistedof n diÔ¨Äerentsoftmaxunitstopredictasequenceof ncharacters.Thesesoftmaxunits weretrainedexactlythesameasifthetaskwereclassiÔ¨Åcation,witheachsoftmax unittrainedindependently. OurrecommendedmethodologyistoiterativelyreÔ¨Ånethebaselineandtest whethereachchangemakesanimprovement.TheÔ¨ÅrstchangetotheStreetView transcriptionsystemwasmotivatedbyatheoreticalunderstandingofthecoverage metricandthestructureofthedata.SpeciÔ¨Åcally,thenetworkrefusestoclassify aninput xwhenevertheprobabilityoftheoutputsequence p( y x|) < tfor somethreshold t.Initially,thedeÔ¨Ånitionof p( y x|)wasad-hoc,basedonsimply multiplyingallofthesoftmaxoutputstogether.Thismotivatedthedevelopment ofaspecializedoutputlayerandcostfunctionthatactuallycomputedaprincipled log-likelihood.Thisapproachallowedtheexamplerejectionmechanismtofunction muchmoreeÔ¨Äectively. Atthispoint,coveragewasstillbelow90%,yettherewerenoobvioustheoretical problemswiththeapproach.Ourmethodologythereforesuggeststoinstrument thetrainandtestsetperformanceinordertodeterminewhethertheproblem isunderÔ¨ÅttingoroverÔ¨Åtting.Inthiscase,trainandtestseterrorwerenearly identical.Indeed,themainreasonthisprojectproceededsosmoothlywasthe availabilityofadatasetwithtensofmillionsoflabeledexamples.Becausetrain andtestseterrorweresosimilar,thissuggestedthattheproblemwaseitherdue 4 4 1 CHAPTER11.PRACTICALMETHODOLOGY tounderÔ¨Åttingorduetoaproblemwiththetrainingdata.Oneofthedebugging strategieswerecommendistovisualizethemodel‚Äôsworsterrors.Inthiscase,that meantvisualizingtheincorrecttrainingsettranscriptionsthatthemodelgavethe highestconÔ¨Ådence.Theseprovedtomostlyconsistofexampleswheretheinput imagehadbeencroppedtootightly,withsomeofthedigitsoftheaddressbeing removedbythecroppingoperation.Forexample,aphotoofanaddress‚Äú1849‚Äù mightbecroppedtootightly,withonlythe‚Äú849‚Äùremainingvisible.Thisproblem couldhavebeenresolvedbyspendingweeksimprovingtheaccuracyoftheaddress numberdetectionsystemresponsiblefordeterminingthecroppingregions.Instead, theteamtookamuchmorepracticaldecision,tosimplyexpandthewidthofthe cropregiontobesystematicallywiderthantheaddressnumberdetectionsystem predicted.Thissinglechangeaddedtenpercentagepointstothetranscription system‚Äôscoverage. Finally,thelastfewpercentagepointsofperformancecamefromadjusting hyperparameters.Thismostlyconsistedofmakingthemodellargerwhilemain- tainingsomerestrictionsonitscomputational cost.Becausetrainandtesterror remainedroughlyequal,itwasalwaysclearthatanyperformancedeÔ¨Åcitsweredue tounderÔ¨Åtting, aswellasduetoafewremainingproblemswiththedatasetitself. Overall,thetranscriptionprojectwasagreatsuccess,andallowedhundredsof millionsofaddressestobetranscribedbothfasterandatlowercostthanwould havebeenpossibleviahumaneÔ¨Äort. Wehopethatthedesignprinciplesdescribedinthischapterwillleadtomany othersimilarsuccesses. 4 4 2 C h a p t e r 1 2 A p p l i cat i on s Inthischapter,wedescribehowtousedeeplearningtosolveapplicationsincom- putervision,speechrecognition,naturallanguageprocessing,andotherapplication areasofcommercialinterest.Webeginbydiscussingthelargescaleneuralnetwork implementationsrequiredformostseriousAIapplications.Next,wereviewseveral speciÔ¨Åcapplicationareasthatdeeplearninghasbeenusedtosolve. Whileone goalofdeeplearningistodesignalgorithmsthatarecapableofsolvingabroad varietyoftasks,sofarsomedegreeofspecializationisneeded.Forexample,vision tasksrequireprocessingalargenumberofinputfeatures(pixels)perexample. Languagetasksrequiremodelingalargenumberofpossiblevalues(wordsinthe vocabulary)perinputfeature. 12. 1 L arge- S c a l e D eep L earni n g Deeplearningisbasedonthephilosophyofconnectionism: whileanindividual biologicalneuronoranindividualfeatureinamachinelearningmodelisnot intelligent,alargepopulationoftheseneuronsorfeaturesactingtogethercan exhibitintelligentbehavior.Ittrulyisimportanttoemphasizethefactthatthe numberofneuronsmustbe l a r g e.Oneofthekeyfactorsresponsibleforthe improvementinneuralnetwork‚Äôsaccuracyandtheimprovementofthecomplexity oftaskstheycansolvebetweenthe1980sandtodayisthedramaticincreasein thesizeofthenetworksweuse.Aswesawinsection,networksizeshave 1.2.3 grownexponentiallyforthepastthreedecades,yetartiÔ¨Åcialneuralnetworksare onlyaslargeasthenervoussystemsofinsects. Becausethesizeofneuralnetworksisofparamountimportance,deeplearning 443 CHAPTER12.APPLICATIONS requireshighperformancehardwareandsoftwareinfrastructure. 12.1.1FastCPUImplementations Traditionally,neuralnetworksweretrainedusingtheCPUofasinglemachine. Today,thisapproachisgenerallyconsideredinsuÔ¨Écient.WenowmostlyuseGPU computingortheCPUsofmanymachinesnetworkedtogether.Beforemovingto theseexpensivesetups,researchersworkedhardtodemonstratethatCPUscould notmanagethehighcomputational workloadrequiredbyneuralnetworks. AdescriptionofhowtoimplementeÔ¨ÉcientnumericalCPUcodeisbeyond thescopeofthisbook,butweemphasizeherethatcarefulimplementation for speciÔ¨ÅcCPUfamiliescanyieldlargeimprovements.Forexample,in2011,thebest CPUsavailablecouldrunneuralnetworkworkloadsfasterwhenusingÔ¨Åxed-point arithmeticratherthanÔ¨Çoating-pointarithmetic.BycreatingacarefullytunedÔ¨Åxed- pointimplementation,Vanhoucke2011 e t a l .()obtainedathreefoldspeedupover astrongÔ¨Çoating-pointsystem.EachnewmodelofCPUhasdiÔ¨Äerentperformance characteristics,sosometimesÔ¨Çoating-pointimplementations canbefastertoo. Theimportantprincipleisthatcarefulspecializationofnumericalcomputation routinescanyieldalargepayoÔ¨Ä.Otherstrategies,besideschoosingwhethertouse Ô¨ÅxedorÔ¨Çoatingpoint,includeoptimizingdatastructurestoavoidcachemisses andusingvectorinstructions.Manymachinelearningresearchersneglectthese implementationdetails,butwhentheperformanceofanimplementation restricts thesizeofthemodel,theaccuracyofthemodelsuÔ¨Äers. 12.1.2GPUImplementations Mostmodernneuralnetworkimplementationsarebasedongraphicsprocessing units.Graphicsprocessingunits(GPUs)arespecializedhardwarecomponents thatwereoriginallydevelopedforgraphicsapplications.Theconsumermarketfor videogamingsystemsspurreddevelopmentofgraphicsprocessinghardware.The performancecharacteristicsneededforgoodvideogamingsystemsturnouttobe beneÔ¨Åcialforneuralnetworksaswell. Videogamerenderingrequiresperformingmanyoperationsinparallelquickly. Modelsof characters and environments arespeciÔ¨Åed intermsof listsof 3-D coordinatesofvertices.Graphicscardsmustperformmatrixmultiplication and divisiononmanyverticesinparalleltoconvertthese3-Dcoordinatesinto2-D on-screencoordinates.Thegraphicscardmustthenperformmanycomputations ateachpixelinparalleltodeterminethecolorofeachpixel. Inbothcases,the 4 4 4 CHAPTER12.APPLICATIONS computations arefairlysimpleanddonotinvolvemuchbranchingcomparedto thecomputational workloadthataCPUusuallyencounters.Forexample,each vertexinthesamerigidobjectwillbemultipliedbythesamematrix;thereisno needtoevaluateanifstatementper-vertextodeterminewhichmatrixtomultiply by.Thecomputations arealsoentirelyindependentofeachother,andthusmay beparallelizedeasily.Thecomputations alsoinvolveprocessingmassivebuÔ¨Äersof memory,containingbitmapsdescribingthetexture(colorpattern)ofeachobject toberendered.Together,thisresultsingraphicscardshavingbeendesignedto haveahighdegreeofparallelismandhighmemorybandwidth,atthecostof havingalowerclockspeedandlessbranchingcapabilityrelativetotraditional CPUs. Neuralnetworkalgorithmsrequirethesameperformancecharacteristicsasthe real-timegraphicsalgorithmsdescribedabove.Neuralnetworksusuallyinvolve largeandnumerousbuÔ¨Äersofparameters,activationvalues,andgradientvalues, eachofwhichmustbecompletelyupdatedduringeverystepoftraining.These buÔ¨Äersarelargeenoughtofalloutsidethecacheofatraditionaldesktopcomputer sothememorybandwidthofthesystemoftenbecomestheratelimitingfactor. GPUsoÔ¨ÄeracompellingadvantageoverCPUsduetotheirhighmemorybandwidth. Neuralnetworktrainingalgorithmstypicallydonotinvolvemuchbranchingor sophisticatedcontrol,sotheyareappropriateforGPUhardware.Sinceneural networkscanbedividedintomultipleindividual‚Äúneurons‚Äùthatcanbeprocessed independentlyfromtheotherneuronsinthesamelayer,neuralnetworkseasily beneÔ¨ÅtfromtheparallelismofGPUcomputing. GPUhardwarewasoriginallysospecializedthatitcouldonlybeusedfor graphicstasks.Overtime,GPUhardwarebecamemoreÔ¨Çexible,allowingcustom subroutinestobeusedtotransformthecoordinatesofverticesorassigncolorsto pixels.Inprinciple,therewasnorequirementthatthesepixelvaluesactuallybe basedonarenderingtask.TheseGPUscouldbeusedforscientiÔ¨Åccomputingby writingtheoutputofacomputationtoabuÔ¨Äerofpixelvalues.Steinkrau e t a l . ()implemented atwo-layerfullyconnectedneuralnetworkonaGPUand 2005 reportedathreefoldspeedupovertheirCPU-basedbaseline.Shortlythereafter, Chellapilla 2006 e t a l .()demonstratedthatthesametechniquecouldbeusedto acceleratesupervisedconvolutionalnetworks. Thepopularityofgraphicscardsforneuralnetworktrainingexplodedafter theadventofgeneralpurposeGPUs.TheseGP-GPUscouldexecutearbitrary code,notjustrenderingsubroutines. NVIDIA‚ÄôsCUDAprogramming language providedawaytowritethisarbitrarycodeinaC-likelanguage.Withtheir relativelyconvenientprogramming model,massiveparallelism,andhighmemory 4 4 5</div>
        </div>
    </div>

    <div class="question-card" id="q107">
        <div class="question-header">
            <span class="question-number">Question 107</span>
            <span class="question-type">multiple-choice</span>
        </div>

        <div class="question-text">Variational inference is a key technique in probabilistic modeling that approximates complex posterior distributions, often using tractable families such as Gaussians. Recent advances focus on learned inference strategies that use neural networks to improve scalability and accuracy in generative modeling.

Which statement best describes the "self-fulfilling prophecy" effect observed when training models with unimodal approximate posteriors in variational learning?

1) The training procedure prevents the true posterior from changing its shape and keeps it multimodal.   
2) Variational learning always ensures that the true posterior matches the real data distribution regardless of the approximation.   
3) Training with unimodal approximate posteriors leads the true posterior to remain multimodal and complex.   
4) The use of multimodal approximate posteriors always forces the true posterior to become unimodal.   
5) The learning process can cause the true posterior to become more unimodal, even if the data supports multiple modes.   
6) The variational gap is reduced by assuming the true posterior is always Gaussian during training.   
7) Using mean-field variational inference increases the number of modes in the true posterior.</div>

        <div class="answer-section">
            <div class="answer-label">‚úì Correct Answer:</div>
            <div class="answer-text">The correct answer is 5) The learning process can cause the true posterior to become more unimodal, even if the data supports multiple modes..</div>
        </div>

        <div class="reference-section">
            <div class="reference-label">üìö Reference Text:</div>
            <button class="toggle-button" onclick="toggleReference(107)">
                Show/Hide Reference
            </button>
            <div id="ref107" class="reference-text hidden">v h)ÓÄÅ (19.63) =expÓÄí ‚àí1 2Eh 2‚àº q ( h 2| v )ÓÄÇ h2 1+ h2 2+ v2+ h2 1 w2 1+ h2 2 w2 2(19.64) ‚àí2 v h 1 w 1‚àí2 v h 2 w 2+2 h 1 w 1 h 2 w 2]ÓÄì . (19.65) Fromthis,wecanseethatthereareeÔ¨Äectivelyonlytwovaluesweneedtoobtain from q( h 2| v): E h 2‚àº | q ( h v )[ h 2]and E h 2‚àº | q ( h v )[ h2 2]. WritingtheseasÓÅ® h 2ÓÅ©andÓÅ® h2 2ÓÅ©, weobtain Àú q h( 1| v) = expÓÄí ‚àí1 2ÓÄÇ h2 1+ÓÅ® h2 2ÓÅ©+ v2+ h2 1 w2 1+ÓÅ® h2 2ÓÅ© w2 2(19.66) ‚àí2 v h 1 w 1‚àíÓÅ®2 v h 2ÓÅ© w 2+2 h 1 w 1ÓÅ® h 2ÓÅ© w 2]ÓÄì .(19.67) Fromthis,wecanseethatÀú qhasthefunctionalformofaGaussian.Wecan thusconclude q( h v|)=N( h; ¬µ Œ≤ ,‚àí 1)where ¬µanddiagonal Œ≤arevariational parametersthatwecanoptimizeusinganytechniquewechoose.Itisimportant torecallthatwedidnoteverassumethat qwouldbeGaussian;itsGaussian formwasderivedautomatically byusingcalculusofvariationstomaximize qwith 6 4 9 CHAPTER19.APPROXIMATEINFERENCE respecttoL.UsingthesameapproachonadiÔ¨ÄerentmodelcouldyieldadiÔ¨Äerent functionalformof. q Thiswasofcourse,justasmallcaseconstructedfordemonstrationpurposes. Forexamplesofrealapplicationsofvariationallearningwithcontinuousvariables inthecontextofdeeplearning,see (). Goodfellow e t a l .2013d 19.4.4InteractionsbetweenLearningandInference Usingapproximate inferenceaspartofalearningalgorithmaÔ¨Äectsthelearning process,andthisinturnaÔ¨Äectstheaccuracyoftheinferencealgorithm. SpeciÔ¨Åcally,thetrainingalgorithmtendstoadaptthemodelinawaythatmakes theapproximating assumptionsunderlyingtheapproximate inferencealgorithm becomemoretrue.Whentrainingtheparameters,variationallearningincreases E h‚àº qlog( ) p v h , . (19.68) ForaspeciÔ¨Åc v,thisincreases p( h v|)forvaluesof hthathavehighprobability under q( h v|)anddecreases p( h v|)forvaluesof hthathavelowprobability under . q( ) h v| Thisbehaviorcausesourapproximating assumptionstobecomeself-fulÔ¨Ålling prophecies.Ifwetrainthemodelwithaunimodalapproximate posterior,wewill obtainamodelwithatrueposteriorthatisfarclosertounimodalthanwewould haveobtainedbytrainingthemodelwithexactinference. Computingthetrueamountofharmimposedonamodelbyavariational approximationisthusverydiÔ¨Écult.Thereexistseveralmethodsforestimating log p( v).Weoftenestimate log p( v; Œ∏)aftertrainingthemodel,andÔ¨Åndthat thegapwith L( v Œ∏ , , q)issmall.Fromthis,wecanconcludethatourvariational approximationisaccurateforthespeciÔ¨Åcvalueof Œ∏thatweobtainedfromthe learningprocess.Weshouldnotconcludethatourvariationalapproximation is accurateingeneralorthatthevariationalapproximation didlittleharmtothe learningprocess.Tomeasurethetrueamountofharminducedbythevariational approximation,wewouldneedtoknow Œ∏‚àó=max Œ∏log p( v; Œ∏). Itispossiblefor L( v Œ∏ , , q)‚âàlog p( v; Œ∏)andlog p( v; Œ∏)ÓÄúlog p( v; Œ∏‚àó)toholdsimultaneously.If max qL( v Œ∏ ,‚àó, q)ÓÄúlog p( v; Œ∏‚àó),because Œ∏‚àóinducestoocomplicatedofaposterior distributionforour qfamilytocapture, then thelearningprocesswillnever approach Œ∏‚àó.SuchaproblemisverydiÔ¨Éculttodetect,becausewecanonlyknow forsurethatithappenedifwehaveasuperiorlearningalgorithmthatcanÔ¨Ånd Œ∏‚àó forcomparison. 6 5 0 CHAPTER19.APPROXIMATEINFERENCE 19.5LearnedApproximateInference Wehaveseenthatinferencecanbethoughtofasanoptimization procedure thatincreasesthevalueofafunction L.Explicitlyperformingoptimization via iterativeproceduressuchasÔ¨Åxedpointequationsorgradient-basedoptimization isoftenveryexpensiveandtime-consuming. Manyapproachestoinferenceavoid thisexpensebylearningtoperformapproximateinference. SpeciÔ¨Åcally ,wecan thinkoftheoptimization processasafunction fthatmapsaninput vtoan approximatedistribution q‚àó=argmaxqL( v , q).Oncewethinkofthemulti-step iterativeoptimization processasjustbeingafunction,wecanapproximateitwith aneuralnetworkthatimplementsanapproximation ÀÜ f(;) v Œ∏. 19.5.1Wake-Sleep OneofthemaindiÔ¨Écultieswithtrainingamodeltoinfer hfrom visthatwe donothaveasupervisedtrainingsetwithwhichtotrainthemodel.Givena v, wedonotknowtheappropriate h.Themappingfrom vto hdependsonthe choiceofmodelfamily,andevolvesthroughoutthelearningprocessas Œ∏changes. Thewake-sleepalgorithm(Hinton1995bFrey1996 e t a l .,; e t a l .,)resolvesthis problembydrawingsamplesofboth hand vfromthemodeldistribution. For example,inadirectedmodel,thiscanbedonecheaplybyperformingancestral samplingbeginningat handendingat v.Theinferencenetworkcanthenbe trainedtoperformthereversemapping: predicting which hcausedthepresent v.Themaindrawbacktothisapproachisthatwewillonlybeabletotrainthe inferencenetworkonvaluesof vthathavehighprobabilityunderthemodel.Early inlearning,themodeldistributionwillnotresemblethedatadistribution,sothe inferencenetworkwillnothaveanopportunitytolearnonsamplesthatresemble data. Insectionwesawthatonepossibleexplanationfortheroleofdreamsleep 18.2 inhumanbeingsandanimalsisthatdreamscouldprovidethenegativephase samplesthatMonteCarlotrainingalgorithmsusetoapproximatethenegative gradientofthelogpartitionfunctionofundirectedmodels.Anotherpossible explanationforbiologicaldreamingisthatitisprovidingsamplesfrom p( h v ,) whichcanbeusedtotrainaninferencenetworktopredict hgiven v.Insome senses,thisexplanationismoresatisfyingthanthepartitionfunctionexplanation. MonteCarloalgorithmsgenerallydonotperformwelliftheyarerunusingonly thepositivephaseofthegradientforseveralstepsthenwithonlythenegative phaseofthegradientforseveralsteps.Humanbeingsandanimalsareusually awakeforseveralconsecutivehoursthenasleepforseveralconsecutivehours.Itis 6 5 1 CHAPTER19.APPROXIMATEINFERENCE notreadilyapparenthowthisschedulecouldsupportMonteCarlotrainingofan undirectedmodel.Learningalgorithmsbasedonmaximizing Lcanberunwith prolongedperiodsofimproving qandprolongedperiodsofimproving Œ∏,however. Iftheroleofbiologicaldreamingistotrainnetworksforpredicting q,thenthis explainshowanimalsareabletoremainawakeforseveralhours(thelongerthey areawake,thegreaterthegapbetweenLandlog p( v),butLwillremainalower bound)andtoremainasleepforseveralhours(thegenerativemodelitselfisnot modiÔ¨Åedduringsleep)withoutdamagingtheirinternalmodels.Ofcourse,these ideasarepurelyspeculative,andthereisnohardevidencetosuggestthatdreaming accomplisheseitherofthesegoals.Dreamingmayalsoservereinforcementlearning ratherthanprobabilisticmodeling,bysamplingsyntheticexperiencesfromthe animal‚Äôstransitionmodel,onwhichtotraintheanimal‚Äôspolicy.Orsleepmay servesomeotherpurposenotyetanticipatedbythemachinelearningcommunity. 19.5.2OtherFormsofLearnedInference Thisstrategyoflearnedapproximateinferencehasalsobeenappliedtoother models.SalakhutdinovandLarochelle2010()showedthatasinglepassina learnedinferencenetworkcouldyieldfasterinferencethaniteratingthemeanÔ¨Åeld Ô¨ÅxedpointequationsinaDBM.Thetrainingprocedureisbasedonrunningthe inferencenetwork,thenapplyingonestepofmeanÔ¨Åeldtoimproveitsestimates, andtrainingtheinferencenetworktooutputthisreÔ¨Ånedestimateinsteadofits originalestimate. Wehavealreadyseeninsectionthatthepredictivesparsedecomposition 14.8 modeltrainsashallowencodernetworktopredictasparsecodefortheinput. Thiscanbeseenasahybridbetweenanautoencoderandsparsecoding.Itis possibletodeviseprobabilisticsemanticsforthemodel,underwhichtheencoder maybeviewedasperforminglearnedapproximate MAPinference.Duetoits shallowencoder,PSDisnotabletoimplementthekindofcompetitionbetween unitsthatwehaveseeninmeanÔ¨Åeldinference.However,thatproblemcanbe remediedbytrainingadeepencodertoperformlearnedapproximateinference,as intheISTAtechnique( ,). GregorandLeCun2010b Learned approximate inference hasrecently become one of the dominant approachestogenerativemodeling,intheformofthevariationalautoencoder (,; ,).Inthiselegantapproach,thereisnoneedto Kingma2013Rezende e t a l .2014 constructexplicittargetsfortheinferencenetwork.Instead,theinferencenetwork issimplyusedtodeÔ¨Åne L,andthentheparametersoftheinferencenetworkare adaptedtoincrease.Thismodelisdescribedindepthlater,insection. L 20.10.3 6 5 2 CHAPTER19.APPROXIMATEINFERENCE Usingapproximateinference,itispossibletotrainanduseawidevarietyof models.Manyofthesemodelsaredescribedinthenextchapter. 6 5 3 C h a p t e r 2 0 D e e p Ge n e rat i v e Mo d e l s Inthischapter,wepresentseveralofthespeciÔ¨Åckindsofgenerativemodelsthat canbebuiltandtrainedusingthetechniquespresentedinchapters‚Äì.Allof1619 thesemodelsrepresentprobabilitydistributionsovermultiplevariablesinsome way.Someallowtheprobabilitydistributionfunctiontobeevaluatedexplicitly. Othersdonotallowtheevaluationoftheprobabilitydistributionfunction,but supportoperationsthatimplicitlyrequireknowledgeofit,suchasdrawingsamples fromthedistribution.Someofthesemodelsarestructuredprobabilisticmodels describedintermsofgraphsandfactors,usingthelanguageofgraphicalmodels presentedinchapter. Otherscannoteasilybedescribedintermsoffactors, 16 butrepresentprobabilitydistributionsnonetheless. 20.1BoltzmannMachines Boltzmannmachineswereoriginallyintroducedasageneral‚Äúconnectionist‚Äù ap- proachtolearningarbitraryprobabilitydistributionsoverbinaryvectors(Fahlman e t a l .,;1983Ackley1985Hinton1984HintonandSejnowski1986 e t a l .,; e t a l .,; ,). VariantsoftheBoltzmannmachinethatincludeotherkindsofvariableshavelong agosurpassedthepopularityoftheoriginal.InthissectionwebrieÔ¨Çyintroduce thebinaryBoltzmannmachineanddiscusstheissuesthatcomeupwhentryingto trainandperforminferenceinthemodel. WedeÔ¨ÅnetheBoltzmannmachineovera d-dimensionalbinaryrandomvector x</div>
        </div>
    </div>

    <div class="question-card" id="q108">
        <div class="question-header">
            <span class="question-number">Question 108</span>
            <span class="question-type">multiple-choice</span>
        </div>

        <div class="question-text">Generative models like mean-covariance RBMs (mcRBMs) and mean-product of t-distributions (mPoT) are designed to capture complex dependencies in high-dimensional data, such as images, by introducing richer covariance structures and advanced hidden unit distributions. Training these models efficiently is crucial for practical applications in unsupervised learning and image modeling.

Which training or sampling method is specifically recommended to address the computational challenges posed by the inversion of non-diagonal covariance matrices in models like mcRBM and mPoT?

1) Gibbs sampling from the conditional distribution   
2) Hamiltonian (hybrid) Monte Carlo sampling from the marginal distribution   
3) Maximum likelihood estimation using diagonal approximations   
4) Contrastive divergence with block updates   
5) Expectation-Maximization with restricted covariance   
6) Importance sampling from the conditional distribution   
7) Variational inference using mean-field assumptions</div>

        <div class="answer-section">
            <div class="answer-label">‚úì Correct Answer:</div>
            <div class="answer-text">The correct answer is 2) Hamiltonian (hybrid) Monte Carlo sampling from the marginal distribution.</div>
        </div>

        <div class="reference-section">
            <div class="reference-label">üìö Reference Text:</div>
            <button class="toggle-button" onclick="toggleReference(108)">
                Show/Hide Reference
            </button>
            <div id="ref108" class="reference-text hidden">c) = E m( x h ,( ) m)+ E c( x h ,( ) c) ,(20.43) where E misthestandardGaussian-Bernoulli RBMenergyfunction:2 E m( x h ,( ) m) =1 2xÓÄæx‚àíÓÅò jxÓÄæW : , j h( ) m j‚àíÓÅò jb( ) m j h( ) m j ,(20.44) and E cisthecRBMenergy function that models the conditionalcovariance information: E c( x h ,( ) c) =1 2ÓÅò jh( ) c jÓÄê xÓÄær( ) jÓÄë2 ‚àíÓÅò jb( ) c j h( ) c j .(20.45) Theparameter r( ) jcorrespondstothecovarianceweightvectorassociatedwith h( ) c jand b( ) cisavectorofcovarianceoÔ¨Äsets.ThecombinedenergyfunctiondeÔ¨Ånes ajointdistribution: p m c( x h ,( ) m, h( ) c) =1 ZexpÓÅÆ ‚àí E m c( x h ,( ) m, h( ) c)ÓÅØ ,(20.46) andacorrespondingconditionaldistributionovertheobservationsgiven h( ) mand h( ) casamultivariateGaussiandistribution: p m c( x h|( ) m, h( ) c) = NÔ£´ Ô£≠ x C;m c x h|Ô£´ Ô£≠ÓÅò jW : , j h( ) m jÔ£∂ Ô£∏ , Cm c x h|Ô£∂ Ô£∏ .(20.47) Notethatthecovariancematrix Cm c x h|=ÓÄêÓÅê j h( ) c j r( ) jr( ) jÓÄæ+ IÓÄë‚àí 1 isnon-diagonal andthat WistheweightmatrixassociatedwiththeGaussianRBMmodelingthe 2Th i s v e rs i o n o f t h e Ga u s s i a n - B e rn o u l l i R B M e n e rg y f u n c t i o n a s s u m e s t h e i m a g e d a t a h a s z e ro m e a n , p e r p i x e l . P i x e l o Ô¨Ä s e t s c a n e a s i l y b e a d d e d t o t h e m o d e l t o a c c o u n t f o r n o n z e ro p i x e l m e a n s . 6 7 9 CHAPTER20.DEEPGENERATIVEMODELS conditionalmeans.ItisdiÔ¨ÉculttotrainthemcRBMviacontrastivedivergenceor persistentcontrastivedivergencebecauseofitsnon-diagonal conditionalcovariance structure.CDandPCDrequiresamplingfromthejointdistributionof x h ,( ) m, h( ) c which,inastandardRBM,isaccomplishedbyGibbssamplingovertheconditionals. However,inthemcRBM,samplingfrom p m c( x h|( ) m, h( ) c)requirescomputing ( Cm c)‚àí 1ateveryiterationoflearning.Thiscanbeanimpracticalcomputational burdenforlargerobservations. ()avoiddirectsampling RanzatoandHinton2010 fromtheconditional p m c( x h|( ) m, h( ) c)bysamplingdirectlyfromthemarginal p( x)usingHamiltonian(hybrid)MonteCarlo(,)onthemcRBMfree Neal1993 energy. Mean-ProductofStudent‚Äôs-distributions t Themean-productofStudent‚Äôs t-distribution(mPoT)model( ,)extendsthePoTmodel( Ranzato e t a l .2010b Welling e t a l .,)inamannersimilartohowthemcRBMextendsthecRBM.This 2003a isachievedbyincludingnonzeroGaussianmeansbytheadditionofGaussian RBM-likehiddenunits.LikethemcRBM,thePoTconditionaldistributionoverthe observationisamultivariateGaussian(withnon-diagonal covariance)distribution; however,unlikethemcRBM,thecomplementaryconditionaldistributionoverthe hiddenvariablesisgivenbyconditionallyindependentGammadistributions.The GammadistributionG( k , Œ∏) isaprobabilitydistributionoverpositiverealnumbers, withmean k Œ∏.Itisnotnecessarytohaveamoredetailedunderstandingofthe GammadistributiontounderstandthebasicideasunderlyingthemPoTmodel. ThemPoTenergyfunctionis: E m P o T( x h ,( ) m, h( ) c) (20.48) = E m( x h ,( ) m)+ÓÅò jÓÄí h( ) c jÓÄí 1+1 2ÓÄê r( ) jÓÄæxÓÄë2ÓÄì +(1‚àí Œ≥ j)log h( ) c jÓÄì (20.49) where r( ) jisthecovarianceweightvectorassociatedwithunit h( ) c jand E m( x h ,( ) m) isasdeÔ¨Ånedinequation.20.44 JustaswiththemcRBM,themPoTmodelenergyfunctionspeciÔ¨Åesamul- tivariateGaussian,withaconditionaldistributionover xthathasnon-diagonal covariance.LearninginthemPoTmodel‚Äîagain,likethemcRBM‚Äîiscompli- catedbytheinabilityto samplefromthenon-diagonal Gaussianconditional p m P o T( x h|( ) m, h( ) c),so ()alsoadvocatedirectsamplingof Ranzato e t a l .2010b p()</div>
        </div>
    </div>

    <div class="question-card" id="q109">
        <div class="question-header">
            <span class="question-number">Question 109</span>
            <span class="question-type">multiple-choice</span>
        </div>

        <div class="question-text">Training undirected probabilistic models often faces computational challenges due to the intractability of the partition function, especially in high-dimensional settings. Several alternative estimation methods have been developed to address these issues, each with specific applicability and limitations.

Which estimation method avoids explicit computation of the partition function by matching the gradients of the model and data log probability densities, but is restricted to models with continuous variables?

1) Generalized pseudolikelihood   
2) Score matching   
3) Ratio matching   
4) Maximum likelihood estimation   
5) Variational inference   
6) Contrastive divergence   
7) Stochastic maximum likelihood</div>

        <div class="answer-section">
            <div class="answer-label">‚úì Correct Answer:</div>
            <div class="answer-text">The correct answer is 2) Score matching.</div>
        </div>

        <div class="reference-section">
            <div class="reference-label">üìö Reference Text:</div>
            <button class="toggle-button" onclick="toggleReference(109)">
                Show/Hide Reference
            </button>
            <div id="ref109" class="reference-text hidden">pcontributionandthelog Zcontribution. Wecanthenuseanyothermethodtotacklelog Àú p(x),andjustaddournegative phasegradientontotheothermethod‚Äôsgradient.Inparticular,thismeansthat ourpositivephasecanmakeuseofmethodsthatprovideonlyalowerboundon Àú p.Mostoftheothermethodsofdealingwith log Zpresentedinthischapterare 614 CHAPTER18.CONFRONTINGTHEPARTITIONFUNCTION incompatible withbound-basedpositivephasemethods. 18.3Pseudolikelihood MonteCarloapproximations tothepartitionfunctionanditsgradientdirectly confrontthepartitionfunction.Otherapproachessidesteptheissue,bytraining themodelwithoutcomputingthepartitionfunction.Mostoftheseapproachesare basedontheobservationthatitiseasytocomputeratiosofprobabilities inan undirectedprobabilisticmodel.Thisisbecausethepartitionfunctionappearsin boththenumeratorandthedenominator oftheratioandcancelsout: p()x p()y=1 ZÀú p()x 1 ZÀú p()y=Àú p()x Àú p()y. (18.17) Thepseudolikelihoodisbasedontheobservationthatconditionalprobabilities takethisratio-basedform,andthuscanbecomputedwithoutknowledgeofthe partitionfunction.Supposethatwepartition xintoa,bandc,where acontains thevariableswewanttoÔ¨Åndtheconditionaldistributionover,bcontainsthe variableswewanttoconditionon,andccontainsthevariablesthatarenotpart ofourquery. p( ) = ab|p ,(ab) p()b=p ,(ab)ÓÅê a c , p , ,(abc)=Àú p ,(ab)ÓÅê a c ,Àú p , ,(abc).(18.18) Thisquantityrequiresmarginalizing outa,whichcanbeaveryeÔ¨Écientoperation providedthataandcdonotcontainverymanyvariables.Intheextremecase,a canbeasinglevariableandccanbeempty,makingthisoperationrequireonlyas manyevaluationsofÀú pastherearevaluesofasinglerandomvariable. Unfortunately,inordertocomputethelog-likelihood,weneedtomarginalize outlargesetsofvariables.Ifthereare nvariablestotal,wemustmarginalizeaset ofsize.Bythechainruleofprobability, n‚àí1 log() = log( px p x 1)+log( p x 2| x 1)+ +( ¬∑¬∑¬∑ p x n|x 1 : 1 n ‚àí) .(18.19) Inthiscase,wehavemade amaximallysmall,butccanbeaslargeasx 2 : n.What ifwesimplymovecintobtoreducethecomputational cost?Thisyieldsthe pseudolik e l i ho o d(,)objectivefunction,basedonpredictingthevalue Besag1975 offeature x igivenalloftheotherfeatures x ‚àí i: n ÓÅò i = 1log( p x i| x ‚àí i) . (18.20) 615 CHAPTER18.CONFRONTINGTHEPARTITIONFUNCTION Ifeachrandomvariablehas kdiÔ¨Äerentvalues,thisrequiresonly k n√óevaluations ofÀú ptocompute,asopposedtothe knevaluationsneededtocomputethepartition function. Thismaylooklikeanunprincipled hack,butitcanbeproventhatestimation bymaximizingthepseudolikelihoodisasymptoticallyconsistent(,).Mase1995 Ofcourse,inthecaseofdatasetsthatdonotapproachthelargesamplelimit, pseudolikelihoodmaydisplaydiÔ¨Äerentbehaviorfromthemaximumlikelihood estimator. Itispossibletotradecomputational complexityfordeviationfrommaximum likelihoodbehaviorbyusingthe g e ner al i z e d pseudolikel i h o o destimator(Huang andOgata2002,).Thegeneralizedpseudolikelihoodestimatoruses mdiÔ¨Äerentsets S( ) i, i= 1 , . . . , mofindicesofvariablesthatappeartogetherontheleftsideofthe conditioningbar.Intheextremecaseof m= 1and S( 1 )=1 , . . . , nthegeneralized pseudolikelihoodrecoversthelog-likelihood. Intheextremecaseof m= nand S( ) i={} i,thegeneralizedpseudolikelihoodrecoversthepseudolikelihood.The generalizedpseudolikelihoodobjectivefunctionisgivenby m ÓÅò i = 1log( pxS() i|x‚àí S() i) . (18.21) Theperformanceofpseudolikelihood-basedapproachesdependslargelyonhow themodelwillbeused.Pseudolikelihoodtendstoperformpoorlyontasksthat requireagoodmodelofthefulljoint p(x),suchasdensityestimationandsampling. However,itcanperformbetterthanmaximumlikelihoodfortasksthatrequireonly theconditionaldistributionsusedduringtraining,suchasÔ¨Ållinginsmallamounts ofmissingvalues.Generalizedpseudolikelihoodtechniquesareespeciallypowerfulif thedatahasregularstructurethatallowsthe Sindexsetstobedesignedtocapture themostimportantcorrelationswhileleavingoutgroupsofvariablesthatonly havenegligiblecorrelation.Forexample,innaturalimages,pixelsthatarewidely separatedinspacealsohaveweakcorrelation,sothegeneralizedpseudolikelihood canbeappliedwitheachsetbeingasmall,spatiallylocalizedwindow. S Oneweaknessofthepseudolikelihoodestimatoristhatitcannotbeusedwith otherapproximationsthatprovideonlyalowerboundonÀú p(x),suchasvariational inference,whichwillbecoveredinchapter.Thisisbecause 19 Àú pappearsinthe denominator. Alowerboundonthedenominator providesonlyanupperboundon theexpressionasawhole,andthereisnobeneÔ¨Åttomaximizinganupperbound. ThismakesitdiÔ¨Éculttoapplypseudolikelihoodapproachestodeepmodelssuch asdeepBoltzmannmachines,sincevariationalmethodsareoneofthedominant approachestoapproximately marginalizing outthemanylayersofhiddenvariables 616 CHAPTER18.CONFRONTINGTHEPARTITIONFUNCTION thatinteractwitheachother. However,pseudolikelihoodisstillusefulfordeep learning,becauseitcanbeusedtotrainsinglelayermodels,ordeepmodelsusing approximateinferencemethodsthatarenotbasedonlowerbounds. PseudolikelihoodhasamuchgreatercostpergradientstepthanSML,dueto itsexplicitcomputationofalloftheconditionals.However,generalizedpseudo- likelihoodandsimilarcriteriacanstillperformwellifonlyonerandomlyselected conditionaliscomputedperexample(Goodfellow2013b e t a l .,),therebybringing thecomputational costdowntomatchthatofSML. Thoughthepseudolikelihoodestimatordoesnotexplicitlyminimize log Z,it canstillbethoughtofashavingsomethingresemblinganegativephase.The denominators ofeachconditionaldistributionresultinthelearningalgorithm suppressingtheprobabilityofallstatesthathaveonlyonevariablediÔ¨Äeringfrom atrainingexample. SeeMarlinanddeFreitas2011()foratheoreticalanalysisoftheasymptotic eÔ¨Éciencyofpseudolikelihood. 18.4ScoreMatchingandRatioMatching Scorematching(,)providesanotherconsistentmeansoftraininga Hyv√§rinen2005 modelwithoutestimating Zoritsderivatives.Thenamescorematchingcomes fromterminologyinwhichthederivativesofalogdensitywithrespecttoits argument,‚àá xlog p( x),arecalledits sc o r e.Thestrategyusedbyscorematching istominimizetheexpectedsquareddiÔ¨Äerencebetweenthederivativesofthe model‚Äôslogdensitywithrespecttotheinputandthederivativesofthedata‚Äôslog densitywithrespecttotheinput: L ,( x Œ∏) =1 2||‚àá xlog p m o de l(;) x Œ∏‚àí‚àá xlog pdata() x||2 2(18.22) J() = Œ∏1 2E pdata ( ) x L ,( x Œ∏) (18.23) Œ∏‚àó= min Œ∏J() Œ∏ (18.24) ThisobjectivefunctionavoidsthediÔ¨ÉcultiesassociatedwithdiÔ¨Äerentiating thepartitionfunction Zbecause Zisnotafunctionof xandtherefore ‚àá x Z= 0. Initially,scorematchingappearstohaveanewdiÔ¨Éculty: comput ingthescore ofthedatadistributionrequiresknowledgeofthetruedistributiongenerating thetrainingdata, pdata.Fortunately,minimizingtheexpectedvalueofis L ,( x Œ∏) 617 CHAPTER18.CONFRONTINGTHEPARTITIONFUNCTION equivalenttominimizingtheexpectedvalueof Àú L ,( x Œ∏) =n ÓÅò j = 1ÓÄ† ‚àÇ2 ‚àÇ x2 jlog p m o de l(;)+ x Œ∏1 2ÓÄí‚àÇ ‚àÇ x jlog p m o de l(;) x Œ∏ÓÄì2ÓÄ° (18.25) whereisthedimensionalityof. n x Becausescorematchingrequirestakingderivativeswithrespecttox,itisnot applicabletomodelsofdiscretedata.However,thelatentvariablesinthemodel maybediscrete. Likethepseudolikelihood,scorematchingonlyworkswhenweareableto evaluate log Àú p(x)anditsderivativesdirectly.Itisnotcompatiblewithmethods thatonlyprovidealowerboundonlog Àú p(x),becausescorematchingrequires thederivativesandsecondderivativesoflog Àú p(x)andalowerboundconveysno informationaboutitsderivatives.Thismeansthatscorematchingcannotbe appliedtoestimatingmodelswithcomplicatedinteractionsbetweenthehidden units,suchassparsecodingmodelsordeepBoltzmannmachines.Whilescore matchingcanbeusedtopretraintheÔ¨Årsthiddenlayerofalargermodel,ithas notbeenappliedasapretrainingstrategyforthedeeperlayersofalargermodel. Thisisprobablybecausethehiddenlayersofsuchmodelsusuallycontainsome discretevariables. Whilescorematchingdoesnotexplicitlyhaveanegativephase,itcanbe viewedasaversionofcontrastivedivergenceusingaspeciÔ¨ÅckindofMarkovchain (,).TheMarkovchaininthiscaseisnotGibbssampling,but Hyv√§rinen2007a ratheradiÔ¨Äerentapproachthatmakeslocalmovesguidedbythegradient.Score matchingisequivalenttoCDwiththistypeofMarkovchainwhenthesizeofthe localmovesapproacheszero. Lyu2009()generalizedscorematchingtothediscretecase(butmadeanerror intheirderivationthatwascorrectedby ()). Marlin e t a l .2010Marlin e t a l . ()foundthat 2010 g e ner al i z e d sc o r e m at c hi ng(GSM)doesnotworkinhigh dimensionaldiscretespaceswheretheobservedprobabilityofmanyeventsis0. Amoresuccessfulapproachtoextendingthebasicideasofscorematching todiscretedatais r at i o m at c hi ng(,).Ratiomatchingapplies Hyv√§rinen2007b speciÔ¨Åcallytobinarydata.Ratiomatchingconsistsofminimizingtheaverageover examplesofthefollowingobjectivefunction: L( )RM() = x Œ∏ ,n ÓÅò j = 1Ô£´ Ô£≠1 1+pmodel ( ; ) x Œ∏ pmodel ( ( ) ) ; ) f x , j Œ∏Ô£∂ Ô£∏2 ,(18.26) 618 CHAPTER18.CONFRONTINGTHEPARTITIONFUNCTION where returnswiththebitatpositionÔ¨Çipped.Ratiomatchingavoids f , j( x) x j thepartitionfunctionusingthesametrickasthepseudolikelihoodestimator:ina ratiooftwoprobabilities, thepartitionfunctioncancelsout. ()</div>
        </div>
    </div>

    <div class="question-card" id="q110">
        <div class="question-header">
            <span class="question-number">Question 110</span>
            <span class="question-type">multiple-choice</span>
        </div>

        <div class="question-text">Deep learning has revolutionized automatic speech recognition (ASR) and natural language processing (NLP) by introducing advanced neural architectures and new training strategies. These innovations have led to significant performance improvements and widespread adoption in real-world applications.

Which architectural and methodological combination was key to enabling end-to-end automatic speech recognition systems that bypassed the need for hidden Markov models (HMMs)?

1) Bidirectional convolutional networks with dropout regularization   
2) Deep feedforward networks trained with supervised learning   
3) Deep LSTM recurrent neural networks using connectionist temporal classification (CTC)   
4) Shallow neural networks with Gaussian mixture model (GMM) integration   
5) Transformer-based models with external phoneme sequence prediction   
6) Restricted Boltzmann machines for unsupervised feature extraction   
7) N-gram models enhanced by word embeddings</div>

        <div class="answer-section">
            <div class="answer-label">‚úì Correct Answer:</div>
            <div class="answer-text">The correct answer is 3) Deep LSTM recurrent neural networks using connectionist temporal classification (CTC).</div>
        </div>

        <div class="reference-section">
            <div class="reference-label">üìö Reference Text:</div>
            <button class="toggle-button" onclick="toggleReference(110)">
                Show/Hide Reference
            </button>
            <div id="ref110" class="reference-text hidden">anonlinearityappliedtothehiddenlayersofanetwork,aswellasapreprocessing operationappliedtotheinput. Aswithglobalcontrastnormalization, wetypicallyneedtoregularizelocal contrastnormalization toavoiddivisionbyzero.Infact,becauselocalcontrast normalization typicallyactsonsmallerwindows,itisevenmoreimportantto regularize.Smallerwindowsaremorelikelytocontainvaluesthatareallnearly thesameaseachother,andthusmorelikelytohavezerostandarddeviation. 4 5 7 CHAPTER12.APPLICATIONS 12.2.1.2DatasetAugmentation Asdescribedinsection,itiseasytoimprovethegeneralization ofaclassiÔ¨Åer 7.4 byincreasingthesizeofthetrainingsetbyaddingextracopiesofthetraining examplesthathavebeenmodiÔ¨Åedwithtransformationsthatdonotchangethe class.ObjectrecognitionisaclassiÔ¨Åcationtaskthatisespeciallyamenableto thisform ofdataset augmentationbecause theclass isinvariant toso many transformationsandtheinputcanbeeasilytransformedwithmanygeometric operations.Asdescribedbefore,classiÔ¨ÅerscanbeneÔ¨Åtfromrandomtranslations, rotations,andinsomecases,Ô¨Çipsoftheinputtoaugmentthedataset.Inspecialized computervisionapplications,moreadvancedtransformationsarecommonlyused fordatasetaugmentation. Theseschemesincluderandomperturbationofthe colorsinanimage( ,)andnonlineargeometricdistortionsof Krizhevsky e t a l .2012 theinput( ,). LeCun e t a l .1998b 12. 3 S p eec h R ec ogn i t i o n Thetaskofspeechrecognitionistomapanacousticsignalcontainingaspoken naturallanguageutteranceintothecorrespondingsequenceofwordsintendedby thespeaker.LetX= (x(1),x(2), . . . ,x() T)denotethesequenceofacousticinput vectors(traditionallyproducedbysplittingtheaudiointo20msframes).Most speechrecognitionsystemspreprocesstheinputusingspecializedhand-designed features,butsome( ,)deeplearningsystemslearnfeatures JaitlyandHinton2011 fromrawinput.Lety= ( y1 , y2 , . . . , y N)denotethetargetoutputsequence(usually asequenceofwordsorcharacters).Theautomaticspeechrecognition(ASR) taskconsistsofcreatingafunction f‚àó ASRthatcomputesthemostprobablelinguistic sequencegiventheacousticsequence: y X f‚àó ASR() = argmaxX yP‚àó( = ) y X|X (12.4) where P‚àóisthetrueconditionaldistributionrelatingtheinputsXtothetargets y. Sincethe1980sanduntilabout2009‚Äì2012,state-of-theartspeechrecognition systemsprimarilycombinedhiddenMarkovmodels(HMMs)andGaussianmixture models(GMMs).GMMsmodeledtheassociationbetweenacousticfeaturesand phonemes(,),whileHMMsmodeledthesequenceofphonemes. Bahl e t a l .1987 TheGMM-HMM modelfamilytreats acousticwaveformsasbeinggenerated bythefollowingprocess: Ô¨ÅrstanHMMgeneratesasequenceofphonemesand discretesub-phonemicstates(suchasthebeginning,middle,andendofeach 4 5 8 CHAPTER12.APPLICATIONS phoneme),thenaGMMtransformseachdiscretesymbolintoabriefsegmentof audiowaveform.AlthoughGMM-HMMsystemsdominatedASRuntilrecently, speechrecognitionwasactuallyoneoftheÔ¨Årstareaswhereneuralnetworkswere applied,andnumerousASRsystemsfromthelate1980sandearly1990sused neuralnets(BourlardandWellekens1989Waibel1989Robinsonand ,; e t a l .,; Fallside1991Bengio19911992Konig 1996 ,; e t a l .,,; e t a l .,).Atthetime,the performanceofASRbasedonneuralnetsapproximately matchedtheperformance ofGMM-HMMsystems.Forexample,RobinsonandFallside1991()achieved 26%phonemeerrorrateontheTIMIT( ,)corpus(with39 Garofolo e t a l .1993 phonemestodiscriminatebetween), whichwasbetterthanorcomparableto HMM-basedsystems.Sincethen,TIMIThasbeenabenchmarkforphoneme recognition,playingarolesimilartotheroleMNISTplaysforobjectrecognition. However,becauseofthecomplexengineeringinvolvedinsoftwaresystemsfor speechrecognitionandtheeÔ¨Äortthathadbeeninvestedinbuildingthesesystems onthebasisofGMM-HMMs,theindustrydidnotseeacompellingargument forswitchingtoneuralnetworks.Asaconsequence,untilthelate2000s,both academicandindustrialresearchinusingneuralnetsforspeechrecognitionmostly focusedonusingneuralnetstolearnextrafeaturesforGMM-HMMsystems. Later,with m u c h l a r g e r a nd d e e p e r m o d e l sandmuchlargerdatasets,recognition accuracywasdramatically improvedbyusingneuralnetworkstoreplaceGMMs forthetaskofassociatingacousticfeaturestophonemes(orsub-phonemicstates). Startingin2009,speechresearchersappliedaformofdeeplearningbasedon unsupervisedlearningtospeechrecognition.Thisapproachtodeeplearningwas basedontrainingundirectedprobabilisticmodelscalledrestrictedBoltzmann machines(RBMs)tomodeltheinputdata.RBMswillbedescribedinpart.III Tosolvespeechrecognitiontasks,unsupervisedpretrainingwasusedtobuild deepfeedforwardnetworkswhoselayerswereeachinitializedbytraininganRBM. ThesenetworkstakespectralacousticrepresentationsinaÔ¨Åxed-sizeinputwindow (aroundacenterframe)andpredicttheconditionalprobabilities ofHMMstates forthatcenterframe.TrainingsuchdeepnetworkshelpedtosigniÔ¨Åcantlyimprove therecognitionrateonTIMIT( ,,),bringingdownthe Mohamed e t a l .20092012a phonemeerrorratefromabout26%to20.7%.See ()foran Mohamed e t a l .2012b analysisofreasonsforthesuccessofthesemodels.Extensionstothebasicphone recognitionpipelineincludedtheadditionofspeaker-adaptivefeatures(Mohamed e t a l .,)thatfurtherreducedtheerrorrate.Thiswasquicklyfollowedup 2011 byworktoexpandthearchitecturefromphonemerecognition(whichiswhat TIMITisfocusedon)tolarge-vocabulary speechrecognition(,), Dahl e t a l .2012 whichinvolvesnotjustrecognizingphonemesbutalsorecognizingsequencesof wordsfromalargevocabulary.Deepnetworksforspeechrecognitioneventually 4 5 9 CHAPTER12.APPLICATIONS shiftedfrombeingbasedonpretrainingandBoltzmannmachinestobeingbased ontechniquessuchasrectiÔ¨Åedlinearunitsanddropout(,; Zeiler e t a l .2013Dahl e t a l .,). Bythattime,severalofthemajorspeechgroupsinindustryhad 2013 startedexploringdeeplearningincollaborationwithacademicresearchers.Hinton e t a l .()describethebreakthroughs achievedbythesecollaborators,which 2012a arenowdeployedinproductssuchasmobilephones. Later,asthesegroupsexploredlargerandlargerlabeleddatasetsandincorpo- ratedsomeofthemethodsforinitializing,training,andsettingupthearchitecture ofdeepnets,theyrealizedthattheunsupervisedpretrainingphasewaseither unnecessaryordidnotbringanysigniÔ¨Åcantimprovement. Thesebreakthroughs inrecognitionperformanceforworderrorrateinspeech recognitionwereunprecedented (around30%improvement)andwerefollowinga longperiodofabouttenyearsduringwhicherrorratesdidnotimprovemuchwith thetraditionalGMM-HMMtechnology,inspiteofthecontinuouslygrowingsizeof trainingsets(seeÔ¨Ågure2.4ofDengandYu2014()).Thiscreatedarapidshiftin thespeechrecognitioncommunitytowardsdeeplearning.Inamatterofroughly twoyears,mostoftheindustrialproductsforspeechrecognitionincorporateddeep neuralnetworksandthissuccessspurredanewwaveofresearchintodeeplearning algorithmsandarchitectures forASR,whichisstillongoingtoday. Oneoftheseinnovationswastheuseofconvolutionalnetworks( , Sainath e t a l . 2013)thatreplicateweightsacrosstimeandfrequency,improvingovertheearlier time-delayneuralnetworksthatreplicatedweightsonlyacrosstime.Thenew two-dimensionalconvolutionalmodelsregardtheinputspectrogramnotasone longvectorbutasanimage,withoneaxiscorrespondingtotimeandtheotherto frequencyofspectralcomponents. Anotherimportantpush, stillongoing,hasbeentowardsend-to-enddeep learningspeechrecognitionsystemsthatcompletelyremovetheHMM.TheÔ¨Årst majorbreakthrough inthisdirectioncamefromGraves2013 e t a l .()whotrained adeepLSTMRNN(seesection),usingMAPinferenceovertheframe-to- 10.10 phonemealignment,asin ()andintheCTCframework( LeCun e t a l .1998b Graves e t a l .,;2006Graves2012 Graves2013 ,).AdeepRNN( e t a l .,)hasstatevariables fromseverallayersateachtimestep,givingtheunfoldedgraphtwokindsofdepth: ordinarydepthduetoastackoflayers,anddepthduetotimeunfolding. This workbroughtthephonemeerrorrateonTIMITtoarecordlowof17.7%.See Pascanu2014aChung2014 e t a l .()and e t a l .()forothervariantsofdeepRNNs, appliedinothersettings. Anothercontemporarysteptowardend-to-enddeeplearningASRistoletthe systemlearnhowto‚Äúalign‚Äùtheacoustic-levelinformationwiththephonetic-level 4 6 0 CHAPTER12.APPLICATIONS information( ,;,). Chorowski e t a l .2014Lu e t a l .2015 12. 4 Nat u ra l L an gu a g e Pro c es s i n g Naturallanguageprocessing(NLP)istheuseofhumanlanguages,suchas EnglishorFrench,byacomputer.Computerprogramstypicallyreadandemit specializedlanguagesdesignedtoalloweÔ¨Écientandunambiguousparsingbysimple programs.Morenaturallyoccurringlanguagesareoftenambiguousanddefyformal description. Naturallanguageprocessingincludesapplicationssuchasmachine translation,inwhichthelearnermustreadasentenceinonehumanlanguageand emitanequivalentsentenceinanotherhumanlanguage.ManyNLPapplications arebasedonlanguagemodelsthatdeÔ¨Åneaprobabilitydistributionoversequences ofwords,charactersorbytesinanaturallanguage. Aswiththeotherapplicationsdiscussedinthischapter,verygenericneural networktechniquescanbesuccessfullyappliedtonaturallanguageprocessing. However,toachieveexcellentperformanceandtoscalewelltolargeapplications, somedomain-speciÔ¨Åcstrategiesbecomeimportant.TobuildaneÔ¨Écientmodelof naturallanguage,wemustusuallyusetechniquesthatarespecializedforprocessing sequentialdata.Inmanycases,wechoosetoregardnaturallanguageasasequence ofwords,ratherthanasequenceofindividualcharactersorbytes.Becausethetotal numberofpossiblewordsissolarge,word-basedlanguagemodelsmustoperateon anextremelyhigh-dimensionalandsparsediscretespace.Severalstrategieshave beendevelopedtomakemodelsofsuchaspaceeÔ¨Écient,bothinacomputational andinastatisticalsense. 12.4.1-grams n AlanguagemodeldeÔ¨Ånesaprobabilitydistributionoversequencesoftokens inanaturallanguage.Dependingonhowthemodelisdesigned,atokenmay beaword,acharacter,orevenabyte.Tokensarealwaysdiscreteentities.The earliestsuccessfullanguagemodelswerebasedonmodelsofÔ¨Åxed-lengthsequences oftokenscalled-grams.An-gramisasequenceoftokens. n n n Modelsbasedon n-gramsdeÔ¨Ånetheconditionalprobabilityofthe n-thtoken giventhepreceding n‚àí1tokens.Themodelusesproductsoftheseconditional distributionstodeÔ¨Ånetheprobabilitydistributionoverlongersequences: P x(1 , . . . , x œÑ) = ( P x1 , . . . , x n‚àí1)œÑÓÅô t n=P x( t| x t n‚àí+1 , . . . , x t‚àí1) .(12.5) 4 6 1 CHAPTER12.APPLICATIONS ThisdecompositionisjustiÔ¨Åedbythechainruleofprobability.Theprobability distributionovertheinitialsequence P( x1 , . . . , x n‚àí1)maybemodeledbyadiÔ¨Äerent modelwithasmallervalueof. n Training n-grammodelsisstraightforwardbecausethemaximumlikelihood estimatecanbecomputedsimplybycountinghowmanytimeseachpossible n gramoccursinthetrainingset.Modelsbasedon n-gramshavebeenthecore buildingblockofstatisticallanguagemodelingformanydecades(Jelinekand Mercer1980Katz1987ChenandGoodman1999 ,;,; ,). Forsmallvaluesof n,modelshaveparticularnames:unigramfor n=1,bigram for n=2,andtrigramfor n=3. ThesenamesderivefromtheLatinpreÔ¨Åxesfor thecorrespondingnumbersandtheGreeksuÔ¨Éx‚Äú-gram‚Äùdenotingsomethingthat iswritten.</div>
        </div>
    </div>

    <div class="question-card" id="q111">
        <div class="question-header">
            <span class="question-number">Question 111</span>
            <span class="question-type">multiple-choice</span>
        </div>

        <div class="question-text">Sampling-based methods are essential for estimating partition functions and performing inference in undirected probabilistic models, particularly when exact computations are infeasible. Techniques such as annealed importance sampling (AIS) and bridge sampling offer different approaches to overcome these challenges in deep generative modeling.

Which statement accurately characterizes the role of the bridging distribution in bridge sampling for partition function estimation?

1) It is fixed throughout the sampling process and independent of both distributions.   
2) Its optimal form depends on the ratio of partition functions and is designed to overlap significantly with both distributions.   
3) It is only necessary when the distributions are identical and the partition functions are known.   
4) It requires that the Kullback-Leibler divergence between distributions be maximized for robust estimation.   
5) It serves solely to generate samples for the known distribution, not the unknown one.   
6) Its efficiency does not depend on the amount of overlap with the target distributions.   
7) It must be chosen such that it minimizes the variance of the importance weights by ignoring partition function ratios.</div>

        <div class="answer-section">
            <div class="answer-label">‚úì Correct Answer:</div>
            <div class="answer-text">The correct answer is 2) Its optimal form depends on the ratio of partition functions and is designed to overlap significantly with both distributions..</div>
        </div>

        <div class="reference-section">
            <div class="reference-label">üìö Reference Text:</div>
            <button class="toggle-button" onclick="toggleReference(111)">
                Show/Hide Reference
            </button>
            <div id="ref111" class="reference-text hidden">‚àí1( x 1| x Œ∑ n ‚àí1)Àú p Œ∑1( x Œ∑1)n ‚àí 2 ÓÅô i = 1Àú p Œ∑ i+1( x Œ∑ i+1) Àú p Œ∑ i( x Œ∑ i+1)T Œ∑ i( x Œ∑ i+1| x Œ∑ i) . (18.59) Wenowhavemeansofgeneratingsamplesfromthejointproposaldistribution qovertheextendedsampleviaasamplingschemegivenabove,withthejoint distributiongivenby: q( x Œ∑1 , . . . , x Œ∑ n ‚àí1 , x 1) = p 0( x Œ∑1) T Œ∑1( x Œ∑2| x Œ∑1) . . . T Œ∑ n ‚àí1( x 1| x Œ∑ n ‚àí1) .(18.60) Wehaveajointdistributionontheextendedspacegivenbyequation.Taking18.59 q( x Œ∑1 , . . . , x Œ∑ n ‚àí1 , x 1)astheproposaldistributionontheextendedstatespacefrom whichwewilldrawsamples,itremainstodeterminetheimportanceweights: w( ) k=Àú p( x Œ∑1 , . . . , x Œ∑ n ‚àí1 , x 1) q( x Œ∑1 , . . . , x Œ∑ n ‚àí1 , x 1)=Àú p 1( x( ) k 1) Àú p Œ∑ n ‚àí1( x( ) k Œ∑ n ‚àí1). . .Àú p Œ∑2( x( ) k Œ∑2) Àú p 1( x( ) k Œ∑1)Àú p Œ∑1( x( ) k Œ∑1) Àú p 0( x( ) k 0).(18.61) TheseweightsarethesameasproposedforAIS.ThuswecaninterpretAISas simpleimportancesamplingappliedtoanextendedstateanditsvalidityfollows immediatelyfromthevalidityofimportancesampling. Annealedimportancesampling(AIS)wasÔ¨Årstdiscoveredby () Jarzynski1997 andthenagain,independently,by().Itiscurrentlythemostcommon Neal2001 wayofestimatingthepartitionfunctionforundirectedprobabilisticmodels.The reasonsforthismayhavemoretodowiththepublicationofaninÔ¨Çuentialpaper (SalakhutdinovandMurray2008,)describingitsapplicationtoestimatingthe partitionfunctionofrestrictedBoltzmannmachinesanddeepbeliefnetworksthan withanyinherentadvantagethemethodhasovertheothermethoddescribed below. AdiscussionofthepropertiesoftheAISestimator(e.g..itsvarianceand eÔ¨Éciency)canbefoundin().Neal2001 1 8 . 7 . 2 B ri d g e S a m p l i n g Bridgesampling ()isanothermethodthat,likeAIS,addressesthe Bennett1976 shortcomingsofimportancesampling.Ratherthanchainingtogetheraseriesof 628 CHAPTER18.CONFRONTINGTHEPARTITIONFUNCTION intermediatedistributions,bridgesamplingreliesonasingledistribution p ‚àó,known asthebridge,tointerpolatebetweenadistributionwithknownpartitionfunction, p 0,andadistribution p 1forwhichwearetryingtoestimatethepartitionfunction Z 1. Bridgesamplingestimatestheratio Z 1 / Z 0astheratiooftheexpectedimpor- tanceweightsbetweenÀú p 0andÀú p ‚àóandbetweenÀú p 1andÀú p ‚àó: Z 1 Z 0‚âàK ÓÅò k = 1Àú p ‚àó( x( ) k 0) Àú p 0( x( ) k 0)ÓÄ¨K ÓÅò k = 1Àú p ‚àó( x( ) k 1) Àú p 1( x( ) k 1)(18.62) Ifthebridgedistribution p ‚àóischosencarefullytohavealargeoverlapofsupport withboth p 0and p 1,thenbridgesamplingcanallowthedistancebetweentwo distributions(ormoreformally, D K L( p 0ÓÅ´ p 1))tobemuchlargerthanwithstandard importancesampling. Itcanbeshownthattheoptimalbridgingdistributionisgivenby p( ) op t ‚àó(x)‚àù Àú p0 ( ) Àú x p1 ( ) x r Àú p0 ( ) + Àú x p1 ( ) xwhere r= Z 1 / Z 0.AtÔ¨Årst,thisappearstobeanunworkablesolution asitwouldseemtorequiretheveryquantitywearetryingtoestimate, Z 1 / Z 0. However,itispossibletostartwithacoarseestimateof randusetheresulting bridgedistributiontoreÔ¨Åneourestimateiteratively(,).Thatis,we Neal2005 iterativelyre-estimatetheratioanduseeachiterationtoupdatethevalueof. r L i nk e d i m p o r t anc e samplingBothAISandbridgesamplinghavetheirad- vantages.If D K L( p 0ÓÅ´ p 1)isnottoolarge(because p 0and p 1aresuÔ¨Écientlyclose) bridgesamplingcanbeamoreeÔ¨Äectivemeansofestimatingtheratioofpartition functionsthanAIS.If,however,thetwodistributionsaretoofarapartforasingle distribution p ‚àótobridgethegapthenonecanatleastuseAISwithpotentially manyintermediate distributionstospanthedistancebetween p 0and p 1.Neal ()showedhowhislinkedimportancesamplingmethodleveragedthepowerof 2005 thebridgesamplingstrategytobridgetheintermediatedistributionsusedinAIS tosigniÔ¨Åcantlyimprovetheoverallpartitionfunctionestimates. E st i m at i n g t he par t i t i o n f unc t i o n whi l e t r ai ni ngWhileAIShasbecome acceptedasthestandardmethodforestimatingthepartitionfunctionformany undirectedmodels, itissuÔ¨Écientlycomputationally intensivethatitremains infeasibletouseduringtraining.However,alternativestrategiesthathavebeen exploredtomaintainanestimateofthepartitionfunctionthroughouttraining Usingacombinationofbridgesampling,short-chainAISandparalleltempering, Desjardins2011 e t a l .()devisedaschemetotrackthepartitionfunctionofan 629 CHAPTER18.CONFRONTINGTHEPARTITIONFUNCTION RBMthroughoutthetrainingprocess.Thestrategyisbasedonthemaintenanceof independentestimatesofthepartitionfunctionsoftheRBMateverytemperature operatingintheparalleltemperingscheme.Theauthorscombinedbridgesampling estimatesoftheratiosofpartitionfunctionsofneighboringchains(i.e.from paralleltempering)withAISestimatesacrosstimetocomeupwithalowvariance estimateofthepartitionfunctionsateveryiterationoflearning. ThetoolsdescribedinthischapterprovidemanydiÔ¨Äerentwaysofovercoming theproblemofintractablepartitionfunctions,buttherecanbeseveralother diÔ¨Écultiesinvolvedintrainingandusinggenerativemodels.Foremostamongthese istheproblemofintractableinference,whichweconfrontnext. 630 C h a p t e r 1 9 ApproximateInference ManyprobabilisticmodelsarediÔ¨ÉculttotrainbecauseitisdiÔ¨Éculttoperform inferenceinthem.Inthecontextofdeeplearning,weusuallyhaveasetofvisible variables vandasetoflatentvariables h.Thechallengeofinferenceusually referstothediÔ¨Écultproblemofcomputing p( h v|)ortakingexpectationswith respecttoit.Suchoperationsareoftennecessaryfortaskslikemaximumlikelihood learning. Manysimplegraphicalmodelswithonlyonehiddenlayer,suchasrestricted BoltzmannmachinesandprobabilisticPCA,aredeÔ¨Ånedinawaythatmakes inferenceoperationslikecomputing p( h v|),ortakingexpectationswithrespect toit,simple.Unfortunately,mostgraphicalmodelswithmultiplelayersofhidden variableshaveintractableposteriordistributions.Exactinferencerequiresan exponentialamountoftimeinthesemodels.Evensomemodelswithonlyasingle layer,suchassparsecoding,havethisproblem. Inthischapter,weintroduceseveralofthetechniquesforconfrontingthese intractableinferenceproblems.Later,inchapter,wewilldescribehowtouse 20 thesetechniquestotrainprobabilisticmodelsthatwouldotherwisebeintractable, suchasdeepbeliefnetworksanddeepBoltzmannmachines. Intractableinferenceproblemsindeeplearningusuallyarisefrominteractions betweenlatentvariablesinastructuredgraphicalmodel.SeeÔ¨Ågureforsome19.1 examples.Theseinteractionsmaybeduetodirectinteractionsinundirected modelsor‚Äúexplainingaway‚Äùinteractionsbetweenmutualancestorsofthesame visibleunitindirectedmodels. 631 CHAPTER19.APPROXIMATEINFERENCE Figure19.1:Intractableinferenceproblemsindeeplearningareusuallytheresultof interactionsbetweenlatentvariablesinastructuredgraphicalmodel.Thesecanbe duetoedgesdirectlyconnectingonelatentvariabletoanother,orduetolongerpaths thatareactivatedwhenthechildofaV-structureisobserved. ( L e f t )Asemi-restricted Boltzmannmachine( ,)withconnectionsbetweenhidden OsinderoandHinton2008 units.Thesedirectconnectionsbetweenlatentvariablesmaketheposteriordistribution</div>
        </div>
    </div>

    <div class="question-card" id="q112">
        <div class="question-header">
            <span class="question-number">Question 112</span>
            <span class="question-type">multiple-choice</span>
        </div>

        <div class="question-text">Recurrent neural networks (RNNs) are widely used for modeling sequences, with different architectures providing trade-offs between modeling capacity and training efficiency. Understanding how information flows and training methods such as teacher forcing impact these models is essential when designing sequence models.

Which of the following statements best characterizes a key limitation of output-to-hidden recurrent neural networks in sequence modeling?

1) They always require Backpropagation Through Time for training, making parallelization impossible.   
2) Their hidden state is updated using the previous hidden state, which increases memory requirements.   
3) They must rely on the output units to transmit all historical information, which can cause loss of important sequence context if the outputs are not sufficiently expressive.   
4) They use attention mechanisms to capture dependencies instead of recurrence.   
5) They cannot be trained with teacher forcing due to their architecture.   
6) They are inherently superior for capturing long-term dependencies compared to standard RNNs.   
7) They summarize sequences by outputting a fixed-size vector at each timestep.</div>

        <div class="answer-section">
            <div class="answer-label">‚úì Correct Answer:</div>
            <div class="answer-text">The correct answer is 3) They must rely on the output units to transmit all historical information, which can cause loss of important sequence context if the outputs are not sufficiently expressive..</div>
        </div>

        <div class="reference-section">
            <div class="reference-label">üìö Reference Text:</div>
            <button class="toggle-button" onclick="toggleReference(112)">
                Show/Hide Reference
            </button>
            <div id="ref112" class="reference-text hidden">t x( t ‚àí 1 )x( t ‚àí 1 )x( ) tx( ) tx( + 1 ) tx( + 1 ) tW W W Wo( ) . . .o( ) . . . h( ) . . .h( ) . . .V V V U U UU nf ol d Figure10.4:AnRNNwhoseonlyrecurrenceisthefeedbackconnectionfromtheoutput tothehiddenlayer.Ateachtimestep t,theinputisxt,thehiddenlayeractivationsare h( ) t,theoutputsareo( ) t,thetargetsarey( ) tandthelossis L( ) t. ( L e f t )Circuitdiagram. ( R i g h t )Unfoldedcomputationalgraph.SuchanRNNislesspowerful(canexpressa smallersetoffunctions)thanthoseinthefamilyrepresentedbyÔ¨Ågure.TheRNN 10.3 inÔ¨Ågurecanchoosetoputanyinformationitwantsaboutthepastintoitshidden 10.3 representationhandtransmithtothefuture.TheRNNinthisÔ¨Ågureistrainedto putaspeciÔ¨Åcoutputvalueintoo,andoistheonlyinformationitisallowedtosend tothefuture.Therearenodirectconnectionsfromhgoingforward.Theprevioush isconnectedtothepresentonlyindirectly,viathepredictionsitwasusedtoproduce. Unlessoisveryhigh-dimensionalandrich,itwillusuallylackimportantinformation fromthepast.ThismakestheRNNinthisÔ¨Ågurelesspowerful,butitmaybeeasierto trainbecauseeachtimestepcanbetrainedinisolationfromtheothers,allowinggreater parallelizationduringtraining,asdescribedinsection.10.2.1 3 8 0 CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS t t œÑ = 1to= ,weapplythefollowingupdateequations: a( ) t= +bWh( 1 ) t ‚àí+Ux( ) t(10.8) h( ) t=tanh(a( ) t) (10.9) o( ) t= +cVh( ) t(10.10) ÀÜy( ) t=softmax(o( ) t) (10.11) wheretheparametersarethebiasvectorsbandcalongwiththeweightmatrices U,VandW,respectivelyforinput-to-hidden, hidden-to-output andhidden-to- hiddenconnections.Thisisanexampleofarecurrentnetworkthatmapsan inputsequencetoanoutputsequenceofthesamelength.Thetotallossfora givensequenceofvaluespairedwithasequenceofvalueswouldthenbejust x y thesumofthelossesoverallthetimesteps.Forexample,if L( ) tisthenegative log-likelihoodof y( ) tgivenx( 1 ), . . . ,x( ) t,then LÓÄê {x( 1 ), . . . ,x( ) œÑ}{ ,y( 1 ), . . . ,y( ) œÑ}ÓÄë (10.12) =ÓÅò tL( ) t(10.13) =‚àíÓÅò tlog p m o de lÓÄê y( ) t|{x( 1 ), . . . ,x( ) t}ÓÄë , (10.14) where p m o de lÓÄÄ y( ) t|{x( 1 ), . . . ,x( ) t}ÓÄÅ isgivenbyreadingtheentryfor y( ) tfromthe model‚ÄôsoutputvectorÀÜy( ) t.Computingthegradientofthislossfunctionwithrespect totheparametersisanexpensiveoperation.Thegradientcomputationinvolves performingaforwardpropagationpassmovinglefttorightthroughourillustration oftheunrolledgraphinÔ¨Ågure,followedbyabackwardpropagationpass 10.3 movingrighttoleftthroughthegraph.Theruntimeis O( œÑ) andcannotbereduced byparallelization becausetheforwardpropagationgraphisinherentlysequential; eachtimestepmayonlybecomputedafterthepreviousone. Statescomputed intheforwardpassmustbestoreduntiltheyarereusedduringthebackward pass,sothememorycostisalso O( œÑ).Theback-propagation algorithmapplied totheunrolledgraphwith O( œÑ)costiscalledback-propagationthroughtime orBPTTandisdiscussedfurtherinsection.Thenetworkwithrecurrence 10.2.2 betweenhiddenunitsisthusverypowerfulbutalsoexpensivetotrain.Istherean alternative? 10.2.1TeacherForcingandNetworkswithOutputRecurrence Thenetworkwithrecurrentconnectionsonlyfromtheoutputatonetimestepto thehiddenunitsatthenexttimestep(showninÔ¨Ågure)isstrictlylesspowerful 10.4 3 8 1 CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS becauseitlackshidden-to-hidden recurrentconnections.Forexample,itcannot simulateauniversalTuringmachine.Becausethisnetworklackshidden-to-hidden recurrence,itrequiresthattheoutputunitscapturealloftheinformationabout thepastthatthenetworkwillusetopredictthefuture.Becausetheoutputunits areexplicitlytrainedtomatchthetrainingsettargets,theyareunlikelytocapture thenecessaryinformationaboutthepasthistoryoftheinput,unlesstheuser knowshowtodescribethefullstateofthesystemandprovidesitaspartofthe trainingsettargets.Theadvantageofeliminatinghidden-to-hidden recurrence isthat,foranylossfunctionbasedoncomparingthepredictionattime ttothe trainingtargetattime t,allthetimestepsaredecoupled.Trainingcanthusbe parallelized,withthegradientforeachstep tcomputedinisolation.Thereisno needtocomputetheoutputfortheprevioustimestepÔ¨Årst,becausethetraining setprovidestheidealvalueofthatoutput. h( t ‚àí 1 )h( t ‚àí 1 ) Wh( ) th( ) t . . . . . . x( t ‚àí 1 )x( t ‚àí 1 )x( ) tx( ) tx( ) . . .x( ) . . .W W U U Uh( ) œÑh( ) œÑ x( ) œÑx( ) œÑW Uo( ) œÑo( ) œÑy( ) œÑy( ) œÑL( ) œÑL( ) œÑ V . . . . . . Figure10.5:Time-unfoldedrecurrentneuralnetworkwithasingleoutputattheend ofthesequence.Suchanetworkcanbeusedtosummarizeasequenceandproducea Ô¨Åxed-sizerepresentationusedasinputforfurtherprocessing. Theremightbeatarget rightattheend(asdepictedhere)orthegradientontheoutputo( ) tcanbeobtainedby back-propagatingfromfurtherdownstreammodules. Modelsthathaverecurrentconnectionsfromtheiroutputsleadingbackinto themodelmaybetrainedwithteacherforcing.Teacherforcingisaprocedure thatemergesfromthemaximumlikelihoodcriterion,inwhichduringtrainingthe modelreceivesthegroundtruthoutput y( ) tasinputattime t+1. Wecansee thisbyexaminingasequencewithtwotimesteps.Theconditionalmaximum 3 8 2 CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS o( t ‚àí 1 )o( t ‚àí 1 )o( ) to( ) t h( t ‚àí 1 )h( t ‚àí 1 )h( ) th( ) t x( t ‚àí 1 )x( t ‚àí 1 )x( ) tx( ) tW V V U Uo( t ‚àí 1 )o( t ‚àí 1 )o( ) to( ) tL( t ‚àí 1 )L( t ‚àí 1 )L( ) tL( ) ty( t ‚àí 1 )y( t ‚àí 1 )y( ) ty( ) t h( t ‚àí 1 )h( t ‚àí 1 )h( ) th( ) t x( t ‚àí 1 )x( t ‚àí 1 )x( ) tx( ) tW V V U U T r ai n t i m e T e s t t i m e Figure10.6:Illustrationofteacherforcing.Teacherforcingisatrainingtechniquethatis applicabletoRNNsthathaveconnectionsfromtheiroutputtotheirhiddenstatesatthe nexttimestep. ( L e f t )Attraintime,wefeedthe c</div>
        </div>
    </div>

    <div class="question-card" id="q113">
        <div class="question-header">
            <span class="question-number">Question 113</span>
            <span class="question-type">multiple-choice</span>
        </div>

        <div class="question-text">Convolutional neural networks (CNNs) utilize specialized architectural components, such as convolution and pooling layers, to efficiently process and extract features from high-dimensional data like images. The way these components structure parameter sharing and information flow has significant implications for model complexity and generalization.

Which statement best characterizes the effect of imposing an infinitely strong prior through convolutional layers in a CNN?

1) It allows each hidden unit to learn entirely independent weights for every spatial location.   
2) It results in pooling regions being dynamically determined for each image by clustering algorithms.   
3) It forces all weights for hidden units to be identical except for spatial shifts and zero outside small receptive fields.   
4) It makes the architecture strictly invertible, preserving all input information for reconstruction.   
5) It requires every pooling region to be the same size, regardless of input dimensions.   
6) It introduces high statistical complexity by increasing the number of learnable parameters.   
7) It prevents the structuring of information flow and feature hierarchies within the network.</div>

        <div class="answer-section">
            <div class="answer-label">‚úì Correct Answer:</div>
            <div class="answer-text">The correct answer is 3) It forces all weights for hidden units to be identical except for spatial shifts and zero outside small receptive fields..</div>
        </div>

        <div class="reference-section">
            <div class="reference-label">üìö Reference Text:</div>
            <button class="toggle-button" onclick="toggleReference(113)">
                Show/Hide Reference
            </button>
            <div id="ref113" class="reference-text hidden">i n g.Hereweusemax-poolingwithapoolwidthof threeandastridebetweenpoolsoftwo.Thisreducestherepresentationsizebyafactor oftwo,whichreducesthecomputationalandstatisticalburdenonthenextlayer.Note thattherightmostpoolingregionhasasmallersize,butmustbeincludedifwedonot wanttoignoresomeofthedetectorunits. 3 4 4 CHAPTER9.CONVOLUTIONALNETWORKS Sometheoreticalworkgivesguidanceastowhichkindsofpoolingoneshould useinvarioussituations( ,).Itisalsopossibletodynamically Boureauetal.2010 poolfeaturestogether,forexample,byrunningaclusteringalgorithmonthe locationsofinterestingfeatures( ,).Thisapproachyieldsa Boureauetal.2011 diÔ¨Äerentsetofpoolingregionsforeachimage.Anotherapproachistolearna singlepoolingstructurethatisthenappliedtoallimages(,). Jiaetal.2012 Poolingcancomplicatesomekindsofneuralnetworkarchitecturesthatuse top-downinformation, suchasBoltzmannmachinesandautoencoders.These issueswillbediscussedfurtherwhenwepresentthesetypesofnetworksinpart.III PoolinginconvolutionalBoltzmannmachinesispresentedinsection. The20.6 inverse-likeoperationsonpoolingunitsneededinsomediÔ¨Äerentiablenetworkswill becoveredinsection.20.10.6 SomeexamplesofcompleteconvolutionalnetworkarchitecturesforclassiÔ¨Åcation usingconvolutionandpoolingareshowninÔ¨Ågure.9.11 9.4Convolutionand Pooling asan InÔ¨ÅnitelyStrong Prior Recalltheconceptofa pr i o r pr o babili t y di st r i but i o nfromsection.Thisis5.2 aprobabilitydistributionovertheparametersofamodelthatencodesourbeliefs aboutwhatmodelsarereasonable,beforewehaveseenanydata. Priorscanbeconsideredweakorstrongdependingonhowconcentratedthe probabilitydensityintheprioris.Aweakpriorisapriordistributionwithhigh entropy,suchasaGaussiandistributionwithhighvariance.Suchapriorallows thedatatomovetheparametersmoreorlessfreely.Astrongpriorhasverylow entropy,suchasaGaussiandistributionwithlowvariance.Suchapriorplaysa moreactiveroleindeterminingwheretheparametersendup. AninÔ¨Ånitelystrongpriorplaceszeroprobabilityonsomeparametersandsays thattheseparametervaluesarecompletelyforbidden,regardlessofhowmuch supportthedatagivestothosevalues. Wecanimagineaconvolutionalnetasbeingsimilartoafullyconnectednet, butwithaninÔ¨Ånitelystrongprioroveritsweights.ThisinÔ¨Ånitelystrongprior saysthattheweightsforonehiddenunitmustbeidenticaltotheweightsofits neighbor,butshiftedinspace.Theprioralsosaysthattheweightsmustbezero, exceptforinthesmall,spatiallycontiguousreceptiveÔ¨Åeldassignedtothathidden unit.Overall,wecanthinkoftheuseofconvolutionasintroducinganinÔ¨Ånitely strongpriorprobabilitydistributionovertheparametersofalayer.Thisprior 3 4 5</div>
        </div>
    </div>

    <div class="question-card" id="q114">
        <div class="question-header">
            <span class="question-number">Question 114</span>
            <span class="question-type">multiple-choice</span>
        </div>

        <div class="question-text">Deep generative models often rely on hierarchical architectures such as stacked autoencoders or Restricted Boltzmann Machines (RBMs) to learn complex data distributions. Effective sampling and training in these models can be hindered by intractable normalization constants and poor mixing between latent modes.

Which practical technique is commonly used to address the intractable partition function during training of deep generative models with undirected graphical structures?

1) Variational Autoencoding   
2) Backpropagation Through Time   
3) Contrastive Divergence   
4) L1 Regularization   
5) Dropout   
6) Batch Normalization   
7) Learning Rate Scheduling</div>

        <div class="answer-section">
            <div class="answer-label">‚úì Correct Answer:</div>
            <div class="answer-text">The correct answer is 3) Contrastive Divergence.</div>
        </div>

        <div class="reference-section">
            <div class="reference-label">üìö Reference Text:</div>
            <button class="toggle-button" onclick="toggleReference(114)">
                Show/Hide Reference
            </button>
            <div id="ref114" class="reference-text hidden">a l .()observedthatdeeperstacksofregularizedautoencodersor RBMsyieldmarginaldistributionsinthetop-level h-spacethatappearedmore spreadoutandmoreuniform,withlessofagapbetweentheregionscorresponding todiÔ¨Äerentmodes(categories,intheexperiments).TraininganRBMinthat higher-levelspaceallowedGibbssamplingtomixfasterbetweenmodes.Itremains howeverunclearhowtoexploitthisobservationtohelpbettertrainandsample fromdeepgenerativemodels. DespitethediÔ¨Écultyofmixing,MonteCarlotechniquesareusefulandare oftenthebesttoolavailable.Indeed,theyaretheprimarytoolusedtoconfront theintractablepartitionfunctionofundirectedmodels,discussednext. 6 0 4 C h a p t e r 1 8 C on f ron t i n g t h e P art i t i on F u n ct i on Insectionwesawthatmanyprobabilisticmodels(commonlyknownasundi- 16.2.2 rectedgraphicalmodels)aredeÔ¨Ånedbyanunnormalized probabilitydistribution Àú p(x; Œ∏).Wemustnormalize Àú pbydividingbyapartitionfunction Z( Œ∏)inorderto obtainavalidprobabilitydistribution: p(;) =x Œ∏1 Z() Œ∏Àú p . (;)x Œ∏ (18.1) Thepartitionfunctionisanintegral(forcontinuousvariables)orsum(fordiscrete variables)overtheunnormalized probabilityofallstates: ÓÅö Àú p d() x x (18.2) or ÓÅò xÀú p .() x (18.3) Thisoperationisintractableformanyinterestingmodels. Aswewillseeinchapter,severaldeeplearningmodelsaredesignedto 20 haveatractablenormalizingconstant,oraredesignedtobeusedinwaysthatdo notinvolvecomputing p(x)atall. However,othermodelsdirectlyconfrontthe challengeofintractablepartitionfunctions.Inthischapter,wedescribetechniques usedfortrainingandevaluatingmodelsthathaveintractablepartitionfunctions. 605</div>
        </div>
    </div>

    <div class="question-card" id="q115">
        <div class="question-header">
            <span class="question-number">Question 115</span>
            <span class="question-type">multiple-choice</span>
        </div>

        <div class="question-text">Artificial intelligence systems have evolved from rule-based programs to learning algorithms capable of handling complex, intuitive tasks. Understanding why knowledge-base approaches struggled and how deep learning addressed these limitations is crucial in the field of modern AI.

Which of the following best explains why deep learning outperformed early AI systems that relied on hard-coded knowledge bases?

1) Deep learning models require no mathematical notation or probability theory.   
2) Deep learning algorithms can autonomously learn hierarchical representations and extract features from raw data, handling complex intuitive tasks without explicit formalization.   
3) Rule-based systems are inherently better at tasks needing subjective human intuition.   
4) Knowledge-base approaches succeed when world knowledge is encoded exhaustively by human experts.   
5) Deep learning eliminates the need for large amounts of training data.   
6) Hard-coded knowledge bases excel at image and speech recognition through formal logic.   
7) Deep learning systems cannot adapt to new situations once trained.</div>

        <div class="answer-section">
            <div class="answer-label">‚úì Correct Answer:</div>
            <div class="answer-text">The correct answer is 2) Deep learning algorithms can autonomously learn hierarchical representations and extract features from raw data, handling complex intuitive tasks without explicit formalization..</div>
        </div>

        <div class="reference-section">
            <div class="reference-label">üìö Reference Text:</div>
            <button class="toggle-button" onclick="toggleReference(115)">
                Show/Hide Reference
            </button>
            <div id="ref115" class="reference-text hidden">B GAgraph P a G(x i)Theparentsofx iinG I ndexing a iElement iofvectora,withindexingstartingat1 a ‚àí iAllelementsofvectorexceptforelementa i A i , jElementofmatrix i , jA A i , :Rowofmatrix iA A : , iColumnofmatrix iA A i , j , kElementofa3-Dtensor ( ) i , j , k A A : : , , i2-Dsliceofa3-Dtensor a iElementoftherandomvector i a L i near Al g e br a O p e r at i o ns AÓÄæTransposeofmatrixA A+Moore-PenrosepseudoinverseofA ABÓÄåElement-wise(Hadamard)productofandAB det()ADeterminantofA x i i CO NTE NT S Cal c ul usd y d xDerivativeofwithrespectto y x ‚àÇ y ‚àÇ xPartialderivativeofwithrespectto y x ‚àá x yGradientofwithrespectto y x ‚àá X yMatrixderivativesofwithrespectto y X ‚àá X yTensorcontainingderivativesof ywithrespectto X ‚àÇ f ‚àÇxJacobianmatrixJ‚àà Rm n √óof f: Rn‚Üí Rm ‚àá2 x f f f () (xorH)()xTheHessianmatrixofatinputpointxÓÅö f d()xxDeÔ¨Åniteintegralovertheentiredomainofx ÓÅö Sf d()xx x DeÔ¨Åniteintegralwithrespecttoovertheset S P r o babil i t y and I nf o r m at i o n T heor y abTherandomvariablesaandbareindependent ‚ä• abcTheyareconditionallyindependentgivenc ‚ä•| P()aAprobabilitydistributionoveradiscretevariable p()aAprobabilitydistributionoveracontinuousvari- able,oroveravariablewhosetypehasnotbeen speciÔ¨Åed a Randomvariableahasdistribution ‚àº P P E x ‚àº P[()] () () () f xor E f xExpectationof f xwithrespectto Px Var(()) f xVarianceofunderx f x() P() Cov(()()) f x , g xCovarianceofandunderx f x() g x() P() H()xShannonentropyoftherandomvariablex D K L( ) P QÓÅ´Kullback-LeiblerdivergenceofPandQ N(; )x¬µ ,Œ£Gaussiandistributionoverxwithmean¬µand covarianceŒ£ x i i i CO NTE NT S F unc t i o ns f f : A B‚ÜíThefunctionwithdomainandrange A B f g f g ‚ó¶Compositionofthefunctionsand f(;)xŒ∏Afunctionofxparametrized byŒ∏. (Sometimes wewrite f(x)andomittheargumentŒ∏tolighten notation) log x x Naturallogarithmof œÉ x()Logisticsigmoid,1 1+exp()‚àí x Œ∂ x x () log(1+exp( Softplus, )) ||||x p Lpnormofx ||||x L2normofx x+Positivepartof,i.e., x max(0) , x 1 c o ndi t i o nis1iftheconditionistrue,0otherwise Sometimesweuseafunction fwhoseargumentisascalarbutapplyittoa vector,matrix,ortensor: f(x), f(X),or f( X).Thisdenotestheapplicationof f tothearrayelement-wise. Forexample,if C= œÉ( X),then C i , j , k= œÉ( X i , j , k)forall validvaluesof,and. i j k D at aset s and D i st r i but i o n s p da t aThedatageneratingdistribution ÀÜ p da t aTheempiricaldistributiondeÔ¨Ånedbythetraining set XAsetoftrainingexamples x( ) iThe-thexample(input)fromadataset i y( ) iory( ) iThetargetassociatedwithx( ) iforsupervisedlearn- ing XThe m n√ómatrixwithinputexamplex( ) iinrow X i , : x i v C h a p t e r 1 I n t ro d u ct i on Inventorshavelongdreamedofcreatingmachinesthatthink.Thisdesiredates backtoatleastthetimeofancientGreece.ThemythicalÔ¨ÅguresPygmalion, Daedalus,andHephaestusmayallbeinterpretedaslegendaryinventors,and Galatea,Talos,andPandoramayallberegardedasartiÔ¨Åciallife( , OvidandMartin 2004Sparkes1996Tandy1997 ;,;,). Whenprogrammable computerswereÔ¨Årstconceived,peoplewonderedwhether suchmachinesmightbecomeintelligent,overahundredyearsbeforeonewas built(Lovelace1842,).Today, ar t i Ô¨Åc i al i n t e l l i g e nc e(AI)isathrivingÔ¨Åeldwith manypracticalapplicationsandactiveresearchtopics.Welooktointelligent softwaretoautomateroutinelabor,understandspeechorimages,makediagnoses inmedicineandsupportbasicscientiÔ¨Åcresearch. IntheearlydaysofartiÔ¨Åcialintelligence,theÔ¨Åeldrapidlytackledandsolved problemsthatareintellectually diÔ¨Écultforhumanbeingsbutrelativelystraight- forwardforcomputers‚Äîproblemsthatcanbedescribedbyalistofformal,math- ematicalrules. ThetruechallengetoartiÔ¨Åcialintelligenceprovedtobesolving thetasksthatareeasyforpeopletoperformbuthardforpeopletodescribe formally‚Äîprobl emsthatwesolveintuitively,thatfeelautomatic,likerecognizing spokenwordsorfacesinimages. Thisbookisaboutasolutiontothesemoreintuitiveproblems.Thissolutionis toallowcomputerstolearnfromexperienceandunderstandtheworldintermsofa hierarchyofconcepts,witheachconceptdeÔ¨Ånedintermsofitsrelationtosimpler concepts.Bygatheringknowledgefromexperience,thisapproachavoidstheneed forhumanoperatorstoformallyspecifyalloftheknowledgethatthecomputer needs.Thehierarchyofconceptsallowsthecomputertolearncomplicatedconcepts bybuildingthemoutofsimplerones.Ifwedrawagraphshowinghowthese 1 CHAPTER1.INTRODUCTION conceptsarebuiltontopofeachother,thegraphisdeep,withmanylayers.For thisreason,wecallthisapproachtoAI . deep l e ar ni ng ManyoftheearlysuccessesofAItookplaceinrelativelysterileandformal environmentsanddidnotrequirecomputerstohavemuchknowledgeabout theworld.Forexample,IBM‚ÄôsDeepBluechess-playingsystemdefeatedworld championGarryKasparovin1997(,).Chessisofcourseaverysimple Hsu2002 world,containingonlysixty-fourlocationsandthirty-twopiecesthatcanmove inonlyrigidlycircumscribedways.Devisingasuccessfulchessstrategyis a tremendousaccomplishment, butthechallengeisnotduetothediÔ¨Écultyof describingthesetofchesspiecesandallowablemovestothecomputer.Chess canbecompletelydescribedbyaverybrieflistofcompletelyformalrules,easily providedaheadoftimebytheprogrammer. Ironically,abstractandformaltasksthatareamongthemostdiÔ¨Écultmental undertakings forahumanbeingareamongtheeasiestforacomputer.Computers havelongbeenabletodefeateventhebesthumanchessplayer,butareonly recentlymatchingsomeoftheabilitiesofaveragehumanbeingstorecognizeobjects orspeech.Aperson‚Äôseverydayliferequiresanimmenseamountofknowledge abouttheworld.Muchofthisknowledgeissubjectiveandintuitive,andtherefore diÔ¨Éculttoarticulateinaformalway.Computersneedtocapturethissame knowledgeinordertobehaveinanintelligentway.Oneofthekeychallengesin artiÔ¨Åcialintelligenceishowtogetthisinformalknowledgeintoacomputer. SeveralartiÔ¨Åcialintelligenceprojectshavesoughttohard-codeknowledgeabout theworldinformallanguages.Acomputercanreasonaboutstatementsinthese formallanguagesautomatically usinglogicalinferencerules.Thisisknownasthe k no wl e dge baseapproachtoartiÔ¨Åcialintelligence.Noneoftheseprojectshasled toamajorsuccess.OneofthemostfamoussuchprojectsisCyc( , LenatandGuha 1989).Cycisaninferenceengineandadatabaseofstatementsinalanguage calledCycL.ThesestatementsareenteredbyastaÔ¨Äofhumansupervisors.Itisan unwieldyprocess.Peoplestruggletodeviseformalruleswithenoughcomplexity toaccuratelydescribetheworld.Forexample,Cycfailedtounderstandastory aboutapersonnamedFredshavinginthemorning(,).Itsinference Linde1992 enginedetectedaninconsistencyinthestory: itknewthatpeopledonothave electricalparts,butbecauseFredwasholdinganelectricrazor,itbelievedthe entity‚ÄúFredWhileShaving‚Äùcontainedelectricalparts.Itthereforeaskedwhether Fredwasstillapersonwhilehewasshaving. ThediÔ¨Écultiesfacedbysystemsrelyingonhard-codedknowledgesuggest thatAIsystemsneedtheabilitytoacquiretheirownknowledge,byextracting patternsfromrawdata.Thiscapabilityisknownas m ac hi ne l e ar ni ng.The 2 CHAPTER1.INTRODUCTION introductionofmachinelearningallowedcomputerstotackleproblemsinvolving knowledgeoftherealworldandmakedecisionsthatappearsubjective.Asimple machinelearningalgorithmcalled</div>
        </div>
    </div>

    <div class="question-card" id="q116">
        <div class="question-header">
            <span class="question-number">Question 116</span>
            <span class="question-type">multiple-choice</span>
        </div>

        <div class="question-text">Recurrent neural networks (RNNs) are widely used for modeling sequential data by capturing temporal dependencies through shared network parameters and hidden state variables. Their training involves specialized techniques to handle the propagation of gradients across time steps.

Which statement best explains why RNNs can efficiently model the full joint distribution over a sequence without explicitly representing all inter-variable dependencies?

1) They use separate weight matrices for each possible dependency in the sequence.   
2) They compute gradients only with respect to the output variables, ignoring hidden states.   
3) They do not share parameters across time steps, allowing independent modeling of each input.   
4) They rely exclusively on mean squared error loss for all sequence modeling tasks.   
5) They treat each output as conditionally independent by excluding previous outputs from the model.   
6) They avoid using hidden states, directly modeling dependencies between observed variables at every time step.   
7) They introduce hidden state variables and share parameters across time steps, allowing a compact representation that summarizes relevant past information for efficient joint modeling. </div>

        <div class="answer-section">
            <div class="answer-label">‚úì Correct Answer:</div>
            <div class="answer-text">The correct answer is 7) They introduce hidden state variables and share parameters across time steps, allowing a compact representation that summarizes relevant past information for efficient joint modeling..</div>
        </div>

        <div class="reference-section">
            <div class="reference-label">üìö Reference Text:</div>
            <button class="toggle-button" onclick="toggleReference(116)">
                Show/Hide Reference
            </button>
            <div id="ref116" class="reference-text hidden">CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS Oncethegradientsonthe internalnodesofthe computational graphare obtained, wecanobtainthegradientsontheparameternodes.Becausethe parametersaresharedacrossmanytimesteps,wemusttakesomecarewhen denotingcalculusoperationsinvolvingthesevariables.Theequationswewishto implementusethebpropmethodofsection,thatcomputesthecontribution 6.5.6 ofasingleedgeinthecomputational graphtothegradient.However,the‚àá W f operatorusedincalculustakesintoaccountthecontributionofWtothevalue of fduetoedgesinthecomputational graph.Toresolvethisambiguity,we a l l introducedummyvariablesW( ) tthataredeÔ¨ÅnedtobecopiesofWbutwitheach W( ) tusedonlyattimestep t.Wemaythenuse‚àáW( ) ttodenotethecontribution oftheweightsattimesteptothegradient. t Usingthisnotation,thegradientontheremainingparametersisgivenby: ‚àá c L=ÓÅò tÓÄ† ‚àÇo( ) t ‚àÇcÓÄ°ÓÄæ ‚àáo( ) t L=ÓÅò t‚àáo( ) t L (10.22) ‚àá b L=ÓÅò tÓÄ† ‚àÇh( ) t ‚àÇb( ) tÓÄ°ÓÄæ ‚àáh( ) t L=ÓÅò tdiagÓÄí 1‚àíÓÄê h( ) tÓÄë2ÓÄì ‚àáh( ) t L(10.23) ‚àá V L=ÓÅò tÓÅò iÓÄ† ‚àÇ L ‚àÇ o( ) t iÓÄ° ‚àá V o( ) t i=ÓÅò t(‚àáo( ) t L)h( ) tÓÄæ(10.24) ‚àá W L=ÓÅò tÓÅò iÓÄ† ‚àÇ L ‚àÇ h( ) t iÓÄ° ‚àáW( ) t h( ) t i (10.25) =ÓÅò tdiagÓÄí 1‚àíÓÄê h( ) tÓÄë2ÓÄì (‚àáh( ) t L)h( 1 ) t ‚àíÓÄæ(10.26) ‚àá U L=ÓÅò tÓÅò iÓÄ† ‚àÇ L ‚àÇ h( ) t iÓÄ° ‚àáU( ) t h( ) t i (10.27) =ÓÅò tdiagÓÄí 1‚àíÓÄê h( ) tÓÄë2ÓÄì (‚àáh( ) t L)x( ) tÓÄæ(10.28) Wedonotneedtocomputethegradientwithrespecttox( ) tfortrainingbecause itdoesnothaveanyparametersasancestorsinthecomputational graphdeÔ¨Åning theloss. 3 8 6 CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS 10.2.3RecurrentNetworksasDirectedGraphicalModels Intheexamplerecurrentnetworkwehavedevelopedsofar,thelosses L( ) twere cross-entropiesbetweentrainingtargetsy( ) tandoutputso( ) t.Aswithafeedforward network,itisinprinciplepossibletousealmostanylosswitharecurrentnetwork. Thelossshouldbechosenbasedonthetask.Aswithafeedforwardnetwork,we usuallywishtointerprettheoutputoftheRNNasaprobabilitydistribution,and weusuallyusethecross-entropyassociatedwiththatdistributiontodeÔ¨Ånetheloss. Meansquarederroristhecross-entropylossassociatedwithanoutputdistribution thatisaunitGaussian,forexample,justaswithafeedforwardnetwork. When we use apredictivelog-likelihood trainingobjective,such asequa- tion,wetraintheRNNtoestimatetheconditionaldistributionofthenext 10.12 sequenceelementy( ) tgiventhepastinputs.Thismaymeanthatwemaximize thelog-likelihood log( py( ) t|x( 1 ), . . . ,x( ) t) , (10.29) or,ifthemodelincludesconnectionsfromtheoutputatonetimesteptothenext timestep, log( py( ) t|x( 1 ), . . . ,x( ) t,y( 1 ), . . . ,y( 1 ) t ‚àí) . (10.30) Decomposingthejointprobabilityoverthesequenceofyvaluesasaseriesof one-stepprobabilisticpredictionsisonewaytocapturethefulljointdistribution acrossthewholesequence.Whenwedonotfeedpastyvaluesasinputsthat conditionthenextstepprediction,thedirectedgraphicalmodelcontainsnoedges fromanyy( ) iinthepasttothecurrenty( ) t.Inthiscase,theoutputsyare conditionallyindependentgiventhesequenceofxvalues.Whenwedofeedthe actualyvalues(nottheirprediction,buttheactualobservedorgeneratedvalues) backintothenetwork,thedirectedgraphicalmodelcontainsedgesfromally( ) i valuesinthepasttothecurrent y( ) tvalue. Asasimpleexample,letusconsiderthecasewheretheRNNmodelsonlya sequenceofscalarrandomvariables Y={y( 1 ), . . . ,y( ) œÑ},withnoadditionalinputs x.Theinputattimestep tissimplytheoutputattimestep t‚àí1.TheRNNthen deÔ¨Ånesadirectedgraphicalmodelovertheyvariables.Weparametrizethejoint distributionoftheseobservationsusingthechainrule(equation)forconditional3.6 probabilities: P P () = Y ( y( 1 ), . . . , y( ) œÑ) =œÑÓÅô t = 1P( y( ) t| y( 1 ) t ‚àí, y( 2 ) t ‚àí, . . . , y( 1 ))(10.31) wheretheright-handsideofthebarisemptyfor t=1,ofcourse.Hencethe negativelog-likelihoodofasetofvalues { y( 1 ), . . . , y( ) œÑ}accordingtosuchamodel 3 8 7 CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS y( 1 )y( 1 )y( 2 )y( 2 )y( 3 )y( 3 )y( 4 )y( 4 )y( 5 )y( 5 )y( ) . . .y( ) . . . Figure10.7:Fullyconnectedgraphicalmodelforasequence y( 1 ), y( 2 ), . . . , y( ) t, . . .:every pastobservation y( ) imayinÔ¨Çuencetheconditionaldistributionofsome y( ) t(for t > i), giventhepreviousvalues.Parametrizingthegraphicalmodeldirectlyaccordingtothis graph(asinequation)mightbeveryineÔ¨Écient,withanevergrowingnumberof 10.6 inputsandparametersforeachelementofthesequence.RNNsobtainthesamefull connectivitybuteÔ¨Écientparametrization,asillustratedinÔ¨Ågure.10.8 is L=ÓÅò tL( ) t(10.32) where L( ) t= log( ‚àí Py( ) t= y( ) t| y( 1 ) t ‚àí, y( 2 ) t ‚àí, . . . , y( 1 )) .(10.33) y( 1 )y( 1 )y( 2 )y( 2 )y( 3 )y( 3 )y( 4 )y( 4 )y( 5 )y( 5 )y( ) . . .y( ) . . .h( 1 )h( 1 )h( 2 )h( 2 )h( 3 )h( 3 )h( 4 )h( 4 )h( 5 )h( 5 )h( ) . . .h( ) . . . Figure10.8:IntroducingthestatevariableinthegraphicalmodeloftheRNN,even thoughitisadeterministicfunctionofitsinputs,helpstoseehowwecanobtainavery eÔ¨Écientparametrization,basedonequation.Everystageinthesequence(for 10.5 h( ) t andy( ) t)involvesthesamestructure(thesamenumberofinputsforeachnode)andcan sharethesameparameterswiththeotherstages. Theedgesinagraphicalmodelindicatewhichvariablesdependdirectlyonother</div>
        </div>
    </div>

    <div class="question-card" id="q117">
        <div class="question-header">
            <span class="question-number">Question 117</span>
            <span class="question-type">multiple-choice</span>
        </div>

        <div class="question-text">Optimization algorithms often rely on matrix calculus concepts to efficiently find minima or maxima in high-dimensional spaces. Understanding the role of derivatives and matrix structures is essential for designing and analyzing such algorithms.

Which statement best describes the relationship between the Hessian matrix and optimal learning rate selection in gradient descent for quadratic functions?

1) The optimal learning rate is inversely proportional to the largest eigenvalue of the Hessian matrix.   
2) The optimal learning rate is directly proportional to the trace of the Hessian matrix.   
3) The optimal learning rate is determined solely by the Jacobian matrix.   
4) The optimal learning rate is the geometric mean of all eigenvalues of the Hessian matrix.   
5) The optimal learning rate is proportional to the determinant of the Hessian matrix.   
6) The optimal learning rate does not depend on the Hessian matrix in quadratic functions.   
7) The optimal learning rate is inversely proportional to the smallest eigenvalue of the Hessian matrix.</div>

        <div class="answer-section">
            <div class="answer-label">‚úì Correct Answer:</div>
            <div class="answer-text">The correct answer is 1) The optimal learning rate is inversely proportional to the largest eigenvalue of the Hessian matrix..</div>
        </div>

        <div class="reference-section">
            <div class="reference-label">üìö Reference Text:</div>
            <button class="toggle-button" onclick="toggleReference(117)">
                Show/Hide Reference
            </button>
            <div id="ref117" class="reference-text hidden">CHAPTER4.NUMERICALCOMPUTATION where ÓÄèisthe l e ar ni ng r at e,apositivescalardeterminingthesizeofthestep. Wecanchoose ÓÄèinseveraldiÔ¨Äerentways.Apopularapproachistoset ÓÄètoasmall constant.Sometimes,wecansolveforthestepsizethatmakesthedirectional derivativevanish.Anotherapproachistoevaluate f ÓÄè ( x‚àí‚àá x f()) xforseveral valuesof ÓÄèandchoosetheonethatresultsinthesmallestobjectivefunctionvalue. Thislaststrategyiscalleda l i ne se ar c h. Steepestdescentconvergeswheneveryelementofthegradientiszero(or,in practice,veryclosetozero).Insomecases,wemaybeabletoavoidrunningthis iterativealgorithm,andjustjumpdirectlytothecriticalpointbysolvingthe equation ‚àá x f() = 0 xfor. x Althoughgradientdescentislimitedtooptimization incontinuousspaces,the generalconceptofrepeatedlymakingasmallmove(thatisapproximately thebest smallmove)towardsbetterconÔ¨Ågurations canbegeneralizedtodiscretespaces. Ascendinganobjectivefunctionofdiscreteparametersiscalled hi l l c l i m bi ng ( ,). RusselandNorvig2003 4 . 3 . 1 B ey o n d t h e G ra d i en t : Ja co b i a n a n d Hessi a n Ma t ri ces SometimesweneedtoÔ¨Åndallofthepartialderivativesofafunctionwhoseinput andoutputarebothvectors.Thematrixcontainingallsuchpartialderivativesis knownasa J ac o bi an m at r i x.SpeciÔ¨Åcally,ifwehaveafunction f: Rm‚Üí Rn, thentheJacobianmatrix J‚àà Rn m √óofisdeÔ¨Ånedsuchthat f J i , j=‚àÇ ‚àÇ x jf() x i. Wearealsosometimesinterestedinaderivativeofaderivative.Thisisknown asa se c o nd der i v at i v e.Forexample,forafunction f: Rn‚Üí R,thederivative withrespectto x iofthederivativeof fwithrespectto x jisdenotedas‚àÇ2 ‚àÇ x i ‚àÇ x jf. Inasingledimension,wecandenoted2 d x2 fby fÓÄ∞ ÓÄ∞( x).Thesecondderivativetells ushowtheÔ¨Årstderivativewillchangeaswevarytheinput.Thisisimportant becauseittellsuswhetheragradientstepwillcauseasmuchofanimprovement aswewouldexpectbasedonthegradientalone.Wecanthinkofthesecond derivativeasmeasuring c ur v at ur e.Supposewehaveaquadraticfunction(many functionsthatariseinpracticearenotquadraticbutcanbeapproximated well asquadratic,atleastlocally).Ifsuchafunctionhasasecondderivativeofzero, thenthereisnocurvature.ItisaperfectlyÔ¨Çatline,anditsvaluecanbepredicted usingonlythegradient.Ifthegradientis,thenwecanmakeastepofsize 1 ÓÄè alongthenegativegradient,andthecostfunctionwilldecreaseby ÓÄè.Ifthesecond derivativeisnegative,thefunctioncurvesdownward,sothecostfunctionwill actuallydecreasebymorethan ÓÄè.Finally,ifthesecondderivativeispositive,the functioncurvesupward,sothecostfunctioncandecreasebylessthan ÓÄè.See 8 6 CHAPTER4.NUMERICALCOMPUTATION xf x()N e g a t i v e c u r v a t u r e xf x()N o c u r v a t u r e xf x()P o s i t i v e c u r v a t u r e Figure4.4:Thesecondderivativedeterminesthecurvatureofafunction.Hereweshow quadraticfunctionswithvariouscurvature.Thedashedlineindicatesthevalueofthecost functionwewouldexpectbasedonthegradientinformationaloneaswemakeagradient stepdownhill.Inthecaseofnegativecurvature,thecostfunctionactuallydecreasesfaster thanthegradientpredicts.Inthecaseofnocurvature,thegradientpredictsthedecrease correctly.Inthecaseofpositivecurvature,thefunctiondecreasesslowerthanexpected andeventuallybeginstoincrease,sostepsthataretoolargecanactuallyincreasethe functioninadvertently. Ô¨ÅguretoseehowdiÔ¨ÄerentformsofcurvatureaÔ¨Äecttherelationshipbetween 4.4 thevalueofthecostfunctionpredictedbythegradientandthetruevalue. Whenourfunctionhasmultipleinputdimensions,therearemanysecond derivatives.Thesederivativescanbecollectedtogetherintoamatrixcalledthe Hessian m at r i x.TheHessianmatrix isdeÔ¨Ånedsuchthat H x()( f) H x()( f) i , j=‚àÇ2 ‚àÇ x i ‚àÇ x jf .() x (4.6) Equivalently,theHessianistheJacobianofthegradient. Anywherethatthesecondpartialderivativesarecontinuous,thediÔ¨Äerential operatorsarecommutative,i.e.theirordercanbeswapped: ‚àÇ2 ‚àÇ x i ‚àÇ x jf() = x‚àÇ2 ‚àÇ x j ‚àÇ x if .() x (4.7) Thisimpliesthat H i , j= H j , i,sotheHessianmatrixissymmetricatsuchpoints. Mostofthefunctionsweencounterinthecontextofdeeplearninghaveasymmetric Hessianalmosteverywhere. Because theHessianmatrixisrealandsymmetric, wecandecomposeitintoasetofrealeigenvaluesandanorthogonalbasisof 8 7 CHAPTER4.NUMERICALCOMPUTATION eigenvectors.ThesecondderivativeinaspeciÔ¨Åcdirectionrepresentedbyaunit vector disgivenby dÓÄæH d.When disaneigenvectorof H,thesecondderivative inthatdirectionisgivenbythecorrespondingeigenvalue.Forotherdirectionsof d,thedirectionalsecondderivativeisaweightedaverageofalloftheeigenvalues, withweightsbetween0and1,andeigenvectorsthathavesmalleranglewith d receivingmoreweight.Themaximumeigenvaluedeterminesthemaximumsecond derivativeandtheminimumeigenvaluedeterminestheminimumsecondderivative. The(directional)secondderivativetellsushowwellwecanexpectagradient descentsteptoperform.Wecanmakeasecond-orderTaylorseriesapproximation tothefunction aroundthecurrentpoint f() x x( 0 ): f f () x‚âà( x( 0 ))+( x x‚àí( 0 ))ÓÄæg+1 2( x x‚àí( 0 ))ÓÄæH x x (‚àí( 0 )) .(4.8) where gisthegradientand HistheHessianat x( 0 ). Ifweusealearningrate of ÓÄè,thenthenewpoint xwillbegivenby x( 0 )‚àí ÓÄè g.Substitutingthisintoour approximation,weobtain f( x( 0 )‚àí ‚âà ÓÄè g) f( x( 0 ))‚àí ÓÄè gÓÄæg+1 2ÓÄè2gÓÄæH g . (4.9) Therearethree termshere:theoriginalvalue ofthefunction, the expected improvementduetotheslopeofthefunction,andthecorrectionwemustapply toaccountforthecurvatureofthefunction.Whenthislasttermistoolarge,the gradientdescentstepcanactuallymoveuphill.When gÓÄæH giszeroornegative, theTaylorseriesapproximationpredictsthatincreasing ÓÄèforeverwilldecrease f forever.Inpractice,theTaylorseriesisunlikelytoremainaccurateforlarge ÓÄè,so onemustresorttomoreheuristicchoicesof ÓÄèinthiscase.When gÓÄæH gispositive, solvingfortheoptimalstepsizethatdecreasestheTaylorseriesapproximation of thefunctionthemostyields ÓÄè‚àó=gÓÄæg gÓÄæH g. (4.10) Intheworstcase,when galignswiththeeigenvectorof Hcorrespondingtothe maximaleigenvalue Œª m a x,thenthisoptimalstepsizeisgivenby1 Œªmax.Tothe extentthatthefunctionweminimizecanbeapproximatedwellbyaquadratic function,theeigenvaluesoftheHessianthusdeterminethescaleofthelearning rate. Thesecondderivativecanbeusedtodeterminewhetheracriticalpointis alocalmaximum,alocalminimum,orsaddlepoint.Recallthatonacritical point, fÓÄ∞( x) = 0.Whenthesecondderivative fÓÄ∞ ÓÄ∞( x) >0,theÔ¨Årstderivative fÓÄ∞( x) increasesaswemovetotherightanddecreasesaswemovetotheleft.Thismeans 8 8 CHAPTER4.NUMERICALCOMPUTATION fÓÄ∞( x ÓÄè‚àí) <0and fÓÄ∞( x+ ÓÄè) >0forsmallenough ÓÄè.Inotherwords,aswemove right,theslopebeginstopointuphilltotheright,andaswemoveleft,theslope beginstopointuphilltotheleft. Thus,when fÓÄ∞( x)=0and fÓÄ∞ ÓÄ∞( x) >0,wecan concludethat xisalocalminimum.Similarly,when fÓÄ∞( x) = 0and fÓÄ∞ ÓÄ∞( x) <0,we canconcludethat xisalocalmaximum.Thisisknownasthe se c o nd der i v at i v e t e st.Unfortunately,when fÓÄ∞ ÓÄ∞( x) = 0,thetestisinconclusive.Inthiscase xmay beasaddlepoint,orapartofaÔ¨Çatregion. Inmultipledimensions,weneedtoexamineallofthesecondderivativesofthe</div>
        </div>
    </div>

    <div class="question-card" id="q118">
        <div class="question-header">
            <span class="question-number">Question 118</span>
            <span class="question-type">multiple-choice</span>
        </div>

        <div class="question-text">Energy-based models (EBMs) are a foundational approach in probabilistic machine learning, defining probability distributions in terms of an energy function and closely related to graphical models. Understanding how EBMs relate to concepts like conditional independence and graphical structure is crucial for designing effective models.

In undirected probabilistic graphical models such as Markov random fields, which criterion determines whether two sets of variables are conditionally independent given a set of observed variables?

1) Marginalization over latent variables   
2) Maximizing log-likelihood of observed data   
3) D-separation through directed edges   
4) Factorization based on chain rule   
5) Separation‚Äîif all paths between the sets pass through observed variables   
6) Parameterization using clique potentials   
7) Deterministic mapping of variable domains</div>

        <div class="answer-section">
            <div class="answer-label">‚úì Correct Answer:</div>
            <div class="answer-text">The correct answer is 5) Separation‚Äîif all paths between the sets pass through observed variables.</div>
        </div>

        <div class="reference-section">
            <div class="reference-label">üìö Reference Text:</div>
            <button class="toggle-button" onclick="toggleReference(118)">
                Show/Hide Reference
            </button>
            <div id="ref118" class="reference-text hidden">j=1)for jÓÄ∂= i. Often,itispossibletoleverage theeÔ¨Äectofacarefullychosendomainofavariableinordertoobtaincomplicated behaviorfromarelativelysimplesetof œÜfunctions.Wewillexploreapractical applicationofthisidealater,insection.20.6 1 6 . 2 . 4 E n erg y-B a s ed Mo d el s Manyinterestingtheoreticalresultsaboutundirectedmodelsdependontheas- sumptionthat‚àÄx ,Àú p(x) >0.Aconvenientwaytoenforcethisconditionistouse an (EBM)where energy-basedmodel Àú p E () = exp( x ‚àí())x (16.7) 5 6 9 CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING a b c d e f Figure16.4:Thisgraphimpliesthat p(abcdef , , , , ,)canbewrittenas 1 ZœÜ a b ,(ab ,) œÜ b c ,(bc ,) œÜ a d ,(ad ,) œÜ b e ,(be ,) œÜ e f ,(ef ,)foranappropriatechoiceofthe œÜfunc- tions. and E(x)isknownastheenergyfunction.Becauseexp( z)ispositiveforall z,thisguaranteesthatnoenergyfunctionwillresultinaprobabilityofzero foranystatex.Beingcompletely free to choose theenergyfunction makes learningsimpler.Ifwelearnedthecliquepotentialsdirectly,wewouldneedtouse constrainedoptimization toarbitrarilyimposesomespeciÔ¨Åcminimalprobability value.Bylearningtheenergyfunction,wecanuseunconstrainedoptimization.5 Theprobabilitiesinanenergy-basedmodelcanapproacharbitrarilyclosetozero butneverreachit. Anydistributionoftheformgivenbyequationisanexampleofa 16.7 Boltz- mann distribution.For this reason, manyenergy-based models are called Boltzmannmachines(Fahlman 1983Ackley1985Hinton e t a l .,; e t a l .,; e t a l ., 1984HintonandSejnowski1986 ; ,).Thereisnoacceptedguidelineforwhentocall amodelanenergy-basedmodelandwhentocallitaBoltzmannmachine.The termBoltzmannmachinewasÔ¨Årstintroducedtodescribeamodelwithexclusively binaryvariables,buttodaymanymodelssuchasthemean-covariancerestricted Boltzmannmachineincorporatereal-valuedvariablesaswell.WhileBoltzmann machineswereoriginallydeÔ¨Ånedtoencompassbothmodelswithandwithoutla- tentvariables,thetermBoltzmannmachineistodaymostoftenusedtodesignate modelswithlatentvariables,whileBoltzmannmachineswithoutlatentvariables aremoreoftencalledMarkovrandomÔ¨Åeldsorlog-linearmodels. Cliquesinanundirectedgraphcorrespondtofactorsoftheunnormalized probabilityfunction.Becauseexp( a)exp( b) =exp( a+ b),thismeansthatdiÔ¨Äerent cliquesintheundirectedgraphcorrespondtothediÔ¨Äerenttermsoftheenergy function.Inotherwords,anenergy-basedmodelisjustaspecialkindofMarkov network:theexponentiationmakeseachtermintheenergyfunctioncorrespond toafactorforadiÔ¨Äerentclique.SeeÔ¨Ågureforanexampleofhowtoreadthe 16.5 5F o r s o m e m o d e l s , we m a y s t i l l n e e d t o u s e c o n s t ra i n e d o p t i m i z a t i o n t o m a k e s u re e x i s t s . Z 5 7 0 CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING a b c d e f Figure 16.5:Thisgraph impliesthat E(abcdef , , , , ,)can be writtenas E a b ,(ab ,)+ E b c ,(bc ,)+ E a d ,(ad ,)+ E b e ,(be ,)+ E e f ,(ef ,)foranappropriatechoiceoftheper-clique energyfunctions.Notethatwecanobtainthe œÜfunctionsinÔ¨Ågurebysettingeach 16.4 œÜ totheexponentialofthecorrespondingnegativeenergy,e.g., œÜ a b ,(ab ,) =exp(()) ‚àí Eab ,. formoftheenergyfunctionfromanundirectedgraphstructure.Onecanviewan energy-basedmodelwithmultipletermsinitsenergyfunctionasbeingaproduct ofexperts(Hinton1999,).Eachtermintheenergyfunctioncorrespondsto anotherfactorintheprobabilitydistribution.Eachtermoftheenergyfunctioncan bethoughtofasan‚Äúexpert‚Äùthatdetermineswhetheraparticularsoftconstraint issatisÔ¨Åed.Eachexpertmayenforceonlyoneconstraintthatconcernsonly alow-dimensionalprojectionoftherandomvariables,butwhencombinedby multiplicationofprobabilities, theexpertstogetherenforceacomplicatedhigh- dimensionalconstraint. OnepartofthedeÔ¨Ånitionofanenergy-basedmodelservesnofunctionalpurpose fromamachinelearningpointofview:the‚àísigninequation.This16.7 ‚àísign couldbeincorporatedintothedeÔ¨Ånitionof E.Formanychoicesofthefunction E,thelearningalgorithmisfreetodeterminethesignoftheenergyanyway.The ‚àísignispresentprimarilytopreservecompatibilitybetweenthemachinelearning literatureandthephysicsliterature.Manyadvancesinprobabilisticmodeling wereoriginallydevelopedbystatisticalphysicists,forwhom Ereferstoactual, physicalenergyanddoesnothavearbitrarysign. Terminologysuchas‚Äúenergy‚Äù and‚Äúpartitionfunction‚Äùremainsassociatedwiththesetechniques,eventhough theirmathematical applicabilityisbroaderthanthephysicscontextinwhichthey weredeveloped.Somemachinelearningresearchers(e.g., (),who Smolensky1986 referredtonegativeenergyasharmony)havechosentoemitthenegation,but thisisnotthestandardconvention. Manyalgorithmsthatoperateonprobabilisticmodelsdonotneedtocompute p m o de l( x)butonly log Àú p m o de l( x).Forenergy-basedmodelswithlatentvariables h, thesealgorithmsaresometimesphrasedintermsofthenegativeofthisquantity, 5 7 1 CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING a s b a s b (a) (b) Figure16.6:(a)Thepathbetweenrandomvariableaandrandomvariablebthroughsis active,becausesisnotobserved.Thismeansthataandbarenotseparated.(b)Heres isshadedin,toindicatethatitisobserved.Becausetheonlypathbetweenaandbis throughs,andthatpathisinactive,wecanconcludethataandbareseparatedgivens. calledthe :freeenergy F ‚àí () = x logÓÅò hexp(( )) ‚àí E x h , . (16.8) Inthisbook,weusuallypreferthemoregeneral log Àú p m o de l() xformulation. 1 6 . 2 . 5 S ep a ra t i o n a n d D - S ep a r a t i o n Theedgesinagraphicalmodeltelluswhichvariablesdirectlyinteract.Weoften needtoknowwhichvariables i ndir e c t l yinteract.Someoftheseindirectinteractions canbeenabledordisabledbyobservingothervariables.Moreformally,wewould liketoknowwhichsubsetsofvariablesareconditionallyindependentfromeach other,giventhevaluesofothersubsetsofvariables. Identifyingtheconditionalindependencesinagraphisverysimpleinthecase ofundirectedmodels.Inthiscase,conditionalindependenceimpliedbythegraph iscalledseparation.Wesaythatasetofvariables Aisseparatedfromanother setofvariables Bgivenathirdsetofvariables Sifthegraphstructureimpliesthat Aisindependentfrom Bgiven S.Iftwovariablesaandbareconnectedbyapath involvingonlyunobservedvariables,thenthosevariablesarenotseparated.Ifno pathexistsbetweenthem,orallpathscontainanobservedvariable,thentheyare separated.Werefertopathsinvolvingonlyunobservedvariablesas‚Äúactive‚Äùand pathsincludinganobservedvariableas‚Äúinactive.‚Äù Whenwedrawagraph,wecanindicateobservedvariablesbyshadingthemin. SeeÔ¨Ågureforadepictionofhowactiveandinactivepathsinanundirected 16.6 modellookwhendrawninthisway.SeeÔ¨Ågureforanexampleofreading 16.7 separationfromanundirectedgraph. Similar concepts apply todirected models ,except that inthe context of directedmodels,theseconceptsarereferredtoasd-separation.The‚Äúd‚Äùstands for‚Äúdependence.‚Äù D-separati onfordirectedgraphsisdeÔ¨Ånedthesameasseparation 5 7 2 CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING a b c d Figure16.7:Anexampleofreadingseparationpropertiesfromanundirectedgraph.Here bisshadedtoindicatethatitisobserved.Becauseobservingbblockstheonlypathfrom atoc,wesaythataandcareseparatedfromeachothergivenb.Theobservationofb alsoblocksonepathbetweenaandd,butthereisasecond,activepathbetweenthem. Therefore,aanddarenotseparatedgivenb. forundirectedgraphs:Wesaythatasetofvariables Aisd-separatedfromanother setofvariables Bgivenathirdsetofvariables Sifthegraphstructureimplies thatisindependentfromgiven. A B S Aswithundirectedmodels,wecanexaminetheindependencesimpliedbythe graphbylookingatwhatactivepathsexistinthegraph.Asbefore,twovariables aredependentifthereisanactivepathbetweenthem,andd-separatedifnosuch pathexists.Indirectednets,determiningwhetherapathisactiveissomewhat morecomplicated. SeeÔ¨Ågureforaguidetoidentifyingactivepathsina 16.8 directedmodel.SeeÔ¨Ågureforanexampleofreadingsomepropertiesfroma 16.9 graph. Itisimportanttorememberthatseparationandd-separationtellusonly aboutthoseconditionalindependences t h a t a r e i m p l i e d b y t h</div>
        </div>
    </div>

    <div class="question-card" id="q119">
        <div class="question-header">
            <span class="question-number">Question 119</span>
            <span class="question-type">multiple-choice</span>
        </div>

        <div class="question-text">In machine learning, the effectiveness of algorithms often hinges on the way data is represented and the features extracted from that data. Deep learning has introduced advanced methods for automating the discovery of useful data representations, enabling significant progress in tasks such as image and speech recognition.

Which approach enables neural networks to automatically discover hierarchical representations by composing multiple layers, allowing them to disentangle complex factors of variation from raw data?

1) Feature selection using principal component analysis (PCA)   
2) Manual feature engineering by domain experts   
3) Training shallow decision trees   
4) Applying K-means clustering for representation   
5) Deep learning with multilayer neural networks   
6) Aggregating features using ensemble methods   
7) Using support vector machines with kernel tricks</div>

        <div class="answer-section">
            <div class="answer-label">‚úì Correct Answer:</div>
            <div class="answer-text">The correct answer is 5) Deep learning with multilayer neural networks.</div>
        </div>

        <div class="reference-section">
            <div class="reference-label">üìö Reference Text:</div>
            <button class="toggle-button" onclick="toggleReference(119)">
                Show/Hide Reference
            </button>
            <div id="ref119" class="reference-text hidden">l o g i st i c r e g r e ssi o ncandeterminewhetherto recommendcesareandelivery(Mor-Yosef1990 e t a l .,).Asimplemachinelearning algorithmcalled nai v e B a y e scanseparatelegitimatee-mailfromspame-mail. Theperformanceofthesesimplemachinelearningalgorithmsdependsheavily onthe r e pr e se n t at i o nofthedatatheyaregiven.Forexample,whenlogistic regressionisusedtorecommendcesareandelivery,theAIsystemdoesnotexamine thepatientdirectly.Instead,thedoctortellsthesystemseveralpiecesofrelevant information, suchasthepresenceorabsenceofauterinescar.Eachpieceof informationincludedintherepresentationofthepatientisknownasa f e at ur e. Logisticregressionlearnshoweachofthesefeaturesofthepatientcorrelateswith variousoutcomes.However,itcannotinÔ¨Çuencethewaythatthefeaturesare deÔ¨Ånedinanyway. IflogisticregressionwasgivenanMRIscanofthepatient, ratherthanthedoctor‚Äôsformalizedreport,itwouldnotbeabletomakeuseful predictions.IndividualpixelsinanMRIscanhavenegligiblecorrelationwithany complications thatmightoccurduringdelivery. Thisdependenceonrepresentationsisageneralphenomenon thatappears throughoutcomputerscienceandevendailylife.Incomputerscience,opera- tionssuchassearchingacollectionofdatacanproceedexponentiallyfasterif thecollectionisstructuredandindexedintelligently.Peoplecaneasilyperform arithmeticonArabicnumerals,butÔ¨ÅndarithmeticonRomannumeralsmuch moretime-consuming. Itisnotsurprisingthatthechoiceofrepresentationhasan enormouseÔ¨Äectontheperformanceofmachinelearningalgorithms.Forasimple visualexample,seeÔ¨Ågure.1.1 ManyartiÔ¨Åcialintelligencetaskscanbesolvedbydesigningtherightsetof featurestoextractforthattask,thenprovidingthesefeaturestoasimplemachine learningalgorithm.Forexample,ausefulfeatureforspeakeridentiÔ¨Åcationfrom soundisanestimateofthesizeofspeaker‚Äôsvocaltract.Itthereforegivesastrong clueastowhetherthespeakerisaman,woman,orchild. However,formanytasks,itisdiÔ¨Éculttoknowwhatfeaturesshouldbeextracted. Forexample,supposethatwewouldliketowriteaprogramtodetectcarsin photographs. Weknowthatcarshavewheels,sowemightliketousethepresence ofawheelasafeature.Unfortunately,itisdiÔ¨Éculttodescribeexactlywhata wheellookslikeintermsofpixelvalues.Awheelhasasimplegeometricshapebut itsimagemaybecomplicatedbyshadowsfallingonthewheel,thesunglaringoÔ¨Ä themetalpartsofthewheel,thefenderofthecaroranobjectintheforeground obscuringpartofthewheel,andsoon. 3 CHAPTER1.INTRODUCTION ÓÅ∏ÓÅπÓÅÉ ÓÅ° ÓÅ≤ ÓÅ¥ ÓÅ• ÓÅ≥ÓÅ©ÓÅ°ÓÅÆÓÄ† ÓÅ£ ÓÅØ ÓÅØ ÓÅ≤ ÓÅ§ ÓÅ© ÓÅÆ ÓÅ° ÓÅ¥ ÓÅ• ÓÅ≥ ÓÅ≤ÓÇµÓÅê ÓÅØ ÓÅ¨ ÓÅ° ÓÅ≤ ÓÄ† ÓÅ£ ÓÅØ ÓÅØ ÓÅ≤ ÓÅ§ ÓÅ© ÓÅÆ ÓÅ° ÓÅ¥ ÓÅ• ÓÅ≥ Figure1.1:ExampleofdiÔ¨Äerentrepresentations:supposewewanttoseparatetwo categoriesofdatabydrawingalinebetweentheminascatterplot.Intheplotontheleft, werepresentsomedatausingCartesiancoordinates,andthetaskisimpossible.Intheplot ontheright,werepresentthedatawithpolarcoordinatesandthetaskbecomessimpleto solvewithaverticalline.FigureproducedincollaborationwithDavidWarde-Farley. Onesolutiontothisproblemistousemachinelearningtodiscovernotonly themappingfromrepresentationtooutputbutalsotherepresentationitself. Thisapproachisknownas r e pr e se n t at i o n l e ar ni ng. Learnedrepresentations oftenresultinmuchbetterperformancethancanbeobtainedwithhand-designed representations.TheyalsoallowAIsystemstorapidlyadapttonewtasks,with minimalhumanintervention.Arepresentationlearningalgorithmcandiscovera goodsetoffeaturesforasimpletaskinminutes,oracomplextaskinhoursto months.Manuallydesigningfeaturesforacomplextaskrequiresagreatdealof humantimeandeÔ¨Äort;itcantakedecadesforanentirecommunityofresearchers. Thequintessentialexampleofarepresentationlearningalgorithmisthe au- t o e nc o der.Anautoencoderisthecombinationofan e nc o derfunctionthat convertstheinputdataintoadiÔ¨Äerentrepresentation,anda dec o derfunction thatconvertsthenewrepresentationbackintotheoriginalformat.Autoencoders aretrainedtopreserveasmuchinformationaspossiblewhenaninputisrun throughtheencoderandthenthedecoder,butarealsotrainedtomakethenew representationhavevariousniceproperties.DiÔ¨Äerentkindsofautoencodersaimto achievediÔ¨Äerentkindsofproperties. Whendesigningfeaturesoralgorithmsforlearningfeatures,ourgoalisusually toseparatethe f ac t o r s o f v ar i at i o nthatexplaintheobserveddata.Inthis context,weusetheword‚Äúfactors‚ÄùsimplytorefertoseparatesourcesofinÔ¨Çuence; thefactorsareusuallynotcombinedbymultiplication. Suchfactorsareoftennot 4 CHAPTER1.INTRODUCTION quantitiesthataredirectlyobserved.Instead,theymayexisteitherasunobserved objectsorunobservedforcesinthephysicalworldthataÔ¨Äectobservablequantities. Theymayalsoexistasconstructsinthehumanmindthatprovideusefulsimplifying explanationsorinferredcausesoftheobserveddata.Theycanbethoughtofas conceptsorabstractionsthathelpusmakesenseoftherichvariabilityinthedata. Whenanalyzingaspeechrecording,thefactorsofvariationincludethespeaker‚Äôs age,theirsex,theiraccentandthewordsthattheyarespeaking.Whenanalyzing animageofacar,thefactorsofvariationincludethepositionofthecar,itscolor, andtheangleandbrightnessofthesun. AmajorsourceofdiÔ¨Écultyinmanyreal-worldartiÔ¨Åcialintelligenceapplications isthatmanyofthefactorsofvariationinÔ¨Çuenceeverysinglepieceofdataweare abletoobserve.Theindividualpixelsinanimageofaredcarmightbeveryclose toblackatnight.Theshapeofthecar‚Äôssilhouettedependsontheviewingangle. Mostapplicationsrequireusto thefactorsofvariationanddiscardthe d i s e nt a ng l e onesthatwedonotcareabout. Ofcourse,itcanbeverydiÔ¨Éculttoextractsuchhigh-level,abstractfeatures fromrawdata.Manyofthesefactorsofvariation,suchasaspeaker‚Äôsaccent, canbeidentiÔ¨Åedonlyusingsophisticated,nearlyhuman-levelunderstandingof thedata.WhenitisnearlyasdiÔ¨Éculttoobtainarepresentationastosolvethe originalproblem,representationlearningdoesnot,atÔ¨Årstglance,seemtohelpus. D e e p l e ar ni ngsolvesthiscentralprobleminrepresentationlearningbyintro- ducingrepresentationsthatareexpressedintermsofother,simplerrepresentations. Deeplearningallowsthecomputertobuildcomplexconceptsoutofsimplercon- cepts.Figureshowshowadeeplearningsystemcanrepresenttheconceptof 1.2 animageofapersonbycombiningsimplerconcepts,suchascornersandcontours, whichareinturndeÔ¨Ånedintermsofedges. Thequintessentialexampleofadeeplearningmodelisthefeedforwarddeep networkor m ul t i l a y e r p e r c e pt r o n(MLP).Amultilayerperceptronisjusta mathematical functionmappingsomesetofinputvaluestooutputvalues.The functionisformedbycomposingmanysimplerfunctions.Wecanthinkofeach applicationofadiÔ¨Äerentmathematical functionasprovidinganewrepresentation oftheinput. Theideaoflearningtherightrepresentationforthedataprovidesoneperspec- tiveondeeplearning.Anotherperspectiveondeeplearningisthatdepthallowsthe computertolearnamulti-stepcomputerprogram.Eachlayeroftherepresentation canbethoughtofasthestateofthecomputer‚Äôsmemoryafterexecutinganother setofinstructionsinparallel.Networkswithgreaterdepthcanexecutemore instructionsinsequence.SequentialinstructionsoÔ¨Äergreatpowerbecauselater 5</div>
        </div>
    </div>

    <div class="question-card" id="q120">
        <div class="question-header">
            <span class="question-number">Question 120</span>
            <span class="question-type">multiple-choice</span>
        </div>

        <div class="question-text">Deep Boltzmann Machines (DBMs) are probabilistic models designed for hierarchical representation learning, featuring layers of binary hidden units and undirected inter-layer connectivity. Their inference and training methods rely heavily on variational techniques and specialized sampling strategies due to computational challenges.

Which of the following procedures is specifically used in DBMs to approximate the intractable posterior by assuming conditional independence among hidden units and iteratively minimizing KL divergence?

1) Contrastive divergence   
2) Gibbs sampling with ancestral passes   
3) Maximum likelihood estimation via gradient ascent   
4) Pseudolikelihood optimization   
5) Persistent chains with stochastic gradient descent   
6) Annealed importance sampling   
7) Mean-field variational inference </div>

        <div class="answer-section">
            <div class="answer-label">‚úì Correct Answer:</div>
            <div class="answer-text">The correct answer is 7) Mean-field variational inference.</div>
        </div>

        <div class="reference-section">
            <div class="reference-label">üìö Reference Text:</div>
            <button class="toggle-button" onclick="toggleReference(120)">
                Show/Hide Reference
            </button>
            <div id="ref120" class="reference-text hidden">CHAPTER20.DEEPGENERATIVEMODELS fromoptimal.InDBMs,allofthehiddenunitswithinalayerareconditionally independentgiventheotherlayers. Thislackofintralayerinteractionmakesit possibletouseÔ¨Åxedpointequationstoactuallyoptimizethevariationallower boundandÔ¨ÅndthetrueoptimalmeanÔ¨Åeldexpectations(towithinsomenumerical tolerance). TheuseofpropermeanÔ¨Åeldallowstheapproximate inferenceprocedurefor DBMstocapturetheinÔ¨Çuenceoftop-downfeedbackinteractions. Thismakes DBMsinterestingfromthepointofviewofneuroscience,becausethehumanbrain isknowntousemanytop-downfeedbackconnections.Becauseofthisproperty, DBMshavebeenusedascomputational modelsofrealneuroscientiÔ¨Åcphenomena (,; Series e t a l .2010Reichert2011 e t a l .,). OneunfortunatepropertyofDBMsisthatsamplingfromthemisrelatively diÔ¨Écult.DBNsonlyneedtouseMCMCsamplingintheirtoppairoflayers.The otherlayersareusedonlyattheendofthesamplingprocess,inoneeÔ¨Écient ancestralsamplingpass.TogenerateasamplefromaDBM,itisnecessaryto useMCMCacrossalllayers,witheverylayerofthemodelparticipating inevery Markovchaintransition. 20.4.2DBMMeanFieldInference TheconditionaldistributionoveroneDBMlayergiventheneighboringlayersis factorial.IntheexampleoftheDBMwithtwohiddenlayers,thesedistributions are P( v h|( 1 )), P( h( 1 )| v h ,( 2 ))and P( h( 2 )| h( 1 )).Thedistributionover a l l hiddenlayersgenerallydoesnotfactorizebecauseofinteractionsbetweenlayers. Intheexamplewithtwohiddenlayers, P( h( 1 ), h( 2 )| v)doesnotfactorizeduedue totheinteractionweights W( 2 )between h( 1 )and h( 2 )whichrenderthesevariables mutuallydependent. AswasthecasewiththeDBN,wearelefttoseekoutmethodstoapproximate theDBMposteriordistribution. However,unliketheDBN,theDBMposterior distributionovertheirhiddenunits‚Äîwhilecomplicated‚Äîis easytoapproximate withavariationalapproximation(asdiscussedinsection), speciÔ¨Åcallya 19.4 meanÔ¨Åeldapproximation. ThemeanÔ¨Åeldapproximation isasimpleformof variationalinference,wherewerestricttheapproximatingdistributiontofully factorialdistributions.InthecontextofDBMs,themeanÔ¨Åeldequationscapture thebidirectionalinteractionsbetweenlayers.Inthissectionwederivetheiterative approximateinferenceprocedureoriginallyintroducedinSalakhutdinovandHinton ().2009a Invariationalapproximations toinference,weapproachthetaskofapproxi- 6 6 6 CHAPTER20.DEEPGENERATIVEMODELS matingaparticulartargetdistribution‚Äîinourcase,theposteriordistributionover thehiddenunitsgiventhevisibleunits‚Äîbysomereasonablysimplefamilyofdis- tributions.InthecaseofthemeanÔ¨Åeldapproximation, theapproximating family isthesetofdistributionswherethehiddenunitsareconditionallyindependent. WenowdevelopthemeanÔ¨Åeldapproachfortheexamplewithtwohidden layers.Let Q( h( 1 ), h( 2 )| v)betheapproximation of P( h( 1 ), h( 2 )| v).Themean Ô¨Åeldassumptionimpliesthat Q( h( 1 ), h( 2 )| v) =ÓÅô jQ h(( 1 ) j| v)ÓÅô kQ h(( 2 ) k| v) .(20.29) ThemeanÔ¨ÅeldapproximationattemptstoÔ¨Åndamemberofthisfamilyof distributionsthatbestÔ¨Åtsthetrueposterior P( h( 1 ), h( 2 )| v). Importantly ,the inferenceprocessmustberunagaintoÔ¨ÅndadiÔ¨Äerentdistribution Qeverytime weuseanewvalueof. v Onecanconceiveofmanywaysofmeasuringhowwell Q( h v|)Ô¨Åts P( h v|). ThemeanÔ¨Åeldapproachistominimize KL( ) = Q PÓÅ´ÓÅò hQ( h( 1 ), h( 2 )| v)logÓÄ† Q( h( 1 ), h( 2 )| v) P( h( 1 ) , h( 2 )| v)ÓÄ° .(20.30) Ingeneral,wedonothavetoprovideaparametricformoftheapproximating distributionbeyondenforcingtheindependenceassumptions.Thevariational approximationprocedureisgenerallyabletorecoverafunctionalformofthe approximatedistribution.However,inthecaseofameanÔ¨Åeldassumptionon binaryhiddenunits(thecasewearedevelopinghere)thereisnolossofgenerality resultingfromÔ¨Åxingaparametrization ofthemodelinadvance. Weparametrize QasaproductofBernoullidistributions,thatisweassociate theprobabilityofeachelementof h( 1 )withaparameter.SpeciÔ¨Åcally,foreach j, ÀÜh( 1 ) j= Q( h( 1 ) j= 1| v),where ÀÜh( 1 ) j‚àà[0 ,1]andforeach k,ÀÜh( 2 ) k= Q( h( 2 ) k= 1| v), where ÀÜ h( 2 ) k‚àà[01] ,.Thuswehavethefollowingapproximationtotheposterior: Q( h( 1 ), h( 2 )| v) =ÓÅô jQ h(( 1 ) j| v)ÓÅô kQ h(( 2 ) k| v) (20.31) =ÓÅô j(ÀÜ h( 1 ) j)h( 1 ) j(1‚àíÀÜh( 1 ) j)( 1‚àí h( 1 ) j )√óÓÅô k(ÀÜh( 2 ) k)h( 2 ) k(1‚àíÀÜh( 2 ) k)( 1‚àí h( 2 ) k). (20.32) Ofcourse,forDBMswithmorelayerstheapproximateposteriorparametrization canbeextendedintheobviousway,exploitingthebipartitestructureofthegraph 6 6 7 CHAPTER20.DEEPGENERATIVEMODELS toupdatealloftheevenlayerssimultaneouslyandthentoupdatealloftheodd layerssimultaneously,followingthesamescheduleasGibbssampling. NowthatwehavespeciÔ¨Åedourfamilyofapproximating distributions Q,it remainstospecifyaprocedureforchoosingthememberofthisfamilythatbest Ô¨Åts P.ThemoststraightforwardwaytodothisistousethemeanÔ¨Åeldequations speciÔ¨Åedbyequation.Theseequationswerederivedbysolvingforwherethe 19.56 derivativesofthevariationallowerboundarezero.Theydescribeinanabstract mannerhowtooptimizethevariationallowerboundforanymodel,simplyby takingexpectationswithrespectto. Q Applyingthesegeneralequations,weobtaintheupdaterules(again,ignoring biasterms): ÀÜh( 1 ) j= œÉÓÄ†ÓÅò iv i W( 1 ) i , j+ÓÅò kÓÄ∞W( 2 ) j , kÓÄ∞ÀÜ h( 2 ) kÓÄ∞ÓÄ° , j‚àÄ (20.33) ÀÜh( 2 ) k= œÉÔ£´ Ô£≠ÓÅò jÓÄ∞W( 2 ) jÓÄ∞ , kÀÜh( 1 ) jÓÄ∞Ô£∂ Ô£∏ , k .‚àÄ (20.34) AtaÔ¨Åxedpointofthissystemofequations,wehavealocalmaximumofthe variationallowerbound L( Q).ThustheseÔ¨ÅxedpointupdateequationsdeÔ¨Ånean iterativealgorithmwherewealternateupdatesofÀÜh( 1 ) j(usingequation)and20.33 updatesofÀÜh( 2 ) k(usingequation).OnsmallproblemssuchasMNIST,asfew 20.34 asteniterationscanbesuÔ¨ÉcienttoÔ¨Åndanapproximate positivephasegradient forlearning,andÔ¨ÅftyusuallysuÔ¨Écetoobtainahighqualityrepresentationof asinglespeciÔ¨Åcexampletobeusedforhigh-accuracy classiÔ¨Åcation.Extending approximatevariationalinferencetodeeperDBMsisstraightforward. 20.4.3DBMParameterLearning LearningintheDBMmustconfrontboththechallengeofanintractablepartition function,usingthetechniquesfromchapter,andthechallengeofanintractable 18 posteriordistribution,usingthetechniquesfromchapter.19 Asdescribedinsection,variationalinferenceallowstheconstructionof 20.4.2 adistribution Q( h v|)thatapproximates theintractable P( h v|).Learningthen proceedsbymaximizing L( v Œ∏ , Q ,),thevariationallowerboundontheintractable log-likelihood, . log(;) P v Œ∏ 6 6 8 CHAPTER20.DEEPGENERATIVEMODELS ForadeepBoltzmannmachinewithtwohiddenlayers,isgivenby L L( ) = Q , Œ∏ÓÅò iÓÅò jÓÄ∞v i W( 1 ) i , jÓÄ∞ÀÜh( 1 ) jÓÄ∞+ÓÅò jÓÄ∞ÓÅò kÓÄ∞ÀÜh( 1 ) jÓÄ∞ W( 2 ) jÓÄ∞ , kÓÄ∞ÀÜh( 2 ) kÓÄ∞‚àí H log()+ Z Œ∏ () Q .(20.35) Thisexpressionstillcontainsthelogpartitionfunction, log Z( Œ∏).Becauseadeep BoltzmannmachinecontainsrestrictedBoltzmannmachinesascomponents,the hardnessresultsforcomputingthepartitionfunctionandsamplingthatapplyto restrictedBoltzmannmachinesalsoapplytodeepBoltzmannmachines.Thismeans thatevaluatingtheprobabilitymassfunctionofaBoltzmannmachinerequires approximatemethodssuchasannealedimportancesampling.Likewise,training themodelrequiresapproximationstothegradientofthelogpartitionfunction.See chapterforageneraldescriptionofthesemethods.DBMsaretypicallytrained 18 usingstochasticmaximumlikelihood.Manyoftheothertechniquesdescribedin chapterarenotapplicable.Techniquessuchaspseudolikelihoodrequirethe 18 abilitytoevaluatetheunnormalized probabilities, ratherthanmerelyobtaina variationallowerboundonthem.ContrastivedivergenceisslowfordeepBoltzmann machinesbecausetheydonotalloweÔ¨Écientsamplingofthehiddenunitsgiventhe visibleunits‚Äîinstead,contrastivedivergencewouldrequireburninginaMarkov chaineverytimeanewnegativephasesampleisneeded. Thenon-variationalversionofstochasticmaximumlikelihoodalgorithmwas discussedearlier,insection. Variationalstochasticmaximumlikelihoodas 18.2 appliedtotheDBMisgiveninalgorithm .RecallthatwedescribeasimpliÔ¨Åed</div>
        </div>
    </div>

    <div class="question-card" id="q121">
        <div class="question-header">
            <span class="question-number">Question 121</span>
            <span class="question-type">multiple-choice</span>
        </div>

        <div class="question-text">Convolutional neural networks (CNNs) are foundational models in computer vision and pattern recognition, incorporating architectural features inspired by biological neural systems. Their design choices impact performance, computational efficiency, and applicability to tasks like image segmentation and structured prediction.

Which architectural strategy in CNNs is specifically employed to preserve high spatial resolution in output, making the network suitable for pixel-wise classification tasks such as segmentation?

1) Using unit stride and minimizing pooling layers   
2) Increasing kernel size in deeper layers   
3) Applying global average pooling after each convolution   
4) Stacking fully connected layers after convolutional blocks   
5) Maximizing the number of output channels per layer   
6) Employing aggressive downsampling via pooling   
7) Using random weight initialization for all layers</div>

        <div class="answer-section">
            <div class="answer-label">‚úì Correct Answer:</div>
            <div class="answer-text">The correct answer is 1) Using unit stride and minimizing pooling layers.</div>
        </div>

        <div class="reference-section">
            <div class="reference-label">üìö Reference Text:</div>
            <button class="toggle-button" onclick="toggleReference(121)">
                Show/Hide Reference
            </button>
            <div id="ref121" class="reference-text hidden">convolutional,wecanusethefunction htoperformthetransposeoftheconvolution operation.Supposewehavehiddenunits Hinthesameformatas ZandwedeÔ¨Åne areconstruction R K H = ( h , , s .) (9.14) Inordertotraintheautoencoder,wewillreceivethegradientwithrespect to Rasatensor E.Totrainthedecoder,weneedtoobtainthegradientwith respectto K.Thisisgivenby g( H E , , s).Totraintheencoder,weneedtoobtain thegradientwithrespectto H.Thisisgivenby c( K E , , s).Itisalsopossibleto diÔ¨Äerentiatethrough gusing cand h,buttheseoperationsarenotneededforthe back-propagationalgorithmonanystandardnetworkarchitectures. Generally,wedonotuseonlyalinearoperationinordertotransformfrom theinputstotheoutputsinaconvolutionallayer.Wegenerallyalsoaddsome biastermtoeachoutputbeforeapplyingthenonlinearity.Thisraisesthequestion ofhowtoshareparametersamongthebiases. Forlocallyconnectedlayersitis naturaltogiveeachunititsownbias,andfortiledconvolution,itisnaturalto sharethebiaseswiththesametilingpatternasthekernels.Forconvolutional layers,itistypicaltohaveonebiasperchanneloftheoutputandshareitacross alllocationswithineachconvolutionmap.However,iftheinputisofknown,Ô¨Åxed size,itisalsopossibletolearnaseparatebiasateachlocationoftheoutputmap. SeparatingthebiasesmayslightlyreducethestatisticaleÔ¨Éciencyofthemodel,but alsoallowsthemodeltocorrectfordiÔ¨ÄerencesintheimagestatisticsatdiÔ¨Äerent locations.Forexample,whenusingimplicitzeropadding,detectorunitsatthe edgeoftheimagereceivelesstotalinputandmayneedlargerbiases. 9.6StructuredOutputs Convolutionalnetworkscanbeusedtooutputahigh-dimensional,structured object,ratherthanjustpredictingaclasslabelforaclassiÔ¨Åcationtaskorareal valueforaregressiontask.Typicallythisobjectisjustatensor,emittedbya standardconvolutionallayer.Forexample,themodelmightemitatensor S,where S i , j , kistheprobabilitythatpixel ( j , k)oftheinputtothenetworkbelongstoclass i.Thisallowsthemodeltolabeleverypixelinanimageanddrawprecisemasks thatfollowtheoutlinesofindividualobjects. Oneissuethatoftencomesupisthattheoutputplanecanbesmallerthanthe 3 5 8 CHAPTER9.CONVOLUTIONALNETWORKS ÀÜ Y( 1 )ÀÜ Y( 1 )ÀÜ Y( 2 )ÀÜ Y( 2 )ÀÜ Y( 3 )ÀÜ Y( 3 ) H( 1 )H( 1 )H( 2 )H( 2 )H( 3 )H( 3 ) XXU U UV V V W W Figure9.17:Anexampleofarecurrentconvolutionalnetworkforpixellabeling.The inputisanimagetensor,withaxescorrespondingtoimagerows,imagecolumns,and X channels(red,green,blue).ThegoalistooutputatensoroflabelsÀÜ Y,withaprobability distributionoverlabelsforeachpixel.Thistensorhasaxescorrespondingtoimagerows, imagecolumns,andthediÔ¨Äerentclasses.RatherthanoutputtingÀÜ Yinasingleshot,the recurrentnetworkiterativelyreÔ¨ÅnesitsestimateÀÜ YbyusingapreviousestimateofÀÜ Y asinputforcreatinganewestimate. Thesameparametersareusedforeachupdated estimate,andtheestimatecanbereÔ¨Ånedasmanytimesaswewish.Thetensorof convolutionkernels Uisusedoneachsteptocomputethehiddenrepresentationgiventhe inputimage.Thekerneltensor Visusedtoproduceanestimateofthelabelsgiventhe hiddenvalues.OnallbuttheÔ¨Årststep,thekernels WareconvolvedoverÀÜ Ytoprovide inputtothehiddenlayer.OntheÔ¨Årsttimestep,thistermisreplacedbyzero.Because thesameparametersareusedoneachstep,thisisanexampleofarecurrentnetwork,as describedinchapter.10 inputplane,asshowninÔ¨Ågure.Inthekindsofarchitectures typicallyusedfor 9.13 classiÔ¨Åcationofasingleobjectinanimage,thegreatestreductioninthespatial dimensionsofthenetworkcomesfromusingpoolinglayerswithlargestride.In ordertoproduceanoutputmapofsimilarsizeastheinput,onecanavoidpooling altogether(,).Anotherstrategyistosimplyemitalower-resolution Jainetal.2007 gridoflabels( ,,).Finally,inprinciple,onecould PinheiroandCollobert20142015 useapoolingoperatorwithunitstride. Onestrategyforpixel-wiselabelingofimagesistoproduceaninitialguess oftheimagelabels,thenreÔ¨Ånethisinitialguessusingtheinteractionsbetween neighboringpixels.RepeatingthisreÔ¨Ånementstepseveraltimescorrespondsto usingthesameconvolutionsateachstage,sharingweightsbetweenthelastlayersof thedeepnet(,).Thismakesthesequenceofcomputationsperformed Jainetal.2007 bythesuccessiveconvolutionallayerswithweightssharedacrosslayersaparticular kindofrecurrentnetwork( ,,).Figureshows PinheiroandCollobert20142015 9.17 thearchitectureofsucharecurrentconvolutionalnetwork. 3 5 9 CHAPTER9.CONVOLUTIONALNETWORKS Onceapredictionforeachpixelismade,variousmethodscanbeusedto furtherprocessthesepredictionsinordertoobtainasegmentationoftheimage intoregions( ,; Briggman etal.2009Turaga 2010Farabet2013 etal.,; etal.,). Thegeneralideaistoassumethatlargegroupsofcontiguouspixelstendtobe associatedwiththesamelabel.Graphicalmodelscandescribetheprobabilistic relationshipsbetweenneighboringpixels.Alternatively,theconvolutionalnetwork canbetrainedtomaximizeanapproximation ofthegraphicalmodeltraining objective(,; ,). Ningetal.2005Thompsonetal.2014 9.7DataTypes Thedatausedwithaconvolutionalnetworkusuallyconsistsofseveralchannels, eachchannelbeingtheobservationofadiÔ¨Äerentquantityatsomepointinspace ortime.SeetableforexamplesofdatatypeswithdiÔ¨Äerentdimensionalities 9.1 andnumberofchannels. Foranexampleofconvolutionalnetworksappliedtovideo,seeChenetal. ().2010 Sofarwehavediscussedonlythecasewhereeveryexampleinthetrainandtest datahasthesamespatialdimensions.Oneadvantagetoconvolutionalnetworks isthattheycanalsoprocessinputswithvaryingspatialextents.Thesekindsof inputsimplycannotberepresentedbytraditional,matrixmultiplication-based neuralnetworks.Thisprovidesacompellingreasontouseconvolutionalnetworks evenwhencomputational costandoverÔ¨ÅttingarenotsigniÔ¨Åcantissues. Forexample,consideracollectionofimages,whereeachimagehasadiÔ¨Äerent widthandheight.Itisunclearhowtomodelsuchinputswithaweightmatrixof Ô¨Åxedsize.Convolutionisstraightforwardtoapply;thekernelissimplyapplieda diÔ¨Äerentnumberoftimesdependingonthesizeoftheinput,andtheoutputofthe convolutionoperationscalesaccordingly.Convolutionmaybeviewedasmatrix multiplication; thesameconvolutionkernelinducesadiÔ¨Äerentsizeofdoublyblock circulantmatrixforeachsizeofinput. Sometimes theoutputofthenetworkis allowedtohavevariablesizeaswellastheinput,forexampleifwewanttoassign aclasslabeltoeachpixeloftheinput.Inthiscase,nofurtherdesignworkis necessary.Inothercases,thenetworkmustproducesomeÔ¨Åxed-sizeoutput,for exampleifwewanttoassignasingleclasslabeltotheentireimage.Inthiscase wemustmakesomeadditionaldesignsteps,likeinsertingapoolinglayerwhose poolingregionsscaleinsizeproportionaltothesizeoftheinput,inorderto maintainaÔ¨Åxednumberofpooledoutputs.Someexamplesofthiskindofstrategy areshowninÔ¨Ågure.9.11 3 6 0 CHAPTER9.CONVOLUTIONALNETWORKS Singlechannel Multi-channel 1-DAudio waveform:The axis we convolveovercorrespondsto time.Wediscretizetimeand measuretheamplitudeofthe waveformoncepertimestep.Skeletonanimationdata:Anima- tionsof3-Dcomputer-rendered charactersaregeneratedbyalter- ingtheposeofa‚Äúskeleton‚Äùover time.Ateachpointintime,the poseofthecharacterisdescribed byaspeciÔ¨Åcationoftheanglesof eachofthejointsinthecharac- ter‚Äôsskeleton.Eachchannelin thedatawefeedtotheconvolu- tionalmodelrepresentstheangle aboutoneaxisofonejoint. 2-DAudiodatathathasbeenprepro- cessedwithaFouriertransform: Wecantransformtheaudiowave- formintoa2Dtensorwithdif- ferentrowscorrespondingtodif- ferentfrequencies anddiÔ¨Äerent columnscorrespondingtodiÔ¨Äer- entpointsintime.Usingconvolu- tioninthetimemakesthemodel equivarianttoshiftsintime.Us- ingconvolutionacrossthefre- quencyaxismakesthemodel equivarianttofrequency,sothat thesamemelodyplayedinadif- ferentoctaveproducesthesame representationbutatadiÔ¨Äerent heightinthenetwork‚Äôsoutput.Colorimagedata:Onechannel containstheredpixels,onethe green pixels, and one theblue pixels.Theconvolutionkernel movesoverboththehorizontal andverticalaxesofthe image, conferringtranslationequivari- anceinbothdirections. 3-DVolumetricdata:Acommon sourceofthiskindofdataismed- icalimagingtechnology,suchas CTscans.Colorvideodata:Oneaxiscorre- spondstotime,onetotheheight ofthevideoframe,andoneto thewidthofthevideoframe. Table9.1:ExamplesofdiÔ¨Äerentformatsofdatathatcanbeusedwithconvolutional networks. 3 6 1 CHAPTER9.CONVOLUTIONALNETWORKS Notethattheuseofconvolutionforprocessingvariablesizedinputsonlymakes senseforinputsthathavevariablesizebecausetheycontainvaryingamounts ofobservationofthesamekindofthing‚ÄîdiÔ¨Äeren tlengthsofrecordingsover time,diÔ¨Äerentwidthsofobservationsoverspace,etc.Convolutiondoesnotmake senseiftheinputhasvariablesizebecauseitcanoptionallyincludediÔ¨Äerent kindsofobservations.Forexample,ifweareprocessingcollegeapplications,and ourfeaturesconsistofbothgradesandstandardizedtestscores,butnotevery applicanttookthestandardizedtest,thenitdoesnotmakesensetoconvolvethe sameweightsoverboththefeaturescorrespondingtothegradesandthefeatures correspondingtothetestscores. 9.8EÔ¨ÉcientConvolutionAlgorithms Modernconvolutionalnetworkapplicationsofteninvolvenetworkscontainingmore thanonemillionunits.Powerfulimplementations exploitingparallelcomputation resources,asdiscussedinsection,areessential. However,inmanycasesit 12.1 isalsopossibletospeedupconvolutionbyselectinganappropriateconvolution algorithm. Convolutionisequivalenttoconvertingboththeinputandthekerneltothe frequencydomainusingaFouriertransform,performingpoint-wisemultiplication ofthetwosignals, andconvertingbacktothetimedomainusinganinverse Fouriertransform.Forsomeproblemsizes,thiscanbefasterthanthenaive implementationofdiscreteconvolution. Whena d-dimensionalkernelcanbeexpressedas theouterproductof d vectors,onevectorperdimension,thekerneliscalled se par abl e.Whenthe kernelisseparable,naiveconvolutionisineÔ¨Écient.Itisequivalenttocompose d one-dimensional convolutionswitheachofthesevectors.Thecomposedapproach issigniÔ¨Åcantlyfasterthanperformingone d-dimensionalconvolutionwiththeir outerproduct.Thekernelalsotakesfewerparameterstorepresentasvectors. Ifthekernelis welementswideineachdimension,thennaivemultidimensional convolutionrequires O( wd)runtimeandparameterstoragespace,whileseparable convolutionrequires O( w d √ó)runtimeandparameterstoragespace.Ofcourse, noteveryconvolutioncanberepresentedinthisway. Devisingfasterwaysofperformingconvolutionorapproximateconvolution withoutharmingtheaccuracyofthemodelisanactiveareaofresearch.Eventech- niquesthatimprovetheeÔ¨Éciencyofonlyforwardpropagationareusefulbecause inthecommercialsetting,itistypicaltodevotemoreresourcestodeploymentof anetworkthantoitstraining. 3 6 2 CHAPTER9.CONVOLUTIONALNETWORKS 9.9RandomorUnsupervisedFeatures Typically,themostexpensivepartofconvolutionalnetworktrainingislearningthe features.Theoutputlayerisusuallyrelativelyinexpensiveduetothesmallnumber offeaturesprovidedasinputtothislayerafterpassingthroughseverallayersof pooling.Whenperformingsupervisedtrainingwithgradientdescent,everygradient steprequiresacompleterunofforwardpropagationandbackwardpropagation throughtheentirenetwork.Onewaytoreducethecostofconvolutionalnetwork trainingistousefeaturesthatarenottrainedinasupervisedfashion. Therearethreebasicstrategiesforobtaining con volutionkernelswithout supervisedtraining.Oneistosimplyinitializethemrandomly.Anotheristo designthembyhand,forexamplebysettingeachkerneltodetectedgesata certainorientationorscale.Finally,onecanlearnthekernelswithanunsupervised criterion.Forexample, ()apply Coatesetal.2011 k-meansclusteringtosmall imagepatches,thenuseeachlearnedcentroidasaconvolutionkernel. PartIII describesmanymoreunsupervisedlearningapproaches.Learningthefeatures withanunsupervisedcriterionallowsthemtobedeterminedseparatelyfromthe classiÔ¨Åerlayeratthetopofthearchitecture.Onecanthenextractthefeaturesfor theentiretrainingsetjustonce,essentiallyconstructinganewtrainingsetforthe lastlayer.Learningthelastlayeristhentypicallyaconvexoptimization problem, assumingthelastlayerissomethinglikelogisticregressionoranSVM. RandomÔ¨Åltersoftenworksurprisinglywellinconvolutionalnetworks(Jarrett etal. etal. etal. ,;2009Saxe,;2011Pinto,;2011CoxandPinto2011Saxe,).etal. ()showedthatlayersconsistingofconvolutionfollowingbypoolingnaturally 2011 becomefrequencyselectiveandtranslationinvariantwhenassignedrandomweights. Theyarguethatthisprovidesaninexpensivewaytochoosethearchitectureof aconvolutionalnetwork:Ô¨Årstevaluatetheperformanceofseveralconvolutional networkarchitecturesbytrainingonlythelastlayer,thentakethebestofthese architecturesandtraintheentirearchitectureusingamoreexpensiveapproach. Anintermediate approachistolearnthefeatures,butusingmethodsthatdo notrequirefullforwardandback-propagationateverygradientstep.Aswith multilayerperceptrons,weusegreedylayer-wisepretraining,totraintheÔ¨Årstlayer inisolation,thenextractallfeaturesfromtheÔ¨Årstlayeronlyonce,thentrainthe secondlayerinisolationgiventhosefeatures,andsoon.Chapterhasdescribed 8 howtoperformsupervisedgreedylayer-wisepretraining,andpartextendsthisIII togreedylayer-wisepretrainingusinganunsupervisedcriterionateachlayer.The canonicalexampleofgreedylayer-wisepretrainingofaconvolutionalmodelisthe convolutionaldeepbeliefnetwork(,).ConvolutionalnetworksoÔ¨Äer Leeetal.2009 3 6 3 CHAPTER9.CONVOLUTIONALNETWORKS ustheopportunitytotakethepretrainingstrategyonestepfurtherthanispossible withmultilayerperceptrons.Insteadoftraininganentireconvolutionallayerata time,wecantrainamodelofasmallpatch,as ()dowith Coatesetal.2011 k-means. Wecanthenusetheparametersfromthispatch-basedmodeltodeÔ¨Ånethekernels ofaconvolutionallayer.Thismeansthatitispossibletouseunsupervisedlearning totrainaconvolutionalnetworkwithouteverusingconvolutionduringthetraining process.Usingthisapproach,wecantrainverylargemodelsandincurahigh computational costonlyatinferencetime( ,; , Ranzatoetal.2007bJarrettetal. 2009Kavukcuoglu2010Coates 2013 ; etal.,; etal.,).Thisapproachwaspopular fromroughly2007‚Äì2013,whenlabeleddatasetsweresmallandcomputational powerwasmorelimited.Today,mostconvolutionalnetworksaretrainedina purelysupervisedfashion,usingfullforwardandback-propagation throughthe entirenetworkoneachtrainingiteration. Aswithotherapproachestounsupervisedpretraining,itremainsdiÔ¨Écultto teaseapartthecauseofsomeofthebeneÔ¨Åtsseenwiththisapproach.Unsupervised pretrainingmayoÔ¨Äersomeregularizationrelativetosupervisedtraining,oritmay simplyallowustotrainmuchlargerarchitectures duetothereducedcomputational costofthelearningrule. 9.10TheNeuroscientiÔ¨ÅcBasisforConvolutionalNet- works Convolutional networksare perhaps the greatest successstory ofbiologically inspiredartiÔ¨Åcialintelligence.Thoughconvolutionalnetworkshavebeenguided bymanyotherÔ¨Åelds,someofthekeydesignprinciplesofneuralnetworkswere drawnfromneuroscience. ThehistoryofconvolutionalnetworksbeginswithneuroscientiÔ¨Åcexperiments longbeforetherelevantcomputational modelsweredeveloped.Neurophysiologists DavidHubelandTorstenWieselcollaboratedforseveralyearstodeterminemany ofthemostbasicfactsabouthowthemammalianvisionsystemworks(Hubeland Wiesel195919621968 ,,,).Theiraccomplishmentswereeventuallyrecognizedwith aNobelprize.TheirÔ¨ÅndingsthathavehadthegreatestinÔ¨Çuenceoncontemporary deeplearningmodelswerebasedonrecordingtheactivityofindividualneuronsin cats.Theyobservedhowneuronsinthecat‚Äôsbrainrespondedtoimagesprojected inpreciselocationsonascreeninfrontofthecat.Theirgreatdiscoverywas thatneuronsintheearlyvisualsystemrespondedmoststronglytoveryspeciÔ¨Åc patternsoflight,suchaspreciselyorientedbars,butrespondedhardlyatallto otherpatterns. 3 6 4 CHAPTER9.CONVOLUTIONALNETWORKS Theirworkhelpedtocharacterizemanyaspectsofbrainfunctionthatare beyondthescopeofthisbook.Fromthepointofviewofdeeplearning,wecan focusonasimpliÔ¨Åed,cartoonviewofbrainfunction. InthissimpliÔ¨Åedview,wefocusonapartofthebraincalledV1,alsoknown asthe pr i m ar y v i sual c o r t e x. V1istheÔ¨Årstareaofthebrainthatbeginsto performsigniÔ¨Åcantlyadvancedprocessingofvisualinput. Inthiscartoonview, imagesareformedbylightarrivingintheeyeandstimulatingtheretina,the light-sensitivetissueinthebackoftheeye.Theneuronsintheretinaperform somesimplepreprocessingoftheimagebutdonotsubstantiallyalterthewayitis represented.Theimagethenpassesthroughtheopticnerveandabrainregion calledthelateralgeniculatenucleus. Themainrole,asfarasweareconcerned here,ofbothoftheseanatomicalregionsisprimarilyjusttocarrythesignalfrom theeyetoV1,whichislocatedatthebackofthehead. AconvolutionalnetworklayerisdesignedtocapturethreepropertiesofV1: 1.V1isarrangedinaspatialmap.Itactuallyhasatwo-dimensionalstructure mirroring the</div>
        </div>
    </div>

    <div class="question-card" id="q122">
        <div class="question-header">
            <span class="question-number">Question 122</span>
            <span class="question-type">multiple-choice</span>
        </div>

        <div class="question-text">In linear algebra and its applications to data science, understanding matrix decompositions is crucial for analyzing transformations, optimizing quadratic forms, and solving linear equations. Singular value decomposition (SVD) and eigendecomposition are two foundational methods with distinct properties and uses.

Which of the following statements about singular value decomposition (SVD) is true?

1) SVD only applies to real symmetric matrices.   
2) SVD factors a matrix into two orthogonal matrices and one lower triangular matrix.   
3) SVD is applicable to any real matrix, whether square or non-square.   
4) The singular values in SVD are always negative.   
5) SVD requires all eigenvalues of the matrix to be distinct.   
6) SVD is identical to eigendecomposition for all matrices.   
7) SVD cannot be used to compute the Moore-Penrose pseudoinverse.</div>

        <div class="answer-section">
            <div class="answer-label">‚úì Correct Answer:</div>
            <div class="answer-text">The correct answer is 3) SVD is applicable to any real matrix, whether square or non-square..</div>
        </div>

        <div class="reference-section">
            <div class="reference-label">üìö Reference Text:</div>
            <button class="toggle-button" onclick="toggleReference(122)">
                Show/Hide Reference
            </button>
            <div id="ref122" class="reference-text hidden">(2.39) Thescalar Œªisknownastheeigenvaluecorrespondingtothiseigenvector.(One canalsoÔ¨ÅndalefteigenvectorsuchthatvÓÄæA= ŒªvÓÄæ, butweareusually concernedwithrighteigenvectors). IfvisaneigenvectorofA,thensoisanyrescaledvector svfor s , s ‚àà RÓÄ∂= 0. Moreover, svstillhasthesameeigenvalue.Forthisreason,weusuallyonlylook foruniteigenvectors. SupposethatamatrixAhas nlinearlyindependenteigenvectors,{v( 1 ), . . . , v( ) n},withcorrespondingeigenvalues { Œª 1 , . . . , Œª n}.Wemayconcatenateallofthe 4 2 CHAPTER2.LINEARALGEBRA  ÓÄ≥  ÓÄ≤  ÓÄ± ÓÄ∞ ÓÄ± ÓÄ≤ ÓÄ≥ ÓÅ∏ÓÄ∞ ÓÄ≥ ÓÄ≤ ÓÄ±ÓÄ∞ÓÄ±ÓÄ≤ÓÄ≥ÓÅ∏ÓÄ±ÓÅ∂ÓÄ®ÓÄ± ÓÄ© ÓÅ∂ÓÄ®ÓÄ≤ ÓÄ©ÓÅÇ ÓÅ• ÓÅ¶ ÓÅØ ÓÅ≤ ÓÅ• ÓÄ† ÓÅ≠ ÓÅµ ÓÅ¨ ÓÅ¥ ÓÅ© ÓÅ∞ ÓÅ¨ ÓÅ© ÓÅ£ ÓÅ° ÓÅ¥ ÓÅ© ÓÅØ ÓÅÆ  ÓÄ≥  ÓÄ≤  ÓÄ± ÓÄ∞ ÓÄ± ÓÄ≤ ÓÄ≥ ÓÅ∏ÓÄ∞ ÓÄ∞ ÓÄ≥ ÓÄ≤ ÓÄ±ÓÄ∞ÓÄ±ÓÄ≤ÓÄ≥ÓÅ∏ÓÄ∞ ÓÄ±ÓÅ∂ÓÄ®ÓÄ± ÓÄ©ÓÇ∏ÓÄ± ÓÅ∂ÓÄ®ÓÄ± ÓÄ© ÓÅ∂ÓÄ®ÓÄ≤ ÓÄ©ÓÇ∏ÓÄ≤ÓÅ∂ÓÄ®ÓÄ≤ ÓÄ©ÓÅÅ ÓÅ¶ ÓÅ¥ ÓÅ• ÓÅ≤ ÓÄ† ÓÅ≠ ÓÅµ ÓÅ¨ ÓÅ¥ ÓÅ© ÓÅ∞ ÓÅ¨ ÓÅ© ÓÅ£ ÓÅ° ÓÅ¥ ÓÅ© ÓÅØ ÓÅÆÓÅÖ ÓÅ¶ ÓÅ¶ ÓÅ• ÓÅ£ ÓÅ¥ ÓÄ† ÓÅØÓÅ¶ ÓÄ† ÓÅ• ÓÅ© ÓÅß ÓÅ• ÓÅÆ ÓÅ∂ ÓÅ• ÓÅ£ ÓÅ¥ ÓÅØÓÅ≤ ÓÅ≥ ÓÄ† ÓÅ°ÓÅÆ ÓÅ§ ÓÄ† ÓÅ• ÓÅ© ÓÅßÓÅ• ÓÅÆ ÓÅ∂ ÓÅ°ÓÅ¨ ÓÅµ ÓÅ• ÓÅ≥ Figure2.3:AnexampleoftheeÔ¨Äectofeigenvectorsandeigenvalues.Here,wehave amatrixAwithtwoorthonormaleigenvectors,v( 1 )witheigenvalue Œª 1andv( 2 )with eigenvalue Œª 2. ( L e f t )Weplotthesetofallunitvectorsu‚àà R2asaunitcircle. ( R i g h t )We plotthesetofallpointsAu.ByobservingthewaythatAdistortstheunitcircle,we canseethatitscalesspaceindirectionv( ) iby Œª i. eigenvectorstoformamatrixVwithoneeigenvectorpercolumn:V= [v( 1 ), . . . , v( ) n].Likewise,wecanconcatenatetheeigenvaluestoformavectorŒª= [ Œª 1 , . . . , Œª n]ÓÄæ.The ofisthengivenby eigendecompositionA AVŒªV = diag()‚àí 1. (2.40) WehaveseenthatconstructingmatriceswithspeciÔ¨Åceigenvaluesandeigenvec- torsallowsustostretchspaceindesireddirections. Ho wever,weoftenwantto decomposematricesintotheireigenvaluesandeigenvectors.Doingsocanhelp ustoanalyzecertainpropertiesofthematrix,muchasdecomposinganinteger intoitsprimefactorscanhelpusunderstandthebehaviorofthatinteger. Noteverymatrixcanbedecomposedintoeigenvaluesandeigenvectors.Insome 4 3 CHAPTER2.LINEARALGEBRA cases,thedecompositionexists,butmayinvolvecomplexratherthanrealnumbers. Fortunately,inthisbook,weusuallyneedtodecomposeonlyaspeciÔ¨Åcclassof matricesthathaveasimpledecomposition.SpeciÔ¨Åcally,everyrealsymmetric matrixcanbedecomposedintoanexpressionusingonlyreal-valuedeigenvectors andeigenvalues: AQQ = ŒõÓÄæ, (2.41) whereQisanorthogonalmatrixcomposedofeigenvectorsofA,and Œõisa diagonalmatrix.TheeigenvalueŒõ i , iisassociatedwiththeeigenvectorincolumn i ofQ,denotedasQ : , i.BecauseQisanorthogonalmatrix,wecanthinkofAas scalingspaceby Œª iindirectionv( ) i.SeeÔ¨Ågureforanexample.2.3 WhileanyrealsymmetricmatrixAisguaranteedtohaveaneigendecomposi- tion,theeigendecompositionmaynotbeunique.Ifanytwoormoreeigenvectors sharethesameeigenvalue,thenanysetoforthogonalvectorslyingintheirspan arealsoeigenvectorswiththateigenvalue,andwecouldequivalentlychooseaQ usingthoseeigenvectorsinstead.Byconvention,weusuallysorttheentriesof Œõ indescendingorder.Underthisconvention,theeigendecompositionisuniqueonly ifalloftheeigenvaluesareunique. Theeigendecompositionof amatrix tellsus many usefulfactsabout the matrix.Thematrixissingularifandonlyifanyoftheeigenvaluesarezero. Theeigendecomposition ofarealsymmetricmatrixcanalsobeusedtooptimize quadraticexpressionsoftheform f(x) =xÓÄæAxsubjectto||||x 2= 1.Wheneverx isequaltoaneigenvectorofA, ftakesonthevalueofthecorrespondingeigenvalue. Themaximumvalueof fwithintheconstraintregionisthemaximumeigenvalue anditsminimumvaluewithintheconstraintregionistheminimumeigenvalue. AmatrixwhoseeigenvaluesareallpositiveiscalledpositivedeÔ¨Ånite.A matrixwhoseeigenvaluesareallpositiveorzero-valuediscalledpositivesemideÔ¨Å- nite.Likewise,ifalleigenvaluesarenegative,thematrixisnegativedeÔ¨Ånite,and ifalleigenvaluesarenegativeorzero-valued,itisnegativesemideÔ¨Ånite.Positive semideÔ¨Ånitematricesareinterestingbecausetheyguaranteethat‚àÄxx ,ÓÄæAx‚â•0. PositivedeÔ¨ÅnitematricesadditionallyguaranteethatxÓÄæAxx = 0 ‚áí = 0. 2.8SingularValueDecomposition Insection,wesawhowtodecomposeamatrixintoeigenvectorsandeigenvalues. 2.7 Thesingularvaluedecomposition(SVD)providesanotherwaytofactorize amatrix,intosingularvectorsandsingularvalues.TheSVDallowsusto discoversomeofthesamekindofinformationastheeigendecomposition.However, 4 4 CHAPTER2.LINEARALGEBRA theSVDismoregenerallyapplicable.Everyrealmatrixhasasingularvalue decomposition,butthesameisnottrueoftheeigenvaluedecomposition.For example,ifamatrixisnotsquare,theeigendecompositionisnotdeÔ¨Åned,andwe mustuseasingularvaluedecompositioninstead. RecallthattheeigendecompositioninvolvesanalyzingamatrixAtodiscover amatrixVofeigenvectorsandavectorofeigenvaluesŒªsuchthatwecanrewrite Aas AVŒªV = diag()‚àí 1. (2.42) Thesingularvaluedecompositionissimilar,exceptthistimewewillwriteA asaproductofthreematrices: AUDV = ÓÄæ. (2.43) SupposethatAisan m n√ómatrix.ThenUisdeÔ¨Ånedtobean m m√ómatrix, D V tobeanmatrix,and m n√ó tobeanmatrix. n n√ó EachofthesematricesisdeÔ¨Ånedtohaveaspecialstructure.ThematricesU andVarebothdeÔ¨Ånedtobeorthogonalmatrices.ThematrixDisdeÔ¨Ånedtobe adiagonalmatrix.Notethatisnotnecessarilysquare. D TheelementsalongthediagonalofDareknownasthesingularvaluesof thematrixA.ThecolumnsofUareknownastheleft-singularvectors.The columnsofareknownasasthe V right-singularvectors. WecanactuallyinterpretthesingularvaluedecompositionofAintermsof theeigendecomposition offunctionsofA.Theleft-singularvectorsofAarethe eigenvectorsofAAÓÄæ.Theright-singularvectorsofAaretheeigenvectorsofAÓÄæA. Thenon-zerosingularvaluesofAarethesquarerootsoftheeigenvaluesofAÓÄæA. ThesameistrueforAAÓÄæ. PerhapsthemostusefulfeatureoftheSVDisthatwecanuseittopartially generalizematrixinversiontonon-squarematrices,aswewillseeinthenext section. 2.9TheMoore-PenrosePseudoinverse MatrixinversionisnotdeÔ¨Ånedformatricesthatarenotsquare.Supposewewant tomakealeft-inverseofamatrix,sothatwecansolvealinearequation BA Axy= (2.44) 4 5</div>
        </div>
    </div>

    <div class="question-card" id="q123">
        <div class="question-header">
            <span class="question-number">Question 123</span>
            <span class="question-type">multiple-choice</span>
        </div>

        <div class="question-text">Autoencoders are neural network models used for unsupervised representation learning by compressing and reconstructing data. Regularization techniques are often employed to ensure that autoencoders learn meaningful features rather than simply copying their inputs.

Which of the following statements correctly describes the role of sparsity regularization in sparse autoencoders?

1) It penalizes the model parameters to prevent overfitting by shrinking their values toward zero.   
2) It encourages most hidden units to remain inactive for most inputs, promoting the discovery of interpretable and robust features.   
3) It forces the encoding dimension to be smaller than the input dimension, ensuring only the most salient features are captured.   
4) It guarantees that autoencoders learn the same principal subspace as PCA, regardless of activation functions.   
5) It increases model capacity to enable memorization of the training data through exact input-output copying.   
6) It adds a probabilistic interpretation to the autoencoder by maximizing the likelihood of the data.   
7) It acts as a Bayesian prior over network weights, similar to weight decay regularization.</div>

        <div class="answer-section">
            <div class="answer-label">‚úì Correct Answer:</div>
            <div class="answer-text">The correct answer is 2) It encourages most hidden units to remain inactive for most inputs, promoting the discovery of interpretable and robust features..</div>
        </div>

        <div class="reference-section">
            <div class="reference-label">üìö Reference Text:</div>
            <button class="toggle-button" onclick="toggleReference(123)">
                Show/Hide Reference
            </button>
            <div id="ref123" class="reference-text hidden">e t e.Learninganundercomplete representationforcestheautoencodertocapturethemostsalientfeaturesofthe trainingdata. Thelearningprocessisdescribedsimplyasminimizingalossfunction L , g f ( x(())) x (14.1) where Lisalossfunctionpenalizing g( f( x))forbeingdissimilarfrom x,suchas themeansquarederror. Whenthedecoderislinearand Listhemeansquarederror,anundercomplete autoencoderlearnstospanthesamesubspaceasPCA.Inthiscase,anautoencoder trainedtoperformthecopyingtaskhaslearnedtheprincipalsubspaceofthe trainingdataasaside-eÔ¨Äect. Autoencoderswithnonlinearencoderfunctions fandnonlineardecoderfunc- tions gcanthuslearnamorepowerfulnonlineargeneralization ofPCA.Unfortu- 5 0 3 CHAPTER14.AUTOENCODERS nately,iftheencoderanddecoderareallowedtoomuchcapacity,theautoencoder canlearntoperformthecopyingtaskwithoutextractingusefulinformationabout thedistributionofthedata.Theoretically,onecouldimaginethatanautoencoder withaone-dimensional codebutaverypowerfulnonlinearencodercouldlearnto representeachtrainingexample x() iwiththecode i.Thedecodercouldlearnto maptheseintegerindicesbacktothevaluesofspeciÔ¨Åctrainingexamples.This speciÔ¨Åcscenariodoesnotoccurinpractice,butitillustratesclearlythatanautoen- codertrainedtoperformthecopyingtaskcanfailtolearnanythingusefulabout thedatasetifthecapacityoftheautoencoderisallowedtobecometoogreat. 14.2RegularizedAutoencoders Undercomplete autoencoders,withcodedimensionlessthantheinputdimension, canlearnthemostsalientfeaturesofthedatadistribution.Wehaveseenthat theseautoencodersfailtolearnanythingusefuliftheencoderanddecoderare giventoomuchcapacity. Asimilarproblemoccursifthehiddencodeisallowedtohavedimension equaltotheinput,andinthe o v e r c o m pl e t ecaseinwhichthehiddencodehas dimensiongreaterthantheinput.Inthesecases,evenalinearencoderandlinear decodercanlearntocopytheinputtotheoutputwithoutlearninganythinguseful aboutthedatadistribution. Ideally,onecouldtrainanyarchitectureofautoencodersuccessfully,choosing thecodedimensionandthecapacityoftheencoderanddecoderbasedonthe complexityofdistributiontobemodeled.Regularizedautoencodersprovidethe abilitytodoso.Ratherthanlimitingthemodelcapacitybykeepingtheencoder anddecodershallowandthecodesizesmall,regularizedautoencodersusealoss functionthatencouragesthemodeltohaveotherpropertiesbesidestheability tocopyitsinputtoitsoutput.Theseotherpropertiesincludesparsityofthe representation,smallnessofthederivativeoftherepresentation,androbustness tonoiseortomissinginputs.Aregularizedautoencodercanbenonlinearand overcompletebutstilllearnsomethingusefulaboutthedatadistributionevenif themodelcapacityisgreatenoughtolearnatrivialidentityfunction. Inadditiontothemethodsdescribedherewhicharemostnaturallyinterpreted asregularizedautoencoders,nearlyanygenerativemodelwithlatentvariables andequippedwithaninferenceprocedure(forcomputinglatentrepresentations giveninput)maybeviewedasaparticularformofautoencoder.Twogenerative modelingapproachesthatemphasizethisconnectionwithautoencodersarethe descendantsoftheHelmholtzmachine( ,),suchasthevariational Hinton e t a l .1995b 5 0 4 CHAPTER14.AUTOENCODERS autoencoder(section)andthegenerativestochasticnetworks(section). 20.10.3 20.12 Thesemodelsnaturallylearnhigh-capacity,overcompleteencodingsoftheinput anddonotrequireregularizationfortheseencodingstobeuseful.Theirencodings arenaturallyusefulbecausethemodelsweretrainedtoapproximatelymaximize theprobabilityofthetrainingdataratherthantocopytheinputtotheoutput. 1 4 . 2 . 1 S p a rse A u t o en co d ers Asparseautoencoderissimplyanautoencoderwhosetrainingcriterioninvolvesa sparsitypenalty‚Ñ¶( h)onthecodelayer h,inadditiontothereconstructionerror: L , g f ( x(()))+‚Ñ¶() x h (14.2) where g( h)isthedecoderoutputandtypicallywehave h= f( x),theencoder output. Sparseautoencodersaretypicallyusedtolearnfeaturesforanothertasksuch asclassiÔ¨Åcation.Anautoencoderthathasbeenregularizedtobesparsemust respondtouniquestatisticalfeaturesofthedatasetithasbeentrainedon,rather thansimplyactingasanidentityfunction.Inthisway,trainingtoperformthe copyingtaskwithasparsitypenaltycanyieldamodelthathaslearneduseful featuresasabyproduct. Wecanthink ofthepenalty ‚Ñ¶( h)simplyasaregularizertermaddedto afeedforwardnetworkwhoseprimarytaskistocopytheinputtotheoutput (unsupervisedlearningobjective)andpossiblyalsoperformsomesupervisedtask (with asupervised learning ob jective) thatdepends on thesesparsefeatures. Unlikeotherregularizerssuchasweightdecay,thereisnotastraightforward Bayesianinterpretationtothisregularizer.Asdescribedinsection,training5.6.1 withweightdecayandotherregularizationpenaltiescanbeinterpretedasa MAPapproximationtoBayesianinference,withtheaddedregularizingpenalty correspondingtoapriorprobabilitydistributionoverthemodelparameters.In thisview,regularizedmaximumlikelihoodcorrespondstomaximizing p( Œ∏ x|), whichisequivalenttomaximizing log p( x Œ∏|)+log p( Œ∏). The log p( x Œ∏|)term istheusualdatalog-likelihoodtermandthelog p( Œ∏)term,thelog-priorover parameters,incorporatesthepreferenceoverparticularvaluesof Œ∏.Thisviewwas describedinsection.Regularizedautoencodersdefysuchaninterpretation 5.6 becausetheregularizerdependsonthedataandisthereforebydeÔ¨Ånitionnota priorintheformalsenseoftheword.Wecanstillthinkoftheseregularization termsasimplicitlyexpressingapreferenceoverfunctions. Ratherthanthinkingofthesparsitypenaltyasaregularizerforthecopying task,wecanthinkoftheentiresparseautoencoderframeworkasapproximating 5 0 5</div>
        </div>
    </div>

    <div class="question-card" id="q124">
        <div class="question-header">
            <span class="question-number">Question 124</span>
            <span class="question-type">multiple-choice</span>
        </div>

        <div class="question-text">Neural network training relies on specialized optimization methods to handle the complexity and scale of deep models. Techniques such as stochastic gradient descent and regularization strategies play crucial roles in achieving generalization and computational feasibility.

Which strategy is specifically used to prevent a deep neural network from overfitting by halting training before full minimization of the objective function, based on monitoring performance on a validation set?

1) Early stopping   
2) Batch normalization   
3) Learning rate scheduling   
4) Weight initialization   
5) Data augmentation   
6) Gradient clipping   
7) Momentum optimization</div>

        <div class="answer-section">
            <div class="answer-label">‚úì Correct Answer:</div>
            <div class="answer-text">The correct answer is 1) Early stopping.</div>
        </div>

        <div class="reference-section">
            <div class="reference-label">üìö Reference Text:</div>
            <button class="toggle-button" onclick="toggleReference(124)">
                Show/Hide Reference
            </button>
            <div id="ref124" class="reference-text hidden">problemsinvolvedindeeplearning,themost diÔ¨Écultisneuralnetworktraining.Itisquitecommontoinvestdaystomonthsof timeonhundredsofmachinesinordertosolveevenasingleinstanceoftheneural networktrainingproblem.Becausethisproblemissoimportantandsoexpensive, aspecializedsetofoptimization techniqueshavebeendevelopedforsolvingit. Thischapterpresentstheseoptimization techniquesforneuralnetworktraining. Ifyouareunfamiliarwiththebasicprinciplesofgradient-basedoptimization, wesuggestreviewingchapter.Thatchapterincludesabriefoverviewofnumerical 4 optimization ingeneral. Thischapterfocusesononeparticularcaseofoptimization: Ô¨Åndingtheparam- etersŒ∏ofaneuralnetworkthatsigniÔ¨Åcantlyreduceacostfunction J(Œ∏),which typicallyincludesaperformancemeasureevaluatedontheentiretrainingsetas wellasadditionalregularizationterms. Webeginwithadescriptionofhowoptimization usedasatrainingalgorithm foramachinelearningtaskdiÔ¨Äersfrompureoptimization. Next,wepresentseveral oftheconcretechallengesthatmakeoptimization ofneuralnetworksdiÔ¨Écult.We thendeÔ¨Åneseveralpracticalalgorithms,includingbothoptimization algorithms themselvesandstrategiesforinitializingtheparameters.Moreadvancedalgorithms adapttheirlearningratesduringtrainingorleverageinformationcontainedin 274 CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS thesecondderivativesofthecostfunction.Finally,weconcludewithareviewof severaloptimization strategiesthatareformedbycombiningsimpleoptimization algorithmsintohigher-levelprocedures. 8.1HowLearningDiÔ¨ÄersfromPureOptimization Optimization algorithmsusedfortrainingofdeepmodelsdiÔ¨Äerfromtraditional optimization algorithmsinseveralways.Machinelearningusuallyactsindirectly. Inmostmachinelearningscenarios,wecareaboutsomeperformancemeasure P,thatisdeÔ¨Ånedwithrespecttothetestsetandmayalsobeintractable.We thereforeoptimize Ponlyindirectly.WereduceadiÔ¨Äerentcostfunction J(Œ∏)in thehopethatdoingsowillimprove P.Thisisincontrasttopureoptimization, whereminimizing Jisagoalinandofitself.Optimization algorithmsfortraining deepmodelsalsotypicallyincludesomespecializationonthespeciÔ¨Åcstructureof machinelearningobjectivefunctions. Typically,thecostfunctioncanbewrittenasanaverageoverthetrainingset, suchas J() = Œ∏ E ( ) ÀÜ x ,y ‚àº pdataL f , y , ((;)xŒ∏) (8.1) where Listheper-examplelossfunction, f(x;Œ∏)isthepredictedoutputwhen theinputisx,ÀÜ p da t aistheempiricaldistribution.Inthesupervisedlearningcase, yisthetargetoutput.Throughoutthischapter,wedeveloptheunregularized supervisedcase,wheretheargumentsto Lare f(x;Œ∏)and y.However,itistrivial toextendthisdevelopment,forexample,toincludeŒ∏orxasarguments,orto exclude yasarguments,inordertodevelopvariousformsofregularizationor unsupervisedlearning. EquationdeÔ¨Ånesanobjectivefunctionwithrespecttothetrainingset.We 8.1 wouldusuallyprefertominimizethecorrespondingobjectivefunctionwherethe expectationistakenacrossthedatageneratingdistribution p da t aratherthanjust overtheÔ¨Ånitetrainingset: J‚àó() = Œ∏ E ( ) x ,y ‚àº pdataL f , y . ((;)xŒ∏) (8.2) 8.1.1EmpiricalRiskMinimization Thegoalofamachinelearningalgorithmistoreducetheexpectedgeneralization errorgivenbyequation.Thisquantityisknownasthe 8.2 risk.Weemphasizehere thattheexpectationistakenoverthetrueunderlyingdistribution p da t a.Ifweknew thetruedistribution p da t a(x , y),riskminimization wouldbeanoptimization task 2 7 5 CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS solvablebyanoptimization algorithm.However,whenwedonotknow p da t a(x , y) butonlyhaveatrainingsetofsamples,wehaveamachinelearningproblem. Thesimplestwaytoconvertamachinelearningproblembackintoanop- timizationproblemistominimizetheexpectedlossonthetrainingset.This meansreplacingthetruedistribution p(x , y) withtheempiricaldistributionÀÜ p(x , y) deÔ¨Ånedbythetrainingset.Wenowminimizetheempiricalrisk E x ,y ‚àº ÀÜ pdata ( ) x , y[((;))] = L fxŒ∏ , y1 mm ÓÅò i = 1L f((x( ) i;)Œ∏ , y( ) i)(8.3) whereisthenumberoftrainingexamples. m Thetrainingprocessbasedonminimizingthisaveragetrainingerrorisknown asempiricalriskminimization.Inthissetting,machinelearningisstillvery similartostraightforwardoptimization. Ratherthanoptimizingtheriskdirectly, weoptimizetheempiricalrisk,andhopethattheriskdecreasessigniÔ¨Åcantlyas well.Avarietyoftheoreticalresultsestablishconditionsunderwhichthetruerisk canbeexpectedtodecreasebyvariousamounts. However,empiricalriskminimization ispronetooverÔ¨Åtting.Modelswith highcapacitycansimplymemorizethetrainingset.Inmanycases,empirical riskminimization isnotreallyfeasible.ThemosteÔ¨Äectivemodernoptimization algorithmsarebasedongradientdescent,butmanyusefullossfunctions,such as0-1loss,havenousefulderivatives(thederivativeiseitherzeroorundeÔ¨Åned everywhere).Thesetwoproblemsmeanthat,inthecontextofdeeplearning,we rarelyuseempiricalriskminimization. Instead,wemustuseaslightlydiÔ¨Äerent approach,inwhichthequantitythatweactuallyoptimizeisevenmorediÔ¨Äerent fromthequantitythatwetrulywanttooptimize. 8.1.2SurrogateLossFunctionsandEarlyStopping Sometimes,thelossfunctionweactuallycareabout(sayclassiÔ¨Åcationerror)isnot onethatcanbeoptimizedeÔ¨Éciently.Forexample,exactlyminimizingexpected0-1 lossistypicallyintractable(exponentialintheinputdimension),evenforalinear classiÔ¨Åer(MarcotteandSavard1992,).Insuchsituations,onetypicallyoptimizes asurrogatelossfunctioninstead,whichactsasaproxybuthasadvantages. Forexample,thenegativelog-likelihoodofthecorrectclassistypicallyusedasa surrogateforthe0-1loss.Thenegativelog-likelihoodallowsthemodeltoestimate theconditionalprobabilityoftheclasses,giventheinput,andifthemodelcan dothatwell,thenitcanpicktheclassesthatyieldtheleastclassiÔ¨Åcationerrorin expectation. 2 7 6 CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS Insomecases,asurrogatelossfunctionactuallyresultsinbeingabletolearn more.Forexample,thetestset0-1lossoftencontinuestodecreaseforalong timeafterthetrainingset0-1losshasreachedzero,whentrainingusingthe log-likelihoodsurrogate.Thisisbecauseevenwhentheexpected0-1lossiszero, onecanimprovetherobustnessoftheclassiÔ¨Åerbyfurtherpushingtheclassesapart fromeachother,obtainingamoreconÔ¨ÅdentandreliableclassiÔ¨Åer,thusextracting moreinformationfromthetrainingdatathanwouldhavebeenpossiblebysimply minimizingtheaverage0-1lossonthetrainingset. AveryimportantdiÔ¨Äerencebetweenoptimization ingeneralandoptimization asweuseitfortrainingalgorithmsisthattrainingalgorithmsdonotusuallyhalt atalocalminimum.Instead,amachinelearningalgorithmusuallyminimizes asurrogatelossfunctionbuthaltswhenaconvergencecriterionbasedonearly stopping(section)issatisÔ¨Åed.Typicallytheearlystoppingcriterionisbased 7.8 onthetrueunderlyinglossfunction,suchas0-1lossmeasuredonavalidationset, andisdesignedtocausethealgorithmtohaltwheneveroverÔ¨Åttingbeginstooccur. Trainingoftenhaltswhilethesurrogatelossfunctionstillhaslargederivatives, whichisverydiÔ¨Äerentfromthepureoptimization setting,whereanoptimization algorithmisconsideredtohaveconvergedwhenthegradientbecomesverysmall. 8.1.3BatchandMinibatchAlgorithms Oneaspectofmachinelearningalgorithmsthatseparatesthemfromgeneral optimization algorithmsisthattheobjectivefunctionusuallydecomposesasasum overthetrainingexamples.Optimization algorithmsformachinelearningtypically computeeachupdatetotheparametersbasedonanexpectedvalueofthecost functionestimatedusingonlyasubsetofthetermsofthefullcostfunction. Forexample,maximumlikelihoodestimationproblems,whenviewedinlog space,decomposeintoasumovereachexample: Œ∏ M L= argmax Œ∏m ÓÅò i = 1log p m o de l(x( ) i, y( ) i;)Œ∏ . (8.4) Maximizingthissumisequivalenttomaximizingtheexpectationoverthe empiricaldistributiondeÔ¨Ånedbythetrainingset: J() = Œ∏ E x ,y ‚àº ÀÜ pdatalog p m o de l(;)x , yŒ∏ . (8.5) Mostofthepropertiesoftheobjectivefunction Jusedbymostofouropti- mizationalgorithmsarealsoexpectationsoverthetrainingset.Forexample,the 2 7 7 CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS mostcommonlyusedpropertyisthegradient: ‚àá Œ∏ J() = Œ∏ E x ,y ‚àº ÀÜ pdata‚àá Œ∏log p m o de l(;)x , yŒ∏ . (8.6) Computing this expectation exactly isvery expensive because it requires evaluatingthemodeloneveryexampleintheentiredataset.Inpractice,wecan computetheseexpectationsbyrandomlysamplingasmallnumberofexamples fromthedataset,thentakingtheaverageoveronlythoseexamples. Recallthatthestandarderrorofthemean(equation)estimatedfrom 5.46 n samplesisgivenby œÉ /‚àön ,where œÉisthetruestandarddeviationofthevalueof thesamples.Thedenominator of‚àönshowsthattherearelessthanlinearreturns tousingmoreexamplestoestimatethegradient.Comparetwohypothetical estimatesofthegradient,onebasedon100examplesandanotherbasedon10,000 examples.Thelatterrequires100timesmorecomputationthantheformer,but reducesthestandarderrorofthemeanonlybyafactorof10.Mostoptimization algorithmsconvergemuchfaster(intermsoftotalcomputation,notintermsof numberofupdates)iftheyareallowedtorapidlycomputeapproximate estimates ofthegradientratherthanslowlycomputingtheexactgradient. Anotherconsiderationmotivatingstatisticalestimationofthegradientfroma smallnumberofsamplesisredundancyinthetrainingset.Intheworstcase,all msamplesinthetrainingsetcouldbeidenticalcopiesofeachother.Asampling- basedestimateofthegradientcouldcomputethecorrectgradientwithasingle sample,using mtimeslesscomputationthanthenaiveapproach.Inpractice,we areunlikelytotrulyencounterthisworst-casesituation,butwemayÔ¨Åndlarge numbersofexamplesthatallmakeverysimilarcontributionstothegradient. Optimization algorithmsthatusetheentiretrainingsetarecalledbatchor deterministicgradientmethods,becausetheyprocessallofthetrainingexamples simultaneouslyinalargebatch.Thisterminologycanbesomewhatconfusing becausetheword‚Äúbatch‚Äùisalsooftenusedtodescribetheminibatchusedby minibatchstochasticgradientdescent.Typicallytheterm‚Äúbatchgradientdescent‚Äù impliestheuseofthefulltrainingset,whiletheuseoftheterm‚Äúbatch‚Äùtodescribe agroupofexamplesdoesnot. Forexample,itisverycommontousetheterm ‚Äúbatchsize‚Äùtodescribethesizeofaminibatch. Optimization algorithmsthatuseonlyasingleexampleatatimearesometimes calledstochasticorsometimesonlinemethods.Thetermonlineisusually reservedforthecasewheretheexamplesaredrawnfromastreamofcontinually createdexamplesratherthanfromaÔ¨Åxed-sizetrainingsetoverwhichseveral passesaremade. Mostalgorithmsusedfordeeplearningfallsomewhereinbetween,usingmore 2 7 8 CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS thanonebutlessthanallofthetrainingexamples.Theseweretraditionallycalled minibatchorminibatchstochasticmethodsanditisnowcommontosimply callthemstochasticmethods. Thecanonicalexampleofastochasticmethodisstochasticgradientdescent, presentedindetailinsection.8.3.1 Minibatchsizesaregenerallydrivenbythefollowingfactors: ‚Ä¢Largerbatchesprovideamoreaccurateestimateofthegradient,butwith lessthanlinearreturns. ‚Ä¢Multicorearchitectures areusuallyunderutilized byextremelysmallbatches. Thismotivatesusingsomeabsoluteminimumbatchsize,belowwhichthere isnoreductioninthetimetoprocessaminibatch. ‚Ä¢Ifallexamplesinthebatcharetobeprocessedinparallel(asistypically thecase),thentheamountofmemoryscaleswiththebatchsize.Formany hardwaresetupsthisisthelimitingfactorinbatchsize. ‚Ä¢SomekindsofhardwareachievebetterruntimewithspeciÔ¨Åcsizesofarrays. EspeciallywhenusingGPUs,itiscommonforpowerof2batchsizestooÔ¨Äer betterruntime.Typicalpowerof2batchsizesrangefrom32to256,with16 sometimesbeingattemptedforlargemodels. ‚Ä¢SmallbatchescanoÔ¨ÄeraregularizingeÔ¨Äect( ,), WilsonandMartinez2003 perhapsduetothenoisetheyaddtothelearningprocess.Generalization errorisoftenbestforabatchsizeof1. Trainingwithsuchasmallbatch sizemightrequireasmalllearningratetomaintainstabilityduetothehigh varianceintheestimateofthegradient.Thetotalruntimecanbeveryhigh duetotheneedtomakemoresteps,bothbecauseofthereducedlearning rateandbecauseittakesmorestepstoobservetheentiretrainingset. DiÔ¨ÄerentkindsofalgorithmsusediÔ¨Äerentkindsofinformationfromthemini- batchindiÔ¨Äerentways.Somealgorithmsaremoresensitivetosamplingerrorthan others,eitherbecausetheyuseinformationthatisdiÔ¨Éculttoestimateaccurately withfewsamples,orbecausetheyuseinformationinwaysthatamplifysampling errorsmore.Methodsthatcomputeupdatesbasedonlyonthegradientgare usuallyrelativelyrobustandcanhandlesmallerbatchsizeslike100.Second-order methods,whichusealsotheHessianmatrixHandcomputeupdatessuchas H‚àí 1g,typicallyrequiremuchlargerbatchsizeslike10,000.Theselargebatch sizesarerequiredtominimizeÔ¨ÇuctuationsintheestimatesofH‚àí 1g.Suppose thatHisestimatedperfectlybuthasapoorconditionnumber.Multiplication by 2 7 9 CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS HoritsinverseampliÔ¨Åespre-existingerrors,inthiscase,estimationerrorsing. Verysmallchangesintheestimateofgcanthuscauselargechangesintheupdate H‚àí 1g,evenifHwereestimatedperfectly.Ofcourse,Hwillbeestimatedonly approximately,sotheupdateH‚àí 1gwillcontainevenmoreerrorthanwewould predictfromapplyingapoorlyconditionedoperationtotheestimateof.g Itisalsocrucialthattheminibatchesbeselectedrandomly.Computingan unbiasedestimateoftheexpectedgradientfromasetofsamplesrequiresthatthose samplesbeindependent.Wealsowishfortwosubsequentgradientestimatestobe independentfromeachother,sotwosubsequentminibatchesofexamplesshould alsobeindependentfromeachother.Manydatasetsaremostnaturallyarranged inawaywheresuccessiveexamplesarehighlycorrelated.Forexample,wemight haveadatasetofmedicaldatawithalonglistofbloodsampletestresults.This listmightbearrangedsothatÔ¨ÅrstwehaveÔ¨ÅvebloodsamplestakenatdiÔ¨Äerent timesfromtheÔ¨Årstpatient,thenwehavethreebloodsamplestakenfromthe secondpatient,thenthebloodsamplesfromthethirdpatient,andsoon.Ifwe weretodrawexamplesinorderfromthislist,theneachofourminibatcheswould beextremelybiased,becauseitwouldrepresentprimarilyonepatientoutofthe manypatientsinthedataset.Incasessuchasthesewheretheorderofthedataset holdssomesigniÔ¨Åcance,itisnecessarytoshuÔ¨Ñetheexamplesbeforeselecting minibatches.Forverylargedatasets,forexampledatasetscontainingbillionsof examplesinadatacenter,itcanbeimpracticaltosampleexamplestrulyuniformly atrandomeverytimewewanttoconstructaminibatch.Fortunately,inpractice itisusuallysuÔ¨ÉcienttoshuÔ¨Ñetheorderofthedatasetonceandthenstoreitin shuÔ¨Ñedfashion.ThiswillimposeaÔ¨Åxedsetofpossibleminibatchesofconsecutive examplesthatallmodelstrainedthereafterwilluse,andeachindividualmodel willbeforcedtoreusethisorderingeverytimeitpassesthroughthetraining data.However,thisdeviationfromtruerandomselectiondoesnotseemtohavea signiÔ¨ÅcantdetrimentaleÔ¨Äect.FailingtoevershuÔ¨Ñetheexamplesinanywaycan seriouslyreducetheeÔ¨Äectivenessofthealgorithm. Manyoptimization problemsinmachinelearningdecomposeoverexamples wellenoughthatwecancomputeentireseparateupdatesoverdiÔ¨Äerentexamples inparallel.Inotherwords,wecancomputetheupdatethatminimizes J(X)for oneminibatchofexamplesXatthesametimethatwecomputetheupdatefor severalotherminibatches.Suchasynchronousparalleldistributedapproachesare discussedfurtherinsection.12.1.3 Aninterestingmotivationforminibatchstochasticgradientdescentisthatit followsthegradientofthetruegeneralizationerror(equation)solongasno 8.2 examplesarerepeated.Mostimplementations ofminibatchstochasticgradient 2 8 0 CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS descentshuÔ¨Ñethedatasetonceandthenpassthroughitmultipletimes.Onthe Ô¨Årstpass,eachminibatchisusedtocomputeanunbiasedestimateofthetrue generalization error.Onthesecondpass,theestimatebecomesbiasedbecauseitis formedbyre-samplingvaluesthathavealreadybeenused,ratherthanobtaining newfairsamplesfromthedatageneratingdistribution. Thefactthatstochasticgradientdescentminimizesgeneralization erroris easiesttoseeintheonlinelearningcase,whereexamplesorminibatchesaredrawn fromastreamofdata.Inotherwords,insteadofreceivingaÔ¨Åxed-sizetraining set,thelearnerissimilartoalivingbeingwhoseesanewexampleateachinstant, witheveryexample (x , y)comingfromthedatageneratingdistribution p da t</div>
        </div>
    </div>

    <div class="question-card" id="q125">
        <div class="question-header">
            <span class="question-number">Question 125</span>
            <span class="question-type">multiple-choice</span>
        </div>

        <div class="question-text">In constrained optimization, the generalized Lagrangian function and the Karush-Kuhn-Tucker (KKT) conditions are central tools for handling problems with both equality and inequality constraints. Understanding how these methods reformulate and solve optimization problems is critical in fields like engineering, economics, and machine learning.

Which of the following statements best describes the role of KKT multipliers in the generalized Lagrangian approach to constrained optimization?

1) They eliminate the need for inequality constraints by converting them to equalities.   
2) They directly compute the global minimum without considering feasibility.   
3) They are used only for unconstrained optimization problems.   
4) They assign a penalty to objective functions, regardless of constraint satisfaction.   
5) They ensure all constraints are inactive at the optimal solution.   
6) They act as variables that enforce both equality and inequality constraints by penalizing violations, causing the Lagrangian to become infinite if constraints are not satisfied.   
7) They guarantee sufficiency for optimality in all types of constrained optimization problems.</div>

        <div class="answer-section">
            <div class="answer-label">‚úì Correct Answer:</div>
            <div class="answer-text">The correct answer is 6) They act as variables that enforce both equality and inequality constraints by penalizing violations, causing the Lagrangian to become infinite if constraints are not satisfied..</div>
        </div>

        <div class="reference-section">
            <div class="reference-label">üìö Reference Text:</div>
            <button class="toggle-button" onclick="toggleReference(125)">
                Show/Hide Reference
            </button>
            <div id="ref125" class="reference-text hidden">newfunctioncalledthe g e ner al i z e d L agr angi a nor g e ner al i z e d L agr ange f unc t i o n. TodeÔ¨ÅnetheLagrangian,weÔ¨Årstneedtodescribe Sintermsofequations andinequalities. W ewantadescriptionof Sintermsof mfunctions g( ) iand n functions h( ) jsothat S={|‚àÄ x i , g( ) i( x) = 0and‚àÄ j , h( ) j( x)‚â§0}.Theequations involving g( ) iarecalledthe e q ual i t y c o nst r ai n t sandtheinequalitiesinvolving h( ) jarecalled . i neq ual i t y c o nst r ai n t s Weintroducenewvariables Œª iand Œ± jforeachconstraint,thesearecalledthe KKTmultipliers.ThegeneralizedLagrangianisthendeÔ¨Ånedas L , , f ( x Œª Œ±) = ()+ xÓÅò iŒª i g( ) i()+ xÓÅò jŒ± j h( ) j() x .(4.14) Wecannowsolveaconstrainedminimization problemusingunconstrained optimization ofthegeneralizedLagrangian.Observethat,solongasatleastone feasiblepointexistsandisnotpermittedtohavevalue,then f() x ‚àû min xmax Œªmax Œ± Œ± , ‚â• 0L , , . ( x Œª Œ±) (4.15) hasthesameoptimalobjectivefunctionvalueandsetofoptimalpointsas x min x ‚àà Sf .() x (4.16) ThisfollowsbecauseanytimetheconstraintsaresatisÔ¨Åed, max Œªmax Œ± Œ± , ‚â• 0L , , f , ( x Œª Œ±) = () x (4.17) whileanytimeaconstraintisviolated, max Œªmax Œ± Œ± , ‚â• 0L , , . ( x Œª Œ±) = ‚àû (4.18) 1Th e K K T a p p ro a c h g e n e ra l i z e s t h e m e t h o d o f La gra n ge m u lt ip lie r s wh i c h a l l o ws e q u a l i t y c o n s t ra i n t s b u t n o t i n e q u a l i t y c o n s t ra i n t s . 9 4 CHAPTER4.NUMERICALCOMPUTATION Thesepropertiesguaranteethatnoinfeasiblepointcanbeoptimal,andthatthe optimumwithinthefeasiblepointsisunchanged. Toperformconstrainedmaximization, wecanconstructthegeneralizedLa- grangefunctionof,whichleadstothisoptimization problem: ‚àí f() x min xmax Œªmax Œ± Œ± , ‚â• 0‚àí f()+ xÓÅò iŒª i g( ) i()+ xÓÅò jŒ± j h( ) j() x .(4.19) Wemayalsoconvertthistoaproblemwithmaximization intheouterloop: max xmin Œªmin Œ± Œ± , ‚â• 0f()+ xÓÅò iŒª i g( ) i() x‚àíÓÅò jŒ± j h( ) j() x .(4.20) Thesignofthetermfortheequalityconstraintsdoesnotmatter;wemaydeÔ¨Åneit withadditionorsubtractionaswewish,becausetheoptimization isfreetochoose anysignforeach Œª i. Theinequalityconstraintsareparticularlyinteresting.Wesaythataconstraint h( ) i( x)is ac t i v eif h( ) i( x‚àó) = 0.Ifaconstraintisnotactive,thenthesolutionto theproblemfoundusingthatconstraintwouldremainatleastalocalsolutionif thatconstraintwereremoved.Itispossiblethataninactiveconstraintexcludes othersolutions.Forexample,aconvexproblemwithanentireregionofglobally optimalpoints(awide,Ô¨Çat,regionofequalcost)couldhaveasubsetofthis regioneliminatedbyconstraints,oranon-convexproblemcouldhavebetterlocal stationarypointsexcludedbyaconstraintthatisinactiveatconvergence.However, thepointfoundatconvergenceremainsastationarypointwhetherornotthe inactiveconstraintsareincluded.Becauseaninactive h( ) ihasnegativevalue,then thesolutiontomin xmax Œªmax Œ± Œ± , ‚â• 0 L( x Œª Œ± , ,)willhave Œ± i=0.Wecanthus observethatatthesolution, Œ± hÓÄå( x)= 0.Inotherwords,forall i,weknow thatatleastoneoftheconstraints Œ± i‚â•0and h( ) i( x)‚â§0mustbeactiveatthe solution.Togainsomeintuitionforthisidea,wecansaythateitherthesolution isontheboundaryimposedbytheinequalityandwemustuseitsKKTmultiplier toinÔ¨Çuencethesolutionto x,ortheinequalityhasnoinÔ¨Çuenceonthesolution andwerepresentthisbyzeroingoutitsKKTmultiplier. Asimplesetofpropertiesdescribetheoptimalpointsofconstrainedopti- mizationproblems.ThesepropertiesarecalledtheKarush-Kuhn-Tucker(KKT) conditions(,;Karush1939KuhnandTucker1951,).Theyarenecessaryconditions, butnotalwayssuÔ¨Écientconditions,forapointtobeoptimal.Theconditionsare: ‚Ä¢ThegradientofthegeneralizedLagrangianiszero. ‚Ä¢AllconstraintsonbothandtheKKTmultipliersaresatisÔ¨Åed. x 9 5 CHAPTER4.NUMERICALCOMPUTATION ‚Ä¢Theinequalityconstraintsexhibit‚Äúcomplementary slackness‚Äù: Œ± hÓÄå( x) = 0. FormoreinformationabouttheKKTapproach,seeNocedalandWright2006(). 4. 5 E x am p l e: L i n ear L eas t S q u are s SupposewewanttoÔ¨Åndthevalueofthatminimizes x f() = x1 2||‚àí|| A x b2 2 . (4.21) TherearespecializedlinearalgebraalgorithmsthatcansolvethisproblemeÔ¨Éciently. However,wecanalsoexplorehowtosolveitusinggradient-basedoptimization as asimpleexampleofhowthesetechniqueswork. First,weneedtoobtainthegradient: ‚àá x f() = x AÓÄæ( ) = A x b‚àí AÓÄæA x A‚àíÓÄæb . (4.22) Wecanthenfollowthisgradientdownhill,takingsmallsteps.Seealgorithm4.1 fordetails. Al g o r</div>
        </div>
    </div>

    <div class="question-card" id="q126">
        <div class="question-header">
            <span class="question-number">Question 126</span>
            <span class="question-type">multiple-choice</span>
        </div>

        <div class="question-text">Deep generative models utilize various strategies to efficiently model high-dimensional data, with parameter sharing playing a crucial role in reducing complexity and improving generalization. Neural auto-regressive networks have introduced innovations that distinguish them from linear and tabular approaches.

Which property specifically enables neural auto-regressive models to overcome the curse of dimensionality and achieve scalable density estimation?

1) Modeling each conditional distribution separately without shared features   
2) Using tabular representations for conditional probabilities   
3) Sharing features and parameters across conditional distributions   
4) Applying kernel methods to each variable independently   
5) Restricting the model to linear relationships among variables   
6) Employing closed-form solutions for parameter estimation   
7) Limiting the architecture to fully visible belief networks</div>

        <div class="answer-section">
            <div class="answer-label">‚úì Correct Answer:</div>
            <div class="answer-text">The correct answer is 3) Sharing features and parameters across conditional distributions.</div>
        </div>

        <div class="reference-section">
            <div class="reference-label">üìö Reference Text:</div>
            <button class="toggle-button" onclick="toggleReference(126)">
                Show/Hide Reference
            </button>
            <div id="ref126" class="reference-text hidden">CHAPTER20.DEEPGENERATIVEMODELS insection below,wecanintroduceaformofparametersharingthat 20.10.10 bringsbothastatisticaladvantage(feweruniqueparameters)andacomputational advantage(lesscomputation). Thisisonemoreinstanceoftherecurringdeep learningmotifof r e u s e o f f e a t u r e s. x 1 x 1 x 2 x 2 x 3 x 3 x 4 x 4P x ( 4| x 1 , x 2 , x 3 ) P x ( 4| x 1 , x 2 , x 3 )P x ( 3| x 1 , x 2 ) P x ( 3| x 1 , x 2 ) P x ( 2| x 1 ) P x ( 2| x 1 )P x ( 1 ) P x ( 1 )x 1 x 1 x 2 x 2 x 3 x 3 x 4 x 4 Figure20.8:A fullyvisiblebelief networkpredictsthe i-thvariable fromthe i‚àí1 previousones. ( T o p ) ( Bottom ) ThedirectedgraphicalmodelforanFVBN. Corresponding computationalgraph,inthecaseofthelogisticFVBN,whereeachpredictionismadeby alinearpredictor. 20.10.8LinearAuto-RegressiveNetworks Thesimplestformofauto-regressiv enetworkhasnohiddenunitsandnosharing ofparametersorfeatures.Each P( x i| x i‚àí 1 , . . . , x 1)isparametrized asalinear model(linearregressionforreal-valueddata,logisticregressionforbinarydata, softmaxregressionfordiscretedata).ThismodelwasintroducedbyFrey1998() andhas O( d2)parameterswhenthereare dvariablestomodel.Itisillustratedin Ô¨Ågure.20.8 Ifthevariablesarecontinuous,alinearauto-regressive modelismerelyanother waytoformulateamultivariateGaussiandistribution,capturinglinearpairwise interactionsbetweentheobservedvariables. Linearauto-regressiv enetworksareessentiallythegeneralization oflinear classiÔ¨Åcationmethodstogenerativemodeling.Theythereforehavethesame 7 0 6 CHAPTER20.DEEPGENERATIVEMODELS advantagesanddisadvantagesaslinearclassiÔ¨Åers.LikelinearclassiÔ¨Åers,theymay betrainedwithconvexlossfunctions,andsometimesadmitclosedformsolutions (asintheGaussiancase).LikelinearclassiÔ¨Åers,themodelitselfdoesnotoÔ¨Äer awayofincreasingitscapacity,socapacitymustberaisedusingtechniqueslike basisexpansionsoftheinputorthekerneltrick. x 1 x 1 x 2 x 2 x 3 x 3 x 4 x 4h 1 h 1 h 2 h 2 h 3 h 3P x ( 4| x 1 , x 2 , x 3 ) P x ( 4| x 1 , x 2 , x 3 )P x ( 3| x 1 , x 2 ) P x ( 3| x 1 , x 2 ) P x ( 2| x 1 ) P x ( 2| x 1 )P x ( 1 ) P x ( 1 ) Figure20.9:Aneuralauto-regressivenetworkpredictsthe i-thvariable x ifromthe i‚àí1 previousones,butisparametrizedsothatfeatures(groupsofhiddenunitsdenoted h i) thatarefunctionsof x 1 , . . . , x icanbereusedinpredictingallofthesubsequentvariables x i + 1 , x i + 2 , . . . , x d. 20.10.9NeuralAuto-RegressiveNetworks Neuralauto-regressiv enetworks( ,,)havethesame BengioandBengio2000ab left-to-rightgraphicalmodelaslogisticauto-regressiv enetworks(Ô¨Ågure)but20.8 employadiÔ¨Äerentparametrization oftheconditionaldistributionswithinthat graphicalmodelstructure.Thenewparametrization ismorepowerfulinthesense thatitscapacitycanbeincreasedasmuchasneeded,allowingapproximation of anyjointdistribution.Thenewparametrization canalsoimprovegeneralization byintroducingaparametersharingandfeaturesharingprinciplecommontodeep learningingeneral.Themodelsweremotivatedbytheobjectiveofavoidingthe curseofdimensionalityarisingoutoftraditionaltabulargraphicalmodels,sharing thesamestructureasÔ¨Ågure.Intabulardiscreteprobabilisticmodels,each 20.8 conditionaldistributionisrepresentedbyatableofprobabilities, withoneentry andoneparameterforeachpossibleconÔ¨Ågurationofthevariablesinvolved.By usinganeuralnetworkinstead,twoadvantagesareobtained: 7 0 7 CHAPTER20.DEEPGENERATIVEMODELS 1.Theparametrization ofeach P( x i| x i‚àí 1 , . . . , x 1)byaneuralnetworkwith ( i‚àí1)√ó kinputsand koutputs(ifthevariablesarediscreteandtake k values,encodedone-hot)allowsonetoestimatetheconditionalprobability withoutrequiringanexponentialnumberofparameters(andexamples),yet stillisabletocapturehigh-orderdependenciesbetweentherandomvariables. 2.InsteadofhavingadiÔ¨Äerentneuralnetworkforthepredictionofeach x i, a connectivityillustratedinÔ¨Ågureallowsonetomergeall l e f t - t o - r i g h t 20.9 theneuralnetworksintoone.Equivalently,itmeansthatthehiddenlayer featurescomputedforpredicting x icanbereusedforpredicting x i k +( k >0). Thehiddenunitsarethusorganizedin g r o u p sthathavetheparticularity thatalltheunitsinthe i-thgrouponlydependontheinputvalues x 1 , . . . , x i. Theparametersusedtocomputethesehiddenunitsarejointlyoptimized to improvethe prediction ofall thevariables inthe sequence.This is aninstanceofthe r e u s e p r i nc i p l ethatrecursthroughoutdeeplearningin scenariosrangingfromrecurrentandconvolutionalnetworkarchitectures to multi-taskandtransferlearning. Each P( x i| x i‚àí 1 , . . . , x 1)canrepresentaconditionaldistributionbyhaving outputsoftheneuralnetworkpredict p a r a m e t e r softheconditionaldistribution of x i,asdiscussedinsection.Althoughtheoriginalneuralauto-regressive 6.2.1.1 networkswereinitiallyevaluatedinthecontextofpurelydiscretemultivariate data(withasigmoidoutputforaBernoullivariableorsoftmaxoutputfora multinoullivariable)itisnaturaltoextendsuchmodelstocontinuousvariablesor jointdistributionsinvolvingbothdiscreteandcontinuousvariables. 20.10.10NADE Theneuralautoregressivedensityestimator(NADE)isaverysuccessful recentformofneuralauto-regressive network(LarochelleandMurray2011,).The connectivityisthesameasfortheoriginalneuralauto-regressive networkofBengio andBengio2000b()butNADEintroducesanadditionalparametersharingscheme, asillustratedinÔ¨Ågure.TheparametersofthehiddenunitsofdiÔ¨Äerentgroups 20.10 jareshared. Theweights WÓÄ∞ j , k, ifromthe i-thinput x itothe k-thelementofthe j-thgroup ofhiddenunit h( ) j k()aresharedamongthegroups: j i‚â• WÓÄ∞ j , k, i=</div>
        </div>
    </div>

    <div class="question-card" id="q127">
        <div class="question-header">
            <span class="question-number">Question 127</span>
            <span class="question-type">multiple-choice</span>
        </div>

        <div class="question-text">In machine learning, optimization techniques such as gradient descent are used to minimize or maximize objective functions, which often depend on multiple parameters. Understanding the behavior of derivatives and gradients is crucial for effectively finding optimal solutions in high-dimensional spaces.

Which of the following statements most accurately describes a saddle point in the context of high-dimensional optimization?

1) It is a point where the function achieves its lowest possible value globally.   
2) It is a point where the gradient is nonzero but the function value does not change.   
3) It is a point where the function achieves its highest possible value locally.   
4) It is a point where the gradient (or all partial derivatives) is zero, but the point is neither a local minimum nor a local maximum.   
5) It is a point where the function value can only decrease in every direction.   
6) It is a point where the gradient descent algorithm always converges.   
7) It is a point that can only occur in functions of a single variable.</div>

        <div class="answer-section">
            <div class="answer-label">‚úì Correct Answer:</div>
            <div class="answer-text">The correct answer is 4) It is a point where the gradient (or all partial derivatives) is zero, but the point is neither a local minimum nor a local maximum..</div>
        </div>

        <div class="reference-section">
            <div class="reference-label">üìö Reference Text:</div>
            <button class="toggle-button" onclick="toggleReference(127)">
                Show/Hide Reference
            </button>
            <div id="ref127" class="reference-text hidden">t i v e f unc - t i o nor c r i t e r i o n.Whenweareminimizingit, wemayalsocallitthe c o st f unc t i o n, l o ss f unc t i o n,or e r r o r f unc t i o n. Inthisbook,weusetheseterms interchangeably,thoughsomemachinelearningpublicationsassignspecialmeaning tosomeoftheseterms. Weoftendenotethevaluethatminimizesormaximizesafunctionwitha superscript.Forexample,wemightsay ‚àó x‚àó= argmin() f x. 8 2 CHAPTER4.NUMERICALCOMPUTATION ‚àí ‚àí ‚àí ‚àí 20. 15. 10. 05 00 05 10 15 20 ...... x‚àí20.‚àí15.‚àí10.‚àí05.00.05.10.15.20. Globalminimumat= 0.x SincefÓÄ∞() = 0,gradient x descent haltshere. For 0,wehave x< fÓÄ∞() 0,x< sowecandecreasebyf moving rightward.For 0,wehave x> fÓÄ∞() 0,x> sowecandecreasebyf moving leftward. f x() =1 2x2 fÓÄ∞() = x x Figure4.1:Anillustrationofhowthegradientdescentalgorithmusesthederivativesofa functioncanbeusedtofollowthefunctiondownhilltoaminimum. Weassumethereaderisalreadyfamiliarwithcalculus,butprovideabrief reviewofhowcalculusconceptsrelatetooptimization here. Supposewehaveafunction y= f( x),whereboth xand yarerealnumbers. The der i v at i v eofthisfunctionisdenotedas fÓÄ∞( x)orasd y d x.Thederivative fÓÄ∞( x) givestheslopeof f( x)atthepoint x.Inotherwords,itspeciÔ¨Åeshowtoscale asmallchangeintheinputinordertoobtainthecorrespondingchangeinthe output: f x ÓÄè f x ÓÄè f (+) ‚âà()+ÓÄ∞() x. Thederivativeisthereforeusefulforminimizingafunctionbecauseittells ushowtochange xinordertomakeasmallimprovementin y.Forexample, weknowthat f( x ÓÄè‚àísign( fÓÄ∞( x)))islessthan f( x)forsmallenough ÓÄè.Wecan thusreduce f( x)bymoving xinsmallstepswithoppositesignofthederivative. Thistechniqueiscalled g r adi e n t desc e n t(Cauchy1847,).SeeÔ¨Ågureforan4.1 exampleofthistechnique. When fÓÄ∞( x) = 0,thederivativeprovidesnoinformationaboutwhichdirection tomove.Pointswhere fÓÄ∞( x)=0areknownas c r i t i c al p o i nt sor st at i o na r y p o i n t s.A l o c al m i ni m umisapointwhere f( x)islowerthanatallneighboring points,soitisnolongerpossibletodecrease f( x)bymakinginÔ¨Ånitesimalsteps. A l o c al m ax i m u misapointwhere f( x)ishigherthanatallneighboringpoints, 8 3 CHAPTER4.NUMERICALCOMPUTATION Minimum Maximum Saddlepoint Figure4.2:Examplesofeachofthethreetypesofcriticalpointsin1-D.Acriticalpointis apointwithzeroslope.Suchapointcaneitherbealocalminimum,whichislowerthan theneighboringpoints,alocalmaximum,whichishigherthantheneighboringpoints,or asaddlepoint,whichhasneighborsthatarebothhigherandlowerthanthepointitself. soitisnotpossibletoincrease f( x)bymakinginÔ¨Ånitesimalsteps.Somecritical pointsareneithermaximanorminima.Theseareknownas saddle p o i nt s.See Ô¨Ågureforexamplesofeachtypeofcriticalpoint. 4.2 Apointthatobtainstheabsolutelowestvalueof f( x)isa g l o bal m i ni m um. Itispossiblefortheretobeonlyoneglobalminimumormultipleglobalminimaof thefunction.Itisalsopossiblefortheretobelocalminimathatarenotglobally optimal.Inthecontextofdeeplearning,weoptimizefunctionsthatmayhave manylocalminimathatarenotoptimal,andmanysaddlepointssurroundedby veryÔ¨Çatregions.Allofthismakesoptimization verydiÔ¨Écult,especiallywhenthe inputtothefunctionismultidimensional.WethereforeusuallysettleforÔ¨Åndinga valueof fthatisverylow,butnotnecessarilyminimalinanyformalsense.See Ô¨Ågureforanexample.4.3 Weoftenminimizefunctionsthathavemultipleinputs: f: Rn‚Üí R.Forthe conceptof‚Äúminimization‚Äù to makesense,theremuststillbeonlyone(scalar) output. Forfunctionswithmultipleinputs,wemustmakeuseoftheconceptof par t i al der i v at i v e s.Thepartialderivative‚àÇ ‚àÇ x if( x)measureshow fchangesasonlythe variable x iincreasesatpoint x.The g r adi e n tgeneralizesthenotionofderivative tothecasewherethederivativeiswithrespecttoavector:thegradientof fisthe vectorcontainingallofthepartialderivatives,denoted ‚àá x f( x).Element iofthe gradientisthepartialderivativeof fwithrespectto x i.Inmultipledimensions, 8 4 CHAPTER4.NUMERICALCOMPUTATION xf x() Ideally,wewouldlike toarriveattheglobal minimum, butthis might notbepossible.Thislocalminimum performsnearlyaswellas theglobalone, soitisanacceptable haltingpoint. Thislocalminimumperforms poorlyandshouldbeavoided. Figure4.3:OptimizationalgorithmsmayfailtoÔ¨Åndaglobalminimumwhenthereare multiplelocalminimaorplateauspresent.Inthecontextofdeeplearning,wegenerally acceptsuchsolutionseventhoughtheyarenottrulyminimal,solongastheycorrespond tosigniÔ¨Åcantlylowvaluesofthecostfunction. criticalpointsarepointswhereeveryelementofthegradientisequaltozero. The di r e c t i o n a l der i v at i v eindirection(aunitvector)istheslopeofthe u function findirection u.Inotherwords,thedirectionalderivativeisthederivative ofthefunction f( x+ Œ± u)withrespectto Œ±,evaluatedat Œ±= 0.Usingthechain rule,wecanseethat‚àÇ ‚àÇ Œ±f Œ± (+ x u)evaluatesto uÓÄæ‚àá x f Œ± () xwhen = 0. Tominimize f,wewouldliketoÔ¨Åndthedirectioninwhich fdecreasesthe fastest.Wecandothisusingthedirectionalderivative: min u u ,ÓÄæ u = 1uÓÄæ‚àá x f() x (4.3) =min u u ,ÓÄæ u = 1|||| u 2||‚àá x f() x|| 2cos Œ∏ (4.4) where Œ∏istheanglebetween uandthegradient.Substitutingin|||| u 2= 1and ignoringfactorsthatdonotdependon u,thissimpliÔ¨Åestomin ucos Œ∏.Thisis minimizedwhen upointsintheoppositedirectionasthegradient.Inother words,thegradientpointsdirectlyuphill,andthenegativegradientpointsdirectly downhill.Wecandecrease fbymovinginthedirectionofthenegativegradient. Thisisknownasthe or . m e t ho d o f st e e p e st desc e nt g r adi e nt desc e nt Steepestdescentproposesanewpoint xÓÄ∞= x‚àí‚àá ÓÄè x f() x (4.5) 8 5</div>
        </div>
    </div>

    <div class="question-card" id="q128">
        <div class="question-header">
            <span class="question-number">Question 128</span>
            <span class="question-type">multiple-choice</span>
        </div>

        <div class="question-text">Recurrent neural networks (RNNs) are widely used in machine learning for modeling sequential data such as language, audio, and time series. They process sequences by maintaining and updating a hidden state over time, allowing information to persist across steps.

Which statement best explains why RNNs can generalize to input sequences longer than those encountered during training?

1) They use different sets of parameters for each time step, adapting to sequence length dynamically.   
2) Their hidden state is reset after each input, enabling flexibility across varied sequence lengths.   
3) RNNs store the entire sequence history within the output layer, supporting longer sequences.   
4) The output probabilities are normalized using softmax, which allows arbitrary sequence length.   
5) They employ multiple activation functions at each time step, increasing generalization ability.   
6) The same transition function with shared parameters is applied at every time step, making the model applicable to unseen sequence lengths.   
7) They utilize unfolded computational graphs to memorize all possible sequence variations.</div>

        <div class="answer-section">
            <div class="answer-label">‚úì Correct Answer:</div>
            <div class="answer-text">The correct answer is 6) The same transition function with shared parameters is applied at every time step, making the model applicable to unseen sequence lengths..</div>
        </div>

        <div class="reference-section">
            <div class="reference-label">üìö Reference Text:</div>
            <button class="toggle-button" onclick="toggleReference(128)">
                Show/Hide Reference
            </button>
            <div id="ref128" class="reference-text hidden">t ‚àí 1 )x( ) tx( ) tx( + 1 ) tx( + 1 ) th( ) . . .h( ) . . .h( ) . . .h( ) . . . f f U nf ol df f f f f Figure10.2:Arecurrentnetworkwithnooutputs.Thisrecurrentnetworkjustprocesses informationfromtheinputxbyincorporatingitintothestatehthatispassedforward throughtime. ( L e f t )Circuitdiagram.Theblacksquareindicatesadelayofasingletime step.Thesamenetworkseenasanunfoldedcomputationalgraph,whereeach ( R i g h t ) nodeisnowassociatedwithoneparticulartimeinstance. EquationcanbedrawnintwodiÔ¨Äerentways.OnewaytodrawtheRNN 10.5 iswithadiagramcontainingonenodeforeverycomponentthatmightexistina 3 7 6 CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS physicalimplementationofthemodel,suchasabiologicalneuralnetwork.Inthis view,thenetworkdeÔ¨Ånesacircuitthatoperatesinrealtime,withphysicalparts whosecurrentstatecaninÔ¨Çuencetheirfuturestate,asintheleftofÔ¨Ågure.10.2 Throughoutthischapter,weuseablacksquareinacircuitdiagramtoindicate thataninteractiontakesplacewithadelayofasingletimestep,fromthestate attime ttothestateattime t+1.TheotherwaytodrawtheRNNisasan unfoldedcomputational graph,inwhicheachcomponentisrepresentedbymany diÔ¨Äerentvariables,withonevariablepertimestep,representingthestateofthe componentatthatpointintime.Eachvariableforeachtimestepisdrawnasa separatenodeofthecomputational graph,asintherightofÔ¨Ågure.Whatwe10.2 callunfoldingistheoperationthatmapsacircuitasintheleftsideoftheÔ¨Ågure toacomputational graphwithrepeatedpiecesasintherightside.Theunfolded graphnowhasasizethatdependsonthesequencelength. Wecanrepresenttheunfoldedrecurrenceafterstepswithafunction t g( ) t: h( ) t= g( ) t(x( ) t,x( 1 ) t ‚àí,x( 2 ) t ‚àí, . . . ,x( 2 ),x( 1 )) (10.6) =( fh( 1 ) t ‚àí,x( ) t;)Œ∏ (10.7) Thefunction g( ) ttakesthewholepastsequence (x( ) t,x( 1 ) t ‚àí,x( 2 ) t ‚àí, . . . ,x( 2 ),x( 1 )) asinputandproducesthecurrentstate,buttheunfoldedrecurrentstructure allowsustofactorize g( ) tintorepeatedapplicationofafunction f.Theunfolding processthusintroducestwomajoradvantages: 1.Regardlessofthesequencelength,thelearnedmodelalwayshasthesame inputsize,becauseitisspeciÔ¨Åedintermsoftransitionfromonestateto anotherstate,ratherthanspeciÔ¨Åedintermsofavariable-length historyof states. 2.Itispossibletousethetransitionfunction s a m e fwiththesameparameters ateverytimestep. Thesetwofactorsmakeitpossibletolearnasinglemodel fthatoperateson alltimestepsandallsequencelengths,ratherthanneedingtolearnaseparate model g( ) tforallpossibletimesteps.Learningasingle,sharedmodelallows generalization tosequencelengthsthatdidnotappearinthetrainingset,and allowsthemodeltobeestimatedwithfarfewertrainingexamplesthanwouldbe requiredwithoutparametersharing. Boththerecurrentgraphandtheunrolledgraphhavetheiruses.Therecurrent graphissuccinct.Theunfoldedgraphprovidesanexplicitdescriptionofwhich computations toperform.Theunfoldedgraphalsohelpstoillustratetheideaof 3 7 7 CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS informationÔ¨Çowforwardintime(computingoutputsandlosses)andbackward intime(computinggradients)byexplicitlyshowingthepathalongwhichthis informationÔ¨Çows. 10.2RecurrentNeuralNetworks Armedwiththegraphunrollingandparametersharingideasofsection,we10.1 candesignawidevarietyofrecurrentneuralnetworks. UUV V WWo( t ‚àí 1 )o( t ‚àí 1 ) hhooy y LL x xo( ) to( ) to( + 1 ) to( + 1 ) tL( t ‚àí 1 )L( t ‚àí 1 )L( ) tL( ) tL( + 1 ) tL( + 1 ) ty( t ‚àí 1 )y( t ‚àí 1 )y( ) ty( ) ty( +1 ) ty( +1 ) t h( t ‚àí 1 )h( t ‚àí 1 )h( ) th( ) th( + 1 ) th( + 1 ) t x( t ‚àí 1 )x( t ‚àí 1 )x( ) tx( ) tx( + 1 ) tx( + 1 ) tWW WW WW WW h( ) . . .h( ) . . .h( ) . . .h( ) . . .V V V V V V UU UU UUU nf ol d Figure10.3:Thecomputationalgraphtocomputethetraininglossofarecurrentnetwork thatmapsaninputsequenceofxvaluestoacorrespondingsequenceofoutputovalues. Aloss Lmeasureshowfareachoisfromthecorrespondingtrainingtargety.Whenusing softmaxoutputs,weassumeoistheunnormalizedlogprobabilities.Theloss Linternally computesÀÜy=softmax(o) andcomparesthistothetargety.TheRNNhasinputtohidden connectionsparametrizedbyaweightmatrixU,hidden-to-hiddenrecurrentconnections parametrizedbyaweightmatrixW,andhidden-to-outputconnectionsparametrizedby aweightmatrixV.EquationdeÔ¨Ånesforwardpropagationinthismodel. 10.8 ( L e f t )The RNNanditslossdrawnwithrecurrentconnections. ( R i g h t )Thesameseenasantime- unfoldedcomputationalgraph,whereeachnodeisnowassociatedwithoneparticular timeinstance. Someexamplesofimportantdesignpatternsforrecurrentneuralnetworks includethefollowing: 3 7 8 CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS ‚Ä¢Recurrentnetworksthatproduceanoutputateachtimestepandhave recurrentconnectionsbetweenhiddenunits,illustratedinÔ¨Ågure.10.3 ‚Ä¢Recurrentnetworksthatproduceanoutputateachtimestepandhave recurrentconnectionsonlyfromtheoutputatonetimesteptothehidden unitsatthenexttimestep,illustratedinÔ¨Ågure10.4 ‚Ä¢Recurrentnetworkswithrecurrentconnectionsbetweenhiddenunits,that readanentiresequenceandthenproduceasingleoutput,illustratedin Ô¨Ågure.10.5 Ô¨Ågureisareasonablyrepresentativeexamplethatwereturntothroughout 10.3 mostofthechapter. TherecurrentneuralnetworkofÔ¨Ågureandequationisuniversalinthe 10.3 10.8 sensethatanyfunctioncomputablebyaTuringmachinecanbecomputedbysuch arecurrentnetworkofaÔ¨Ånitesize.TheoutputcanbereadfromtheRNNafter anumberoftimestepsthatisasymptoticallylinearinthenumberoftimesteps usedbytheTuringmachineandasymptoticallylinearinthelengthoftheinput (SiegelmannandSontag1991Siegelmann1995SiegelmannandSontag1995 ,;,; ,; Hyotyniemi1996,).ThefunctionscomputablebyaTuringmachinearediscrete, sotheseresultsregardexactimplementation ofthefunction,notapproximations . TheRNN,whenusedasaTuringmachine,takesabinarysequenceasinputandits outputsmustbediscretizedtoprovideabinaryoutput.Itispossibletocomputeall functionsinthissettingusingasinglespeciÔ¨ÅcRNNofÔ¨Ånitesize(Siegelmannand Sontag1995()use886units).The‚Äúinput‚ÄùoftheTuringmachineisaspeciÔ¨Åcation ofthefunctiontobecomputed,sothesamenetworkthatsimulatesthisTuring machineissuÔ¨Écientforallproblems.ThetheoreticalRNNusedfortheproof cansimulateanunboundedstackbyrepresentingitsactivationsandweightswith rationalnumbersofunboundedprecision. WenowdeveloptheforwardpropagationequationsfortheRNNdepictedin Ô¨Ågure.TheÔ¨Åguredoesnotspecifythechoiceofactivationfunctionforthe 10.3 hiddenunits.Hereweassumethehyperbolictangentactivationfunction.Also, theÔ¨Åguredoesnotspecifyexactlywhatformtheoutputandlossfunctiontake. Hereweassumethattheoutputisdiscrete,asiftheRNNisusedtopredictwords orcharacters.Anaturalwaytorepresentdiscretevariablesistoregardtheoutput oasgivingtheunnormalized logprobabilitiesofeachpossiblevalueofthediscrete variable.Wecanthenapplythesoftmaxoperationasapost-processingstepto obtainavectorÀÜyofnormalizedprobabilitiesovertheoutput.Forwardpropagation beginswithaspeciÔ¨Åcationoftheinitialstateh( 0 ).Then,foreachtimestepfrom 3 7 9 CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS UV Wo( t ‚àí 1 )o( t ‚àí 1 ) hhooy y LL x xo( ) to( ) to( + 1 ) to( + 1 ) tL( t ‚àí 1 )L( t ‚àí 1 )L( ) tL( ) tL( + 1 ) tL( + 1 ) ty( t ‚àí 1 )y( t ‚àí 1 )y( ) ty( ) ty( +1 ) ty( +1 ) t h( t ‚àí 1 )h( t ‚àí 1 )h( ) th( ) th( + 1 ) th( + 1 )</div>
        </div>
    </div>

    <div class="question-card" id="q129">
        <div class="question-header">
            <span class="question-number">Question 129</span>
            <span class="question-type">multiple-choice</span>
        </div>

        <div class="question-text">Deep generative models, such as Boltzmann machines and Restricted Boltzmann Machines (RBMs), are widely applied to learning complex distributions and modeling structured or sequential data. Conditional variants of these models are often used to capture dependencies in temporal or structured outputs, such as in speech synthesis or motion generation.

In conditional Restricted Boltzmann Machines (RBMs) designed for sequence modeling, which architectural feature allows the model to adapt its output generation based on recent past inputs without modifying its synaptic weights?

1) Nonlinear activation functions for visible units   
2) Bias parameters that depend linearly on previous sequence values   
3) Weight matrices shared across all time steps   
4) Stochastic sampling of output units   
5) Explicit zero-padding of input vectors   
6) Increased number of hidden units in deeper layers   
7) Mean field fixed point updates for each layer</div>

        <div class="answer-section">
            <div class="answer-label">‚úì Correct Answer:</div>
            <div class="answer-text">The correct answer is 2) Bias parameters that depend linearly on previous sequence values.</div>
        </div>

        <div class="reference-section">
            <div class="reference-label">üìö Reference Text:</div>
            <button class="toggle-button" onclick="toggleReference(129)">
                Show/Hide Reference
            </button>
            <div id="ref129" class="reference-text hidden">d m o d e l with t ra c t a b l e l a y e r- wis e m e a n Ô¨Å e l d Ô¨Å x e d p o i n t u p d a t e s , i t b e s t Ô¨Å t s t h e d e Ô¨Å n i t i o n o f a d e e p B o l t z m a n n m a c h i n e . 6 8 4 CHAPTER20.DEEPGENERATIVEMODELS wellbecausetheylieinthereceptiveÔ¨Åeldoffewerhiddenunits.However,ifwedo implicitlyzero-padtheinput,thenthehiddenunitsattheboundaryaredrivenby fewerinputpixels,andmayfailtoactivatewhenneeded. 20.7BoltzmannMachinesforStructuredorSequential Outputs Inthestructuredoutputscenario,wewishtotrainamodelthatcanmapfrom someinput xtosomeoutput y,andthediÔ¨Äerententriesof yarerelatedtoeach otherandmustobeysomeconstraints.Forexample,inthespeechsynthesistask, yisawaveform,andtheentirewaveformmustsoundlikeacoherentutterance. Anaturalwaytorepresenttherelationshipsbetweentheentriesin yisto useaprobabilitydistribution p(y| x).Boltzmannmachines,extendedtomodel conditionaldistributions,cansupplythisprobabilisticmodel. ThesametoolofconditionalmodelingwithaBoltzmannmachinecanbeused notjustforstructuredoutputtasks,butalsoforsequencemodeling.Inthelatter case,ratherthanmappinganinput xtoanoutput y,themodelmustestimatea probabilitydistributionoverasequenceofvariables, p(x( 1 ), . . . ,x( ) œÑ).Conditional Boltzmannmachinescanrepresentfactorsoftheform p(x( ) t|x( 1 ), . . . ,x( 1 ) t‚àí)in ordertoaccomplishthistask. AnimportantsequencemodelingtaskforthevideogameandÔ¨Ålmindustry ismodelingsequencesofjointanglesofskeletonsusedtorender3-Dcharacters. Thesesequencesareoftencollectedusingmotioncapturesystemstorecordthe movementsofactors.Aprobabilisticmodelofacharacter‚Äôsmovementallows thegenerationofnew, previouslyunseen, but realisticanimations.Tosolve thissequencemodelingtask,Taylor2007 e t a l .()introducedaconditionalRBM modeling p( x( ) t| x( 1 ) t‚àí, . . . , x( ) t m‚àí)forsmall m.ThemodelisanRBMover p( x( ) t)whosebiasparametersarealinearfunctionofthepreceding mvaluesof x. WhenweconditionondiÔ¨Äerentvaluesof x( 1 ) t‚àíandearliervariables,wegetanew RBMoverx.TheweightsintheRBMoverxneverchange,butbyconditioningon diÔ¨Äerentpastvalues,wecanchangetheprobabilityofdiÔ¨Äerenthiddenunitsinthe RBMbeingactive.ByactivatinganddeactivatingdiÔ¨Äerentsubsetsofhiddenunits, wecanmakelargechangestotheprobabilitydistributioninducedonx. Other variantsofconditionalRBM(,)andothervariantsofsequence Mnih e t a l .2011 modelingusingconditionalRBMsarepossible(TaylorandHinton2009Sutskever ,; e t a l .,;2009Boulanger-Lewandowski2012 e t a l .,). Anothersequencemodelingtaskistomodelthedistributionoversequences 6 8 5</div>
        </div>
    </div>

    <div class="question-card" id="q130">
        <div class="question-header">
            <span class="question-number">Question 130</span>
            <span class="question-type">multiple-choice</span>
        </div>

        <div class="question-text">Linear algebra provides the foundational structures and operations for representing and manipulating data in mathematics, engineering, and machine learning. Understanding the properties of vectors, matrices, and their operations is critical for solving complex computational problems.

Which of the following statements accurately describes the rules governing the transpose of a product of two matrices, A and B?

1) (AB)·µó = AB·µó   
2) (AB)·µó = A·µóB   
3) (AB)·µó = B·µó + A·µó   
4) (AB)·µó = A·µó + B·µó   
5) (AB)·µó = B·µóA·µó   
6) (AB)·µó = BA   
7) (AB)·µó = AB</div>

        <div class="answer-section">
            <div class="answer-label">‚úì Correct Answer:</div>
            <div class="answer-text">The correct answer is 5) (AB)·µó = B·µóA·µó.</div>
        </div>

        <div class="reference-section">
            <div class="reference-label">üìö Reference Text:</div>
            <button class="toggle-button" onclick="toggleReference(130)">
                Show/Hide Reference
            </button>
            <div id="ref130" class="reference-text hidden">naturalnumberscalar. ‚Ä¢Vectors: Avectorisanarrayofnumbers.Thenumbersarearrangedin order.Wecanidentifyeachindividualnumberbyitsindexinthatordering. Typicallywegivevectorslowercasenameswritteninboldtypeface,such asx.TheelementsofthevectorareidentiÔ¨Åedbywritingitsnameinitalic typeface,withasubscript.TheÔ¨Årstelementofxis x 1,thesecondelement is x 2andsoon.Wealsoneedtosaywhatkindofnumbersarestoredin thevector.Ifeachelementisin R,andthevectorhas nelements,thenthe vectorliesinthesetformedbytakingtheCartesianproductof R ntimes, denotedas Rn.Whenweneedtoexplicitlyidentifytheelementsofavector, wewritethemasacolumnenclosedinsquarebrackets: x=Ô£Æ Ô£ØÔ£ØÔ£ØÔ£∞x 1 x 2 ... x nÔ£π Ô£∫Ô£∫Ô£∫Ô£ª. (2.1) Wecanthinkofvectorsasidentifyingpointsinspace,witheachelement givingthecoordinatealongadiÔ¨Äerentaxis. Sometimesweneedtoindexasetofelementsofavector.Inthiscase,we deÔ¨Åneasetcontainingtheindicesandwritethesetasasubscript.For example,toaccess x 1, x 3and x 6,wedeÔ¨Ånetheset S={1 ,3 ,6}andwrite x S.Weusethe‚àísigntoindexthecomplementofaset.Forexamplex ‚àí 1is thevectorcontainingallelementsofxexceptfor x 1,andx ‚àí Sisthevector containingalloftheelementsofexceptforx x 1, x 3and x 6. ‚Ä¢Matrices:Amatrixisa2-Darrayofnumbers,soeachelementisidentiÔ¨Åed bytwoindicesinsteadofjustone.Weusuallygivematricesupper-case variablenameswithboldtypeface,suchasA.Ifareal-valuedmatrixAhas aheightof mandawidthof n,thenwesaythatA‚àà Rm n √ó. Weusually identifytheelementsofamatrixusingitsnameinitalicbutnotboldfont, andtheindicesarelistedwithseparatingcommas.Forexample, A 1 1 ,isthe upperleftentryofAand A m , nisthebottomrightentry.Wecanidentifyall ofthenumberswithverticalcoordinate ibywritinga‚Äú‚Äùforthehorizontal : coordinate.Forexample,A i , :denotesthehorizontalcrosssectionofAwith verticalcoordinate i.Thisisknownasthe i-throwofA.Likewise,A : , iis 3 2 CHAPTER2.LINEARALGEBRA A =Ô£Æ Ô£∞A 1 1 , A 1 2 , A 2 1 , A 2 2 , A 3 1 , A 3 2 ,Ô£π Ô£ª ‚áí AÓÄ°=ÓÄ•A 1 1 , A 2 1 , A 3 1 , A 1 2 , A 2 2 , A 3 2 ,ÓÄ¶ Figure2.1:Thetransposeofthematrixcanbethoughtofasamirrorimageacrossthe maindiagonal. the-thof.Whenweneedtoexplicitlyidentifytheelementsof icolumnA amatrix,wewritethemasanarrayenclosedinsquarebrackets: ÓÄîA 1 1 , A 1 2 , A 2 1 , A 2 2 ,ÓÄï . (2.2) Sometimeswemayneedtoindexmatrix-valuedexpressionsthatarenotjust asingleletter.Inthiscase,weusesubscriptsaftertheexpression,butdo notconvertanythingtolowercase.Forexample, f(A) i , jgiveselement( i , j) ofthematrixcomputedbyapplyingthefunctionto. fA ‚Ä¢Tensors:Insomecaseswewillneedanarraywithmorethantwoaxes. Inthegeneralcase,anarrayofnumbersarrangedonaregulargridwitha variablenumberofaxesisknownasatensor.Wedenoteatensornamed‚ÄúA‚Äù withthistypeface: A.Weidentifytheelementof Aatcoordinates ( i , j , k) bywriting A i , j , k. Oneimportantoperationonmatricesisthetranspose. Thetransposeofa matrixisthemirrorimageofthematrixacrossadiagonalline,calledthemain diagonal,runningdownandtotheright,startingfromitsupperleftcorner.See Ô¨Ågureforagraphicaldepictionofthisoperation.Wedenotethetransposeofa 2.1 matrixasAAÓÄæ,anditisdeÔ¨Ånedsuchthat (AÓÄæ) i , j= A j , i . (2.3) Vectorscanbethoughtofasmatricesthatcontainonlyonecolumn.The transposeofavectoristhereforeamatrixwithonlyonerow.Sometimeswe 3 3 CHAPTER2.LINEARALGEBRA deÔ¨Åneavectorbywritingoutitselementsinthetextinlineasarowmatrix, thenusingthetransposeoperatortoturnitintoastandardcolumnvector,e.g., x= [ x 1 , x 2 , x 3]ÓÄæ. Ascalarcanbethoughtofasamatrixwithonlyasingleentry.Fromthis,we canseethatascalarisitsowntranspose: a a= ÓÄæ. Wecanaddmatricestoeachother,aslongastheyhavethesameshape,just byaddingtheircorrespondingelements: whereCAB = + C i , j= A i , j+ B i , j . Wecanalsoaddascalartoamatrixormultiplyamatrixbyascalar,just byperformingthatoperationoneachelementofamatrix:D= a¬∑B+ cwhere D i , j= a B¬∑ i , j+ c. Inthecontextofdeeplearning,wealsousesomelessconventionalnotation. Weallowtheadditionofmatrixandavector,yieldinganothermatrix:C=A+b, where C i , j= A i , j+ b j.Inotherwords,thevectorbisaddedtoeachrowofthe matrix.ThisshorthandeliminatestheneedtodeÔ¨Åneamatrixwithbcopiedinto eachrowbeforedoingtheaddition.Thisimplicitcopyingofbtomanylocations iscalled .broadcasting 2.2MultiplyingMatricesandVectors Oneofthemostimportantoperationsinvolvingmatricesismultiplication oftwo matrices.ThematrixproductofmatricesAandBisathirdmatrixC.In orderforthisproducttobedeÔ¨Åned,Amusthavethesamenumberofcolumnsas Bhasrows.IfAisofshape m n√óandBisofshape n p√ó,thenCisofshape m p√ó.Wecanwritethematrixproductjustbyplacingtwoormorematrices together,e.g. CAB= . (2.4) TheproductoperationisdeÔ¨Ånedby C i , j=ÓÅò kA i , k B k, j . (2.5) Notethatthestandardproductoftwomatricesisjustamatrixcontaining not theproductoftheindividualelements.Suchanoperationexistsandiscalledthe element-wiseproductHadamardproduct or ,andisdenotedas.ABÓÄå Thedotproductbetweentwovectorsxandyofthesamedimensionality isthematrixproductxÓÄæy.WecanthinkofthematrixproductC=ABas computing C i , jasthedotproductbetweenrowofandcolumnof. iA jB 3 4 CHAPTER2.LINEARALGEBRA Matrixproductoperationshavemanyusefulpropertiesthatmakemathematical analysis ofmatrices moreconvenient.For example, matrix m ultiplication is distributive: ABCABAC (+) = + . (2.6) Itisalsoassociative: ABCABC ( ) = ( ) . (2.7) Matrixmultiplication iscommutative(thecondition not AB=BAdoesnot alwayshold),unlikescalarmultiplication. However,thedotproductbetweentwo vectorsiscommutative: xÓÄæyy= ÓÄæx . (2.8) Thetransposeofamatrixproducthasasimpleform: ( )ABÓÄæ= BÓÄæAÓÄæ. (2.9) Thisallowsustodemonstrateequation,byexploitingthefactthatthevalue 2.8 ofsuchaproductisascalarandthereforeequaltoitsowntranspose: xÓÄæy=ÓÄê xÓÄæyÓÄëÓÄæ = yÓÄæx . (2.10) Sincethefocusofthistextbookisnotlinearalgebra,wedonotattemptto developacomprehensivelistofusefulpropertiesofthematrixproducthere,but thereadershouldbeawarethatmanymoreexist. Wenowknowenoughlinearalgebranotationtowritedownasystemoflinear equations: Axb= (2.11) whereA‚àà Rm n √óisaknownmatrix,b‚àà Rmisaknownvector,andx‚àà Rnisa vectorofunknownvariableswewouldliketosolvefor.Eachelement x iofxisone oftheseunknownvariables.EachrowofAandeachelementofbprovideanother constraint.Wecanrewriteequationas:2.11 A 1 : ,x= b 1 (2.12) A 2 : ,x= b 2 (2.13) . . . (2.14) A m , :x= b m (2.15) or,evenmoreexplicitly,as: A 1 1 , x 1+A 1 2 , x 2+ +¬∑¬∑¬∑A 1 , n x n= b 1 (2.16) 3 5 CHAPTER2.LINEARALGEBRA Ô£Æ Ô£∞100 010 001Ô£π Ô£ª Figure2.2:Exampleidentitymatrix:ThisisI 3. A 2 1 , x 1+A 2 2 , x 2+ +¬∑¬∑¬∑A 2 , n x n= b 2 (2.17) . . . (2.18) A m , 1 x 1+A m , 2 x 2+ +¬∑¬∑¬∑A</div>
        </div>
    </div>

    <div class="question-card" id="q131">
        <div class="question-header">
            <span class="question-number">Question 131</span>
            <span class="question-type">multiple-choice</span>
        </div>

        <div class="question-text">Convolutional neural networks (CNNs) are widely used in image and signal processing due to their unique architectural features. Their design principles contribute to computational efficiency and effective learning of spatial patterns.

Which property of convolutional layers is primarily responsible for making them equivariant to translation in input data?

1) Use of nonlinear activation functions   
2) Application of dropout regularization   
3) Increasing the number of channels in deeper layers   
4) Employing pooling operations   
5) Utilizing strided convolutions   
6) Sparse connectivity of neurons   
7) Parameter sharing across spatial locations </div>

        <div class="answer-section">
            <div class="answer-label">‚úì Correct Answer:</div>
            <div class="answer-text">The correct answer is 7) Parameter sharing across spatial locations.</div>
        </div>

        <div class="reference-section">
            <div class="reference-label">üìö Reference Text:</div>
            <button class="toggle-button" onclick="toggleReference(131)">
                Show/Hide Reference
            </button>
            <div id="ref131" class="reference-text hidden">2 s 2 s 1 s 1 s 3 s 3 x 4 x 4s 4 s 4 x 5 x 5s 5 s 5 x 1 x 1 x 2 x 2 x 3 x 3s 2 s 2 s 1 s 1 s 3 s 3 x 4 x 4s 4 s 4 x 5 x 5s 5 s 5 Figure9.2: S p a r s e c o n n e c t i v i t y , v i e w e d f r o m b e l o w :Wehighlightoneinputunit, x 3, andalsohighlighttheoutputunitsin sthatareaÔ¨Äectedbythisunit. ( T o p )When sis formedbyconvolutionwithakernelofwidth,onlythreeoutputsareaÔ¨Äectedby 3 x. ( Bottom )Whenisformedbymatrixmultiplication,connectivityisnolongersparse,so s alloftheoutputsareaÔ¨Äectedby x 3. 3 3 6 CHAPTER9.CONVOLUTIONALNETWORKS x 1 x 1 x 2 x 2 x 3 x 3s 2 s 2 s 1 s 1 s 3 s 3 x 4 x 4s 4 s 4 x 5 x 5s 5 s 5 x 1 x 1 x 2 x 2 x 3 x 3s 2 s 2 s 1 s 1 s 3 s 3 x 4 x 4s 4 s 4 x 5 x 5s 5 s 5 Figure9.3: S p a r s e c o n n e c t i v i t y , v i e w e d f r o m a b o v e : Wehighlightoneoutputunit, s 3, andalsohighlighttheinputunitsin xthataÔ¨Äectthisunit.Theseunitsareknown asthereceptiveÔ¨Åeldof s 3. ( T o p )When sisformedbyconvolutionwithakernelof width,onlythreeinputsaÔ¨Äect 3 s 3.When ( Bottom ) sisformedbymatrixmultiplication, connectivityisnolongersparse,soalloftheinputsaÔ¨Äect s 3. x 1 x 1 x 2 x 2 x 3 x 3h 2 h 2 h 1 h 1 h 3 h 3 x 4 x 4h 4 h 4 x 5 x 5h 5 h 5g 2 g 2 g 1 g 1 g 3 g 3 g 4 g 4 g 5 g 5 Figure9.4:ThereceptiveÔ¨Åeldoftheunitsinthedeeperlayersofaconvolutionalnetwork islargerthanthereceptiveÔ¨Åeldoftheunitsintheshallowlayers.ThiseÔ¨Äectincreasesif thenetworkincludesarchitecturalfeatureslikestridedconvolution(Ô¨Ågure)orpooling 9.12 (section).Thismeansthateventhough 9.3 d i r e c tconnectionsinaconvolutionalnetare verysparse,unitsinthedeeperlayerscanbe i n d i r e c t l yconnectedtoallormostofthe inputimage. 3 3 7 CHAPTER9.CONVOLUTIONALNETWORKS x 1 x 1 x 2 x 2 x 3 x 3s 2 s 2 s 1 s 1 s 3 s 3 x 4 x 4s 4 s 4 x 5 x 5s 5 s 5 x 1 x 1 x 2 x 2 x 3 x 3 x 4 x 4 x 5 x 5s 2 s 2 s 1 s 1 s 3 s 3 s 4 s 4 s 5 s 5 Figure9.5:Parametersharing:Blackarrowsindicatetheconnectionsthatuseaparticular parameterintwodiÔ¨Äerentmodels. ( T o p )Theblackarrowsindicateusesofthecentral elementofa3-elementkernelinaconvolutionalmodel.Duetoparametersharing,this singleparameterisusedatallinputlocations.Thesingleblackarrowindicates ( Bottom ) theuseofthecentralelementoftheweightmatrixinafullyconnectedmodel.Thismodel hasnoparametersharingsotheparameterisusedonlyonce. foreverylocation,welearnonlyoneset.ThisdoesnotaÔ¨Äecttheruntimeof forwardpropagation‚Äîit isstill O( k n √ó)‚Äîbutitdoesfurtherreducethestorage requirementsofthemodelto kparameters.Recallthat kisusuallyseveralorders ofmagnitudelessthan m.Since mand nareusuallyroughlythesamesize, kis practicallyinsigniÔ¨Åcantcomparedto m n √ó.Convolutionisthusdramatically more eÔ¨Écientthandensematrixmultiplication intermsofthememoryrequirements andstatisticaleÔ¨Éciency.Foragraphicaldepictionofhowparametersharingworks, seeÔ¨Ågure.9.5 AsanexampleofbothoftheseÔ¨Årsttwoprinciplesinaction,Ô¨Ågureshows9.6 howsparseconnectivityandparametersharingcandramatically improvethe eÔ¨Éciencyofalinearfunctionfordetectingedgesinanimage. Inthecaseofconvolution,theparticularformofparametersharingcausesthe layertohaveapropertycalled e q ui v ar i anc etotranslation.Tosayafunctionis equivariantmeansthatiftheinputchanges,theoutputchangesinthesameway. SpeciÔ¨Åcally,afunction f( x)isequivarianttoafunction gif f( g( x))= g( f( x)). Inthecaseofconvolution,ifwelet gbeanyfunctionthattranslatestheinput, i.e.,shiftsit,thentheconvolutionfunctionisequivariantto g.Forexample,let I beafunctiongivingimagebrightnessatintegercoordinates.Let gbeafunction 3 3 8 CHAPTER9.CONVOLUTIONALNETWORKS mappingoneimagefunctiontoanotherimagefunction,suchthat IÓÄ∞= g( I)is theimagefunctionwith IÓÄ∞( x , y)=</div>
        </div>
    </div>

    <div class="question-card" id="q132">
        <div class="question-header">
            <span class="question-number">Question 132</span>
            <span class="question-type">multiple-choice</span>
        </div>

        <div class="question-text">Modern deep learning architectures often address challenges in training sequential models by incorporating specialized techniques to stabilize gradient behavior and enhance memory capabilities. These innovations are particularly vital for tasks requiring the handling of long-range dependencies and explicit fact retrieval.

Which approach combines both norm clipping and a regularizer that maintains gradient magnitude across time steps to extend the dependency span that recurrent neural networks can learn, while still being generally less effective than LSTMs for tasks with abundant sequential data?

1) Using only element-wise gradient clipping without regularization   
2) Applying dropout to recurrent connections   
3) Employing content-based addressing in explicit memory architectures   
4) Implementing gated recurrent units (GRUs) without gradient clipping   
5) Utilizing attention mechanisms for dynamic input focus   
6) Applying norm clipping together with a gradient-magnitude regularizer   
7) Training with only implicit memory and no explicit memory component</div>

        <div class="answer-section">
            <div class="answer-label">‚úì Correct Answer:</div>
            <div class="answer-text">The correct answer is 6) Applying norm clipping together with a gradient-magnitude regularizer.</div>
        </div>

        <div class="reference-section">
            <div class="reference-label">üìö Reference Text:</div>
            <button class="toggle-button" onclick="toggleReference(132)">
                Show/Hide Reference
            </button>
            <div id="ref132" class="reference-text hidden">.,).Oneoptionistocliptheparametergradientfromaminibatch e l e m e nt - w i s e(Mikolov2012,)justbeforetheparameterupdate.Anotheristo c l i p t h e norm ||||g o f t h e g r a d i e ntg(Pascanu2013 e t a l .,)justbeforetheparameter update: if||||g > v (10.48) g‚Üêg v ||||g(10.49) 4 1 4 CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS where visthenormthresholdandgisusedtoupdateparameters.Becausethe gradientofalltheparameters(includingdiÔ¨Äerentgroupsofparameters,suchas weightsandbiases)isrenormalizedjointlywithasinglescalingfactor,thelatter methodhastheadvantagethatitguaranteesthateachstepisstillinthegradient direction,butexperimentssuggestthatbothformsworksimilarly.Although theparameterupdatehasthesamedirectionasthetruegradient,withgradient normclipping,theparameterupdatevectornormisnowbounded.Thisbounded gradientavoidsperformingadetrimentalstepwhenthegradientexplodes.In fact,evensimplytakinga r a ndom s t e pwhenthegradientmagnitudeisabove athresholdtendstoworkalmostaswell.Iftheexplosionissoseverethatthe gradientisnumerically InforNan(consideredinÔ¨Åniteornot-a-number),then arandomstepofsize vcanbetakenandwilltypicallymoveawayfromthe numericallyunstableconÔ¨Åguration. Clippingthegradientnormper-minibatchwill notchangethedirectionofthegradientforanindividualminibatch.However, takingtheaverageofthenorm-clippedgradientfrommanyminibatchesisnot equivalenttoclippingthenormofthetruegradient(thegradientformedfrom usingallexamples).Examplesthathavelargegradientnorm,aswellasexamples thatappearinthesameminibatchassuchexamples,willhavetheircontribution totheÔ¨Ånaldirectiondiminished.Thisstandsincontrasttotraditionalminibatch gradientdescent,wherethetruegradientdirectionisequaltotheaverageoverall minibatchgradients.Putanotherway,traditionalstochasticgradientdescentuses anunbiasedestimateofthegradient,whilegradientdescentwithnormclipping introducesaheuristicbiasthatweknowempiricallytobeuseful.Withelement- wiseclipping,thedirectionoftheupdateisnotalignedwiththetruegradient ortheminibatchgradient,butitisstilladescentdirection.Ithasalsobeen proposed(Graves2013,)tocliptheback-propagatedgradient(withrespectto hiddenunits)butnocomparisonhasbeenpublishedbetweenthesevariants;we conjecturethatallthesemethodsbehavesimilarly. 10.11.2RegularizingtoEncourageInformationFlow Gradientclippinghelpstodealwithexplodinggradients,butitdoesnothelpwith vanishinggradients.Toaddressvanishinggradientsandbettercapturelong-term dependencies,wediscussedtheideaofcreatingpathsinthecomputational graphof theunfoldedrecurrentarchitecturealongwhichtheproductofgradientsassociated witharcsisnear1.OneapproachtoachievethisiswithLSTMsandotherself- loopsandgatingmechanisms,describedaboveinsection.Anotherideais 10.10 toregularizeorconstraintheparameterssoastoencourage‚ÄúinformationÔ¨Çow.‚Äù Inparticular,wewouldlikethegradientvector‚àáh( ) t Lbeingback-propagatedto 4 1 5 CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS maintainitsmagnitude,evenifthelossfunctiononlypenalizestheoutputatthe endofthesequence.Formally,wewant (‚àáh( ) t L)‚àÇh( ) t ‚àÇh( 1 ) t ‚àí(10.50) tobeaslargeas ‚àáh( ) t L. (10.51) Withthisobjective,Pascanu2013 e t a l .()proposethefollowingregularizer: ‚Ñ¶ =ÓÅò tÔ£´ Ô£≠ÓÄåÓÄåÓÄå|‚àá(h( ) t L)‚àÇ h( ) t ‚àÇ h( 1 ) t ‚àíÓÄåÓÄåÓÄå| ||‚àáh( ) t L||‚àí1Ô£∂ Ô£∏2 . (10.52) ComputingthegradientofthisregularizermayappeardiÔ¨Écult,butPascanu e t a l .()proposeanapproximation inwhichweconsidertheback-propagated 2013 vectors‚àáh( ) t Lasiftheywereconstants(forthepurposeofthisregularizer,so thatthereisnoneedtoback-propagatethroughthem).Theexperimentswith thisregularizersuggestthat,ifcombinedwiththenormclippingheuristic(which handlesgradientexplosion),theregularizercanconsiderablyincreasethespanof thedependenciesthatanRNNcanlearn. BecauseitkeepstheRNNdynamics ontheedgeofexplosivegradients,thegradientclippingisparticularlyimportant. Withoutgradientclipping,gradientexplosionpreventslearningfromsucceeding. AkeyweaknessofthisapproachisthatitisnotaseÔ¨ÄectiveastheLSTMfor taskswheredataisabundant,suchaslanguagemodeling. 10.12ExplicitMemory Intelligencerequiresknowledgeandacquiringknowledgecanbedonevialearning, whichhasmotivatedthedevelopmentoflarge-scaledeeparchitectures.However, therearediÔ¨Äerentkindsofknowledge.Someknowledgecanbeimplicit,sub- conscious,anddiÔ¨Éculttoverbalize‚Äîsuchashowtowalk,orhowadoglooks diÔ¨Äerentfromacat.Otherknowledgecanbeexplicit,declarative,andrelatively straightforwardtoputintowords‚Äîeverydaycommonsense knowledge,like‚Äúacat isakindofanimal,‚ÄùorveryspeciÔ¨Åcfactsthatyouneedtoknowtoaccomplish yourcurrentgoals,like‚Äúthemeetingwiththesalesteamisat3:00PMinroom 141.‚Äù Neuralnetworksexcelatstoringimplicitknowledge.However,theystruggleto memorizefacts. Stochasticgradientdescentrequiresmanypresentationsofthe 4 1 6 CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS T ask ne t w or k , c ontrol l i ng th e m e m o r yMe m or y c e l l s W r i t i ng m e c hani s mR e adi ng m e c hani s m Figure10.18:Aschematicofanexampleofanetworkwithanexplicitmemory,capturing someofthekeydesignelementsoftheneuralTuringmachine.Inthisdiagramwe distinguishthe‚Äúrepresentation‚Äùpartofthemodel(the‚Äútasknetwork,‚Äùherearecurrent netinthebottom)fromthe‚Äúmemory‚Äùpartofthemodel(thesetofcells),whichcan storefacts.Thetasknetworklearnsto‚Äúcontrol‚Äùthememory,decidingwheretoreadfrom andwheretowritetowithinthememory(throughthereadingandwritingmechanisms, indicatedbyboldarrowspointingatthereadingandwritingaddresses). 4 1 7 CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS sameinputbeforeitcanbestoredinaneuralnetworkparameters,andeventhen, thatinputwillnotbestoredespeciallyprecisely.Graves2014b e t a l .()hypothesized thatthisisbecauseneuralnetworkslacktheequivalentoftheworkingmemory systemthatallowshumanbeingstoexplicitlyholdandmanipulatepiecesof informationthat arerelevantto achieving some goal.Suchexplicit memory componentswouldallowoursystemsnotonlytorapidlyand‚Äúintentionally‚Äùstore andretrievespeciÔ¨Åcfactsbutalsotosequentiallyreasonwiththem.Theneed forneuralnetworksthatcanprocessinformationinasequenceofsteps,changing thewaytheinputisfedintothenetworkateachstep,haslongbeenrecognized asimportantfortheabilitytoreasonratherthantomakeautomatic,intuitive responsestotheinput(,). Hinton1990 ToresolvethisdiÔ¨Éculty,Weston2014 e t a l .()introducedmemorynetworks thatincludeasetofmemorycellsthatcanbeaccessedviaanaddressingmecha- nism.Memorynetworksoriginallyrequiredasupervisionsignalinstructingthem howtousetheirmemorycells.Graves2014b e t a l .()introducedtheneural Turingmachine,whichisabletolearntoreadfromandwritearbitrarycontent tomemorycellswithoutexplicitsupervisionaboutwhichactionstoundertake, andallowedend-to-endtrainingwithoutthissupervisionsignal,viatheuseof acontent-basedsoftattentionmechanism(see ()andsec- Bahdanau e t a l .2015 tion).Thissoftaddressingmechanismhasbecomestandardwithother 12.4.5.1 relatedarchitecturesemulatingalgorithmicmechanismsinawaythatstillallows gradient-basedoptimization ( ,; Sukhbaatar e t a l .2015JoulinandMikolov2015,; Kumar 2015Vinyals2015aGrefenstette2015 e t a l .,; e t a l .,; e t a l .,). Eachmemorycellcanbethoughtofasanextensionofthememorycellsin LSTMsandGRUs.ThediÔ¨Äerenceisthatthenetworkoutputsaninternalstate thatchooseswhichcelltoreadfromorwriteto,justasmemoryaccessesina digitalcomputerreadfromorwritetoaspeciÔ¨Åcaddress. ItisdiÔ¨Éculttooptimizefunctionsthatproduceexact,integeraddresses.To alleviatethisproblem,NTMsactuallyreadtoorwritefrommanymemorycells simultaneously.Toread,theytakeaweightedaverageofmanycells.Towrite,they modifymultiplecellsbydiÔ¨Äerentamounts.ThecoeÔ¨Écientsfortheseoperations arechosentobefocusedonasmallnumberofcells,forexample,byproducing themviaasoftmaxfunction.Usingtheseweightswithnon-zeroderivativesallows thefunctionscontrollingaccesstothememorytobeoptimizedusinggradient descent.ThegradientonthesecoeÔ¨Écientsindicateswhethereachofthemshould beincreasedordecreased,butthegradientwilltypicallybelargeonlyforthose memoryaddressesreceivingalargecoeÔ¨Écient. Thesememorycellsaretypicallyaugmentedtocontainavector,ratherthan 4 1 8 CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS thesinglescalarstoredbyanLSTMorGRUmemorycell.Therearetworeasons toincreasethesizeofthememorycell.Onereasonisthatwehaveincreasedthe costofaccessingamemorycell. Wepaythecomputational costofproducinga coeÔ¨Écientformanycells,butweexpectthesecoeÔ¨Écientstoclusteraroundasmall numberofcells.Byreadingavectorvalue,ratherthanascalarvalue,wecan oÔ¨Äsetsomeofthiscost.Anotherreasontousevector-valuedmemorycellsisthat theyallowforcontent-basedaddressing,wheretheweightusedtoreadtoor writefromacellisafunctionofthatcell.Vector-valuedcellsallowustoretrievea completevector-valuedmemoryifweareabletoproduceapatternthatmatches somebutnotallofitselements.Thisisanalogoustothewaythatpeoplecan recallthelyricsofasongbasedonafewwords.Wecanthinkofacontent-based readinstructionassaying,‚ÄúRetrievethelyricsofthesongthathasthechorus‚ÄòWe allliveinayellowsubmarine.‚Äô‚ÄùContent-basedaddressingismoreusefulwhenwe maketheobjectstoberetrievedlarge‚Äîifeveryletterofthesongwasstoredina separatememorycell,wewouldnotbeabletoÔ¨Åndthemthisway.Bycomparison, location-basedaddressingisnotallowedtorefertothecontentofthememory. Wecanthinkofalocation-basedreadinstructionassaying‚ÄúRetrievethelyricsof thesonginslot347.‚ÄùLocation-basedaddressingcanoftenbeaperfectlysensible mechanismevenwhenthememorycellsaresmall. Ifthecontentofamemorycelliscopied(notforgotten)atmosttimesteps,then theinformationitcontainscanbepropagatedforwardintimeandthegradients propagatedbackwardintimewithouteithervanishingorexploding. TheexplicitmemoryapproachisillustratedinÔ¨Ågure,whereweseethat 10.18 a‚Äútaskneuralnetwork‚Äù iscoupledwithamemory.Althoughthattaskneural networkcouldbefeedforwardorrecurrent,theoverallsystemisarecurrentnetwork. ThetasknetworkcanchoosetoreadfromorwritetospeciÔ¨Åcmemoryaddresses. ExplicitmemoryseemstoallowmodelstolearntasksthatordinaryRNNsorLSTM RNNscannotlearn.Onereasonforthisadvantagemaybebecauseinformationand gradientscanbepropagated(forwardintimeorbackwardsintime,respectively) forverylongdurations. Asanalternativetoback-propagationthroughweightedaveragesofmemory cells,wecaninterpretthememoryaddressingcoeÔ¨Écientsasprobabilities and stochasticallyreadjustonecell(ZarembaandSutskever2015,).Optimizingmodels thatmakediscretedecisionsrequiresspecializedoptimization algorithms,described insection.Sofar,trainingthesestochasticarchitectures thatmakediscrete 20.9.1 decisionsremainsharderthantrainingdeterministicalgorithmsthatmakesoft decisions. Whetheritissoft(allowingback-propagation) orstochasticandhard,the 4 1 9 CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS mechanism forchoosing anaddress isin itsform identical totheattention mechanismwhichhadbeenpreviouslyintroducedinthecontextofmachine translation( ,)anddiscussedinsection. Theidea Bahdanau e t a l .2015 12.4.5.1 ofattentionmechanismsforneuralnetworkswasintroducedevenearlier,inthe contextofhandwritinggeneration(Graves2013,),withanattentionmechanism thatwasconstrainedtomoveonlyforwardintimethroughthesequence.In thecaseofmachinetranslationandmemorynetworks,ateachstep,thefocusof attentioncanmovetoacompletelydiÔ¨Äerentplace,comparedtothepreviousstep. Recurrentneuralnetworksprovideawaytoextenddeeplearningtosequential data.Theyarethelastmajortoolinourdeeplearningtoolbox.Ourdiscussionnow movestohowtochooseandusethesetoolsandhowtoapplythemtoreal-world tasks. 4 2 0 C h a p t e r 1 1 Practical Methodology Successfullyapplyingdeeplearningtechniquesrequiresmorethanjustagood knowledgeofwhatalgorithmsexistandtheprinciplesthatexplainhowthey work.Agoodmachinelearningpractitioneralsoneedstoknowhowtochoosean algorithmforaparticularapplicationandhowtomonitorandrespondtofeedback obtainedfromexperimentsinordertoimproveamachinelearningsystem.During daytodaydevelopmentofmachinelearningsystems,practitioners needtodecide whethertogathermoredata,increaseordecreasemodelcapacity,addorremove regularizingfeatures,improvetheoptimization ofamodel,improveapproximate inferenceinamodel,ordebugthesoftwareimplementationofthemodel.Allof theseoperationsareattheveryleasttime-consuming totryout,soitisimportant tobeabletodeterminetherightcourseofactionratherthanblindlyguessing. MostofthisbookisaboutdiÔ¨Äerentmachinelearningmodels,trainingalgo- rithms,andobjectivefunctions.Thismaygivetheimpressionthatthemost importantingredienttobeingamachinelearningexpertisknowingawidevariety ofmachinelearningtechniquesandbeinggoodatdiÔ¨Äerentkindsofmath.Inprac- tice,onecanusuallydomuchbetterwithacorrectapplicationofacommonplace algorithmthanbysloppilyapplyinganobscurealgorithm.Correctapplicationof analgorithmdependsonmasteringsomefairlysimplemethodology.Manyofthe recommendations inthischapterareadaptedfrom().Ng2015 Werecommendthefollowingpracticaldesignprocess: ‚Ä¢Determineyourgoals‚Äîwhaterrormetrictouse,andyourtargetvaluefor thiserrormetric.Thesegoalsanderrormetricsshouldbedrivenbythe problemthattheapplicationisintendedtosolve. ‚Ä¢Establishaworkingend-to-endpipelineassoonaspossible,includingthe 421 CHAPTER11.PRACTICALMETHODOLOGY estimationoftheappropriateperformancemetrics. ‚Ä¢Instrumentthesystemwelltodeterminebottlenecksinperformance.Diag- nosewhichcomponentsareperformingworsethanexpectedandwhetherit isduetooverÔ¨Åtting,underÔ¨Åtting, oradefectinthedataorsoftware. ‚Ä¢Repeatedlymakeincrementalchangessuchasgatheringnewdata,adjusting hyperparameters,orchangingalgorithms,basedonspeciÔ¨ÅcÔ¨Åndingsfrom yourinstrumentation. Asarunningexample,wewilluseStreetViewaddressnumbertranscription system( ,).Thepurposeofthisapplicationistoadd Goodfellow etal.2014d buildingstoGoogleMaps.StreetViewcarsphotographthebuildingsandrecord theGPScoordinatesassociatedwitheachphotograph. Aconvolutionalnetwork recognizestheaddressnumberineachphotograph, allowingtheGoogleMaps databasetoaddthataddressinthecorrectlocation.Thestoryofhowthis commercialapplicationwasdevelopedgivesanexampleofhowtofollowthedesign methodologyweadvocate. Wenowdescribeeachofthestepsinthisprocess. 11.1PerformanceMetrics Determiningyourgoals,intermsofwhicherrormetrictouse,isanecessaryÔ¨Årst stepbecauseyourerrormetricwillguideallofyourfutureactions. Youshould alsohaveanideaofwhatlevelofperformanceyoudesire. Keepinmindthatformostapplications,itisimpossibletoachieveabsolute</div>
        </div>
    </div>

    <div class="question-card" id="q133">
        <div class="question-header">
            <span class="question-number">Question 133</span>
            <span class="question-type">multiple-choice</span>
        </div>

        <div class="question-text">Energy-based generative models such as Spike-and-slab Restricted Boltzmann Machines (ssRBMs) are used for modeling the covariance structure of real-valued data, particularly in image and signal processing. These models employ specialized hidden units and training techniques to efficiently capture complex dependencies without incurring prohibitive computational costs.

Which of the following statements most accurately characterizes the primary advantage of ssRBMs over mcRBMs and mPoT models in handling overcomplete representations with sparse activations?

1) ssRBMs use only binary hidden units, making them more interpretable for feature selection tasks.   
2) ssRBMs require Hamiltonian Monte Carlo for efficient training, which is not feasible for high-dimensional data.   
3) ssRBMs allow sparse selection of directions where variance is increased by combining binary 'spike' and real-valued 'slab' hidden units.   
4) ssRBMs rely on matrix inversion to compute conditional covariances for each data point.   
5) ssRBMs restrict pooling regions to overlap significantly, enhancing their capacity in deep architectures.   
6) ssRBMs enforce positive-definite covariance matrices through constrained optimization, resulting in optimal flexibility.   
7) ssRBMs activate hidden units that impose global constraints on all directions in data space, limiting their ability to produce sparse representations.</div>

        <div class="answer-section">
            <div class="answer-label">‚úì Correct Answer:</div>
            <div class="answer-text">The correct answer is 3) ssRBMs allow sparse selection of directions where variance is increased by combining binary 'spike' and real-valued 'slab' hidden units..</div>
        </div>

        <div class="reference-section">
            <div class="reference-label">üìö Reference Text:</div>
            <button class="toggle-button" onclick="toggleReference(133)">
                Show/Hide Reference
            </button>
            <div id="ref133" class="reference-text hidden">xviaHamiltonian(hybrid)MonteCarlo. 6 8 0 CHAPTER20.DEEPGENERATIVEMODELS SpikeandSlabRestrictedBoltzmannMachinesSpikeandslabrestricted Boltzmannmachines( ,)orssRBMsprovideanothermeans Courville e t a l .2011 ofmodelingthecovariancestructureofreal-valueddata.ComparedtomcRBMs, ssRBMshavetheadvantageofrequiringneithermatrixinversionnorHamiltonian MonteCarlomethods.LikethemcRBMandthemPoTmodel,thessRBM‚Äôsbinary hiddenunitsencodetheconditionalcovarianceacrosspixelsthroughtheuseof auxiliaryreal-valuedvariables. ThespikeandslabRBMhastwosetsofhiddenunits:binaryspikeunits h, andreal-valuedslabunits s.Themeanofthevisibleunitsconditionedonthe hiddenunitsisgivenby( h sÓÄå) WÓÄæ.Inotherwords,eachcolumn W : , ideÔ¨Ånesa componentthatcanappearintheinputwhen h i= 1.Thecorrespondingspike variableh idetermineswhetherthatcomponentispresentatall.Thecorresponding slabvariables ideterminestheintensityofthatcomponent,ifitispresent.When aspikevariableisactive,thecorrespondingslabvariableaddsvariancetothe inputalongtheaxisdeÔ¨Ånedby W : , i.Thisallowsustomodelthecovarianceofthe inputs.Fortunately,contrastivedivergenceandpersistentcontrastivedivergence withGibbssamplingarestillapplicable.Thereisnoneedtoinvertanymatrix. Formally,thessRBMmodelisdeÔ¨Ånedviaitsenergyfunction: E s s( ) = x s h , , ‚àíÓÅò ixÓÄæW : , i s i h i+1 2xÓÄæÓÄ† Œõ+ÓÅò iŒ¶ i h iÓÄ° x (20.50) +1 2ÓÅò iŒ± i s2 i‚àíÓÅò iŒ± i ¬µ i s i h i‚àíÓÅò ib i h i+ÓÅò iŒ± i ¬µ2 i h i ,(20.51) where b iistheoÔ¨Äsetofthespike h iandŒõisadiagonalprecisionmatrixonthe observations x.Theparameter Œ± i >0isascalarprecisionparameterforthe real-valuedslabvariable s i.Theparameter Œ¶ iisanon-negativediagonalmatrix thatdeÔ¨Ånesan h-modulatedquadraticpenaltyon x.Each ¬µ iisameanparameter fortheslabvariable s i. WiththejointdistributiondeÔ¨Ånedviatheenergyfunction,itisrelatively straightforwardto derivethessRBM conditionaldistributions.For example, bymarginalizing outtheslabvariables s,theconditionaldistributionoverthe observationsgiventhebinaryspikevariablesisgivenby: h p s s( )= x h|1 P() h1 ZÓÅö exp ( ) {‚àí E x s h , ,} d s(20.52) =NÓÄ† x C;s s x h|ÓÅò iW : , i ¬µ i h i , Cs s x h|ÓÄ° (20.53) 6 8 1 CHAPTER20.DEEPGENERATIVEMODELS where Cs s x h|=ÓÄÄ Œõ+ÓÅê iŒ¶ i h i‚àíÓÅê iŒ±‚àí 1 i h i W : , i WÓÄæ : , iÓÄÅ‚àí 1.Thelastequalityholdsonlyif thecovariancematrix Cs s x h|ispositivedeÔ¨Ånite. Gatingbythespikevariablesmeansthatthetruemarginaldistributionover hsÓÄåissparse.ThisisdiÔ¨Äerentfromsparsecoding,wheresamplesfromthemodel ‚Äúalmostnever‚Äù(inthemeasuretheoreticsense)containzerosinthecode,andMAP inferenceisrequiredtoimposesparsity. ComparingthessRBMtothemcRBMandthemPoTmodels,thessRBM parametrizes theconditionalcovarianceoftheobservationinasigniÔ¨ÅcantlydiÔ¨Äerent way.ThemcRBMandmPoTbothmodelthecovariancestructureoftheobservation asÓÄêÓÅê j h( ) c j r( ) jr( ) jÓÄæ+ IÓÄë‚àí 1 ,usingtheactivationofthehiddenunits h j >0to enforceconstraintsontheconditionalcovarianceinthedirection r( ) j.Incontrast, thessRBMspeciÔ¨Åestheconditionalcovarianceoftheobservationsusingthehidden spikeactivations h i= 1topinchtheprecisionmatrixalongthedirectionspeciÔ¨Åed bythecorrespondingweightvector. ThessRBMconditionalcovarianceisvery similartothatgivenbyadiÔ¨Äerentmodel:theproductofprobabilisticprincipal componentsanalysis(PoPPCA)(WilliamsandAgakov2002,).Intheovercomplete setting,sparseactivationswiththessRBMparametrization permitsigniÔ¨Åcant variance(abovethenominalvariancegivenbyŒõ‚àí 1)onlyintheselecteddirections ofthesparselyactivated h i. InthemcRBMormPoTmodels,anovercomplete representationwouldmeanthattocapturevariationinaparticulardirectionin theobservationspacerequiresremovingpotentiallyallconstraintswithpositive projectioninthatdirection. This wouldsuggestthatthesemodelsarelesswell suitedtotheovercompletesetting. TheprimarydisadvantageofthespikeandslabrestrictedBoltzmannmachine isthatsomesettingsoftheparameterscancorrespondtoacovariancematrix thatisnotpositivedeÔ¨Ånite.Suchacovariancematrixplacesmoreunnormalized probabilityonvaluesthatarefartherfromthemean,causingtheintegralover allpossibleoutcomestodiverge.Generallythisissuecanbeavoidedwithsimple heuristictricks.Thereisnotyetanytheoreticallysatisfyingsolution.Using constrainedoptimization toexplicitlyavoidtheregionswheretheprobabilityis undeÔ¨ÅnedisdiÔ¨Éculttodowithoutbeingoverlyconservativeandalsopreventing themodelfromaccessinghigh-performingregionsofparameterspace. Qualitatively,convolutionalvariantsofthessRBMproduceexcellentsamples ofnaturalimages.SomeexamplesareshowninÔ¨Ågure.16.1 ThessRBMallowsforseveralextensions.Includinghigher-orderinteractions andaverage-poolingoftheslabvariables( ,)enablesthemodel Courville e t a l .2014 tolearnexcellentfeaturesforaclassiÔ¨Åerwhenlabeleddataisscarce. Addinga 6 8 2 CHAPTER20.DEEPGENERATIVEMODELS termtotheenergyfunctionthatpreventsthepartitionfunctionfrombecoming undeÔ¨Ånedresultsinasparsecodingmodel,spikeandslabsparsecoding(Goodfellow e t a l .,),alsoknownasS3C. 2013d 20.6ConvolutionalBoltzmannMachines Asseeninchapter,extremelyhighdimensionalinputssuchasimagesplace 9 greatstrainonthecomputation,memoryandstatisticalrequirementsofmachine learningmodels.Replacingmatrixmultiplication bydiscreteconvolutionwitha smallkernelisthestandardwayofsolvingtheseproblemsforinputsthathave translationinvariantspatialortemporalstructure. () DesjardinsandBengio2008 showedthatthisapproachworkswellwhenappliedtoRBMs. Deepconvolutionalnetworksusuallyrequireapoolingoperationsothatthe spatialsizeofeachsuccessivelayerdecreases.Feedforwardconvolutionalnetworks oftenuseapoolingfunctionsuchasthemaximumoftheelementstobepooled. Itisunclearhowtogeneralizethistothesettingofenergy-basedmodels.We couldintroduceabinarypoolingunitpover nbinarydetectorunits dandenforce p=max i d ibysettingtheenergyfunctiontobe‚àûwheneverthatconstraintis violated.Thisdoesnotscalewellthough,asitrequiresevaluating 2ndiÔ¨Äerent energyconÔ¨Ågurations tocomputethenormalization constant.Forasmall 3√ó3 poolingregionthisrequires 29= 512energyfunctionevaluationsperpoolingunit! Lee2009 e t a l .()developedasolutiontothisproblemcalledprobabilistic maxpooling(nottobeconfusedwith‚Äústochasticpooling,‚Äùwhichisatechnique forimplicitlyconstructingensemblesofconvolutionalfeedforwardnetworks).The strategybehindprobabilisticmaxpoolingistoconstrainthedetectorunitsso atmostonemaybeactiveatatime.Thismeansthereareonly n+ 1total states(onestateforeachofthe ndetectorunitsbeingon,andanadditionalstate correspondingtoallofthedetectorunitsbeingoÔ¨Ä).Thepoolingunitisonif andonlyifoneofthedetectorunitsison.ThestatewithallunitsoÔ¨Äisassigned energyzero.Wecanthinkofthisasdescribingamodelwithasinglevariablethat has n+1states,orequivalentlyasamodelthathas n+1variablesthatassigns energytoallbutjointassignmentsofvariables. ‚àû n+1 WhileeÔ¨Écient,probabilisticmaxpoolingdoesforcethedetectorunitstobe mutuallyexclusive,whichmaybeausefulregularizingconstraintinsomecontexts oraharmfullimitonmodelcapacityinothercontexts.Italsodoesnotsupport overlappingpoolingregions.Overlapping poolingregionsareusuallyrequired toobtainthebestperformancefromfeedforwardconvolutionalnetworks,sothis constraintprobablygreatlyreducestheperformanceofconvolutionalBoltzmann 6 8 3 CHAPTER20.DEEPGENERATIVEMODELS machines. Lee2009 e t a l .()demonstratedthatprobabilisticmaxpoolingcouldbeused tobuildconvolutionaldeepBoltzmannmachines.3Thismodelisabletoperform operationssuchasÔ¨Ållinginmissingportionsofitsinput.Whileintellectually appealing,thismodelischallengingtomakeworkinpractice,andusuallydoes notperformaswellasaclassiÔ¨Åerastraditionalconvolutionalnetworkstrained withsupervisedlearning. ManyconvolutionalmodelsworkequallywellwithinputsofmanydiÔ¨Äerent spatialsizes.ForBoltzmannmachines,itisdiÔ¨Éculttochangetheinputsize foravarietyofreasons. Thepartitionfunctionchangesasthesizeoftheinput changes.Moreover,manyconvolutionalnetworksachievesizeinvariancebyscaling upthesizeoftheirpoolingregionsproportionaltothesizeoftheinput,butscaling Boltzmannmachinepoolingregionsisawkward.Traditionalconvolutionalneural networkscanuseaÔ¨Åxednumberofpoolingunitsanddynamicallyincreasethe sizeoftheirpoolingregionsinordertoobtainaÔ¨Åxed-sizerepresentationofa variable-sizedinput.ForBoltzmannmachines,largepoolingregionsbecometoo expensiveforthenaiveapproach. The approachof()ofmaking Lee e t a l .2009 eachofthedetectorunitsinthesamepoolingregionmutuallyexclusivesolves thecomputational problems,butstilldoesnotallowvariable-sizepoolingregions. Forexample,supposewelearnamodelwith 2√ó2probabilisticmaxpoolingover detectorunitsthatlearnedgedetectors. Thisenforcestheconstraintthatonly oneoftheseedgesmayappearineach2√ó2region.Ifwethenincreasethesizeof theinputimageby50%ineachdirection,wewouldexpectthenumberofedgesto increasecorrespondingly.Instead,ifweincreasethesizeofthepoolingregionsby 50%ineachdirectionto3√ó3,thenthemutualexclusivityconstraintnowspeciÔ¨Åes thateachoftheseedgesmayonlyappearonceina3√ó3region.Aswegrow amodel‚Äôsinputimageinthisway,themodelgeneratesedgeswithlessdensity. Ofcourse,theseissuesonlyarisewhenthemodelmustusevariableamountsof poolinginordertoemitaÔ¨Åxed-sizeoutputvector.Modelsthatuseprobabilistic maxpoolingmaystillacceptvariable-sizedinputimagessolongastheoutputof themodelisafeaturemapthatcanscaleinsizeproportionaltotheinputimage. PixelsattheboundaryoftheimagealsoposesomediÔ¨Éculty,whichisexac- erbatedbythefactthatconnectionsinaBoltzmannmachinearesymmetric.If wedonotimplicitlyzero-padtheinput,thentherearefewerhiddenunitsthan visibleunits,andthevisibleunitsattheboundaryoftheimagearenotmodeled 3Th e p u b l i c a t i o n d e s c rib e s t h e m o d e l a s a ‚Äú d e e p b e l i e f n e t w o rk ‚Äù b u t b e c a u s e i t c a n b e d e s c rib e d a s a p u re l y u n d i re c t e</div>
        </div>
    </div>

    <div class="question-card" id="q134">
        <div class="question-header">
            <span class="question-number">Question 134</span>
            <span class="question-type">multiple-choice</span>
        </div>

        <div class="question-text">Back-propagation is a foundational algorithm used for training artificial neural networks efficiently by leveraging computational graphs and dynamic programming. It underpins the automation of gradient computations in modern deep learning frameworks.

Which statement best characterizes the main reason back-propagation is significantly more efficient than naive gradient computation algorithms for deep neural networks?

1) It exclusively uses feedforward architectures, reducing computational overhead.   
2) It approximates gradients using finite differences rather than exact derivatives.   
3) It prunes redundant nodes from the computational graph before training begins.   
4) It stores intermediate gradient values for each node, eliminating repeated subexpression evaluations via dynamic programming.   
5) It only computes gradients for output layer parameters, skipping hidden layers.   
6) It replaces matrix multiplications with element-wise operations to speed up calculations.   
7) It relies on stochastic processes to estimate gradients rather than analytical solutions.</div>

        <div class="answer-section">
            <div class="answer-label">‚úì Correct Answer:</div>
            <div class="answer-text">The correct answer is 4) It stores intermediate gradient values for each node, eliminating repeated subexpression evaluations via dynamic programming..</div>
        </div>

        <div class="reference-section">
            <div class="reference-label">üìö Reference Text:</div>
            <button class="toggle-button" onclick="toggleReference(134)">
                Show/Hide Reference
            </button>
            <div id="ref134" class="reference-text hidden">ofnodesin. T Initialize ,adatastructureassociatingtensorstotheirgradients grad_table g r a d t a b l e_ [] 1 z‚Üê fordo Vin T b u i l d g r a d_ ( V , ,GGÓÄ∞, g r a d t a b l e_ ) endfor Return restrictedto grad_table T Insection,weexplainedthatback-propagation wasdevelopedinorderto 6.5.2 avoidcomputingthesamesubexpressioninthechainrulemultipletimes.Thenaive algorithmcouldhaveexponentialruntimeduetotheserepeatedsubexpressions. NowthatwehavespeciÔ¨Åedtheback-propagationalgorithm,wecanunderstandits computational cost.Ifweassumethateachoperationevaluationhasroughlythe samecost,thenwemayanalyzethecomputational costintermsofthenumber ofoperationsexecuted.Keepinmindherethatwerefertoanoperationasthe fundamentalunitofourcomputational graph,whichmightactuallyconsistofvery manyarithmeticoperations(forexample,wemighthaveagraphthattreatsmatrix multiplicationasasingleoperation).Computingagradientinagraphwith nnodes willneverexecutemorethan O( n2)operationsorstoretheoutputofmorethan O( n2) operations.Herewearecountingoperationsinthecomputational graph,not individualoperationsexecutedbytheunderlyinghardware,soitisimportantto rememberthattheruntimeofeachoperationmaybehighlyvariable.Forexample, multiplyingtwomatricesthateachcontainmillionsofentriesmightcorrespondto asingleoperationinthegraph.Wecanseethatcomputingthegradientrequiresas most O( n2) operationsbecausetheforwardpropagationstagewillatworstexecute all nnodesintheoriginalgraph(dependingonwhichvalueswewanttocompute, wemaynotneedtoexecutetheentiregraph).Theback-propagationalgorithm addsoneJacobian-vectorproduct,whichshouldbeexpressedwith O(1)nodes,per edgeintheoriginalgraph.Becausethecomputational graphisadirectedacyclic graphithasatmost O( n2)edges.Forthekindsofgraphsthatarecommonlyused inpractice,thesituationisevenbetter.Mostneuralnetworkcostfunctionsare 2 1 7 CHAPTER6.DEEPFEEDFORWARDNETWORKS Algorithm6.6Theinnerloopsubroutine b u i l d g r a d_ ( V , ,GGÓÄ∞, g r a d t a b l e_ )of theback-propagationalgorithm,calledbytheback-propagationalgorithmdeÔ¨Åned inalgorithm .6.5 Require: V,thevariablewhosegradientshouldbeaddedtoand . Ggrad_table Require:G,thegraphtomodify. Require:GÓÄ∞,therestrictionoftonodesthatparticipateinthegradient. G Require:grad_table,adatastructuremappingnodestotheirgradients if then Visingrad_table Return_ g r a d t a b l e[] V endif i‚Üê1 for C V in_ g e t c o n s u m e r s( ,GÓÄ∞)do o p g e t o p e r a t i o n ‚Üê_ () C D C ‚Üê b u i l d g r a d_ ( , ,GGÓÄ∞, g r a d t a b l e_ ) G( ) i‚Üê G o p b p r o p g e t i n p u t s . (_ ( C ,ÓÄ∞) ) , , V D i i‚Üê+1 endfor G‚ÜêÓÅê i G( ) i g r a d t a b l e_ [] = V G Insertandtheoperationscreatingitinto G G Return G roughlychain-structured,causingback-propagationtohave O( n)cost.Thisisfar betterthanthenaiveapproach,whichmightneedtoexecuteexponentiallymany nodes.Thispotentiallyexponentialcostcanbeseenbyexpandingandrewriting therecursivechainrule(equation)non-recursively: 6.49 ‚àÇ u( ) n ‚àÇ u( ) j=ÓÅò pa t h ( u( œÄ1), u( œÄ2), . . . , u( œÄ t)) , f r o m œÄ1 = t o j œÄ t = ntÓÅô k = 2‚àÇ u( œÄ k ) ‚àÇ u( œÄ k ‚àí1 ). (6.55) Sincethenumberofpathsfromnode jtonode ncangrowexponentiallyinthe lengthofthesepaths,thenumberoftermsintheabovesum,whichisthenumber ofsuchpaths,cangrowexponentiallywiththedepthoftheforwardpropagation graph.Thislargecostwouldbeincurredbecausethesamecomputationfor ‚àÇ u() i ‚àÇ u() jwouldberedonemanytimes. Toavoidsuchrecomputation, wecanthink ofback-propagation asatable-Ô¨Ållingalgorithmthattakesadvantageofstoring intermediateresults‚àÇ u() n ‚àÇ u() i.Eachnodeinthegraphhasacorrespondingslotina tabletostorethegradientforthatnode.ByÔ¨Ållinginthesetableentriesinorder, 2 1 8 CHAPTER6.DEEPFEEDFORWARDNETWORKS back-propagationavoidsrepeatingmanycommonsubexpressions.Thistable-Ô¨Ålling strategyissometimescalled . dynamicprogramming 6.5.7Example:Back-PropagationforMLPTraining Asanexample,wewalkthroughtheback-propagation algorithmasitisusedto trainamultilayerperceptron. Herewedevelopaverysimplemultilayerperceptionwithasinglehidden layer.Totrainthismodel,wewilluseminibatchstochasticgradientdescent. Theback-propagationalgorithmisusedtocomputethegradientofthecostona singleminibatch.SpeciÔ¨Åcally,weuseaminibatchofexamplesfromthetraining setformattedasadesignmatrixXandavectorofassociatedclasslabelsy. ThenetworkcomputesalayerofhiddenfeaturesH=max{0 ,XW( 1 )}.To simplifythepresentationwedonotusebiasesinthismodel.Weassumethatour graphlanguageincludesareluoperationthatcancompute max{0 ,Z}element- wise.Thepredictionsoftheunnormalized logprobabilities overclassesarethen givenbyHW( 2 ).Weassumethatourgraphlanguageincludesacross_entropy operationthatcomputesthecross-entropybetweenthetargetsyandtheprobability distributiondeÔ¨Ånedbytheseunnormalized logprobabilities. Theresultingcross- entropydeÔ¨Ånesthecost J M LE.Minimizingthiscross-entropyperformsmaximum likelihoodestimationoftheclassiÔ¨Åer.However,tomakethisexamplemorerealistic, wealsoincludearegularizationterm.Thetotalcost J J= M LE+ ŒªÔ£´ Ô£≠ÓÅò i , jÓÄê W( 1 ) i , jÓÄë2 +ÓÅò i , jÓÄê W( 2 ) i , jÓÄë2Ô£∂ Ô£∏ (6.56) consistsofthecross-entropyandaweightdecaytermwithcoeÔ¨Écient Œª.The computational graphisillustratedinÔ¨Ågure.6.11 Thecomputational graphforthegradientofthisexampleislargeenoughthat itwouldbetedioustodrawortoread.ThisdemonstratesoneofthebeneÔ¨Åts oftheback-propagation algorithm,whichisthatitcanautomatically generate gradientsthatwouldbestraightforwardbuttediousforasoftwareengineerto derivemanually. Wecanroughlytraceoutthebehavioroftheback-propagation algorithm bylookingattheforwardpropagationgraphinÔ¨Ågure.Totrain,wewish 6.11 tocomputeboth‚àáW(1) Jand ‚àáW(2) J.TherearetwodiÔ¨Äerentpathsleading backwardfrom Jtotheweights:onethroughthecross-entropycost,andone throughtheweightdecaycost.Theweightdecaycostisrelativelysimple;itwill alwayscontribute 2 ŒªW( ) itothegradientonW( ) i. 2 1 9 CHAPTER6.DEEPFEEDFORWARDNETWORKS XXW( 1 )W( 1 )U( 1 )U( 1 ) m a t m u lHH r e l u U( 3 )U( 3 ) s q ru( 4 )u( 4 ) s u mŒª Œª u( 7 )u( 7 )W( 2 )W( 2 )U( 2 )U( 2 ) m a t m u ly yJ M L</div>
        </div>
    </div>

    <div class="question-card" id="q135">
        <div class="question-header">
            <span class="question-number">Question 135</span>
            <span class="question-type">multiple-choice</span>
        </div>

        <div class="question-text">In machine learning, autoencoders are neural networks used for unsupervised representation learning, with variants such as contractive autoencoders (CAEs) and predictive sparse decomposition (PSD) offering additional regularization and feature extraction benefits. These architectures are particularly relevant for tasks involving dimensionality reduction and efficient data retrieval.

Which approach ensures that a contractive autoencoder learns meaningful, non-trivial representations rather than collapsing the contraction penalty through encoder-decoder collusion?

1) Using dropout regularization in the encoder   
2) Training with a batch normalization layer after each hidden unit   
3) Applying L1 sparsity constraints on the decoder outputs   
4) Increasing the reconstruction error weight in the loss function   
5) Injecting Gaussian noise into the input data during training   
6) Tying the encoder and decoder weights, such as making the decoder's weights the transpose of the encoder's   
7) Utilizing multi-head attention mechanisms in the encoder</div>

        <div class="answer-section">
            <div class="answer-label">‚úì Correct Answer:</div>
            <div class="answer-text">The correct answer is 6) Tying the encoder and decoder weights, such as making the decoder's weights the transpose of the encoder's.</div>
        </div>

        <div class="reference-section">
            <div class="reference-label">üìö Reference Text:</div>
            <button class="toggle-button" onclick="toggleReference(135)">
                Show/Hide Reference
            </button>
            <div id="ref135" class="reference-text hidden">t i v earisesfromthewaythattheCAEwarpsspace.SpeciÔ¨Å- cally,becausetheCAEistrainedtoresistperturbationsofitsinput,itisencouraged tomapaneighborhoodofinputpointstoasmallerneighborhoodofoutputpoints. Wecanthinkofthisascontractingtheinputneighborhoodtoasmalleroutput neighborhood. Toclarify,theCAEiscontractiveonlylocally‚Äîallperturbationsofatraining point xaremappednearto f( x).Globally,twodiÔ¨Äerentpoints xand xÓÄ∞maybe mappedto f( x)and f( xÓÄ∞)pointsthatarefartherapartthantheoriginalpoints. Itisplausiblethat fbeexpandingin-betweenorfarfromthedatamanifolds(see forexamplewhathappensinthe1-DtoyexampleofÔ¨Ågure).Whenthe 14.7 ‚Ñ¶( h) penaltyisappliedtosigmoidalunits,oneeasywaytoshrinktheJacobianisto makethesigmoidunitssaturatetoor.ThisencouragestheCAEtoencode 01 inputpointswithextremevaluesofthesigmoidthatmaybeinterpretedasa binarycode.ItalsoensuresthattheCAEwillspreaditscodevaluesthroughout mostofthehypercubethatitssigmoidalhiddenunitscanspan. WecanthinkoftheJacobianmatrix Jatapoint xasapproximating the nonlinearencoder f( x)asbeingalinearoperator.Thisallowsustousetheword ‚Äúcontractive‚Äùmoreformally. Inthetheoryoflinearoperators,alinearoperator 5 2 1 CHAPTER14.AUTOENCODERS issaidtobecontractiveifthenormof J xremainslessthanorequaltofor1 allunit-norm x.Inotherwords, Jiscontractiveifitshrinkstheunitsphere. WecanthinkoftheCAEaspenalizingtheFrobeniusnormofthelocallinear approximationof f( x)ateverytrainingpoint xinordertoencourageeachof theselocallinearoperatortobecomeacontraction. Asdescribed insection, regularized autoencoderslearnmanifoldsby 14.6 balancingtwoopposingforces.InthecaseoftheCAE,thesetwoforcesare reconstructionerrorandthecontractivepenalty‚Ñ¶( h).Reconstructionerroralone wouldencouragetheCAEtolearnanidentityfunction.Thecontractivepenalty alonewouldencouragetheCAEtolearnfeaturesthatareconstantwithrespectto x. Thecompromisebetweenthesetwoforcesyieldsanautoencoderwhosederivatives ‚àÇ f() x ‚àÇ xaremostlytiny.Onlyasmallnumberofhiddenunits,correspondingtoa smallnumberofdirectionsintheinput,mayhavesigniÔ¨Åcantderivatives. ThegoaloftheCAEistolearnthemanifoldstructureofthedata.Directions xwithlarge J xrapidlychange h,sothesearelikelytobedirectionswhich approximatethetangentplanesofthemanifold.Experimentsby () Rifai e t a l .2011a and ()showthattrainingtheCAEresultsinmostsingularvalues Rifai e t a l .2011b of Jdroppingbelowinmagnitudeandthereforebecomingcontractive.However, 1 somesingularvaluesremainabove,becausethereconstructionerrorpenalty 1 encouragestheCAEtoencodethedirectionswiththemostlocalvariance.The directionscorrespondingtothelargestsingularvaluesareinterpretedasthetangent directionsthatthecontractiveautoencoderhaslearned.Ideally,thesetangent directionsshouldcorrespondtorealvariationsinthedata.Forexample,aCAE appliedtoimagesshouldlearntangentvectorsthatshowhowtheimagechangesas objectsintheimagegraduallychangepose,asshowninÔ¨Ågure.Visualizations 14.6 oftheexperimentallyobtainedsingularvectorsdoseemtocorrespondtomeaningful transformationsoftheinputimage,asshowninÔ¨Ågure.14.10 OnepracticalissuewiththeCAEregularizationcriterionisthatalthoughit ischeaptocomputeinthecaseofasinglehiddenlayerautoencoder,itbecomes muchmoreexpensiveinthecaseofdeeperautoencoders.Thestrategyfollowedby Rifai2011a e t a l .()istoseparatelytrainaseriesofsingle-layerautoencoders,each trainedtoreconstructthepreviousautoencoder‚Äôshiddenlayer.Thecomposition oftheseautoencodersthenformsadeepautoencoder.Becauseeachlayerwas separatelytrainedtobelocallycontractive,thedeepautoencoderiscontractive aswell.Theresultisnotthesameaswhatwouldbeobtainedbyjointlytraining theentirearchitecturewithapenaltyontheJacobianofthedeepmodel,butit capturesmanyofthedesirablequalitativecharacteristics. Anotherpracticalissueisthatthecontractionpenaltycanobtainuselessresults 5 2 2 CHAPTER14.AUTOENCODERS Input pointTangentvectors LocalPCA(nosharingacrossregions) Contractiveautoencoder Figure14.10:IllustrationoftangentvectorsofthemanifoldestimatedbylocalPCA andbyacontractiveautoencoder.ThelocationonthemanifoldisdeÔ¨Ånedbytheinput imageofadogdrawnfromtheCIFAR-10dataset. Thetangentvectorsareestimated bytheleadingsingularvectorsoftheJacobianmatrix‚àÇ h ‚àÇ xoftheinput-to-codemapping. AlthoughbothlocalPCAandtheCAEcancapturelocaltangents,theCAEisableto formmoreaccurateestimatesfromlimitedtrainingdatabecauseitexploitsparameter sharingacrossdiÔ¨Äerentlocationsthatshareasubsetofactivehiddenunits. TheCAE tangentdirectionstypicallycorrespondtomovingorchangingpartsoftheobject(suchas theheadorlegs).Imagesreproducedwithpermissionfrom (). Rifai e t a l .2011c ifwedonotimposesomesortofscaleonthedecoder.Forexample,theencoder couldconsistofmultiplyingtheinputbyasmallconstant ÓÄèandthedecoder couldconsistofdividingthecodeby ÓÄè.As ÓÄèapproaches,theencoderdrivesthe 0 contractivepenalty‚Ñ¶( h)toapproachwithouthavinglearnedanythingaboutthe 0 distribution.Meanwhile,thedecodermaintainsperfectreconstruction.InRifai e t a l .(),thisispreventedbytyingtheweightsof 2011a fand g.Both fand gare standardneuralnetworklayersconsistingofanaÔ¨Énetransformationfollowedby anelement-wisenonlinearity,soitisstraightforwardtosettheweightmatrixof g tobethetransposeoftheweightmatrixof. f 14.8PredictiveSparseDecomposition P r e di c t i v e spar se dec o m p o si t i o n(PSD)isamodelthatisahybridofsparse codingandparametricautoencoders(Kavukcuoglu2008 e t a l .,).Aparametric encoderistrainedtopredicttheoutputofiterativeinference.PSDhasbeen appliedtounsupervisedfeaturelearningforobjectrecognitioninimagesandvideo (Kavukcuoglu20092010Jarrett2009Farabet2011 e t a l .,,; e t a l .,; e t a l .,),aswell asforaudio( ,).Themodelconsistsofanencoder HenaÔ¨Ä e t a l .2011 f( x)anda decoder g( h)thatarebothparametric.Duringtraining, hiscontrolledbythe 5 2 3 CHAPTER14.AUTOENCODERS optimization algorithm.Trainingproceedsbyminimizing ||‚àí || x g() h2+ Œª|| h1+ () Œ≥ f ||‚àí h x||2. (14.19) Likeinsparsecoding,thetrainingalgorithmalternatesbetweenminimization with respectto handminimization withrespecttothemodelparameters.Minimization withrespectto hisfastbecause f( x)providesagoodinitialvalueof handthe costfunctionconstrains htoremainnear f( x)anyway.Simplegradientdescent canobtainreasonablevaluesofinasfewastensteps. h ThetrainingprocedureusedbyPSDisdiÔ¨ÄerentfromÔ¨Årsttrainingasparse codingmodelandthentraining f( x)topredictthevaluesofthesparsecoding features.ThePSDtrainingprocedureregularizesthedecodertouseparameters forwhichcaninfergoodcodevalues. f() x Predictivesparsecodingisanexampleof l e ar ned appr o x i m a t e i nf e r e nc e. Insection,thistopicisdevelopedfurther.Thetoolspresentedinchapter 19.5 19 makeitclearthatPSDcanbeinterpretedastrainingadirectedsparsecoding probabilisticmodelbymaximizingalowerboundonthelog-likelihoodofthe model. InpracticalapplicationsofPSD,theiterativeoptimization isonlyusedduring training.Theparametricencoder fisusedtocomputethelearnedfeatureswhen themodelisdeployed.Evaluating fiscomputationally inexpensivecomparedto inferring hviagradientdescent.Because fisadiÔ¨Äerentiableparametricfunction, PSDmodelsmaybestackedandusedtoinitializeadeepnetworktobetrained withanothercriterion. 14.9ApplicationsofAutoencoders Autoencodershavebeensuccessfullyappliedtodimensionalityreductionandinfor- mationretrievaltasks.DimensionalityreductionwasoneoftheÔ¨Årstapplications ofrepresentationlearninganddeeplearning.Itwasoneoftheearlymotivations forstudyingautoencoders.Forexample,HintonandSalakhutdinov2006()trained astackofRBMsandthenusedtheirweightstoinitializeadeepautoencoder withgraduallysmallerhiddenlayers,culminatinginabottleneckof30units.The resultingcodeyieldedlessreconstructionerrorthanPCAinto30dimensionsand thelearnedrepresentationwasqualitativelyeasiertointerpretandrelatetothe underlyingcategories,withthesecategoriesmanifestingaswell-separatedclusters. Lower-dimensionalrepresentationscanimproveperformanceonmanytasks, suchasclassiÔ¨Åcation.Modelsofsmallerspacesconsumelessmemoryandruntime. 5 2 4 CHAPTER14.AUTOENCODERS Manyformsofdimensionalityreductionplacesemanticallyrelatedexamplesnear eachother,asobservedbySalakhutdinovandHinton2007bTorralba ()and e t a l . ().Thehintsprovidedbythemappingtothelower-dimensionalspaceaid 2008 generalization. OnetaskthatbeneÔ¨Åtsevenmorethanusualfromdimensionalityreductionis i nf o r m at i o n r e t r i e v al,thetaskofÔ¨Åndingentriesinadatabasethatresemblea queryentry. ThistaskderivestheusualbeneÔ¨Åtsfromdimensionalityreduction thatothertasksdo,butalsoderivestheadditionalbeneÔ¨Åtthatsearchcanbecome extremelyeÔ¨Écientincertainkindsoflowdimensionalspaces.SpeciÔ¨Åcally, if wetrainthedimensionalityreductionalgorithmtoproduceacodethatislow- dimensionaland,thenwecanstorealldatabaseentriesinahashtable b i nary mappingbinarycodevectorstoentries.Thishashtableallowsustoperform informationretrievalbyreturningalldatabaseentriesthathavethesamebinary codeasthe query.Wecanalso search overslightlylesssimilar entries very eÔ¨Éciently,justbyÔ¨Çippingindividualbitsfromtheencodingofthequery. This approachtoinformationretrievalviadimensionalityreductionandbinarization iscalled se m an t i c hashing(SalakhutdinovandHinton2007b2009b,,),andhas beenappliedtobothtextualinput(SalakhutdinovandHinton2007b2009b,,)and images(Torralba 2008Weiss2008KrizhevskyandHinton2011 e t a l .,; e t a l .,; ,). Toproducebinarycodesforsemantichashing,onetypicallyusesanencoding functionwithsigmoidsontheÔ¨Ånallayer.Thesigmoidunitsmustbetrainedtobe saturatedtonearly0ornearly1forallinputvalues.Onetrickthatcanaccomplish thisissimplytoinjectadditivenoisejustbeforethesigmoidnonlinearityduring training.Themagnitudeofthenoiseshouldincreaseovertime.ToÔ¨Åghtthat noiseandpreserveasmuchinformationaspossible,thenetworkmustincreasethe magnitudeoftheinputstothesigmoidfunction,untilsaturationoccurs. Theideaoflearningahashingfunctionhasbeenfurtherexploredinseveral directions,includingtheideaoftrainingtherepresentationssoastooptimize alossmoredirectlylinkedtothetaskofÔ¨Åndingnearbyexamplesinthehash table( ,). NorouziandFleet2011 5 2 5</div>
        </div>
    </div>

    <div class="question-card" id="q136">
        <div class="question-header">
            <span class="question-number">Question 136</span>
            <span class="question-type">multiple-choice</span>
        </div>

        <div class="question-text">In neural network classification tasks, the softmax activation function is commonly used in the output layer to convert raw model predictions, called logits, into probabilities for each class. Models are trained by adjusting parameters to maximize the likelihood that their outputs match the true class labels.

When optimizing a neural network for classification using the likelihood function, which process primarily encourages the softmax output to closely predict a target vector such as (1, 8, 5)?

1) Increasing the dropout rate during training   
2) Adjusting model weights and biases via gradient descent   
3) Shuffling the training data before each epoch   
4) Using one-hot encoding for class labels   
5) Applying L2 regularization to the loss function   
6) Reducing the learning rate to zero   
7) Initializing all weights to zero</div>

        <div class="answer-section">
            <div class="answer-label">‚úì Correct Answer:</div>
            <div class="answer-text">The correct answer is 2) Adjusting model weights and biases via gradient descent.</div>
        </div>

        <div class="reference-section">
            <div class="reference-label">üìö Reference Text:</div>
            <button class="toggle-button" onclick="toggleReference(136)">
                Show/Hide Reference
            </button>
            <div id="ref136" class="reference-text hidden">likelihoodwilldrivethemodeltolearnparametersthatdrivethesoftmaxtopredict 1 8 5</div>
        </div>
    </div>

    <div class="question-card" id="q137">
        <div class="question-header">
            <span class="question-number">Question 137</span>
            <span class="question-type">multiple-choice</span>
        </div>

        <div class="question-text">In probability theory and statistics, concepts from measure theory and information theory are essential for rigorously analyzing random variables and data distributions. These foundations enable the development of advanced models and methods in statistical inference and machine learning.

Which of the following best describes a set of "measure zero" in the context of measure theory?

1) A set that contains infinitely many elements in any finite interval   
2) A set that occupies no volume in the space, such as a line in the plane or a countable set of points   
3) A set that is uncountable and dense in every open subset   
4) A set whose elements all have probability one   
5) A set with a strictly positive length, area, or volume   
6) A set that is open and closed simultaneously   
7) A set that forms the basis for a sigma-algebra</div>

        <div class="answer-section">
            <div class="answer-label">‚úì Correct Answer:</div>
            <div class="answer-text">The correct answer is 2) A set that occupies no volume in the space, such as a line in the plane or a countable set of points.</div>
        </div>

        <div class="reference-section">
            <div class="reference-label">üìö Reference Text:</div>
            <button class="toggle-button" onclick="toggleReference(137)">
                Show/Hide Reference
            </button>
            <div id="ref137" class="reference-text hidden">1and S 2suchthatp( x‚àà S 1) +p( x‚àà S 2)>1but S 1‚à© S 2=‚àÖ.Thesesets aregenerallyconstructedmakingveryheavyuseoftheinÔ¨Åniteprecisionofreal numbers,forexamplebymakingfractal-shapedsetsorsetsthataredeÔ¨Ånedby transformingthesetofrationalnumbers.2Oneofthekeycontributionsofmeasure theoryistoprovideacharacterization ofthesetofsetsthatwecancomputethe probabilityofwithoutencounteringparadoxes. Inthisbook,weonlyintegrate oversetswithrelativelysimpledescriptions,sothisaspectofmeasuretheorynever becomesarelevantconcern. Forourpurposes,measuretheoryismoreusefulfordescribingtheoremsthat applytomostpointsin Rnbutdonotapplytosomecornercases.Measuretheory providesarigorouswayofdescribingthatasetofpointsisnegligiblysmall.Such asetissaidtohave m e asur e z e r o.WedonotformallydeÔ¨Ånethisconceptinthis textbook.Forourpurposes,itissuÔ¨Écienttounderstandtheintuitionthataset ofmeasurezerooccupiesnovolumeinthespacewearemeasuring.Forexample, within R2,alinehasmeasurezero,whileaÔ¨Ålledpolygonhaspositivemeasure. Likewise,anindividualpointhasmeasurezero.Anyunionofcountablymanysets thateachhavemeasurezeroalsohasmeasurezero(sothesetofalltherational numbershasmeasurezero,forinstance). Anotherusefultermfrommeasuretheoryis al m o st e v e r y wher e.Aproperty thatholdsalmosteverywhereholdsthroughoutallofspaceexceptforonasetof 2TheBanach-Tarskitheoremprovidesafunexampleofsuchsets. 71 CHAPTER3.PROBABILITYANDINFORMATIONTHEORY measurezero.Becausetheexceptionsoccupyanegligibleamountofspace,they canbesafelyignoredformanyapplications.Someimportantresultsinprobability theoryholdforalldiscretevaluesbutonlyhold‚Äúalmosteverywhere‚Äùforcontinuous values. Anothertechnicaldetailofcontinuousvariablesrelatestohandlingcontinuous randomvariablesthataredeterministicfunctionsofoneanother.Supposewehave tworandomvariables, xand y,suchthat y=g( x),wheregisaninvertible,con- tinuous,diÔ¨Äerentiabletransformation.Onemightexpectthatp y( y) =p x(g‚àí 1( y)). Thisisactuallynotthecase. Asasimpleexample,supposewehavescalarrandomvariablesxandy.Suppose y=x 2andx‚àºU(0,1).Ifweusetherulep y(y)=p x(2y)thenp ywillbe0 everywhereexcepttheinterval[0,1 2] 1 ,anditwillbeonthisinterval.Thismeans ÓÅö p y()=ydy1 2, (3.43) whichviolatesthedeÔ¨Ånitionofaprobabilitydistribution.Thisisacommonmistake. Theproblemwiththisapproachisthatitfailstoaccountforthedistortionof spaceintroducedbythefunctiong.Recallthattheprobabilityof xlyinginan inÔ¨ÅnitesimallysmallregionwithvolumeŒ¥ xisgivenbyp( x)Œ¥ x.Sincegcanexpand orcontractspace,theinÔ¨Ånitesimalvolumesurrounding xin xspacemayhave diÔ¨Äerentvolumeinspace. y Toseehowtocorrecttheproblem,wereturntothescalarcase.Weneedto preservetheproperty |p y(())= gxdy||p x()xdx.| (3.44) Solvingfromthis,weobtain p y() = yp x(g‚àí 1())yÓÄåÓÄåÓÄåÓÄå‚àÇx ‚àÇyÓÄåÓÄåÓÄåÓÄå(3.45) orequivalently p x() = xp y(())gxÓÄåÓÄåÓÄåÓÄå‚àÇgx() ‚àÇxÓÄåÓÄåÓÄåÓÄå. (3.46) Inhigherdimensions,thederivativegeneralizestothedeterminantofthe J ac o bi an m at r i x‚ÄîthematrixwithJ i , j=‚àÇ x i ‚àÇ y j.Thus,forreal-valuedvectorsand, x y p x() = xp y(())g xÓÄåÓÄåÓÄåÓÄådetÓÄí‚àÇg() x ‚àÇ xÓÄì ÓÄåÓÄåÓÄåÓÄå. (3.47) 72 CHAPTER3.PROBABILITYANDINFORMATIONTHEORY 3.13InformationTheory Informationtheory isa branchof appliedmathematics thatrevolvesaround quantifyinghowmuchinformationispresentinasignal.Itwasoriginallyinvented tostudysendingmessagesfromdiscretealphabetsoveranoisychannel,suchas communicationviaradiotransmission.Inthiscontext,informationtheorytellshow todesignoptimalcodesandcalculatetheexpectedlengthofmessagessampledfrom speciÔ¨Åcprobabilitydistributionsusingvariousencodingschemes.Inthecontextof machinelearning,wecanalsoapplyinformationtheorytocontinuousvariables wheresomeofthesemessagelengthinterpretations donotapply.ThisÔ¨Åeldis fundamentaltomanyareasofelectricalengineeringandcomputerscience.Inthis textbook,wemostlyuseafewkeyideasfrominformationtheorytocharacterize probabilitydistributionsorquantifysimilaritybetweenprobabilitydistributions. Formoredetailoninformationtheory,seeCoverandThomas2006MacKay ()or ().2003 Thebasicintuitionbehindinformationtheoryisthatlearningthatanunlikely eventhas occurredismoreinformativethanlearningthata likely eventhas occurred.Amessagesaying‚Äúthesunrosethismorning‚Äùissouninformative as tobeunnecessarytosend,butamessagesaying‚Äútherewasasolareclipsethis morning‚Äùisveryinformative. Wewouldliketoquantifyinformationinawaythatformalizesthisintuition. SpeciÔ¨Åcally, ‚Ä¢Likelyeventsshouldhavelowinformationcontent,andintheextremecase, eventsthatareguaranteedtohappenshouldhavenoinformationcontent whatsoever. ‚Ä¢Lesslikelyeventsshouldhavehigherinformationcontent. ‚Ä¢Independenteventsshouldhaveadditiveinformation. Forexample,Ô¨Ånding outthatatossedcoinhascomeupasheadstwiceshouldconveytwiceas muchinformationasÔ¨Åndingoutthatatossedcoinhascomeupasheads once. Inordertosatisfyallthreeoftheseproperties,wedeÔ¨Ånethe se l f - i nf o r m a t i o n ofaneventxtobe = x IxPx. () = log‚àí () (3.48) Inthisbook,wealwaysuselogtomeanthenaturallogarithm,withbasee.Our deÔ¨ÅnitionofI(x)isthereforewritteninunitsof nat s.Onenatistheamountof 73 CHAPTER3.PROBABILITYANDINFORMATIONTHEORY informationgainedbyobservinganeventofprobability1 e.Othertextsusebase-2 logarithmsandunitscalled bi t sor shannons;informationmeasuredinbitsis justarescalingofinformationmeasuredinnats. Whenxiscontinuous,weusethesamedeÔ¨Ånitionofinformationbyanalogy, butsomeofthepropertiesfromthediscretecasearelost.Forexample,anevent withunitdensitystillhaszeroinformation, despitenotbeinganeventthatis guaranteedtooccur. Self-information dealsonlywithasingleoutcome.Wecanquantifytheamount ofuncertaintyinanentireprobabilitydistributionusingthe Shannon e nt r o p y: H() = x E x ‚àº P[()] = Ix ‚àí E x ‚àº P[log()]Px. (3.49) alsodenotedH(P).Inotherwords,theShannonentropyofadistributionisthe expectedamountofinformationinaneventdrawnfromthatdistribution.Itgives alowerboundonthenumberofbits(ifthelogarithmisbase2,otherwisetheunits arediÔ¨Äerent)neededonaveragetoencodesymbolsdrawnfromadistributionP. Distributionsthatarenearlydeterministic(wheretheoutcomeisnearlycertain) havelowentropy;distributionsthatareclosertouniformhavehighentropy.See Ô¨Ågureforademonstration.When 3.5 xiscontinuous,theShannonentropyis knownasthe di Ô¨Ä e r e n t i al e nt r o p y. IfwehavetwoseparateprobabilitydistributionsP(x)andQ(x)overthesame randomvariablex,wecanmeasurehowdiÔ¨Äerentthesetwodistributionsareusing the K ul l bac k - L e i bl e r ( K L ) di v e r g e nc e: D K L( ) = PQÓÅ´ E x ‚àº PÓÄî logPx() Qx()ÓÄï = E x ‚àº P[log()log()] Px‚àíQx.(3.50) Inthecaseofdiscretevariables,itistheextraamountofinformation(measured inbitsifweusethebaselogarithm,butinmachinelearningweusuallyusenats 2 andthenaturallogarithm)neededtosendamessagecontainingsymbolsdrawn fromprobabilitydistributionP,whenweuseacodethatwasdesignedtominimize thelengthofmessagesdrawnfromprobabilitydistribution.Q TheKLdivergencehasmanyusefulproperties,mostnotablythatitisnon- negative.TheKLdivergenceis0ifandonlyifPandQarethesamedistributionin thecaseofdiscretevariables,orequal‚Äúalmosteverywhere‚Äùinthecaseofcontinuous variables.BecausetheKLdivergenceisnon-negativeandmeasuresthediÔ¨Äerence betweentwodistributions,itisoftenconceptualized asmeasuringsomesortof distancebetweenthesedistributions.However,itisnotatruedistancemeasure becauseitisnotsymmetric:D K L(PQÓÅ´)ÓÄ∂=D K L(QPÓÅ´)forsomePandQ. This 74 CHAPTER3.PROBABILITYANDINFORMATIONTHEORY 0 0 0 2 0 4 0 6 0 8 1 0 . . . . . . p0 0 .0 1 .0 2 .0 3 .0 4 .0 5 .0 6 .0 7 .Sha nno n e ntr o p y i n na t s Figure3.5:Thisplotshowshowdistributionsthatareclosertodeterministichavelow ShannonentropywhiledistributionsthatareclosetouniformhavehighShannonentropy. Onthehorizontalaxis,weplotp,theprobabilityofabinaryrandomvariablebeingequal to.Theentropyisgivenby 1 (p‚àí1)log(1‚àíp)‚àípplog.Whenpisnear0,thedistribution isnearlydeterministic,becausetherandomvariableisnearlyalways0.Whenpisnear1, thedistributionisnearlydeterministic,becausetherandomvariableisnearlyalways1. Whenp= 0.5,theentropyismaximal,becausethedistributionisuniformoverthetwo outcomes. asymmetrymeansthatthereareimportantconsequencestothechoiceofwhether touseD K L( )PQÓÅ´orD K L( )QPÓÅ´.SeeÔ¨Ågureformoredetail.3.6 AquantitythatiscloselyrelatedtotheKLdivergenceisthe c r o ss-en t r o p y H(P,Q) =H(P)+D K L(PQÓÅ´),whichissimilartotheKLdivergencebutlacking thetermontheleft: HP,Q( ) = ‚àí E x ‚àº Plog()Qx. (3.51) Minimizingthecross-entropywithrespecttoQisequivalenttominimizingthe KLdivergence,becausedoesnotparticipateintheomittedterm. Q Whencomputingmanyofthesequantities,itiscommontoencounterexpres- sionsoftheform0log0.Byconvention,inthecontextofinformationtheory,we treattheseexpressionsaslim x ‚Üí 0xxlog= 0. 3.14StructuredProbabilisticModels Machinelearningalgorithmsofteninvolveprobabilitydistributionsoveravery largenumberofrandomvariables.Often,theseprobabilitydistributionsinvolve directinteractionsbetweenrelativelyfewvariables.Usingasinglefunctionto 75 CHAPTER3.PROBABILITYANDINFORMATIONTHEORY xProbability Densityq‚àó= argminq D K L() p q ÓÅ´ p x() q‚àó() x xProbability Densityq‚àó= argminq D K L() q p ÓÅ´ p() x q‚àó() x Figure3.6:TheKLdivergenceisasymmetric.Supposewehaveadistributionp(x)and wishtoapproximateitwithanotherdistributionq(x).Wehavethechoiceofminimizing eitherD KL(pqÓÅ´)orD KL(qpÓÅ´).WeillustratetheeÔ¨Äectofthischoiceusingamixtureof twoGaussiansforp,andasingleGaussianforq.</div>
        </div>
    </div>

    <div class="question-card" id="q138">
        <div class="question-header">
            <span class="question-number">Question 138</span>
            <span class="question-type">multiple-choice</span>
        </div>

        <div class="question-text">Optimization algorithms are fundamental for training machine learning models, especially those with large numbers of parameters. Techniques such as quasi-Newton methods, normalization strategies, and coordinate descent offer different trade-offs in terms of efficiency, scalability, and impact on learning dynamics.

Which statement accurately describes a key advantage of the L-BFGS algorithm over standard BFGS in large-scale optimization problems?

1) L-BFGS computes the exact Hessian matrix at each iteration, ensuring higher accuracy.   
2) L-BFGS applies normalization to activations, improving training stability in deep neural networks.   
3) L-BFGS reduces memory usage by storing only a limited set of recent update vectors rather than the full inverse Hessian matrix.   
4) L-BFGS updates variables one at a time, cycling through coordinates to reach a minimum.   
5) L-BFGS is primarily used for regularizing deep neural networks through learnable scaling and shifting parameters.   
6) L-BFGS requires storing an n √ó n matrix, making it impractical for large models.   
7) L-BFGS implements block coordinate descent to parallelize optimization across variable groups.</div>

        <div class="answer-section">
            <div class="answer-label">‚úì Correct Answer:</div>
            <div class="answer-text">The correct answer is 3) L-BFGS reduces memory usage by storing only a limited set of recent update vectors rather than the full inverse Hessian matrix..</div>
        </div>

        <div class="reference-section">
            <div class="reference-label">üìö Reference Text:</div>
            <button class="toggle-button" onclick="toggleReference(138)">
                Show/Hide Reference
            </button>
            <div id="ref138" class="reference-text hidden">burden.In thatrespect, BFGSissimilartotheconjugategradientmethod. However,BFGStakesamoredirectapproachtotheapproximation ofNewton‚Äôs update.RecallthatNewton‚Äôsupdateisgivenby Œ∏‚àó= Œ∏ 0‚àíH‚àí 1‚àá Œ∏ J(Œ∏ 0) , (8.32) whereHistheHessianof JwithrespecttoŒ∏evaluatedatŒ∏ 0.Theprimary computational diÔ¨ÉcultyinapplyingNewton‚Äôsupdateisthecalculationofthe inverseHessianH‚àí 1.Theapproachadoptedbyquasi-Newtonmethods(ofwhich theBFGSalgorithmisthemostprominent)istoapproximate theinversewith amatrixM tthatisiterativelyreÔ¨Ånedbylowrankupdatestobecomeabetter approximationofH‚àí 1. ThespeciÔ¨ÅcationandderivationoftheBFGSapproximationisgiveninmany textbooksonoptimization, includingLuenberger1984(). OncetheinverseHessianapproximationM tisupdated,thedirectionofdescent œÅ tisdeterminedbyœÅ t=M tg t.Alinesearchisperformedinthisdirectionto determinethesizeofthestep, ÓÄè‚àó,takeninthisdirection.TheÔ¨Ånalupdatetothe parametersisgivenby: Œ∏ t + 1= Œ∏ t+ ÓÄè‚àóœÅ t . (8.33) Likethemethodofconjugategradients,theBFGSalgorithmiteratesaseriesof linesearcheswiththedirectionincorporatingsecond-orderinformation. However 3 1 6 CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS unlikeconjugategradients,thesuccessoftheapproachisnotheavilydependent onthelinesearchÔ¨Åndingapointveryclosetothetrueminimumalongtheline. Thus,relativetoconjugategradients,BFGShastheadvantagethatitcanspend lesstimereÔ¨Åningeachlinesearch.Ontheotherhand,theBFGSalgorithmmust storetheinverseHessianmatrix,M,thatrequires O( n2)memory,makingBFGS impracticalformostmoderndeeplearningmodelsthattypicallyhavemillionsof parameters. Limited Memory BFGS (or L-BFGS)The memory costs ofthe BFGS algorithmcanbesigniÔ¨Åcantlydecreasedbyavoidingstoringthecompleteinverse HessianapproximationM.TheL-BFGSalgorithmcomputestheapproximationM usingthesamemethodastheBFGSalgorithm,butbeginningwiththeassumption thatM( 1 ) t ‚àíistheidentitymatrix,ratherthanstoringtheapproximation fromone steptothenext.Ifusedwithexactlinesearches,thedirectionsdeÔ¨ÅnedbyL-BFGS aremutuallyconjugate.However,unlikethemethodofconjugategradients,this procedureremainswellbehavedwhentheminimumofthelinesearchisreached onlyapproximately .TheL-BFGSstrategywithnostoragedescribedherecanbe generalizedtoincludemoreinformationabouttheHessianbystoringsomeofthe vectorsusedtoupdateateachtimestep,whichcostsonlyperstep. M O n() 8.7OptimizationStrategiesandMeta-Algorithms Manyoptimization techniquesarenotexactlyalgorithms, butrathergeneral templatesthatcanbespecializedtoyieldalgorithms,orsubroutinesthatcanbe incorporatedintomanydiÔ¨Äerentalgorithms. 8.7.1BatchNormalization Batchnormalization ( ,)isoneofthemostexcitingrecent IoÔ¨ÄeandSzegedy2015 innovationsinoptimizingdeepneuralnetworksanditisactuallynotanoptimization algorithmatall.Instead,itisamethodofadaptivereparametrization, motivated bythediÔ¨Écultyoftrainingverydeepmodels. Verydeepmodelsinvolvethecompositionofseveralfunctionsorlayers.The gradienttellshowtoupdateeachparameter,undertheassumptionthattheother layersdonotchange.Inpractice,weupdateallofthelayerssimultaneously. Whenwemaketheupdate,unexpectedresultscanhappenbecausemanyfunctions composedtogetherarechangedsimultaneously,usingupdatesthatwerecomputed undertheassumptionthattheotherfunctionsremainconstant.Asasimple 3 1 7 CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS example,supposewehaveadeepneuralnetworkthathasonlyoneunitperlayer anddoesnotuseanactivationfunctionateachhiddenlayer:ÀÜ y= x w 1 w 2 w 3 . . . w l. Here, w iprovidestheweightusedbylayer i.Theoutputoflayer iis h i= h i ‚àí 1 w i. Theoutput ÀÜ yisalinearfunctionoftheinput x,butanonlinearfunctionofthe weights w i.Supposeourcostfunctionhasputagradientofon1 ÀÜ y,sowewishto decreaseÀÜ yslightly.Theback-propagationalgorithmcanthencomputeagradient g=‚àá wÀÜ y.Considerwhathappenswhenwemakeanupdatewwg ‚Üê ‚àí ÓÄè.The Ô¨Årst-orderTaylorseriesapproximation ofÀÜ ypredictsthatthevalueofÀÜ ywilldecrease by ÓÄègÓÄæg.IfwewantedtodecreaseÀÜ yby .1,thisÔ¨Årst-orderinformationavailablein thegradientsuggestswecouldsetthelearningrate ÓÄèto. 1 gÓÄæg.However,theactual updatewillincludesecond-orderandthird-ordereÔ¨Äects,onuptoeÔ¨Äectsoforder l. ThenewvalueofÀÜ yisgivenby x w( 1‚àí ÓÄè g 1)( w 2‚àí ÓÄè g 2)( . . . w l‚àí ÓÄè g l) . (8.34) Anexampleofonesecond-ordertermarisingfromthisupdateis ÓÄè2g 1 g 2ÓÅël i = 3 w i. ThistermmightbenegligibleifÓÅël i = 3 w iissmall,ormightbeexponentiallylarge iftheweightsonlayersthrough3 laregreaterthan.Thismakesitveryhard 1 tochooseanappropriatelearningrate,becausetheeÔ¨Äectsofanupdatetothe parametersforonelayerdependssostronglyonalloftheotherlayers.Second-order optimizationalgorithmsaddressthisissuebycomputinganupdatethattakesthese second-orderinteractionsintoaccount,butwecanseethatinverydeepnetworks, evenhigher-orderinteractionscanbesigniÔ¨Åcant.Evensecond-orderoptimization algorithmsareexpensiveandusuallyrequirenumerousapproximations thatprevent themfromtrulyaccountingforallsigniÔ¨Åcantsecond-orderinteractions. Building an n-thorderoptimization algorithmfor n >2thusseemshopeless.Whatcanwe doinstead? Batchnormalization providesanelegantwayofreparametrizing almostanydeep network.Thereparametrization signiÔ¨Åcantlyreducestheproblemofcoordinating updatesacrossmanylayers.Batchnormalization canbeappliedtoanyinput orhiddenlayerinanetwork.LetHbeaminibatchofactivationsofthelayer tonormalize,arrangedasadesignmatrix,withtheactivationsforeachexample appearinginarowofthematrix.Tonormalize,wereplaceitwith H HÓÄ∞=H¬µ‚àí œÉ, (8.35) where¬µisavectorcontainingthemeanofeachunitandœÉisavectorcontaining thestandarddeviationofeachunit.Thearithmetichereisbasedonbroadcasting thevector¬µandthevectorœÉtobeappliedtoeveryrowofthematrixH.Within eachrow,thearithmeticiselement-wise,so H i , jisnormalizedbysubtracting ¬µ j 3 1 8 CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS anddividingby œÉ j.TherestofthenetworkthenoperatesonHÓÄ∞inexactlythe samewaythattheoriginalnetworkoperatedon.H Attrainingtime, ¬µ=1 mÓÅò iH i , : (8.36) and œÉ=ÓÅ≥ Œ¥+1 mÓÅò i( )H¬µ‚àí2 i , (8.37) where Œ¥isasmallpositivevaluesuchas10‚àí 8imposedtoavoidencountering theundeÔ¨Ånedgradientof‚àözat z=0.Crucially, weback-propagatethrough theseoperationsforcomputingthemeanandthestandarddeviation,andfor applyingthemtonormalizeH.Thismeansthatthegradientwillneverpropose anoperation that actssimplytoincreasethestandard deviationormeanof h i;thenormalization operationsremovetheeÔ¨Äectofsuchanactionandzero outitscomponentinthegradient.Thiswasamajorinnovationofthebatch normalization approach. Previous approacheshadinvolvedaddingpenaltiesto thecostfunctiontoencourageunitstohavenormalizedactivationstatisticsor involvedinterveningtorenormalizeunitstatisticsaftereachgradientdescentstep. Theformerapproachusuallyresultedinimperfectnormalization andthelatter usuallyresultedinsigniÔ¨Åcantwastedtimeasthelearningalgorithmrepeatedly proposedchangingthemeanandvarianceandthenormalization steprepeatedly undidthischange.Batchnormalization reparametrizes themodeltomakesome unitsalwaysbestandardizedbydeÔ¨Ånition,deftlysidesteppingbothproblems. Attesttime,¬µandœÉmaybereplacedbyrunningaveragesthatwerecollected duringtrainingtime.Thisallowsthemodeltobeevaluatedonasingleexample, withoutneedingtousedeÔ¨Ånitionsof¬µandœÉthatdependonanentireminibatch. RevisitingtheÀÜ y= x w 1 w 2 . . . w lexample,weseethatwecanmostlyresolvethe diÔ¨Écultiesinlearningthismodelbynormalizing h l ‚àí 1.Supposethat xisdrawn fromaunitGaussian.Then h l ‚àí 1willalsocomefromaGaussian,becausethe transformationfrom xto h lislinear.However, h l ‚àí 1willnolongerhavezeromean andunitvariance.Afterapplyingbatchnormalization, weobtainthenormalized ÀÜh l ‚àí 1thatrestoresthezeromeanandunitvarianceproperties.Foralmostany updatetothelowerlayers,ÀÜh l ‚àí 1willremainaunitGaussian.Theoutput ÀÜ ymay thenbelearnedasasimplelinearfunction ÀÜ y= w lÀÜ h l ‚àí 1.Learninginthismodelis nowverysimplebecausetheparametersatthelowerlayerssimplydonothavean eÔ¨Äectinmostcases;theiroutputisalwaysrenormalizedtoaunitGaussian. In somecornercases,thelowerlayerscanhaveaneÔ¨Äect.Changingoneofthelower layerweightstocanmaketheoutputbecomedegenerate,andchangingthesign 0 3 1 9 CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS ofoneofthelowerweightscanÔ¨ÇiptherelationshipbetweenÀÜ h l ‚àí 1and y. These situationsareveryrare.Withoutnormalization, nearlyeveryupdatewouldhave anextremeeÔ¨Äectonthestatisticsof h l ‚àí 1.Batchnormalization hasthusmade thismodelsigniÔ¨Åcantlyeasiertolearn. Inthisexample,theeaseoflearningof coursecameatthecostofmakingthelowerlayersuseless.Inourlinearexample, thelowerlayersnolongerhaveanyharmfuleÔ¨Äect,buttheyalsonolongerhave anybeneÔ¨ÅcialeÔ¨Äect.ThisisbecausewehavenormalizedouttheÔ¨Årstandsecond orderstatistics,whichisallthatalinearnetworkcaninÔ¨Çuence.Inadeepneural networkwithnonlinearactivationfunctions,thelowerlayerscanperformnonlinear transformationsofthedata,sotheyremainuseful.Batchnormalization actsto standardizeonlythemeanandvarianceofeachunitinordertostabilizelearning, butallowstherelationshipsbetweenunitsandthenonlinearstatisticsofasingle unittochange. BecausetheÔ¨Ånallayerofthenetworkisabletolearnalineartransformation, wemayactuallywishtoremovealllinearrelationshipsbetweenunitswithina layer.Indeed,thisistheapproachtakenby (),whoprovided Desjardinsetal.2015 theinspirationforbatchnormalization. Unfortunately, eliminating alllinear interactionsismuchmoreexpensivethanstandardizingthemeanandstandard deviationofeachindividualunit,andsofarbatchnormalization remainsthemost practicalapproach. Normalizingthemeanandstandarddeviationofaunitcanreducetheexpressive powerofthe neuralnetworkcontainingthatunit.Inordertomaintainthe expressivepowerofthenetwork,itiscommontoreplacethebatchofhiddenunit activationsHwithŒ≥HÓÄ∞+Œ≤ratherthansimplythenormalizedHÓÄ∞.Thevariables Œ≥andŒ≤arelearnedparametersthatallowthenewvariabletohaveanymean andstandarddeviation.AtÔ¨Årstglance,thismayseemuseless‚Äîwhydidweset themeanto 0,andthenintroduceaparameterthatallowsittobesetbackto anyarbitraryvalueŒ≤?Theansweristhatthenewparametrization canrepresent thesamefamilyoffunctionsoftheinputastheoldparametrization, butthenew parametrization hasdiÔ¨Äerentlearningdynamics.Intheoldparametrization, the meanofHwasdeterminedbyacomplicatedinteractionbetweentheparameters inthelayersbelowH.Inthenewparametrization, themeanofŒ≥HÓÄ∞+Œ≤is determinedsolelybyŒ≤.Thenewparametrization ismucheasiertolearnwith gradientdescent. Mostneuralnetworklayerstaketheformof œÜ(XW+b)where œÜissome Ô¨ÅxednonlinearactivationfunctionsuchastherectiÔ¨Åedlineartransformation.It isnaturaltowonderwhetherweshouldapplybatchnormalization totheinput X,ortothetransformedvalueXW+b. ()recommend IoÔ¨ÄeandSzegedy2015 3 2 0 CHAPTER8.OPTIMIZATIONFORTRAINING DEEPMODELS thelatter.MorespeciÔ¨Åcally,XW+bshouldbereplacedbyanormalizedversion ofXW.Thebiastermshouldbeomittedbecauseitbecomesredundantwith the Œ≤parameterappliedbythebatchnormalization reparametrization. Theinput toalayerisusuallytheoutputofanonlinearactivationfunctionsuchasthe rectiÔ¨Åedlinearfunctioninapreviouslayer. Thestatisticsoftheinputarethus morenon-Gaussianandlessamenabletostandardizationbylinearoperations. Inconvolutionalnetworks,describedinchapter,itisimportanttoapplythe 9 samenormalizing ¬µand œÉateveryspatiallocationwithinafeaturemap,sothat thestatisticsofthefeaturemapremainthesameregardlessofspatiallocation. 8.7.2CoordinateDescent Insomecases,itmaybepossibletosolveanoptimization problemquicklyby breakingitintoseparatepieces.Ifweminimize f(x)withrespecttoasingle variable x i, then minimize it with respect to another variable x jand soon, repeatedlycyclingthroughallvariables,weareguaranteedtoarriveata(local) minimum.Thispracticeisknownascoordinatedescent,becauseweoptimize onecoordinateatatime. Moregenerally,blockcoordinatedescentrefersto</div>
        </div>
    </div>

    <div class="question-card" id="q139">
        <div class="question-header">
            <span class="question-number">Question 139</span>
            <span class="question-type">multiple-choice</span>
        </div>

        <div class="question-text">Deep generative models often rely on gradient estimation methods that can suffer from high variance, particularly when handling discrete latent variables. Techniques such as baselines and variance normalization are employed to improve learning efficiency and stability in these models.

Which of the following strategies specifically minimizes the variance of gradient estimates for each model parameter in REINFORCE-based deep generative models?

1) Computing an optimal baseline as the expected value over the data distribution for each parameter   
2) Using a fixed, global baseline shared across all parameters   
3) Applying weight decay regularization to the network parameters   
4) Increasing the batch size during training   
5) Employing early stopping based on validation loss   
6) Normalizing gradients by their maximum observed value   
7) Freezing the baseline after initial training epochs</div>

        <div class="answer-section">
            <div class="answer-label">‚úì Correct Answer:</div>
            <div class="answer-text">The correct answer is 1) Computing an optimal baseline as the expected value over the data distribution for each parameter.</div>
        </div>

        <div class="reference-section">
            <div class="reference-label">üìö Reference Text:</div>
            <button class="toggle-button" onclick="toggleReference(139)">
                Show/Hide Reference
            </button>
            <div id="ref139" class="reference-text hidden">=ÓÅò y‚àÇ p() y ‚àÇ œâ(20.64) =‚àÇ ‚àÇ œâÓÅò yp() = y‚àÇ ‚àÇ œâ1 = 0 ,(20.65) 6 9 0 CHAPTER20.DEEPGENERATIVEMODELS whichmeansthat E p ( ) yÓÄî (()()) J y‚àí b œâ‚àÇ plog() y ‚àÇ œâÓÄï = E p ( ) yÓÄî J() y‚àÇ plog() y ‚àÇ œâÓÄï ‚àí b E() œâ p ( ) yÓÄî‚àÇ plog() y ‚àÇ œâÓÄï (20.66) = E p ( ) yÓÄî J() y‚àÇ plog() y ‚àÇ œâÓÄï . (20.67) Furthermore,wecanobtaintheoptimal b( œâ) bycomputingthevarianceof( J( y)‚àí b( œâ))‚àÇ p l o g ( ) y ‚àÇ œâunder p( y)andminimizingwithrespectto b( œâ).WhatweÔ¨Åndis thatthisoptimalbaseline b‚àó() œâ iisdiÔ¨Äerentforeachelement œâ iofthevector: œâ b‚àó() œâ i=Ep ( ) yÓÅ® J() y‚àÇ p l o g ( ) y ‚àÇ œâ i2ÓÅ© E p ( ) yÓÅ® ‚àÇ p l o g ( ) y ‚àÇ œâ i2ÓÅ© . (20.68) Thegradientestimatorwithrespectto œâ ithenbecomes (()() J y‚àí b œâ i)‚àÇ plog() y ‚àÇ œâ i(20.69) where b( œâ) iestimatestheabove b‚àó( œâ) i.Theestimate bisusuallyobtainedby addingextraoutputstotheneuralnetworkandtrainingthenewoutputstoestimate E p ( ) y[ J( y)‚àÇ p l o g ( ) y ‚àÇ œâ i2]and E p ( ) yÓÅ® ‚àÇ p l o g ( ) y ‚àÇ œâ i2ÓÅ© foreachelementof œâ.Theseextra outputscanbetrainedwiththemeansquarederrorobjective,usingrespectively J( y)‚àÇ p l o g ( ) y ‚àÇ œâ i2and‚àÇ p l o g ( ) y ‚àÇ œâ i2astargetswhen yissampledfrom p( y),foragiven œâ.Theestimate bmaythenberecoveredbysubstitutingtheseestimatesinto equation. ()preferredtouseasinglesharedoutput 20.68MnihandGregor2014 (acrossallelements iof œâ)trainedwiththetarget J( y),usingasbaseline b( œâ)‚âà E p ( ) y[()] J y. Variancereductionmethodshavebeenintroducedinthereinforcementlearning context( ,; Sutton e t a l .2000WeaverandTao2001,),generalizingpreviouswork onthecaseofbinaryrewardbyDayan1990Bengio2013bMnih (). See e t a l .(), andGregor2014Ba2014Mnih2014Xu2015 (), e t a l .(), e t a l .(),or e t a l .()for examplesofmodernusesoftheREINFORCEalgorithmwithreducedvariancein thecontextofdeeplearning.Inadditiontotheuseofaninput-dependentbaseline b( œâ) ( , ()foundthatthescaleof MnihandGregor2014 J( y)‚àí b( œâ))couldbe adjustedduringtrainingbydividingitbyitsstandarddeviationestimatedbya movingaverageduringtraining,asakindofadaptivelearningrate,tocounter theeÔ¨Äectofimportantvariationsthatoccurduringthecourseoftraininginthe 6 9 1 CHAPTER20.DEEPGENERATIVEMODELS magnitudeofthisquantity. ()calledthisheuristic MnihandGregor2014 variance normalization. REINFORCE-basedestimatorscanbeunderstoodasestimatingthegradient bycorrelatingchoicesof ywithcorrespondingvaluesof J( y).Ifagoodvalueof y isunlikelyunderthecurrentparametrization, itmighttakealongtimetoobtainit bychance,andgettherequiredsignalthatthisconÔ¨Ågurationshouldbereinforced. 20.10DirectedGenerativeNets Asdiscussedinchapter,directedgraphicalmodelsmakeupaprominentclass 16 ofgraphicalmodels.Whiledirectedgraphicalmodelshavebeenverypopular withinthegreatermachinelearningcommunity,withinthesmallerdeeplearning communitytheyhaveuntilroughly2013beenovershadowedbyundirectedmodels suchastheRBM. Inthissectionwereviewsomeofthestandarddirectedgraphicalmodelsthat havetraditionallybeenassociatedwiththedeeplearningcommunity. Wehavealreadydescribeddeepbeliefnetworks,whichareapartiallydirected model.Wehavealsoalreadydescribedsparsecodingmodels,whichcanbethought ofasshallowdirectedgenerativemodels.Theyareoftenusedasfeaturelearners inthecontextofdeeplearning,thoughtheytendtoperformpoorlyatsample generationanddensityestimation.Wenowdescribeavarietyofdeep,fullydirected models. 20.10.1SigmoidBeliefNets Sigmoidbeliefnetworks(,)areasimpleformofdirectedgraphicalmodel Neal1990 withaspeciÔ¨Åckindofconditionalprobabilitydistribution.Ingeneral,wecan thinkofasigmoidbeliefnetworkashavingavectorofbinarystates s,witheach elementofthestateinÔ¨Çuencedbyitsancestors: p s( i) = œÉÔ£´ Ô£≠ÓÅò j < iW j , i s j+ b iÔ£∂ Ô£∏ . (20.70) Themostcommonstructureofsigmoidbeliefnetworkisonethatisdivided intomanylayers,withancestralsamplingproceedingthroughaseriesofmany hiddenlayersandthenultimatelygeneratingthevisiblelayer.Thisstructureis verysimilartothedeepbeliefnetwork,exceptthattheunitsatthebeginningof 6 9 2 CHAPTER20.DEEPGENERATIVEMODELS thesamplingprocessareindependentfromeachother,ratherthansampledfrom arestrictedBoltzmannmachine. Suchastructureisinterestingforavarietyof reasons.Onereasonisthatthestructureisauniversalapproximator ofprobability distributionsoverthevisibleunits,inthesensethatitcanapproximate any probabilitydistributionoverbinaryvariablesarbitrarilywell,givenenoughdepth, evenifthewidthoftheindividuallayersisrestrictedtothedimensionalityofthe visiblelayer(SutskeverandHinton2008,). WhilegeneratingasampleofthevisibleunitsisveryeÔ¨Écientinasigmoid beliefnetwork,mostotheroperationsarenot.Inferenceoverthehiddenunitsgiven thevisibleunitsisintractable.MeanÔ¨Åeldinferenceisalsointractablebecausethe variationallowerboundinvolvestakingexpectationsofcliquesthatencompass entirelayers.ThisproblemhasremaineddiÔ¨Écultenoughtorestrictthepopularity ofdirecteddiscretenetworks. Oneapproachforperforminginferenceinasigmoidbeliefnetworkistoconstruct adiÔ¨Äerentlowerboundthatisspecializedforsigmoidbeliefnetworks(,Saul e t a l . 1996).Thisapproachhasonlybeenappliedtoverysmallnetworks.Another approachistouselearnedinferencemechanismsasdescribedinsection.The19.5 Helmholtzmachine(Dayan1995DayanandHinton1996 e t a l .,; ,)isasigmoidbelief networkcombinedwithaninferencenetworkthatpredictstheparametersofthe meanÔ¨Åelddistributionoverthehiddenunits.Modernapproaches( ,Gregor e t a l . 2014MnihandGregor2014 ; ,)tosigmoidbeliefnetworksstillusethisinference networkapproach.ThesetechniquesremaindiÔ¨Écultduetothediscretenatureof thelatentvariables.Onecannotsimplyback-propagate throughtheoutputofthe inferencenetwork,butinsteadmustusetherelativelyunreliablemachineryforback- propagatingthroughdiscretesamplingprocesses,describedinsection.Recent 20.9.1 approachesbasedonimportancesampling,reweightedwake-sleep(Bornscheinand Bengio2015 Bornschein2015 ,)andbidirectional Helmholtzmachines( e t a l .,) makeitpossibletoquicklytrainsigmoidbeliefnetworksandreachstate-of-the-art performanceonbenchmarktasks. Aspecialcaseofsigmoidbeliefnetworksisthecasewheretherearenolatent variables.LearninginthiscaseiseÔ¨Écient,becausethereisnoneedtomarginalize latentvariablesoutofthelikelihood. Afamilyofmodelscalledauto-regressive networksgeneralizethisfullyvisiblebeliefnetworktootherkindsofvariables besidesbinaryvariablesandotherstructuresofconditionaldistributionsbesideslog- linearrelationships.Auto-regressive networksaredescribedlater,insection.20.10.7 6 9 3 CHAPTER20.DEEPGENERATIVEMODELS 20.10.2DiÔ¨ÄerentiableGeneratorNets ManygenerativemodelsarebasedontheideaofusingadiÔ¨Äerentiablegenerator network.Themodeltransformssamplesoflatentvariables ztosamples xor todistributionsoversamples xusingadiÔ¨Äerentiablefunction g( z; Œ∏( ) g)whichis typicallyrepresentedbyaneuralnetwork.Thismodelclassincludesvariational autoencoders, whichpair thegeneratornetwithaninferencenet,generative adversarial networks, which pair the generator net work witha discriminator network,andtechniquesthattraingeneratornetworksinisolation. Generatornetworksareessentiallyjustparametrized computational procedures forgeneratingsamples,wherethearchitectureprovidesthefamilyofpossible distributionstosamplefromandtheparametersselectadistributionfromwithin thatfamily. Asanexample,thestandardprocedurefordrawingsamplesfromanormal distributionwithmean ¬µandcovariance Œ£istofeedsamples zfromanormal distributionwithzeromeanandidentitycovarianceintoaverysimplegenerator network.ThisgeneratornetworkcontainsjustoneaÔ¨Énelayer: x z L z = ( g) = + ¬µ (20.71) whereisgivenbytheCholeskydecompositionof. L Œ£ Pseudorandomnumbergeneratorscanalsousenonlineartransformationsof simpledistributions.Forexample,inversetransformsampling(Devroye2013,) drawsascalar zfrom U(0 ,1)andappliesanonlineartransformationtoascalar x.Inthiscase g( z)isgivenbytheinverseofthecumulativedistributionfunction F( x) =ÓÅíx ‚àí‚àûp( v) d</div>
        </div>
    </div>

    <div class="question-card" id="q140">
        <div class="question-header">
            <span class="question-number">Question 140</span>
            <span class="question-type">multiple-choice</span>
        </div>

        <div class="question-text">In statistical machine learning, estimation methods such as maximum likelihood and Bayesian inference are used to fit models to data and reason about uncertainty in predictions. These approaches incorporate concepts like consistency, efficiency, regularization, and the use of prior distributions.

Which of the following statements most accurately describes the relationship between linear regression, mean squared error, and maximum likelihood estimation under a Gaussian noise assumption?

1) Linear regression minimizes cross-entropy loss when the output noise is Gaussian.   
2) Maximum likelihood estimation in linear regression leads to regularization by default under Gaussian noise.   
3) Mean squared error is minimized only in non-parametric regression settings, not under Gaussian noise.   
4) Minimizing mean squared error in linear regression is mathematically equivalent to maximizing the conditional log-likelihood when output noise is assumed to be Gaussian.   
5) Maximum likelihood estimation always requires Bayesian priors in linear regression with Gaussian noise.   
6) Under Gaussian noise, minimizing KL divergence is unrelated to the choice of loss function in linear regression.   
7) Linear regression with Gaussian noise maximizes the posterior distribution regardless of the presence of a prior.</div>

        <div class="answer-section">
            <div class="answer-label">‚úì Correct Answer:</div>
            <div class="answer-text">The correct answer is 4) Minimizing mean squared error in linear regression is mathematically equivalent to maximizing the conditional log-likelihood when output noise is assumed to be Gaussian..</div>
        </div>

        <div class="reference-section">
            <div class="reference-label">üìö Reference Text:</div>
            <button class="toggle-button" onclick="toggleReference(140)">
                Show/Hide Reference
            </button>
            <div id="ref140" class="reference-text hidden">x ‚àºÀÜ pdata[logpmodel()]x (5.61) whichisofcoursethesameasthemaximization inequation.5.59 MinimizingthisKLdivergencecorrespondsexactlytominimizingthecross- entropybetweenthedistributions.Manyauthorsusetheterm‚Äúcross-entropy‚Äùto identifyspeciÔ¨Åcallythenegativelog-likelihoodofaBernoulliorsoftmaxdistribution, butthatisamisnomer.Anylossconsistingofanegativelog-likelihoodisacross- entropybetweentheempiricaldistributiondeÔ¨Ånedbythetrainingsetandthe probabilitydistributiondeÔ¨Ånedbymodel.Forexample,meansquarederroristhe cross-entropybetweentheempiricaldistributionandaGaussianmodel. Wecanthusseemaximumlikelihoodasanattempttomakethemodeldis- tributionmatchtheempiricaldistributionÀÜpdata.Ideally,wewouldliketomatch thetruedatageneratingdistributionpdata,butwehavenodirectaccesstothis distribution. WhiletheoptimalŒ∏isthesameregardlessofwhetherwearemaximizingthe likelihoodorminimizingtheKLdivergence,thevaluesoftheobjectivefunctions 1 3 2 CHAPTER5.MACHINELEARNINGBASICS arediÔ¨Äerent.Insoftware,weoftenphrasebothasminimizingacostfunction. Maximumlikelihoodthusbecomesminimization ofthenegativelog-likelihood (NLL),orequivalently,minimization ofthecrossentropy.Theperspectiveof maximumlikelihoodasminimumKLdivergencebecomeshelpfulinthiscase becausetheKLdivergencehasaknownminimumvalueofzero.Thenegative log-likelihoodcanactuallybecomenegativewhenisreal-valued.x 5.5.1ConditionalLog-LikelihoodandMeanSquaredError Themaximumlikelihoodestimatorcanreadilybegeneralizedtothecasewhere ourgoalistoestimateaconditionalprobabilityP( y x|;Œ∏)inordertopredict y given x.Thisisactuallythemostcommonsituationbecauseitformsthebasisfor mostsupervisedlearning.IfXrepresentsallourinputsandYallourobserved targets,thentheconditionalmaximumlikelihoodestimatoris Œ∏ML= argmax Œ∏P. ( ;)YX|Œ∏ (5.62) Iftheexamplesareassumedtobei.i.d.,thenthiscanbedecomposedinto Œ∏ML= argmax Œ∏mÓÅò i=1log(Py() i|x() i;)Œ∏. (5.63) Example:LinearRegressionasMaximumLikelihoodLinearregression, introducedearlierinsection,maybejustiÔ¨Åedasamaximumlikelihood 5.1.4 procedure.Previously,wemotivatedlinearregressionasanalgorithmthatlearns totakeaninputxandproduceanoutputvalue ÀÜy.ThemappingfromxtoÀÜyis chosentominimizemeansquarederror,acriterionthatweintroducedmoreorless arbitrarily.Wenowrevisitlinearregressionfromthepointofviewofmaximum likelihoodestimation.Insteadofproducingasingleprediction ÀÜy,wenowthink ofthemodelasproducingaconditionaldistributionp(y|x).Wecanimagine thatwithaninÔ¨Ånitelylargetrainingset,wemightseeseveraltrainingexamples withthesameinputvaluexbutdiÔ¨Äerentvaluesofy. Thegoalofthelearning algorithmisnowtoÔ¨Åtthedistributionp(y|x)toallofthosediÔ¨Äerentyvalues thatareallcompatiblewithx.Toderivethesamelinearregressionalgorithm weobtainedbefore,wedeÔ¨Ånep(y|x) =N(y;ÀÜy(x;w),œÉ2).Thefunction ÀÜy(x;w) givesthepredictionofthemeanoftheGaussian.Inthisexample,weassumethat thevarianceisÔ¨ÅxedtosomeconstantœÉ2chosenbytheuser.Wewillseethatthis choiceofthefunctionalformofp(y|x)causesthemaximumlikelihoodestimation proceduretoyieldthesamelearningalgorithmaswedevelopedbefore.Sincethe 1 3 3 CHAPTER5.MACHINELEARNINGBASICS examplesareassumedtobei.i.d.,theconditionallog-likelihood(equation)is5.63 givenby mÓÅò i=1log(py() i|x() i;)Œ∏ (5.64) = log ‚àímœÉ‚àím 2log(2)œÄ‚àímÓÅò i=1ÓÄçÓÄçÀÜy() i‚àíy() iÓÄçÓÄç2 2œÉ2,(5.65) where ÀÜy() iistheoutputofthelinearregressiononthei-thinputx() iandmisthe numberofthetrainingexamples.Comparingthelog-likelihoodwiththemean squarederror, MSEtrain=1 mmÓÅò i=1||ÀÜy() i‚àíy() i||2, (5.66) weimmediately seethatmaximizingthelog-likelihoodwithrespecttowyields thesameestimateoftheparameterswasdoesminimizingthemeansquarederror. ThetwocriteriahavediÔ¨Äerentvaluesbutthesamelocationoftheoptimum.This justiÔ¨ÅestheuseoftheMSEasamaximumlikelihoodestimationprocedure.Aswe willsee,themaximumlikelihoodestimatorhasseveraldesirableproperties. 5.5.2PropertiesofMaximumLikelihood Themainappealofthemaximumlikelihoodestimatoristhatitcanbeshownto bethebestestimatorasymptotically,asthenumberofexamplesm‚Üí‚àû,interms ofitsrateofconvergenceasincreases.m Underappropriate conditions, the maximumlikelihood estimatorhas the propertyofconsistency(seesectionabove),meaningthatasthenumber 5.4.5 oftrainingexamplesapproachesinÔ¨Ånity,themaximumlikelihoodestimateofa parameterconvergestothetruevalueoftheparameter.Theseconditionsare: ‚Ä¢Thetruedistributionpdatamustliewithinthemodelfamilypmodel(¬∑;Œ∏). Otherwise,noestimatorcanrecoverpdata. ‚Ä¢ThetruedistributionpdatamustcorrespondtoexactlyonevalueofŒ∏.Other- wise,maximumlikelihoodcanrecoverthecorrectpdata,butwillnotbeable todeterminewhichvalueofwasusedbythedatageneratingprocessing. Œ∏ Thereareotherinductiveprinciplesbesidesthemaximumlikelihoodestima- tor,manyofwhichsharethepropertyofbeingconsistentestimators. However, 1 3 4 CHAPTER5.MACHINELEARNINGBASICS consistentestimatorscandiÔ¨ÄerintheirstatisticeÔ¨Éciency,meaningthatone consistentestimatormayobtainlowergeneralization errorforaÔ¨Åxednumberof samplesm,orequivalently,mayrequirefewerexamplestoobtainaÔ¨Åxedlevelof generalization error. StatisticaleÔ¨Éciencyistypicallystudiedintheparametriccase(likeinlinear regression)whereourgoalistoestimatethevalueofaparameter(andassuming itispossibletoidentifythetrueparameter),notthevalueofafunction.Awayto measurehowclosewearetothetrueparameterisbytheexpectedmeansquared error,computingthesquareddiÔ¨Äerencebetweentheestimatedandtrueparameter values,wheretheexpectationisovermtrainingsamplesfromthedatagenerating distribution.Thatparametricmeansquarederrordecreasesasmincreases,and formlarge,theCram√©r-Raolowerbound(,;,)showsthatno Rao1945Cram√©r1946 consistentestimatorhasalowermeansquarederrorthanthemaximumlikelihood estimator. Forthesereasons(consistencyandeÔ¨Éciency),maximumlikelihoodisoften consideredthepreferredestimatortouseformachinelearning.Whenthenumber ofexamplesissmallenoughtoyieldoverÔ¨Åttingbehavior,regularizationstrategies suchasweightdecaymaybeusedtoobtainabiasedversionofmaximumlikelihood thathaslessvariancewhentrainingdataislimited. 5.6BayesianStatistics Sofarwehavediscussedfrequentiststatisticsandapproachesbasedonestimat- ingasinglevalueofŒ∏,thenmakingallpredictionsthereafterbasedonthatone estimate.AnotherapproachistoconsiderallpossiblevaluesofŒ∏whenmakinga prediction.ThelatteristhedomainofBayesianstatistics. Asdiscussed insection , the frequen tist perspective isthat thetrue 5.4.1 parametervalueŒ∏isÔ¨Åxedbutunknown,whilethepointestimate ÀÜŒ∏isarandom variableonaccountofitbeingafunctionofthedataset(whichisseenasrandom). TheBayesianperspectiveonstatisticsisquitediÔ¨Äerent. The Bayesianuses probabilitytoreÔ¨Çectdegreesofcertaintyofstatesofknowledge.Thedatasetis directlyobservedandsoisnotrandom.Ontheotherhand,thetrueparameterŒ∏ isunknownoruncertainandthusisrepresentedasarandomvariable. Beforeobservingthedata,werepresentourknowledgeofŒ∏usingtheprior probabilitydistribution,p(Œ∏)(sometimesreferredtoassimply‚Äútheprior‚Äù). Generally,themachinelearningpractitionerselectsapriordistributionthatis quitebroad(i.e.withhighentropy)toreÔ¨Çectahighdegreeofuncertaintyinthe 1 3 5 CHAPTER5.MACHINELEARNINGBASICS valueofŒ∏beforeobservinganydata.Forexample,onemightassume that apriori Œ∏liesinsomeÔ¨Åniterangeorvolume,withauniformdistribution. Manypriors insteadreÔ¨Çectapreferencefor‚Äúsimpler‚Äù solutions(suchassmallermagnitude coeÔ¨Écients,orafunctionthatisclosertobeingconstant). Nowconsiderthatwehaveasetofdatasamples {x(1),...,x() m}.Wecan recovertheeÔ¨ÄectofdataonourbeliefaboutŒ∏bycombiningthedatalikelihood px((1),...,x() m|Œ∏)withthepriorviaBayes‚Äôrule: px(Œ∏|(1),...,x() m) =px((1),...,x() m|Œ∏Œ∏)(p) px((1),...,x() m)(5.67) InthescenarioswhereBayesianestimationistypicallyused,thepriorbeginsasa relativelyuniformorGaussiandistributionwithhighentropy,andtheobservation ofthedatausuallycausestheposteriortoloseentropyandconcentratearounda fewhighlylikelyvaluesoftheparameters. Relativetomaximumlikelihoodestimation,BayesianestimationoÔ¨Äerstwo importantdiÔ¨Äerences.First,unlikethemaximumlikelihoodapproachthatmakes predictionsusingapointestimateofŒ∏,theBayesianapproachistomakepredictions usingafulldistributionoverŒ∏.Forexample,afterobservingmexamples,the predicteddistributionoverthenextdatasample,x(+1) m,isgivenby px((+1) m|x(1),...,x() m) =ÓÅö px((+1) m| |Œ∏Œ∏)(px(1),...,x() m)d.Œ∏(5.68) HereeachvalueofŒ∏withpositiveprobabilitydensitycontributestotheprediction ofthenextexample,withthecontributionweightedbytheposteriordensityitself. Afterhavingobserved{x(1),...,x() m},ifwearestillquiteuncertainaboutthe valueofŒ∏,thenthisuncertaintyisincorporated directlyintoanypredictionswe mightmake. Insection,wediscussedhowthefrequentistapproachaddressestheuncer- 5.4 taintyinagivenpointestimateofŒ∏byevaluatingitsvariance.Thevarianceof theestimatorisanassessmentofhowtheestimatemightchangewithalternative samplingsoftheobserveddata.TheBayesiananswertothequestionofhowtodeal withtheuncertaintyintheestimatoristosimplyintegrateoverit,whichtendsto protectwellagainstoverÔ¨Åtting. Thisintegralisofcoursejustanapplicationof thelawsofprobability,makingtheBayesianapproachsimpletojustify,whilethe frequentistmachineryforconstructinganestimatorisbasedontheratheradhoc decisiontosummarizeallknowledgecontainedinthedatasetwithasinglepoint estimate. ThesecondimportantdiÔ¨ÄerencebetweentheBayesianapproachtoestimation andthemaximumlikelihoodapproachisduetothecontributionoftheBayesian 1 3 6 CHAPTER5.MACHINELEARNINGBASICS priordistribution.ThepriorhasaninÔ¨Çuencebyshiftingprobabilitymassdensity towardsregionsoftheparameterspacethatarepreferred .Inpractice, apriori theprioroftenexpressesapreferenceformodelsthataresimplerormoresmooth. CriticsoftheBayesianapproachidentifythepriorasasourceofsubjectivehuman judgmentimpactingthepredictions. Bayesianmethodstypicallygeneralizemuchbetterwhenlimitedtrainingdata isavailable,buttypicallysuÔ¨Äerfromhighcomputational costwhenthenumberof trainingexamplesislarge. Example:BayesianLinearRegressionHereweconsidertheBayesianesti- mationapproachtolearningthelinearregressionparameters.Inlinearregression, welearnalinearmappingfromaninputvectorx‚àà Rntopredictthevalueofa scalar.Thepredictionisparametrized bythevector y‚àà R w‚àà Rn: ÀÜy= wÓÄæx. (5.69) Givenasetofmtrainingsamples (X()train,y()train),wecanexpresstheprediction ofovertheentiretrainingsetas: y ÀÜy()train= X()trainw. (5.70) ExpressedasaGaussianconditionaldistributionony()train,wehave p(y()train|X()train,wy ) = (N()train;X()trainwI,) (5.71) ‚àùexpÓÄí ‚àí1 2(y()train‚àíX()trainw)ÓÄæ(y()train‚àíX()trainw)ÓÄì , (5.72) wherewefollowthestandardMSEformulationinassumingthattheGaussian varianceonyisone.Inwhatfollows,toreducethenotationalburden,wereferto (X()train,y()train) ( ) assimplyXy,. Todeterminetheposteriordistributionoverthemodelparametervectorw,we Ô¨Årstneedtospecifyapriordistribution.ThepriorshouldreÔ¨Çectournaivebelief aboutthevalueoftheseparameters.WhileitissometimesdiÔ¨Écultorunnatural toexpressourpriorbeliefsintermsoftheparametersofthemodel,inpracticewe typicallyassumeafairlybroaddistributionexpressingahighdegreeofuncertainty aboutŒ∏. Forreal-valuedparametersitiscommontouseaGaussianasaprior distribution: p() = (;w Nw¬µ0, Œõ0) exp‚àùÓÄí ‚àí1 2(w¬µ‚àí0)ÓÄæŒõ‚àí1 0(w¬µ‚àí0)ÓÄì ,(5.73) 1 3 7 CHAPTER5.MACHINELEARNINGBASICS where¬µ0and Œõ0arethepriordistributionmeanvectorandcovariancematrix respectively.1 WiththepriorthusspeciÔ¨Åed,wecannowproceedindeterminingtheposterior distributionoverthemodelparameters. p,p,p (wX|y) ‚àù(yX|w)()w (5.74) ‚àùexpÓÄí ‚àí1 2( )yXw‚àíÓÄæ( )yXw‚àíÓÄì expÓÄí ‚àí1 2(w¬µ‚àí0)ÓÄæŒõ‚àí1 0(w¬µ‚àí0)ÓÄì (5.75) ‚àùexpÓÄí ‚àí1 2ÓÄê ‚àí2yÓÄæXww+ÓÄæXÓÄæXww+ÓÄæŒõ‚àí1 0w¬µ‚àí2ÓÄæ 0 Œõ‚àí1 0wÓÄëÓÄì . (5.76) WenowdeÔ¨Åne Œõ m=ÓÄÄ XÓÄæX+ Œõ‚àí1 0ÓÄÅ ‚àí1and¬µ m= Œõ mÓÄÄ XÓÄæy+ Œõ‚àí1 0¬µ0ÓÄÅ .Using thesenewvariables,weÔ¨ÅndthattheposteriormayberewrittenasaGaussian distribution: p, (wX|y) exp‚àùÓÄí ‚àí1 2(w¬µ‚àí m)ÓÄæŒõ‚àí1 m(w¬µ‚àí m)+1 2¬µÓÄæ m Œõ‚àí1 m¬µ mÓÄì (5.77) ‚àùexpÓÄí ‚àí1 2(w¬µ‚àí m)ÓÄæŒõ‚àí1 m(w¬µ‚àí m)ÓÄì . (5.78) Alltermsthatdonotincludetheparametervectorwhavebeenomitted;they areimpliedbythefactthatthedistributionmustbenormalizedtointegrateto.1 EquationshowshowtonormalizeamultivariateGaussiandistribution. 3.23 Examiningthisposteriordistributionallowsustogainsomeintuitionforthe eÔ¨ÄectofBayesianinference.Inmostsituations,weset¬µ0to 0.Ifweset Œõ0=1 Œ±I, then¬µ mgivesthesameestimateofwasdoesfrequentistlinearregressionwitha weightdecaypenaltyofŒ±wÓÄæw.OnediÔ¨ÄerenceisthattheBayesianestimateis undeÔ¨ÅnedifŒ±issettozero‚Äî-wearenotallowedtobegintheBayesianlearning processwithaninÔ¨Ånitelywideprioronw.ThemoreimportantdiÔ¨Äerenceisthat theBayesianestimateprovidesacovariancematrix,showinghowlikelyallthe diÔ¨Äerentvaluesofare,ratherthanprovidingonlytheestimate w ¬µ m. 5.6.1Maximum (MAP)Estimation A P o s t e ri o ri WhilethemostprincipledapproachistomakepredictionsusingthefullBayesian posteriordistributionovertheparameterŒ∏,itisstilloftendesirabletohavea 1Un l e s s t h e re i s a re a s o n t o a s s u m e a p a rtic u l a r c o v a ria n c e s t ru c t u re , we t y p i c a l l y a s s u m e a d i a g o n</div>
        </div>
    </div>

    <div class="question-card" id="q141">
        <div class="question-header">
            <span class="question-number">Question 141</span>
            <span class="question-type">multiple-choice</span>
        </div>

        <div class="question-text">Probabilistic graphical models are widely used in machine learning to represent complex variable dependencies and facilitate efficient inference and learning. These models can be either directed or undirected, with various approaches for structure, sampling, and the use of latent variables.

Which of the following is a primary reason latent variables are introduced in generative models within structured probabilistic frameworks?

1) They enable efficient modeling of dependencies among observed variables without requiring large cliques or parent sets.   
2) They guarantee exact inference regardless of model complexity.   
3) They eliminate the need for approximate inference techniques such as variational inference.   
4) They ensure that every possible interaction among variables is directly represented with observed data.   
5) They automatically provide semantic meaning to all model parameters.   
6) They allow sampling in undirected models to be performed via ancestral sampling.   
7) They always result in fully interpretable model structures.</div>

        <div class="answer-section">
            <div class="answer-label">‚úì Correct Answer:</div>
            <div class="answer-text">The correct answer is 1) They enable efficient modeling of dependencies among observed variables without requiring large cliques or parent sets..</div>
        </div>

        <div class="reference-section">
            <div class="reference-label">üìö Reference Text:</div>
            <button class="toggle-button" onclick="toggleReference(141)">
                Show/Hide Reference
            </button>
            <div id="ref141" class="reference-text hidden">f 2 f 3 f 3 Figure16.13:Anexampleofhowafactorgraphcanresolveambiguityintheinterpretation ofundirectednetworks. ( L e f t )Anundirectednetworkwithacliqueinvolvingthreevariables: a,bandc.Afactorgraphcorrespondingtothesameundirectedmodel.This ( C e n t e r ) factorgraphhasonefactoroverallthreevariables. Anothervalidfactorgraph ( R i g h t ) forthesameundirectedmodel.Thisfactorgraphhasthreefactors,eachoveronlytwo variables.Representation,inference,andlearningareallasymptoticallycheaperinthis factorgraphthaninthefactorgraphdepictedinthecenter,eventhoughbothrequirethe sameundirectedgraphtorepresent. 16.3SamplingfromGraphicalModels Graphicalmodelsalsofacilitatethetaskofdrawingsamplesfromamodel. OneadvantageofdirectedgraphicalmodelsisthatasimpleandeÔ¨Écientproce- durecalledancestralsamplingcanproduceasamplefromthejointdistribution representedbythemodel. Thebasicideaistosortthevariablesx iinthegraphintoatopologicalordering, sothatforall iand j, jisgreaterthan iifx iisaparentofx j.Thevariables 5 8 0 CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING canthenbesampledinthisorder.Inotherwords,weÔ¨Årstsamplex 1‚àº P(x 1), thensample P(x 2| P aG(x 2)),andsoon,untilÔ¨Ånallywesample P(x n| P aG(x n)). Solongaseachconditionaldistribution p(x i| P aG(x i))iseasytosamplefrom, thenthewholemodeliseasytosamplefrom.Thetopologicalsortingoperation guaranteesthatwecanreadtheconditionaldistributionsinequationand16.1 samplefromtheminorder.Withoutthetopologicalsorting,wemightattemptto sampleavariablebeforeitsparentsareavailable. Forsomegraphs,morethanonetopologicalorderingispossible.Ancestral samplingmaybeusedwithanyofthesetopologicalorderings. Ancestralsamplingisgenerallyveryfast(assumingsamplingfromeachcondi- tionaliseasy)andconvenient. Onedrawbacktoancestralsamplingisthatitonlyappliestodirectedgraphical models.Anotherdrawbackisthatitdoesnotsupporteveryconditionalsampling operation.Whenwewishtosamplefromasubsetofthevariablesinadirected graphicalmodel,givensomeothervariables,weoftenrequirethatallthecondition- ingvariablescomeearlierthanthevariablestobesampledintheorderedgraph. Inthiscase,wecansamplefromthelocalconditionalprobabilitydistributions speciÔ¨Åedbythemodeldistribution.Otherwise,theconditionaldistributionswe needtosamplefromaretheposteriordistributionsgiventheobservedvariables. TheseposteriordistributionsareusuallynotexplicitlyspeciÔ¨Åedandparametrized inthemodel.Inferringtheseposteriordistributionscanbecostly.Inmodelswhere thisisthecase,ancestralsamplingisnolongereÔ¨Écient. Unfortunately,ancestralsamplingisapplicableonlytodirectedmodels.We cansamplefromundirectedmodelsbyconvertingthemtodirectedmodels,butthis oftenrequiressolvingintractableinferenceproblems(todeterminethemarginal distributionovertherootnodesofthenewdirectedgraph)orrequiresintroducing somanyedgesthattheresultingdirectedmodelbecomesintractable.Sampling fromanundirectedmodelwithoutÔ¨Årstconvertingittoadirectedmodelseemsto requireresolvingcyclicaldependencies.Everyvariableinteractswitheveryother variable,sothereisnoclearbeginningpointforthesamplingprocess.Unfortunately, drawingsamplesfromanundirectedgraphicalmodelisanexpensive,multi-pass process.TheconceptuallysimplestapproachisGibbssampling.Supposewe haveagraphicalmodeloveran n-dimensionalvectorofrandomvariables x.We iterativelyvisiteachvariablex ianddrawasampleconditionedonalloftheother variables,from p(x i|x‚àí i).Duetotheseparationpropertiesofthegraphical model,wecanequivalentlyconditionononlytheneighborsofx i.Unfortunately, afterwehavemadeonepassthroughthegraphicalmodelandsampledall n variables,westilldonothaveafairsamplefrom p(x).Instead,wemustrepeatthe 5 8 1 CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING processandresampleall nvariablesusingtheupdatedvaluesoftheirneighbors. Asymptotically,aftermanyrepetitions,thisprocessconvergestosamplingfrom thecorrectdistribution.ItcanbediÔ¨Éculttodeterminewhenthesampleshave reachedasuÔ¨Écientlyaccurateapproximationofthedesireddistribution.Sampling techniquesforundirectedmodelsareanadvancedtopic,coveredinmoredetailin chapter.17 16.4AdvantagesofStructuredModeling Theprimaryadvantageofusingstructuredprobabilisticmodelsisthattheyallow ustodramatically reducethecostofrepresentingprobabilitydistributionsaswell aslearningandinference.Samplingisalsoacceleratedinthecaseofdirected models,whilethesituationcanbecomplicatedwithundirectedmodels.The primarymechanismthatallowsalloftheseoperationstouselessruntimeand memoryischoosingtonotmodelcertaininteractions. Graphicalmodelsconvey informationbyleavingedgesout.Anywherethereisnotanedge,themodel speciÔ¨Åestheassumptionthatwedonotneedtomodeladirectinteraction. AlessquantiÔ¨ÅablebeneÔ¨Åtofusingstructuredprobabilisticmodelsisthat theyallowustoexplicitlyseparaterepresentationofknowledgefromlearningof knowledgeorinferencegivenexistingknowledge.Thismakesourmodelseasierto developanddebug.Wecandesign,analyze,andevaluatelearningalgorithmsand inferencealgorithmsthatareapplicabletobroadclassesofgraphs.Independently, wecandesignmodelsthatcapturetherelationshipswebelieveareimportantinour data.WecanthencombinethesediÔ¨Äerentalgorithmsandstructuresandobtain aCartesianproductofdiÔ¨Äerentpossibilities.ItwouldbemuchmorediÔ¨Écultto designend-to-endalgorithmsforeverypossiblesituation. 16.5LearningaboutDependencies Agoodgenerativemodelneedstoaccuratelycapturethedistributionoverthe observedor‚Äúvisible‚Äù variables v.OftenthediÔ¨Äerentelementsofvarehighly dependentoneachother.Inthecontextofdeeplearning,theapproachmost commonlyusedtomodelthesedependenciesistointroduceseverallatentor ‚Äúhidden‚Äùvariables,h.Themodelcanthencapturedependenciesbetweenanypair ofvariablesv iandv jindirectly,viadirectdependenciesbetweenv iandh,and directdependenciesbetweenandv h j. Agoodmodelofvwhichdidnotcontainanylatentvariableswouldneedto 5 8 2 CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING haveverylargenumbersofparentspernodeinaBayesiannetworkorverylarge cliquesinaMarkovnetwork.Justrepresentingthesehigherorderinteractionsis costly‚Äîbothinacomputational sense,becausethenumberofparametersthat mustbestoredinmemoryscalesexponentiallywiththenumberofmembersina clique,butalsoinastatisticalsense,becausethisexponentialnumberofparameters requiresawealthofdatatoestimateaccurately. Whenthemodelisintendedtocapturedependenciesbetweenvisiblevariables withdirectconnections,itisusuallyinfeasibletoconnectallvariables,sothe graphmustbedesignedtoconnectthosevariablesthataretightlycoupledand omitedgesbetweenothervariables.AnentireÔ¨Åeldofmachinelearningcalled structurelearningisdevotedtothisproblemForagoodreferenceonstructure learning,see(KollerandFriedman2009,).Moststructurelearningtechniquesare aformofgreedysearch.Astructureisproposed,amodelwiththatstructure istrained,thengivenascore.Thescorerewardshightrainingsetaccuracyand penalizesmodelcomplexity.Candidatestructureswithasmallnumberofedges addedorremovedarethenproposedasthenextstepofthesearch.Thesearch proceedstoanewstructurethatisexpectedtoincreasethescore. Usinglatentvariablesinsteadofadaptivestructureavoidstheneedtoperform discretesearchesandmultipleroundsoftraining.AÔ¨Åxedstructureovervisible andhiddenvariablescanusedirectinteractionsbetweenvisibleandhiddenunits toimposeindirectinteractionsbetweenvisibleunits.Usingsimpleparameter learningtechniqueswecanlearnamodelwithaÔ¨Åxedstructurethatimputesthe rightstructureonthemarginal . p()v LatentvariableshaveadvantagesbeyondtheirroleineÔ¨Écientlycapturing p(v). Thenewvariables halsoprovideanalternativerepresentationforv.Forexample, asdiscussedinsection,themixtureofGaussiansmodellearnsalatentvariable 3.9.6 thatcorrespondstowhichcategoryofexamplestheinputwasdrawnfrom.This meansthatthelatentvariableinamixtureofGaussiansmodelcanbeusedtodo classiÔ¨Åcation. Inchapterwesawhowsimpleprobabilisticmodelslikesparse 14 codinglearnlatentvariablesthatcanbeusedasinputfeaturesforaclassiÔ¨Åer, orascoordinatesalongamanifold.Othermodelscanbeusedinthissameway, butdeepermodelsandmodelswithdiÔ¨Äerentkindsofinteractionscancreateeven richerdescriptionsoftheinput.Manyapproachesaccomplishfeaturelearning bylearninglatentvariables.Often,givensomemodelofvandh,experimental observationsshowthat E[hv|]orargmaxh p( h v ,)isagoodfeaturemappingfor v. 5 8 3 CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING 16.6InferenceandApproximateInference Oneofthemainwayswecanuseaprobabilisticmodelistoaskquestionsabout howvariablesarerelatedtoeachother.Givenasetofmedicaltests,wecanask whatdiseaseapatientmighthave.Inalatentvariablemodel,wemightwantto extractfeatures E[hv|]describingtheobservedvariables v.Sometimesweneed tosolvesuchproblemsinordertoperformothertasks.Weoftentrainourmodels usingtheprincipleofmaximumlikelihood.Because log()= p v E h h‚àº p (| v )[log( )log( )] p h v ,‚àí p h v| ,(16.9) weoftenwanttocompute p(h| v)inordertoimplementalearningrule.Allof theseareexamplesofinferenceproblemsinwhichwemustpredictthevalueof somevariablesgivenothervariables,orpredicttheprobabilitydistributionover somevariablesgiventhevalueofothervariables. Unfortunately,formostinterestingdeepmodels,theseinferenceproblemsare intractable,evenwhenweuseastructuredgraphicalmodeltosimplifythem.The graphstructureallowsustorepresentcomplicated,high-dimensionaldistributions withareasonablenumberofparameters,butthegraphsusedfordeeplearningare usuallynotrestrictiveenoughtoalsoalloweÔ¨Écientinference. Itisstraightforwardtoseethatcomputingthemarginalprobabilityofageneral graphicalmodelis#Phard.Thecomplexityclass#Pisageneralization ofthe complexityclassNP.ProblemsinNPrequiredeterminingonlywhetheraproblem hasasolutionandÔ¨Åndingasolutionifoneexists.Problemsin#Prequirecounting thenumberofsolutions.Toconstructaworst-casegraphicalmodel,imaginethat wedeÔ¨Åneagraphicalmodeloverthebinaryvariablesina3-SATproblem. We canimposeauniformdistributionoverthesevariables.Wecanthenaddone binarylatentvariableperclausethatindicateswhethereachclauseissatisÔ¨Åed. Wecanthenaddanotherlatentvariableindicatingwhetheralloftheclausesare satisÔ¨Åed.Thiscanbedonewithoutmakingalargeclique,bybuildingareduction treeoflatentvariables,witheachnodeinthetreereportingwhethertwoother variablesaresatisÔ¨Åed.Theleavesofthistreearethevariablesforeachclause. TherootofthetreereportswhethertheentireproblemissatisÔ¨Åed. Duetothe uniformdistributionovertheliterals,themarginaldistributionovertherootofthe reductiontreespeciÔ¨Åeswhatfractionofassignmentssatisfytheproblem.While thisisacontrivedworst-caseexample,NPhardgraphscommonlyariseinpractical real-worldscenarios. Thismotivatestheuseofapproximate inference.In thecontextof deep learning,thisusuallyreferstovariationalinference,inwhichweapproximate the 5 8 4 CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING truedistribution p(h| v)byseekinganapproximate distribution q(hv|)thatisas closetothetrueoneaspossible.Thisandothertechniquesaredescribedindepth inchapter.19 16.7TheDeepLearningApproachtoStructuredProb- abilisticModels Deeplearningpractitioners generallyusethesamebasiccomputational toolsas othermachinelearningpractitionerswhoworkwithstructuredprobabilisticmodels. However,inthecontextofdeeplearning,weusuallymakediÔ¨Äerentdesigndecisions abouthowtocombinethesetools,resultinginoverallalgorithmsandmodelsthat haveaverydiÔ¨ÄerentÔ¨Çavorfrommoretraditionalgraphicalmodels. Deeplearningdoesnotalwaysinvolveespeciallydeepgraphicalmodels.Inthe contextofgraphicalmodels,wecandeÔ¨Ånethedepthofamodelintermsofthe graphicalmodelgraphratherthanthecomputational graph.Wecanthinkofa latentvariable h iasbeingatdepth jiftheshortestpathfrom h itoanobserved variableis jsteps.Weusuallydescribethedepthofthemodelasbeingthegreatest depthofanysuch h i.ThiskindofdepthisdiÔ¨Äerentfromthedepthinducedby thecomputational graph.Manygenerativemodelsusedfordeeplearninghaveno latentvariablesoronlyonelayeroflatentvariables,butusedeepcomputational graphstodeÔ¨Ånetheconditionaldistributionswithinamodel. Deeplearningessentiallyalwaysmakesuseoftheideaofdistributedrepresen- tations.Evenshallowmodelsusedfordeeplearningpurposes(suchaspretraining shallowmodelsthatwilllaterbecomposedtoformdeepones)nearlyalways haveasingle,largelayeroflatentvariables.Deeplearningmodelstypicallyhave morelatentvariablesthanobservedvariables.Complicated nonlinearinteractions betweenvariablesareaccomplishedviaindirectconnectionsthatÔ¨Çowthrough multiplelatentvariables. Bycontrast,traditionalgraphicalmodelsusuallycontainmostlyvariablesthat areatleastoccasionallyobserved,evenifmanyofthevariablesaremissingat randomfromsometrainingexamples.Traditionalmodelsmostlyusehigher-order termsandstructurelearningtocapturecomplicatednonlinearinteractionsbetween variables.Iftherearelatentvariables,theyareusuallyfewinnumber. ThewaythatlatentvariablesaredesignedalsodiÔ¨Äersindeeplearning.The deeplearningpractitionertypicallydoesnotintendforthelatentvariablesto takeonanyspeciÔ¨Åcsemanticsaheadoftime‚Äîthetrainingalgorithmisfreeto inventtheconceptsitneedstomodelaparticulardataset.Thelatentvariablesare 5 8 5</div>
        </div>
    </div>

    <div class="question-card" id="q142">
        <div class="question-header">
            <span class="question-number">Question 142</span>
            <span class="question-type">multiple-choice</span>
        </div>

        <div class="question-text">In deep learning, the structure and nature of learned representations play a crucial role in a model's ability to generalize and efficiently handle complex data. Various regularization strategies are used to guide representation learning and improve model performance.

Which strategy specifically encourages deep models to isolate independent underlying causes of variations within data, often resulting in disentangled and interpretable latent features?

1) Enforcing smoothness in input-output mappings   
2) Prioritizing linear relationships among activations   
3) Promoting temporal coherence in sequential data   
4) Encouraging multiple explanatory factors in the representation   
5) Maximizing clustering of similar examples   
6) Enforcing sparsity across all activations   
7) Regularizing model depth to prevent overfitting</div>

        <div class="answer-section">
            <div class="answer-label">‚úì Correct Answer:</div>
            <div class="answer-text">The correct answer is 4) Encouraging multiple explanatory factors in the representation.</div>
        </div>

        <div class="reference-section">
            <div class="reference-label">üìö Reference Text:</div>
            <button class="toggle-button" onclick="toggleReference(142)">
                Show/Hide Reference
            </button>
            <div id="ref142" class="reference-text hidden">m i g h t wa n t t o d i Ô¨Ä e r i n f 2dd i Ô¨Ä e re n t re g i o n s , re q u i rin g O ( 2d) t ra i n i n g e x a m p l e s . 5 5 0 CHAPTER15.REPRESENTATIONLEARNING Thisprovidesageometricargumenttoexplainthegeneralization powerof distributedrepresentation:withO(nd)parameters(fornlinear-threshold features in Rd)wecandistinctlyrepresentO(nd) regionsininputspace.Ifinsteadwemade noassumptionatallaboutthedata,andusedarepresentationwithoneunique symbolforeachregion,andseparateparametersforeachsymboltorecognizeits correspondingportionof Rd,thenspecifyingO(nd)regionswouldrequireO(nd) examples.Moregenerally,theargumentinfavorofthedistributedrepresentation couldbeextendedtothecasewhereinsteadofusinglinearthresholdunitswe usenonlinear,possiblycontinuous,featureextractorsforeachoftheattributesin thedistributedrepresentation.Theargumentinthiscaseisthatifaparametric transformationwithkparameterscanlearnaboutrregionsininputspace,with krÓÄú,andifobtainingsucharepresentationwasusefultothetaskofinterest,then wecouldpotentiallygeneralizemuchbetterinthiswaythaninanon-distributed settingwherewewouldneedO(r)examplestoobtainthesamefeaturesand associatedpartitioningoftheinputspaceintorregions.Usingfewerparametersto representthemodelmeansthatwehavefewerparameterstoÔ¨Åt,andthusrequire farfewertrainingexamplestogeneralizewell. Afurtherpartoftheargumentforwhymodelsbasedondistributedrepresen- tationsgeneralizewellisthattheircapacityremainslimiteddespitebeingableto distinctlyencodesomanydiÔ¨Äerentregions.Forexample,theVCdimensionofa neuralnetworkoflinearthresholdunitsisonlyO(wwlog),wherewisthenumber ofweights(Sontag1998,).Thislimitationarisesbecause,whilewecanassignvery manyuniquecodestorepresentationspace,wecannotuseabsolutelyallofthecode space,norcanwelearnarbitraryfunctionsmappingfromtherepresentationspace htotheoutputyusingalinearclassiÔ¨Åer.Theuseofadistributedrepresentation combinedwithalinearclassiÔ¨Åerthusexpressesapriorbeliefthattheclassesto berecognizedarelinearlyseparableasafunctionoftheunderlyingcausalfactors capturedbyh. Wewilltypicallywanttolearncategoriessuchasthesetofall imagesofallgreenobjectsorthesetofallimagesofcars,butnotcategoriesthat requirenonlinear,XORlogic.Forexample,wetypicallydonotwanttopartition thedataintothesetofallredcarsandgreentrucksasoneclassandthesetofall greencarsandredtrucksasanotherclass. Theideasdiscussedsofarhavebeenabstract,buttheymaybeexperimentally validated. ()Ô¨Åndthathiddenunitsinadeepconvolutionalnetwork Zhouetal.2015 trainedontheImageNetandPlacesbenchmarkdatasetslearnfeaturesthatarevery ofteninterpretable,correspondingtoalabelthathumanswouldnaturallyassign. Inpracticeitiscertainlynotalwaysthecasethathiddenunitslearnsomething thathasasimplelinguisticname,butitisinterestingtoseethisemergenearthe toplevelsofthebestcomputervisiondeepnetworks.Whatsuchfeatureshavein 5 5 1 CHAPTER15.REPRESENTATIONLEARNING -+ = Figure15.9:Agenerativemodelhaslearnedadistributedrepresentationthatdisentangles theconceptofgenderfromtheconceptofwearingglasses. Ifwebeginwiththerepre- sentationoftheconceptofamanwithglasses,thensubtractthevectorrepresentingthe conceptofamanwithoutglasses,andÔ¨Ånallyaddthevectorrepresentingtheconcept ofawomanwithoutglasses,weobtainthevectorrepresentingtheconceptofawoman withglasses.Thegenerativemodelcorrectlydecodesalloftheserepresentationvectorsto imagesthatmayberecognizedasbelongingtothecorrectclass.Imagesreproducedwith permissionfrom (). Radford e t a l .2015 commonisthatonecouldimagine learningabouteachofthemwithouthavingto seealltheconÔ¨Ågurationsofalltheothers. ()demonstratedthat Radfordetal.2015 agenerativemodelcanlearnarepresentationofimagesoffaces,withseparate directionsinrepresentationspacecapturingdiÔ¨Äerentunderlyingfactorsofvariation. Figuredemonstratesthatonedirectioninrepresentationspacecorresponds 15.9 towhetherthepersonismaleorfemale,whileanothercorrespondstowhether thepersoniswearingglasses.Thesefeatureswerediscoveredautomatically ,not Ô¨Åxedapriori.ThereisnoneedtohavelabelsforthehiddenunitclassiÔ¨Åers: gradientdescentonanobjectivefunctionofinterestnaturallylearnssemantically interestingfeatures,solongasthetaskrequiressuchfeatures.Wecanlearnabout thedistinctionbetweenmaleandfemale,oraboutthepresenceorabsenceof glasses,withouthavingtocharacterizealloftheconÔ¨Ågurations ofthen‚àí1other featuresbyexamplescoveringallofthesecombinationsofvalues. Thisformof statisticalseparabilityiswhatallowsonetogeneralizetonewconÔ¨Ågurations ofa person‚Äôsfeaturesthathaveneverbeenseenduringtraining. 5 5 2 CHAPTER15.REPRESENTATIONLEARNING 15. 5 E x p on en t i al Gai n s f rom D ep t h Wehaveseeninsectionthatmultilayerperceptronsareuniversalapproxima- 6.4.1 tors,andthatsomefunctionscanberepresentedbyexponentiallysmallerdeep networkscomparedtoshallownetworks.Thisdecreaseinmodelsizeleadsto improvedstatisticaleÔ¨Éciency.Inthissection,wedescribehowsimilarresultsapply moregenerallytootherkindsofmodelswithdistributedhiddenrepresentations. Insection,wesawanexampleofagenerativemodelthatlearnedabout 15.4 theexplanatoryfactorsunderlyingimagesoffaces,includingtheperson‚Äôsgender andwhethertheyarewearingglasses.Thegenerativemodelthataccomplished thistaskwasbasedonadeepneuralnetwork.Itwouldnotbereasonabletoexpect ashallownetwork,suchasalinearnetwork,tolearnthecomplicatedrelationship betweentheseabstractexplanatoryfactorsandthepixelsintheimage. Inthis andotherAItasks,thefactorsthatcanbechosenalmostindependentlyfrom eachotheryetstillcorrespondtomeaningfulinputsaremorelikelytobevery high-levelandrelatedinhighlynonlinearwaystotheinput.Wearguethatthis demands deepdistributedrepresentations,wherethehigherlevelfeatures(seenas functionsoftheinput)orfactors(seenasgenerativecauses)areobtainedthrough thecompositionofmanynonlinearities. IthasbeenproveninmanydiÔ¨Äerentsettingsthatorganizingcomputation throughthecompositionofmanynonlinearities andahierarchyofreusedfeatures cangiveanexponentialboosttostatisticaleÔ¨Éciency,ontopoftheexponential boostgivenbyusingadistributedrepresentation.Manykindsofnetworks(e.g., withsaturatingnonlinearities, Booleangates,sum/products,orRBFunits)with asinglehiddenlayercanbeshowntobeuniversalapproximators.Amodel familythatisauniversalapproximator canapproximatealargeclassoffunctions (includingallcontinuousfunctions)uptoanynon-zerotolerancelevel,givenenough hiddenunits. However,therequirednumberofhiddenunitsmaybeverylarge. Theoreticalresultsconcerningtheexpressivepowerofdeeparchitectures statethat therearefamiliesoffunctionsthatcanberepresentedeÔ¨Écientlybyanarchitecture ofdepthk,butwouldrequireanexponentialnumberofhiddenunits(withrespect totheinputsize)withinsuÔ¨Écientdepth(depth2ordepth).k‚àí1 Insection,wesawthatdeterministicfeedforwardnetworksareuniversal 6.4.1 approximatorsoffunctions.Manystructuredprobabilisticmodelswithasingle hiddenlayeroflatentvariables,includingrestrictedBoltzmannmachinesanddeep beliefnetworks,areuniversalapproximatorsofprobabilitydistributions(LeRoux andBengio20082010Mont√∫farandAy2011Mont√∫far2014Krause ,,; ,;,; etal., 2013). 5 5 3 CHAPTER15.REPRESENTATIONLEARNING Insection,wesawthatasuÔ¨Écientlydeepfeedforwardnetworkcanhave 6.4.1 anexponentialadvantageoveranetworkthatistooshallow.Suchresultscanalso beobtainedforothermodelssuchasprobabilisticmodels.Onesuchprobabilistic modelisthe sum-pr o duc t net w o r korSPN(PoonandDomingos2011,).These modelsusepolynomialcircuitstocomputetheprobabilitydistributionovera setofrandomvariables. ()showedthatthereexist DelalleauandBengio2011 probabilitydistributionsforwhichaminimumdepthofSPNisrequiredtoavoid needinganexponentiallylargemodel.Later, () MartensandMedabalimi 2014 showedthattherearesigniÔ¨ÅcantdiÔ¨ÄerencesbetweeneverytwoÔ¨Ånitedepthsof SPN,andthatsomeoftheconstraintsusedtomakeSPNstractablemaylimit theirrepresentationalpower. Anotherinterestingdevelopmentisasetoftheoreticalresultsfortheexpressive poweroffamiliesofdeepcircuitsrelatedtoconvolutionalnets,highlightingan exponentialadvantageforthedeepcircuitevenwhentheshallowcircuitisallowed toonlyapproximatethefunctioncomputedbythedeepcircuit( ,Cohenetal. 2015).Bycomparison,previoustheoreticalworkmadeclaimsregardingonlythe casewheretheshallowcircuitmustexactlyreplicateparticularfunctions. 15. 6 Pro v i d i n g C l u es t o D i s c o v er Un d erl y i n g C au s es Toclosethischapter,wecomebacktooneofouroriginalquestions:whatmakesone representationbetterthananother?Oneanswer,Ô¨Årstintroducedinsection,is15.3 thatanidealrepresentationisonethatdisentanglestheunderlyingcausalfactorsof variationthatgeneratedthedata,especiallythosefactorsthatarerelevanttoour applications.Moststrategiesforrepresentationlearningarebasedonintroducing cluesthathelpthelearningtoÔ¨Åndtheseunderlyingfactorsofvariations.Theclues canhelpthelearnerseparatetheseobservedfactorsfromtheothers.Supervised learningprovidesaverystrongclue:alabely,presentedwitheachx,thatusually speciÔ¨Åesthevalueofatleastoneofthefactorsofvariationdirectly.Moregenerally, tomakeuseofabundantunlabeleddata,representationlearningmakesuseof other,lessdirect,hintsabouttheunderlyingfactors.Thesehintstaketheformof implicitpriorbeliefsthatwe,thedesignersofthelearningalgorithm,imposein ordertoguidethelearner.Resultssuchasthenofreelunchtheoremshowthat regularizationstrategiesarenecessarytoobtaingoodgeneralization. Whileitis impossibletoÔ¨Åndauniversallysuperiorregularizationstrategy,onegoalofdeep learningistoÔ¨Åndasetoffairlygenericregularizationstrategiesthatareapplicable toawidevarietyofAItasks,similartothetasksthatpeopleandanimalsareable tosolve. 5 5 4 CHAPTER15.REPRESENTATIONLEARNING Weprovideherealistofthesegenericregularizationstrategies.Thelistis clearlynotexhaustive,butgivessomeconcreteexamplesofwaysthatlearning algorithmscanbeencouragedtodiscoverfeaturesthatcorrespondtounderlying factors.Thislistwasintroducedinsection3.1of ()andhas Bengioetal.2013d beenpartiallyexpandedhere. ‚Ä¢Smoothness:Thisistheassumptionthatf(x+ÓÄèd)‚âàf(x)forunitdand smallÓÄè.Thisassumptionallowsthelearnertogeneralizefromtraining examplestonearbypointsininputspace.Manymachinelearningalgorithms leveragethisidea,butitisinsuÔ¨Écienttoovercomethecurseofdimensionality. ‚Ä¢Linearity:Manylearningalgorithmsassumethatrelationshipsbetweensome variablesarelinear.Thisallowsthealgorithmtomakepredictionseven veryfarfromtheobserveddata,butcansometimesleadtooverlyextreme predictions.Mostsimplemachinelearningalgorithmsthatdonotmakethe smoothnessassumptioninsteadmakethelinearityassumption.Theseare infactdiÔ¨Äerentassumptions‚Äîlinearfunctionswithlargeweightsapplied tohigh-dimensionalspacesmaynotbeverysmooth.SeeGoodfellowetal. ()forafurtherdiscussionofthelimitationsofthelinearityassumption. 2014b ‚Ä¢Multipleexplanatoryfactors:Manyrepresentationlearningalgorithmsare motivatedbytheassumptionthatthedataisgeneratedbymultipleunderlying explanatoryfactors,andthatmosttaskscanbesolvedeasilygiventhestate ofeachofthesefactors.Sectiondescribeshowthisviewmotivatessemi- 15.3 supervisedlearningviarepresentationlearning.Learningthestructureofp(x) requireslearningsomeofthesamefeaturesthatareusefulformodelingp(y| x)becausebothrefertothesameunderlyingexplanatoryfactors.Section15.4 describeshowthisviewmotivatestheuseofdistributedrepresentations,with separatedirectionsinrepresentationspacecorrespondingtoseparatefactors ofvariation. ‚Ä¢Causalfactors:themodelisconstructedinsuchawaythatittreatsthe factorsofvariationdescribedbythelearnedrepresentationhasthecauses oftheobserveddatax,andnotvice-versa.Asdiscussedinsection,this15.3 isadvantageousforsemi-supervisedlearningandmakesthelearnedmodel morerobustwhenthedistributionovertheunderlyingcauseschangesor whenweusethemodelforanewtask. ‚Ä¢Depthahierarchical organization ofexplanatory factors , or :High-level, abstractconceptscanbedeÔ¨Ånedintermsofsimpleconcepts,forminga hierarchy.From another point of view, the us e ofa deeparchitecture 5 5 5 CHAPTER15.REPRESENTATIONLEARNING expressesourbeliefthatthetaskshouldbeaccomplishedviaamulti-step program, with eachstep referringbacktothe outputoftheprocessing accomplishedviaprevioussteps. ‚Ä¢Sharedfactors across tasks:In thecontextwherewehavemanytasks, correspondingtodiÔ¨Äerentyivariablessharingthesameinput xorwhere eachtaskisassociatedwithasubsetorafunctionf( ) i(x)ofaglobalinput x,theassumptionisthateachyiisassociatedwithadiÔ¨Äerentsubsetfroma commonpoolofrelevantfactors h.Becausethesesubsetsoverlap,learning alltheP(yi|x)viaasharedintermediate representationP(h x|)allows sharingofstatisticalstrengthbetweenthetasks. ‚Ä¢Manifolds:Probabilitymassconcentrates,andtheregionsinwhichitcon- centratesarelocallyconnectedandoccupyatinyvolume.Inthecontinuous case,theseregionscanbeapproximatedbylow-dimensional manifoldswith amuchsmallerdimensionalitythantheoriginalspacewherethedatalives. Manymachinelearningalgorithmsbehavesensiblyonlyonthismanifold ( ,).Somemachinelearningalgorithms,especially Goodfellow etal.2014b autoencoders,attempttoexplicitlylearnthestructureofthemanifold. ‚Ä¢Naturalclustering:Manymachinelearningalgorithmsassumethateach connectedmanifoldintheinputspacemaybeassignedtoasingleclass.The datamaylieonmanydisconnectedmanifolds,buttheclassremainsconstant withineachoneofthese. Thisassumptionmotivatesavarietyoflearning algorithms,includingtangentpropagation, doublebackprop,themanifold tangentclassiÔ¨Åerandadversarialtraining. ‚Ä¢Temporalandspatialcoherence:Slowfeatureanalysisandrelatedalgorithms maketheassumptionthatthemostimportantexplanatoryfactorschange slowlyovertime,oratleastthatitiseasiertopredictthetrueunderlying explanatoryfactorsthantopredictrawobservationssuchaspixelvalues. Seesectionforfurtherdescriptionofthisapproach. 13.3 ‚Ä¢Sparsity:Mostfeaturesshouldpresumablynotberelevanttodescribingmost inputs‚Äîthereisnoneedtouseafeaturethatdetectselephanttrunkswhen representinganimageofacat.Itisthereforereasonabletoimposeaprior thatanyfeaturethatcanbeinterpretedas‚Äúpresent‚Äùor‚Äúabsent‚Äùshouldbe absentmostofthetime. ‚Ä¢SimplicityofFactorDependencies:Ingoodhigh-levelrepresentations,the factorsarerelatedtoeachotherthroughsimpledependencies.Thesimplest 5 5 6 CHAPTER15.REPRESENTATIONLEARNING possibleismarginalindependence,P(h) =ÓÅë iP(h i),butlineardependencies orthosecapturedbyashallowautoencoderarealsoreasonableassumptions. Thiscanbeseeninmanylawsofphysics,andisassumedwhenplugginga linearpredictororafactorizedpriorontopofalearnedrepresentation. Theconceptofrepresentationlearningtiestogetherallofthemanyforms ofdeeplearning.Feedforwardandrecurrentnetworks,autoencodersanddeep probabilisticmodelsalllearnandexploitrepresentations.Learning thebest possiblerepresentationremainsanexcitingavenueofresearch. 5 5 7 C h a p t e r 1 6 S t ru ct u r e d Probabilis t i c Mo d e l s f or D e e p L e ar n i n g Deeplearningdrawsuponmanymodelingformalismsthatresearcherscanuseto guidetheirdesigneÔ¨Äortsanddescribetheiralgorithms.Oneoftheseformalisms istheideaofstructuredprobabilisticmodels.Wehavealreadydiscussed structuredprobabilisticmodelsbrieÔ¨Çyinsection.Thatbriefpresentationwas 3.14 suÔ¨Écienttounderstandhowtousestructuredprobabilisticmodelsasalanguageto describesomeofthealgorithmsinpart.Now,inpart,structuredprobabilistic II III modelsareakeyingredientofmanyofthemostimportantresearchtopicsindeep</div>
        </div>
    </div>

    <div class="question-card" id="q143">
        <div class="question-header">
            <span class="question-number">Question 143</span>
            <span class="question-type">multiple-choice</span>
        </div>

        <div class="question-text">Machine learning algorithms can be categorized based on their approach to learning from data, with supervised and unsupervised methods offering distinct solutions to predictive and pattern discovery tasks. Probabilistic concepts, such as the chain rule, play a foundational role in connecting these approaches, especially in generative modeling.

Which statement correctly explains how the chain rule of probability enables the decomposition of unsupervised learning problems into supervised tasks?

1) The chain rule allows supervised models to be trained without access to any labeled data.   
2) By using the chain rule, clustering algorithms directly predict target labels for each datapoint.   
3) The chain rule discards input features and focuses only on modeling outputs.   
4) The chain rule enables the modeling of the joint distribution of data (p(x)) by expressing it as a product of conditional distributions (e.g., p(x1|x2)..), each of which can be approached as a supervised learning task.   
5) The chain rule requires all examples in the dataset to belong to a single class.   
6) Applying the chain rule transforms unsupervised problems into reinforcement learning tasks.   
7) The chain rule eliminates the need for feature selection in any machine learning algorithm.</div>

        <div class="answer-section">
            <div class="answer-label">‚úì Correct Answer:</div>
            <div class="answer-text">The correct answer is 4) The chain rule enables the modeling of the joint distribution of data (p(x)) by expressing it as a product of conditional distributions (e.g., p(x1|x2)..), each of which can be approached as a supervised learning task..</div>
        </div>

        <div class="reference-section">
            <div class="reference-label">üìö Reference Text:</div>
            <button class="toggle-button" onclick="toggleReference(143)">
                Show/Hide Reference
            </button>
            <div id="ref143" class="reference-text hidden">Machinelearningalgorithmscanbebroadlycategorizedasunsupervisedor supervisedbywhatkindofexperiencetheyareallowedtohaveduringthe learningprocess. Mostofthelearningalgorithmsinthisbookcanbeunderstoodasbeingallowed toexperienceanentiredataset.Adatasetisacollectionofmanyexamples,as 1 0 4 CHAPTER5.MACHINELEARNINGBASICS deÔ¨Ånedinsection.Sometimeswewillalsocallexamples . 5.1.1 datapoints Oneoftheoldestdatasetsstudiedbystatisticiansandmachinelearningre- searchersistheIrisdataset(,).Itisacollectionofmeasurementsof Fisher1936 diÔ¨Äerentpartsof150irisplants.Eachindividualplantcorrespondstooneexample. Thefeatureswithineachexamplearethemeasurementsofeachofthepartsofthe plant:thesepallength,sepalwidth,petallengthandpetalwidth.Thedataset alsorecordswhichspecieseachplantbelongedto.ThreediÔ¨Äerentspeciesare representedinthedataset. Unsupervisedlearningalgorithmsexperienceadatasetcontainingmany features,thenlearnusefulpropertiesofthestructureofthisdataset.Inthecontext ofdeeplearning,weusuallywanttolearntheentireprobabilitydistributionthat generatedadataset,whetherexplicitlyasindensityestimationorimplicitlyfor taskslikesynthesisordenoising.Someotherunsupervisedlearningalgorithms performotherroles,likeclustering,whichconsistsofdividingthedatasetinto clustersofsimilarexamples. Supervisedlearningalgorithmsexperienceadatasetcontainingfeatures, buteachexampleisalsoassociatedwithalabelortarget.Forexample,theIris datasetisannotatedwiththespeciesofeachirisplant.Asupervisedlearning algorithmcanstudytheIrisdatasetandlearntoclassifyirisplantsintothree diÔ¨Äerentspeciesbasedontheirmeasurements. Roughlyspeaking,unsupervisedlearninginvolvesobservingseveralexamples ofarandomvector x,andattemptingtoimplicitlyorexplicitlylearntheproba- bilitydistributionp( x),orsomeinterestingpropertiesofthatdistribution,while supervisedlearninginvolvesobservingseveralexamplesofarandomvector xand anassociatedvalueorvector y,andlearningtopredict yfrom x,usuallyby estimatingp( y x|).Thetermsupervisedlearningoriginatesfromtheviewof thetarget ybeingprovidedbyaninstructororteacherwhoshowsthemachine learningsystemwhattodo.Inunsupervisedlearning,thereisnoinstructoror teacher,andthealgorithmmustlearntomakesenseofthedatawithoutthisguide. UnsupervisedlearningandsupervisedlearningarenotformallydeÔ¨Ånedterms. Thelinesbetweenthemareoftenblurred.Manymachinelearningtechnologiescan beusedtoperformbothtasks.Forexample,thechainruleofprobabilitystates thatforavector x‚àà Rn,thejointdistributioncanbedecomposedas p() = xnÓÅô i=1p(x i|x1,...,x i ‚àí1). (5.1) Thisdecompositionmeansthatwecansolvetheostensiblyunsupervisedproblemof modelingp( x) bysplittingitintonsupervisedlearningproblems.Alternatively,we 1 0 5</div>
        </div>
    </div>

    <div class="question-card" id="q144">
        <div class="question-header">
            <span class="question-number">Question 144</span>
            <span class="question-type">multiple-choice</span>
        </div>

        <div class="question-text">Mixture density networks (MDNs) are specialized neural architectures used in generative modeling to capture multimodal output distributions. Activation functions in deep neural networks critically affect optimization and model expressiveness.

Which mechanism ensures that the mixing coefficients produced by a mixture density network properly represent probabilities for each component?

1) Application of the softmax function to mixing coefficients   
2) Clipping mixing coefficients to a fixed range   
3) Normalizing means across all components   
4) Using diagonal covariances for computational efficiency   
5) Scaling mixing coefficients by the largest value   
6) Applying ReLU activation to mixing coefficients   
7) Assigning equal values to all mixing coefficients</div>

        <div class="answer-section">
            <div class="answer-label">‚úì Correct Answer:</div>
            <div class="answer-text">The correct answer is 1) Application of the softmax function to mixing coefficients.</div>
        </div>

        <div class="reference-section">
            <div class="reference-label">üìö Reference Text:</div>
            <button class="toggle-button" onclick="toggleReference(144)">
                Show/Hide Reference
            </button>
            <div id="ref144" class="reference-text hidden">e d b y p i c k i n g o n e o f t h e m , a n d m a k e t h a t u n o b s e rv e d c h o i c e a ra n d o m v a ria b l e . 1 8 9 CHAPTER6.DEEPFEEDFORWARDNETWORKS typicallybeobtainedbyasoftmaxoveran n-dimensionalvector,toguarantee thattheseoutputsarepositiveandsumto1. 2.Means¬µ( ) i(x):theseindicatethecenterormeanassociatedwiththe i-th Gaussiancomponent,andareunconstrained(typicallywithnononlinearity atallfortheseoutputunits).If yisa d-vector,thenthenetworkmustoutput an n d√ómatrixcontainingall nofthese d-dimensionalvectors. Learning thesemeanswithmaximumlikelihoodisslightlymorecomplicatedthan learningthemeansofadistributionwithonlyoneoutputmode.Weonly wanttoupdatethemeanforthecomponentthatactuallyproducedthe observation.Inpractice,wedonotknowwhichcomponentproducedeach observation.Theexpressionforthenegativelog-likelihoodnaturallyweights eachexample‚Äôscontributiontothelossforeachcomponentbytheprobability thatthecomponentproducedtheexample. 3.Covariances Œ£( ) i(x):thesespecifythecovariancematrixforeachcomponent i.AswhenlearningasingleGaussiancomponent,wetypicallyuseadiagonal matrixtoavoidneedingtocomputedeterminants. Aswithlearningthemeans ofthemixture,maximumlikelihoodiscomplicatedbyneedingtoassign partialresponsibilityforeachpointtoeachmixturecomponent.Gradient descentwillautomatically followthecorrectprocessifgiventhecorrect speciÔ¨Åcationofthenegativelog-likelihoodunderthemixturemodel. Ithasbeenreportedthatgradient-basedoptimization ofconditionalGaussian mixtures(ontheoutputofneuralnetworks)canbeunreliable,inpartbecauseone getsdivisions(bythevariance)whichcanbenumericallyunstable(whensome variancegetstobesmallforaparticularexample,yieldingverylargegradients). Onesolutionistoclipgradients(seesection)whileanotheristoscale 10.11.1 thegradientsheuristically( ,). MurrayandLarochelle2014 GaussianmixtureoutputsareparticularlyeÔ¨Äectiveingenerativemodelsof speech(Schuster1999,)ormovementsofphysicalobjects(Graves2013,).The mixturedensitystrategygivesawayforthenetworktorepresentmultipleoutput modesandtocontrolthevarianceofitsoutput,whichiscrucialforobtaining ahighdegreeofqualityinthesereal-valueddomains.Anexampleofamixture densitynetworkisshowninÔ¨Ågure.6.4 Ingeneral,wemaywishtocontinuetomodellargervectorsycontainingmore variables,andtoimposericherandricherstructuresontheseoutputvariables.For example,wemaywishforourneuralnetworktooutputasequenceofcharacters thatformsasentence.Inthese cases,wemaycontinuetousetheprinciple ofmaximumlikelihoodappliedtoourmodel p(y;œâ(x)),butthemodelweuse 1 9 0 CHAPTER6.DEEPFEEDFORWARDNETWORKS xy Figure6.4:Samplesdrawnfromaneuralnetworkwithamixturedensityoutputlayer. Theinput xissampledfromauniformdistributionandtheoutput yissampledfrom p m o d e l( y x|).Theneuralnetworkisabletolearnnonlinearmappingsfromtheinputto theparametersoftheoutputdistribution.Theseparametersincludetheprobabilities governingwhichofthreemixturecomponentswillgeneratetheoutputaswellasthe parametersforeachmixturecomponent.EachmixturecomponentisGaussianwith predictedmeanandvariance.Alloftheseaspectsoftheoutputdistributionareableto varywithrespecttotheinput,andtodosoinnonlinearways. x todescribeybecomescomplexenoughtobebeyondthescopeofthischapter. ChapterdescribeshowtouserecurrentneuralnetworkstodeÔ¨Ånesuchmodels 10 oversequences,andpartdescribesadvancedtechniquesformodelingarbitrary III probabilitydistributions. 6. 3 Hi d d en Un i t s Sofarwehavefocusedourdiscussionondesignchoicesforneuralnetworksthat arecommontomostparametricmachinelearningmodelstrainedwithgradient- basedoptimization. Nowweturntoanissuethatisuniquetofeedforwardneural networks:howtochoosethetypeofhiddenunittouseinthehiddenlayersofthe model. Thedesignofhiddenunitsisanextremelyactiveareaofresearchanddoesnot yethavemanydeÔ¨Ånitiveguidingtheoreticalprinciples. RectiÔ¨Åedlinearunitsareanexcellentdefaultchoiceofhiddenunit.Manyother typesofhiddenunitsareavailable.ItcanbediÔ¨Éculttodeterminewhentouse whichkind(thoughrectiÔ¨Åedlinearunitsareusuallyanacceptablechoice). We 1 9 1 CHAPTER6.DEEPFEEDFORWARDNETWORKS describeheresomeofthebasicintuitionsmotivatingeachtypeofhiddenunits. Theseintuitionscanhelpdecidewhentotryouteachoftheseunits.Itisusually impossibletopredictinadvancewhichwillworkbest.Thedesignprocessconsists oftrialanderror,intuitingthatakindofhiddenunitmayworkwell,andthen traininganetworkwiththatkindofhiddenunitandevaluatingitsperformance onavalidationset. SomeofthehiddenunitsincludedinthislistarenotactuallydiÔ¨Äerentiableat allinputpoints.Forexample,therectiÔ¨Åedlinearfunction g( z) =max{0 , z}isnot diÔ¨Äerentiableat z= 0.Thismayseemlikeitinvalidates gforusewithagradient- basedlearningalgorithm.Inpractice,gradientdescentstillperformswellenough forthesemodelstobeusedformachinelearningtasks. Thisisinpartbecause neuralnetworktrainingalgorithmsdonotusuallyarriveatalocalminimumof thecostfunction,butinsteadmerelyreduceitsvaluesigniÔ¨Åcantly,asshownin Ô¨Ågure.Theseideaswillbedescribedfurtherinchapter.Becausewedonot 4.3 8 expecttrainingtoactuallyreachapointwherethegradientis 0,itisacceptable fortheminimaofthecostfunctiontocorrespondtopointswithundeÔ¨Ånedgradient. HiddenunitsthatarenotdiÔ¨Äerentiableareusuallynon-diÔ¨Äerentiable atonlya smallnumberofpoints.Ingeneral,afunction g( z)hasaleftderivativedeÔ¨Åned bytheslopeofthefunctionimmediately totheleftof zandarightderivative deÔ¨Ånedbytheslopeofthefunctionimmediately totherightof z.Afunction isdiÔ¨Äerentiableat zonlyifboththeleftderivativeandtherightderivativeare deÔ¨Ånedandequaltoeachother.Thefunctionsusedinthecontextofneural networksusuallyhavedeÔ¨ÅnedleftderivativesanddeÔ¨Ånedrightderivatives.Inthe caseof g( z) =max{0 , z},theleftderivativeat z= 00isandtherightderivative is.Softwareimplementations ofneuralnetworktrainingusuallyreturnoneof 1 theone-sidedderivativesratherthanreportingthatthederivativeisundeÔ¨Ånedor raisinganerror. ThismaybeheuristicallyjustiÔ¨Åedbyobservingthatgradient- basedoptimization onadigitalcomputerissubjecttonumericalerroranyway. Whenafunctionisaskedtoevaluate g(0),itisveryunlikelythattheunderlying valuetrulywas.Instead,itwaslikelytobesomesmallvalue 0 ÓÄèthatwasrounded to.Insomecontexts,moretheoreticallypleasingjustiÔ¨Åcationsareavailable,but 0 theseusuallydonotapplytoneuralnetworktraining.Theimportantpointisthat inpracticeonecansafelydisregardthenon-diÔ¨Äerentiabilityofthehiddenunit activationfunctionsdescribedbelow. Unlessindicatedotherwise,mosthiddenunitscanbedescribedasaccepting avectorofinputsx,computinganaÔ¨Énetransformationz=WÓÄæx+b,and thenapplyinganelement-wisenonlinearfunction g(z).Mosthiddenunitsare distinguishedfromeachotheronlybythechoiceoftheformoftheactivation function. g()z 1 9 2 CHAPTER6.DEEPFEEDFORWARDNETWORKS 6.3.1RectiÔ¨ÅedLinearUnitsandTheirGeneralizations RectiÔ¨Åedlinearunitsusetheactivationfunction . g z , z () = max0{} RectiÔ¨Åedlinearunitsareeasytooptimizebecausetheyaresosimilartolinear units.TheonlydiÔ¨ÄerencebetweenalinearunitandarectiÔ¨Åedlinearunitis thatarectiÔ¨Åedlinearunitoutputszeroacrosshalfitsdomain. This makesthe derivativesthrougharectiÔ¨Åedlinearunitremainlargewhenevertheunitisactive. Thegradientsarenotonlylargebutalsoconsistent.Thesecondderivativeofthe rectifyingoperationisalmosteverywhere,andthederivativeoftherectifying 0 operationiseverywherethattheunitisactive.Thismeansthatthegradient 1 directionisfarmoreusefulforlearningthanitwouldbewithactivationfunctions thatintroducesecond-ordereÔ¨Äects. RectiÔ¨ÅedlinearunitsaretypicallyusedontopofanaÔ¨Énetransformation: hW= ( gÓÄæxb+) . (6.36) WheninitializingtheparametersoftheaÔ¨Énetransformation,itcanbeagood practicetosetallelementsofbtoasmall,positivevalue,suchas0 .1.Thismakes itverylikelythattherectiÔ¨Åedlinearunitswillbeinitiallyactiveformostinputs inthetrainingsetandallowthederivativestopassthrough. Severalgeneralizations ofrectiÔ¨Åedlinearunitsexist.Mostofthesegeneral- izationsperformcomparablytorectiÔ¨Åedlinearunitsandoccasionallyperform better. OnedrawbacktorectiÔ¨Åedlinearunitsisthattheycannotlearnviagradient- based methods onexamples for which their activ ation iszero.Avariety of generalizations ofrectiÔ¨Åedlinearunitsguaranteethattheyreceivegradientevery- where. Threegeneralizations ofrectiÔ¨Åedlinearunitsarebasedonusinganon-zero slope Œ± iwhen z i <0: h i= g(zŒ± ,) i=max(0 , z i)+ Œ± imin(0 , z i).Absolutevalue rectiÔ¨ÅcationÔ¨Åxes Œ± i=‚àí1toobtain g( z) =|| z.Itisusedforobjectrecognition fromimages( ,),whereitmakessensetoseekfeaturesthatare Jarrett e t a l .2009 invariantunderapolarityreversaloftheinputillumination. Othergeneralizations ofrectiÔ¨Åedlinearunitsaremorebroadlyapplicable.AleakyReLU(,Maas e t a l . 2013)Ô¨Åxes Œ± itoasmallvaluelike0.01whileaparametricReLUorPReLU treats Œ± iasalearnableparameter(,). He e t a l .2015 Maxoutunits( ,)generalizerectiÔ¨Åedlinearunits Goodfellow e t a l .2013a further.Insteadofapplyinganelement-wisefunction g( z),maxoutunitsdividez intogroupsof kvalues.Eachmaxoutunitthenoutputsthemaximumelementof 1 9 3 CHAPTER6.DEEPFEEDFORWARDNETWORKS oneofthesegroups: g()z i=max j‚àà G() iz j (6.37) where G( ) iisthesetofindicesintotheinputsforgroup i,{( i‚àí1) k+1 , . . . , i k}. Thisprovidesawayoflearningapiecewiselinearfunctionthatrespondstomultiple directionsintheinputspace.x Amaxoutunitcanlearnapiecewiselinear,convexfunctionwithupto kpieces. Maxoutunitscanthusbeseenas l e a r ning t h e a c t i v a t i o n f u nc t i o nitselfrather thanjusttherelationshipbetweenunits.Withlargeenough k,amaxoutunitcan learntoapproximateanyconvexfunctionwitharbitraryÔ¨Ådelity.Inparticular, amaxoutlayerwithtwopiecescanlearntoimplementthesamefunctionofthe inputxasatraditionallayerusingtherectiÔ¨Åedlinearactivationfunction,absolute valuerectiÔ¨Åcationfunction,ortheleakyorparametricReLU,orcanlearnto implementatotallydiÔ¨Äerentfunctionaltogether.Themaxoutlayerwillofcourse beparametrized diÔ¨Äerentlyfromanyoftheseotherlayertypes,sothelearning dynamicswillbediÔ¨Äerenteveninthecaseswheremaxoutlearnstoimplementthe samefunctionofasoneoftheotherlayertypes. x Eachmaxoutunitisnowparametrized by kweightvectorsinsteadofjustone, somaxoutunitstypicallyneedmoreregularizationthanrectiÔ¨Åedlinearunits.They canworkwellwithoutregularizationifthetrainingsetislargeandthenumberof piecesperunitiskeptlow(,). Cai e t a l .2013 MaxoutunitshaveafewotherbeneÔ¨Åts.Insomecases,onecangainsomesta- tisticalandcomputational advantagesbyrequiringfewerparameters.SpeciÔ¨Åcally, ifthefeaturescapturedby ndiÔ¨ÄerentlinearÔ¨Ålterscanbesummarizedwithout losinginformationbytakingthemaxovereachgroupof kfeatures,thenthenext layercangetbywithtimesfewerweights. k BecauseeachunitisdrivenbymultipleÔ¨Ålters,maxoutunitshavesomeredun- dancythathelpsthemtoresistaphenomenon calledcatastrophicforgetting inwhichneuralnetworksforgethowtoperformtasksthattheyweretrainedonin thepast( ,). Goodfellow e t a l .2014a RectiÔ¨Åedlinearunitsandallofthesegeneralizations ofthemarebasedonthe principlethatmodelsareeasiertooptimizeiftheirbehaviorisclosertolinear. Thissamegeneralprincipleofusinglinearbehaviortoobtaineasieroptimization alsoappliesinothercontextsbesidesdeeplinearnetworks.Recurrentnetworkscan learnfromsequencesandproduceasequenceofstatesandoutputs.Whentraining them,oneneedstopropagateinformationthroughseveraltimesteps,whichismuch easierwhensomelinearcomputations (withsomedirectionalderivativesbeingof magnitudenear1)areinvolved.Oneofthebest-performingrecurrentnetwork 1 9 4 CHAPTER6.DEEPFEEDFORWARDNETWORKS architectures,theLSTM,propagatesinformationthroughtimeviasummation‚Äîa particularstraightforwardkindofsuchlinearactivation.Thisisdiscussedfurther insection.10.10 6.3.2LogisticSigmoidandHyperbolicTangent Priortotheintroduction ofrectiÔ¨Åedlinearunits,mostneuralnetworksusedthe</div>
        </div>
    </div>

    <div class="question-card" id="q145">
        <div class="question-header">
            <span class="question-number">Question 145</span>
            <span class="question-type">multiple-choice</span>
        </div>

        <div class="question-text">Neural networks are a foundational approach in machine learning, with significant developments occurring since the 1980s. Training techniques and core architectures have evolved over time, shaping the field of deep learning.

Which technique remains the primary method for adjusting the weights in feedforward neural networks, forming the basis for modern deep learning frameworks?

1) Stochastic gradient boosting   
2) Back-propagation   
3) Genetic algorithms   
4) K-means clustering   
5) Bayesian inference   
6) Principal component analysis   
7) Expectation-maximization</div>

        <div class="answer-section">
            <div class="answer-label">‚úì Correct Answer:</div>
            <div class="answer-text">The correct answer is 2) Back-propagation.</div>
        </div>

        <div class="reference-section">
            <div class="reference-label">üìö Reference Text:</div>
            <button class="toggle-button" onclick="toggleReference(145)">
                Show/Hide Reference
            </button>
            <div id="ref145" class="reference-text hidden">e t a l .,).1986 Followingthesuccessofback-propagatio n,neuralnetworkresearchgainedpop- ularityandreachedapeakintheearly1990s.Afterwards,othermachinelearning techniquesbecamemorepopularuntilthemoderndeeplearningrenaissancethat beganin2006. Thecoreideasbehindmodernfeedforwardnetworkshavenotchangedsub- stantiallysincethe1980s. Thesameback-propagationalgorithmandthesame 2 2 5</div>
        </div>
    </div>

    <div class="question-card" id="q146">
        <div class="question-header">
            <span class="question-number">Question 146</span>
            <span class="question-type">multiple-choice</span>
        </div>

        <div class="question-text">Undirected probabilistic graphical models, such as Markov random fields (MRFs), are widely used in machine learning to capture structured relationships where variable interactions are bidirectional or lack clear causality. These models are defined using factors associated with cliques in the graph and often require complex inference techniques.

Which of the following is a primary reason why computing the partition function in undirected probabilistic models can be intractable for large systems?

1) The presence of cycles in the graphical structure prevents factorization.   
2) Directed edges introduce non-trivial conditional dependencies.   
3) Only discrete variable domains are supported, leading to combinatorial explosion.   
4) Clique potentials must be normalized individually before use.   
5) Factors encode asymmetric relationships, complicating summation.   
6) The partition function requires summing or integrating over all possible states, which grows exponentially with the number of variables.   
7) The lack of causal directionality restricts the use of efficient inference algorithms.</div>

        <div class="answer-section">
            <div class="answer-label">‚úì Correct Answer:</div>
            <div class="answer-text">The correct answer is 6) The partition function requires summing or integrating over all possible states, which grows exponentially with the number of variables..</div>
        </div>

        <div class="reference-section">
            <div class="reference-label">üìö Reference Text:</div>
            <button class="toggle-button" onclick="toggleReference(146)">
                Show/Hide Reference
            </button>
            <div id="ref146" class="reference-text hidden">CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING ourconditionaldistributions.ItonlydeÔ¨Åneswhichvariablestheyareallowedto takeinasarguments. 1 6 . 2 . 2 Un d i rec t ed Mo d el s Directedgraphicalmodelsgiveusonelanguagefordescribingstructuredprobabilis- ticmodels.Anotherpopularlanguageisthatofundirectedmodels,otherwise knownasMarkovrandomÔ¨Åelds(MRFs)orMarkovnetworks(Kinder- mann1980,).Astheirnameimplies,undirectedmodelsusegraphswhoseedges areundirected. Directedmodelsaremostnaturallyapplicabletosituationswherethereis aclearreasontodraweacharrowinoneparticulardirection.Oftentheseare situationswhereweunderstandthecausalityandthecausalityonlyÔ¨Çowsinone direction.Onesuchsituationistherelayraceexample.EarlierrunnersaÔ¨Äectthe Ô¨Ånishingtimesoflaterrunners;laterrunnersdonotaÔ¨ÄecttheÔ¨Ånishingtimesof earlierrunners. Notallsituationswemightwanttomodelhavesuchacleardirectiontotheir interactions.Whentheinteractionsseemtohavenointrinsicdirection,orto operateinbothdirections,itmaybemoreappropriatetouseanundirectedmodel. Asanexampleofsuchasituation,supposewewanttomodeladistribution overthreebinaryvariables:whetherornotyouaresick,whetherornotyour coworkerissick,andwhetherornotyourroommateissick.Asintherelayrace example,wecanmakesimplifyingassumptionsaboutthekindsofinteractionsthat takeplace.Assumingthatyourcoworkerandyourroommatedonotknoweach other,itisveryunlikelythatoneofthemwillgivetheotheraninfectionsuchasa colddirectly.Thiseventcanbeseenassorarethatitisacceptablenottomodel it.However,itisreasonablylikelythateitherofthemcouldgiveyouacold,and thatyoucouldpassitontotheother.Wecanmodeltheindirecttransmissionof acoldfromyourcoworkertoyourroommatebymodelingthetransmissionofthe coldfromyourcoworkertoyouandthetransmissionofthecoldfromyoutoyour roommate. Inthiscase,itisjustaseasyforyoutocauseyourroommatetogetsickas itisforyourroommatetomakeyousick,sothereisnotaclean,uni-directional narrativeonwhichtobasethemodel.Thismotivatesusinganundirectedmodel. Aswithdirectedmodels,iftwonodesinanundirectedmodelareconnectedbyan edge,thentherandomvariablescorrespondingtothosenodesinteractwitheach otherdirectly.Unlikedirectedmodels,theedgeinanundirectedmodelhasno arrow,andisnotassociatedwithaconditionalprobabilitydistribution. 5 6 6 CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING h r h r h y h y h c h c Figure16.3:Anundirectedgraphrepresentinghowyourroommate‚Äôshealthh r,your healthh y,andyourworkcolleague‚Äôs healthh caÔ¨Äecteachother.Youandyourroommate mightinfecteachotherwithacold,andyouandyourworkcolleaguemightdothesame, butassumingthatyourroommateandyourcolleaguedonotknoweachother,theycan onlyinfecteachotherindirectlyviayou. Wedenotetherandomvariablerepresentingyourhealthash y,therandom variablerepresentingyourroommate‚Äôshealthash r,andtherandomvariable representingyourcolleague‚Äôshealthash c.SeeÔ¨Ågureforadrawingofthe 16.3 graphrepresentingthisscenario. Formally,anundirectedgraphicalmodelisastructuredprobabilisticmodel deÔ¨Ånedonanundirectedgraph G.Foreachclique Cinthegraph,3afactor œÜ(C) (alsocalledacliquepotential) measurestheaÔ¨Énityofthevariablesinthatclique forbeingineachoftheirpossiblejointstates.Thefactorsareconstrainedtobe non-negative.TogethertheydeÔ¨Åneanunnormalizedprobabilitydistribution Àú p() = Œ†x C‚ààG œÜ .()C (16.3) Theunnormalized probabilitydistributioniseÔ¨Écienttoworkwithsolongas allthecliquesaresmall.ItencodestheideathatstateswithhigheraÔ¨Énityare morelikely.However,unlikeinaBayesiannetwork,thereislittlestructuretothe deÔ¨Ånitionofthecliques,sothereisnothingtoguaranteethatmultiplyingthem togetherwillyieldavalidprobabilitydistribution.SeeÔ¨Ågureforanexample 16.4 ofreadingfactorizationinformationfromanundirectedgraph. Ourexampleofthecoldspreadingbetweenyou,yourroommate,andyour colleaguecontainstwocliques.Onecliquecontainsh yandh c.Thefactorforthis cliquecanbedeÔ¨Ånedbyatable,andmighthavevaluesresemblingthese: h y= 0h y= 1 h c= 021 h c= 1110 3A c l i q u e o f t h e g ra p h i s a s u b s e t o f n o d e s t h a t a re a l l c o n n e c t e d t o e a c h o t h e r b y a n e d g e o f t h e g ra p h . 5 6 7 CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING Astateof1indicatesgoodhealth,whileastateof0indicatespoorhealth (havingbeen infectedwith acold).Both ofyou areusuallyhealthy, sothe correspondingstatehasthehighestaÔ¨Énity.Thestatewhereonlyoneofyouis sickhasthelowestaÔ¨Énity,becausethisisararestate.Thestatewherebothof youaresick(becauseoneofyouhasinfectedtheother)isahigheraÔ¨Énitystate, thoughstillnotascommonasthestatewherebotharehealthy. Tocompletethemodel,wewouldneedtoalsodeÔ¨Åneasimilarfactorforthe cliquecontainingh yandh r. 1 6 . 2 . 3 T h e P a rt i t i o n F u n ct i o n Whiletheunnormalized probabilitydistributionisguaranteedtobenon-negative everywhere,itisnotguaranteedtosumorintegrateto1.Toobtainavalid probabilitydistribution,wemustusethecorrespondingnormalizedprobability distribution:4 p() =x1 ZÀú p()x (16.4) where Zisthevalue thatresultsintheprobability distributionsummingor integratingto1: Z=ÓÅö Àú p d . ()xx (16.5) Youcanthinkof Zasaconstantwhenthe œÜfunctionsareheldconstant.Note thatifthe œÜfunctionshaveparameters,then Zisafunctionofthoseparameters. Itiscommonintheliteraturetowrite Zwithitsargumentsomittedtosavespace. Thenormalizingconstant Zisknownasthepartitionfunction,atermborrowed fromstatisticalphysics. Since Zisanintegralorsumoverallpossiblejointassignmentsofthestatex itisoftenintractabletocompute. Inordertobeabletoobtainthenormalized probabilitydistributionofanundirectedmodel, themodelstructureandthe deÔ¨Ånitionsofthe œÜfunctionsmustbeconducivetocomputing ZeÔ¨Éciently.In thecontextofdeeplearning, Zisusuallyintractable. Due totheintractability ofcomputing Zexactly,wemustresorttoapproximations .Suchapproximate algorithmsarethetopicofchapter.18 Oneimportantconsiderationtokeepinmindwhendesigningundirectedmodels isthatitispossibletospecifythefactorsinsuchawaythat Zdoesnotexist. Thishappensifsomeofthevariablesinthemodelarecontinuousandtheintegral 4A d i s t rib u t i o n d e Ô¨Å n e d b y n o rm a l i z i n g a p ro d u c t o f c l i q u e p o t e n t i a l s i s a l s o c a l l e d a Gib b s d is t rib u t i on . 5 6 8 CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING ofÀú povertheirdomaindiverges.Forexample,supposewewanttomodelasingle scalarvariablexwithasinglecliquepotential ‚àà R œÜ x x () = 2.Inthiscase, Z=ÓÅö x2d x . (16.6) Sincethisintegraldiverges,thereisnoprobabilitydistributioncorrespondingto thischoiceof œÜ( x). Sometimes thechoiceofsomeparameterofthe œÜfunctions determineswhetherthe probabilit ydistribution isdeÔ¨Åned.For example, for œÜ( x; Œ≤) =expÓÄÄ‚àí Œ≤ x2ÓÄÅ ,the Œ≤parameterdetermineswhether Zexists.Positive Œ≤ resultsinaGaussiandistributionoverxbutallothervaluesof Œ≤make œÜimpossible tonormalize. OnekeydiÔ¨Äerencebetweendirectedmodelingandundirectedmodelingisthat directedmodelsaredeÔ¨Åneddirectlyintermsofprobabilitydistributionsfrom thestart,whileundirectedmodelsaredeÔ¨Ånedmorelooselyby œÜfunctionsthat arethenconvertedintoprobabilitydistributions.Thischangestheintuitionsone mustdevelopinordertoworkwiththesemodels.Onekeyideatokeepinmind whileworkingwithundirectedmodelsisthatthedomainofeachofthevariables hasdramaticeÔ¨Äectonthekindofprobabilitydistributionthatagivensetof œÜ functionscorrespondsto.Forexample,consideran n-dimensionalvector-valued randomvariable xandanundirectedmodelparametrized byavectorofbiases b.Supposewehaveonecliqueforeachelementofx, œÜ( ) i(x i) =exp( b ix i).What kindofprobabilitydistributiondoesthisresultin?Theansweristhatwedo nothaveenoughinformation,becausewehavenotyetspeciÔ¨Åedthedomainofx. Ifx ‚àà Rn,thentheintegraldeÔ¨Åning Zdivergesandnoprobabilitydistribution exists.Ifx‚àà{0 ,1}n,then p(x)factorizesinto nindependentdistributions,with p(x i= 1) =sigmoid ( b i).Ifthedomainofxisthesetofelementarybasisvectors ({[1 ,0 , . . . ,0] ,[0 ,1 , . . . ,0] , . . . ,[0 ,0 , . . . ,1]})then p(x)=softmax ( b),soalarge valueof b iactuallyreduces p(x</div>
        </div>
    </div>

    <div class="question-card" id="q147">
        <div class="question-header">
            <span class="question-number">Question 147</span>
            <span class="question-type">multiple-choice</span>
        </div>

        <div class="question-text">Autoencoders are neural network models commonly used for unsupervised representation learning, dimensionality reduction, and generative modeling. Their effectiveness depends on design choices such as priors, activation functions, regularization techniques, and architectural depth.

Which combination of techniques is most effective for inducing truly sparse latent representations in autoencoders?

1) Using sigmoid activations and L2 regularization on latent variables   
2) Applying tanh activation with a Gaussian prior on the latent space   
3) Introducing noise to the input without imposing any penalty on the latent variables   
4) Employing ReLU activations together with a Laplace prior on latent variables   
5) Stacking shallow autoencoders with cross-entropy loss   
6) Using softmax output units and contractive regularization   
7) Training with mean squared error loss and deterministic decoders</div>

        <div class="answer-section">
            <div class="answer-label">‚úì Correct Answer:</div>
            <div class="answer-text">The correct answer is 4) Employing ReLU activations together with a Laplace prior on latent variables.</div>
        </div>

        <div class="reference-section">
            <div class="reference-label">üìö Reference Text:</div>
            <button class="toggle-button" onclick="toggleReference(147)">
                Show/Hide Reference
            </button>
            <div id="ref147" class="reference-text hidden">CHAPTER14.AUTOENCODERS maximumlikelihood trainingofagenerativemodel thathaslatentvariables. Supposewehaveamodelwithvisiblevariables xandlatentvariables h,with anexplicitjointdistribution pmodel( x h ,)= pmodel( h) pmodel( x h|).Wereferto pmodel( h)asthemodel‚Äôspriordistributionoverthelatentvariables,representing themodel‚Äôsbeliefspriortoseeing x.ThisisdiÔ¨Äerentfromthewaywehave previouslyusedtheword‚Äúprior,‚Äùtorefertothedistribution p( Œ∏)encodingour beliefsaboutthemodel‚Äôsparametersbeforewehaveseenthetrainingdata.The log-likelihoodcanbedecomposedas log pmodel() = log xÓÅò hpmodel( ) h x , . (14.3) Wecanthinkoftheautoencoderasapproximatingthissumwithapointestimate forjustonehighlylikelyvaluefor h.Thisissimilartothesparsecodinggenerative model(section),butwith13.4 hbeingtheoutputoftheparametricencoderrather thantheresultofanoptimization thatinfersthemostlikely h.Fromthispointof view,withthischosen,wearemaximizing h log pmodel( ) = log h x , pmodel()+log h pmodel( ) x h| .(14.4) Thelog pmodel() htermcanbesparsity-inducing.Forexample,theLaplaceprior, pmodel( h i) =Œª 2e‚àí| Œª h i|, (14.5) correspondstoanabsolutevaluesparsitypenalty.Expressingthelog-priorasan absolutevaluepenalty,weobtain ‚Ñ¶() = h ŒªÓÅò i| h i| (14.6) ‚àílog pmodel() = hÓÅò iÓÄí Œª h| i|‚àílogŒª 2ÓÄì = ‚Ñ¶()+const h (14.7) wheretheconstanttermdependsonlyon Œªandnot h.Wetypicallytreat Œªasa hyperparameteranddiscardtheconstanttermsinceitdoesnotaÔ¨Äecttheparameter learning.OtherpriorssuchastheStudent- tpriorcanalsoinducesparsity.From thispointofviewofsparsityasresultingfromtheeÔ¨Äectof pmodel( h)onapproximate maximumlikelihoodlearning,thesparsitypenaltyisnotaregularizationtermat all. Itisjustaconsequenceofthemodel‚Äôsdistributionoveritslatentvariables. ThisviewprovidesadiÔ¨Äerentmotivationfortraininganautoencoder:itisaway ofapproximately trainingagenerativemodel.ItalsoprovidesadiÔ¨Äerentreasonfor 5 0 6 CHAPTER14.AUTOENCODERS whythefeatureslearnedbytheautoencoderareuseful:theydescribethelatent variablesthatexplaintheinput. Earlyworkonsparseautoencoders( ,,)explored Ranzato e t a l .2007a2008 variousformsofsparsityandproposedaconnectionbetweenthesparsitypenalty andthelog Ztermthatariseswhenapplyingmaximumlikelihoodtoanundirected probabilisticmodel p( x) =1 ZÀú p( x).Theideaisthatminimizing log Zpreventsa probabilisticmodelfromhavinghighprobabilityeverywhere,andimposingsparsity on anautoencoder preventstheautoencoderfrom having lowreconstruction erroreverywhere.Inthiscase, theconnectionisonthelevelofanintuitive understandingofageneralmechanismratherthanamathematical correspondence. Theinterpretation ofthesparsitypenaltyascorrespondingtolog pmodel( h)ina directedmodel pmodel() h pmodel( ) x h|ismoremathematically straightforward. Onewaytoachieve a c t u a l z e r o sin hforsparse(anddenoising)autoencoders wasintroducedin ().TheideaistouserectiÔ¨Åedlinearunitsto Glorot e t a l .2011b producethecodelayer.Withapriorthatactuallypushestherepresentationsto zero(liketheabsolutevaluepenalty),onecanthusindirectlycontroltheaverage numberofzerosintherepresentation. 1 4 . 2 . 2 D en o i s i n g A u t o en co d ers Ratherthanaddingapenaltytothecostfunction,wecanobtainanautoencoder ‚Ñ¶ thatlearnssomethingusefulbychangingthereconstructionerrortermofthecost function. Traditionally,autoencodersminimizesomefunction L , g f ( x(())) x (14.8) where Lisalossfunctionpenalizing g( f( x))forbeingdissimilarfrom x,suchas the L2normoftheirdiÔ¨Äerence. This encourages g f‚ó¶tolearntobemerelyan identityfunctioniftheyhavethecapacitytodoso. A orDAEinsteadminimizes denoising aut o e nc o der L , g f ( x((Àú x))) , (14.9) where Àú xisacopyof xthathasbeencorruptedbysomeformofnoise.Denoising autoencodersmustthereforeundothiscorruptionratherthansimplycopyingtheir input. Denoisingtrainingforces fand gtoimplicitlylearnthestructureof pdata( x), asshown by () and ().Denoising AlainandBengio2013Bengio e t a l .2013c 5 0 7 CHAPTER14.AUTOENCODERS autoencodersthusprovideyetanotherexampleofhowusefulpropertiescanemerge asabyproductofminimizingreconstructionerror.Theyarealsoanexampleof howovercomplete,high-capacity modelsmaybeusedasautoencoderssolong ascareistakentopreventthemfromlearningtheidentityfunction. Denoising autoencodersarepresentedinmoredetailinsection.14.5 1 4 . 2 . 3 Regu l a ri z i n g b y P en a l i zi n g D eri v a t i v es Anotherstrategyforregularizinganautoencoderistouseapenaltyasinsparse ‚Ñ¶ autoencoders, L , g f , , ( x(()))+‚Ñ¶( x h x) (14.10) butwithadiÔ¨Äerentformof:‚Ñ¶ ‚Ñ¶( ) = h x , ŒªÓÅò i||‚àá x h i||2. (14.11) Thisforcesthemodeltolearnafunctionthatdoesnotchangemuchwhen x changesslightly.Becausethispenaltyisappliedonlyattrainingexamples,itforces theautoencodertolearnfeaturesthatcaptureinformationaboutthetraining distribution. Anautoencoderregularizedinthiswayiscalleda c o n t r ac t i v e aut o e nc o der orCAE.Thisapproachhastheoreticalconnectionstodenoisingautoencoders, manifoldlearningandprobabilisticmodeling.TheCAEisdescribedinmoredetail insection.14.7 14.3RepresentationalPower,LayerSizeandDepth Autoencodersareoftentrainedwithonlyasinglelayerencoderandasinglelayer decoder.However,thisisnotarequirement.Infact,usingdeepencodersand decodersoÔ¨Äersmanyadvantages. Recallfromsectionthattherearemanyadvantagestodepthinafeedfor- 6.4.1 wardnetwork.Becauseautoencodersarefeedforwardnetworks,theseadvantages alsoapplytoautoencoders.Moreover,theencoderisitselfafeedforwardnetwork asisthedecoder,soeachofthesecomponentsoftheautoencodercanindividually beneÔ¨Åtfromdepth. Onemajoradvantageofnon-trivialdepthisthattheuniversalapproximator theoremguaranteesthatafeedforwardneuralnetworkwithatleastonehidden layercanrepresentanapproximationofanyfunction(withinabroadclass)toan 5 0 8 CHAPTER14.AUTOENCODERS arbitrarydegreeofaccuracy,providedthatithasenoughhiddenunits.Thismeans thatanautoencoderwithasinglehiddenlayerisabletorepresenttheidentity functionalongthedomainofthedataarbitrarilywell.However,themappingfrom inputtocodeisshallow.Thismeansthatwearenotabletoenforcearbitrary constraints,suchasthatthecodeshouldbesparse.Adeepautoencoder,withat leastoneadditionalhiddenlayerinsidetheencoderitself,canapproximate any mappingfrominputtocodearbitrarilywell,givenenoughhiddenunits. Depthcanexponentiallyreducethecomputational costofrepresentingsome functions.Depthcanalsoexponentiallydecreasetheamountoftrainingdata neededtolearnsomefunctions.Seesectionforareviewoftheadvantagesof 6.4.1 depthinfeedforwardnetworks. Experimentally,deepautoencodersyieldmuchbettercompressionthancorre- spondingshalloworlinearautoencoders(HintonandSalakhutdinov2006,). Acommonstrategyfortrainingadeepautoencoderistogreedilypretrain thedeeparchitecturebytrainingastackofshallowautoencoders,soweoften encountershallowautoencoders,evenwhentheultimategoalistotrainadeep autoencoder. 14.4StochasticEncodersandDecoders Autoencodersarejustfeedforwardnetworks.Thesamelossfunctionsandoutput unittypesthatcanbeusedfortraditionalfeedforwardnetworksarealsousedfor autoencoders. Asdescribedinsection,ageneralstrategyfordesigningtheoutputunits 6.2.2.4 andthelossfunctionofafeedforwardnetworkistodeÔ¨Åneanoutputdistribution p( y x|)andminimizethenegativelog-likelihood‚àílog p( y x|).Inthatsetting, y wasavectoroftargets,suchasclasslabels. Inthecaseofanautoencoder, xisnowthetargetaswellastheinput.However, wecanstillapplythesamemachineryasbefore.Givenahiddencode h,wemay thinkofthedecoderasprovidingaconditionaldistribution pdecoder( x h|). We maythentraintheautoencoderbyminimizing ‚àílog pdecoder( ) x h|.Theexact formofthislossfunctionwillchangedependingontheformof pdecoder.Aswith traditionalfeedforwardnetworks,weusuallyuselinearoutputunitstoparametrize themeanofaGaussiandistributionif xisreal-valued.Inthatcase,thenegative log-likelihoodyieldsameansquarederrorcriterion.Similarly,binary xvalues correspondtoaBernoullidistributionwhoseparametersaregivenbyasigmoid outputunit,discrete xvaluescorrespondtoasoftmaxdistribution,andsoon. 5 0 9 CHAPTER14.AUTOENCODERS Typically,theoutputvariablesaretreatedasbeingconditionallyindependent given hsothatthisprobabilitydistributionisinexpensivetoevaluate,butsome techniquessuchasmixturedensityoutputsallowtractablemodelingofoutputs withcorrelations. xx rrh h p e n c o d e r ( ) h x| p d e c o d e r ( ) x h| Figure14.2:Thestructureofastochasticautoencoder,inwhichboththeencoderandthe decoderarenotsimplefunctionsbutinsteadinvolvesomenoiseinjection,meaningthat theiroutputcanbeseenassampledfromadistribution, pencoder( h x|)fortheencoder and pdecoder( ) x h|forthedecoder. Tomakeamoreradicaldeparturefromthefeedforwardnetworkswehaveseen previously,wecanalsogeneralizethenotionofan e nc o di</div>
        </div>
    </div>

    <div class="question-card" id="q148">
        <div class="question-header">
            <span class="question-number">Question 148</span>
            <span class="question-type">multiple-choice</span>
        </div>

        <div class="question-text">In recurrent neural networks (RNNs), the spectral radius of the Jacobian plays a crucial role in determining the behavior of both gradient and information propagation over time. Understanding how the spectral radius influences stability and memory retention is essential for designing effective architectures.

Which statement most accurately describes the impact of having a spectral radius less than one in a linear recurrent neural network?

1) The network will maintain past information indefinitely, enabling long-term memory.   
2) Gradients and signals will shrink exponentially over time, leading to vanishing gradients and rapid forgetting of past inputs.   
3) The network will exhibit oscillatory behavior due to complex eigenvalues.   
4) The network will be prone to exploding gradients, destabilizing training.   
5) Nonlinear activation functions become unnecessary for gradient stability.   
6) The spectral radius will have no impact on memory retention or gradient propagation.   
7) The Jacobian will always be a symmetric matrix, ensuring stable dynamics.</div>

        <div class="answer-section">
            <div class="answer-label">‚úì Correct Answer:</div>
            <div class="answer-text">The correct answer is 2) Gradients and signals will shrink exponentially over time, leading to vanishing gradients and rapid forgetting of past inputs..</div>
        </div>

        <div class="reference-section">
            <div class="reference-label">üìö Reference Text:</div>
            <button class="toggle-button" onclick="toggleReference(148)">
                Show/Hide Reference
            </button>
            <div id="ref148" class="reference-text hidden">t ‚àÇ s( 1 ) t ‚àí.OfparticularimportanceisthespectralradiusofJ( ) t,deÔ¨Ånedto bethemaximumoftheabsolutevaluesofitseigenvalues. 4 0 4 CHAPTER10.SEQUENCEMODELING:RECURRENTANDRECURSIVENETS TounderstandtheeÔ¨Äectofthespectralradius,considerthesimplecaseof back-propagationwithaJacobianmatrixJthatdoesnotchangewith t.This casehappens,forexample,whenthenetworkispurelylinear.SupposethatJhas aneigenvectorvwithcorrespondingeigenvalue Œª.Considerwhathappensaswe propagateagradientvectorbackwardsthroughtime.Ifwebeginwithagradient vectorg,thenafteronestepofback-propagation,wewillhaveJg,andafter n stepswewillhaveJng.Nowconsiderwhathappensifweinsteadback-propagate aperturbedversionofg.Ifwebeginwithg+ Œ¥v,thenafteronestep,wewill haveJ(g+ Œ¥v).After nsteps,wewillhaveJn(g+ Œ¥v).Fromthiswecansee thatback-propagationstartingfromgandback-propagationstartingfromg+ Œ¥v divergeby Œ¥Jnvafter nstepsofback-propagation.Ifvischosentobeaunit eigenvectorofJwitheigenvalue Œª,thenmultiplicationbytheJacobiansimply scalesthediÔ¨Äerenceateachstep.Thetwoexecutionsofback-propagationare separatedbyadistanceof Œ¥ Œª||n.Whenvcorrespondstothelargestvalueof|| Œª, thisperturbationachievesthewidestpossibleseparationofaninitialperturbation ofsize. Œ¥ When || Œª >1,thedeviationsize Œ¥ Œª||ngrowsexponentiallylarge.When || Œª <1, thedeviationsizebecomesexponentiallysmall. Ofcourse,thisexampleassumedthattheJacobianwasthesameatevery timestep,correspondingtoarecurrentnetworkwithnononlinearity.Whena nonlinearityispresent,thederivativeofthenonlinearitywillapproachzeroon manytimesteps,andhelptopreventtheexplosionresultingfromalargespectral radius. Indeed,themostrecentworkonechostatenetworksadvocatesusinga spectralradiusmuchlargerthanunity(,;,). Yildiz e t a l .2012Jaeger2012 Everythingwehavesaidaboutback-propagation viarepeatedmatrixmultipli- cationappliesequallytoforwardpropagationinanetworkwithnononlinearity, wherethestateh( + 1 ) t= h( ) t ÓÄæW. WhenalinearmapWÓÄæalwaysshrinkshasmeasuredbythe L2norm,then wesaythatthemapiscontractive.Whenthespectralradiusislessthanone, themappingfromh( ) ttoh( + 1 ) tiscontractive,soasmallchangebecomessmaller aftereachtimestep.Thisnecessarilymakesthenetworkforgetinformationabout thepastwhenweuseaÔ¨Ånitelevelofprecision(suchas32bitintegers)tostore thestatevector. TheJacobianmatrixtellsushowasmallchangeofh( ) tpropagatesonestep forward,orequivalently,howthegradientonh( + 1 ) tpropagatesonestepbackward, duringback-propagation. NotethatneitherWnorJneedtobesymmetric(al- thoughtheyaresquareandreal),sotheycanhavecomplex-valuedeigenvaluesand eigenvectors,withimaginarycomponentscorrespondingtopotentiallyoscillatory 4 0 5</div>
        </div>
    </div>

    <div class="question-card" id="q149">
        <div class="question-header">
            <span class="question-number">Question 149</span>
            <span class="question-type">multiple-choice</span>
        </div>

        <div class="question-text">Deep Boltzmann Machines (DBMs) are a class of probabilistic graphical models used for unsupervised learning, notable for their multilayer, undirected architecture and efficient sampling properties. Their structural and inference characteristics distinguish them from related models such as Restricted Boltzmann Machines (RBMs) and Deep Belief Networks (DBNs).

Which property of Deep Boltzmann Machines is primarily responsible for enabling efficient block-wise Gibbs sampling and conditional independence between layers?

1) The use of directed connections between layers   
2) Inclusion of intra-layer connections among units   
3) Reliance on heuristic inference procedures   
4) Composition of only real-valued units   
5) Bipartite graph structure where units connect only to neighboring layers   
6) Training with strictly deterministic algorithms   
7) Presence of a single hidden layer</div>

        <div class="answer-section">
            <div class="answer-label">‚úì Correct Answer:</div>
            <div class="answer-text">The correct answer is 5) Bipartite graph structure where units connect only to neighboring layers.</div>
        </div>

        <div class="reference-section">
            <div class="reference-label">üìö Reference Text:</div>
            <button class="toggle-button" onclick="toggleReference(149)">
                Show/Hide Reference
            </button>
            <div id="ref149" class="reference-text hidden">1 ) 3 h( 1 ) 3 v 1 v 1 v 2 v 2 v 3 v 3h( 2 ) 1 h( 2 ) 1 h( 2 ) 2 h( 2 ) 2h( 2 ) 3h( 2 ) 3 h( 1 ) 4 h( 1 ) 4 Figure20.2:ThegraphicalmodelforadeepBoltzmannmachinewithonevisiblelayer (bottom)andtwohiddenlayers.Connectionsareonlybetweenunitsinneighboringlayers. Therearenointralayerlayerconnections. 20.4DeepBoltzmannMachines AdeepBoltzmannmachineorDBM(Salakhutdino vandHinton2009a,)is anotherkindofdeep,generativemodel. Unlikethedeepbeliefnetwork(DBN), itisanentirelyundirectedmodel.UnliketheRBM,theDBMhasseverallayers oflatentvariables(RBMshavejustone). ButliketheRBM,withineachlayer, eachofthevariablesaremutuallyindependent,conditionedonthevariablesin theneighboringlayers.SeeÔ¨Ågureforthegraphstructure.DeepBoltzmann 20.2 machineshavebeenappliedtoavarietyoftasksincludingdocumentmodeling (Srivastava2013 e t a l .,). LikeRBMsandDBNs, DBMstypicallycontainonlybinaryunits‚Äîaswe assumeforsimplicityofourpresentationofthemodel‚Äîbutitisstraightforward toincludereal-valuedvisibleunits. ADBMisanenergy-basedmodel,meaningthatthethejointprobability distributionoverthemodelvariablesisparametrized byanenergyfunction E.In thecaseofadeepBoltzmannmachinewithonevisiblelayer, v,andthreehidden layers, h( 1 ), h( 2 )and h( 3 ),thejointprobabilityisgivenby: PÓÄê v h ,( 1 ), h( 2 ), h( 3 )ÓÄë =1 Z() Œ∏expÓÄê ‚àí E ,( v h( 1 ), h( 2 ), h( 3 );) Œ∏ÓÄë .(20.24) Tosimplifyourpresentation,weomitthebiasparametersbelow.TheDBMenergy functionisthendeÔ¨Ånedasfollows: E ,( v h( 1 ), h( 2 ), h( 3 );) = Œ∏ ‚àí vÓÄæW( 1 )h( 1 )‚àí h( 1 )ÓÄæW( 2 )h( 2 )‚àí h( 2 )ÓÄæW( 3 )h( 3 ). (20.25) 6 6 3 CHAPTER20.DEEPGENERATIVEMODELS h( 1 ) 1 h( 1 ) 1 h( 1 ) 2 h( 1 ) 2h( 1 ) 3h( 1 ) 3 v 1 v 1 v 2 v 2h( 2 ) 1h( 2 ) 1h( 2 ) 2h( 2 ) 2h( 2 ) 3 h( 2 ) 3h( 3 ) 1 h( 3 ) 1 h( 3 ) 2 h( 3 ) 2 v1 v2h( 2 ) 1 h( 2 ) 1 h( 2 ) 2 h( 2 ) 2 h( 2 ) 3h( 2 ) 3 h( 1 ) 1 h( 1 ) 1 h( 1 ) 2 h( 1 ) 2 h( 1 ) 3 h( 1 ) 3h( 3 ) 1 h( 3 ) 1 h( 3 ) 2 h( 3 ) 2 Figure20.3:AdeepBoltzmannmachine,re-arrangedtorevealitsbipartitegraphstructure. IncomparisontotheRBMenergyfunction(equation),theDBMenergy 20.5 functionincludesconnectionsbetweenthehiddenunits(latentvariables)inthe formoftheweightmatrices( W( 2 )and W( 3 )).Aswewillsee,theseconnections havesigniÔ¨Åcantconsequencesforboththemodelbehavioraswellashowwego aboutperforminginferenceinthemodel. IncomparisontofullyconnectedBoltzmannmachines(witheveryunitcon- nectedtoeveryotherunit),theDBMoÔ¨Äerssomeadvantagesthataresimilar tothoseoÔ¨ÄeredbytheRBM.SpeciÔ¨Åcally,asillustratedinÔ¨Ågure,theDBM20.3 layerscanbeorganizedintoabipartitegraph,withoddlayersononesideand evenlayersontheother.Thisimmediatelyimpliesthatwhenweconditiononthe variablesintheevenlayer,thevariablesintheoddlayersbecomeconditionally independent.Ofcourse,whenweconditiononthevariablesintheoddlayers,the variablesintheevenlayersalsobecomeconditionallyindependent. ThebipartitestructureoftheDBMmeansthatwecanapplythesameequa- tionswehavepreviouslyusedfortheconditionaldistributionsofanRBMto determinetheconditionaldistributionsinaDBM.Theunitswithinalayerare conditionallyindependentfromeachothergiventhevaluesoftheneighboring layers,sothedistributionsoverbinaryvariablescanbefullydescribedbythe Bernoulliparametersgivingtheprobabilityofeachunitbeingactive.Inour examplewithtwohiddenlayers,theactivationprobabilities aregivenby: P v( i= 1 | h( 1 )) = œÉÓÄê W( 1 ) i , : h( 1 )ÓÄë , (20.26) 6 6 4 CHAPTER20.DEEPGENERATIVEMODELS P h(( 1 ) i= 1 | v h ,( 2 )) = œÉÓÄê vÓÄæW( 1 ) : , i+ W( 2 ) i , : h( 2 )ÓÄë (20.27) and P h(( 2 ) k= 1 | h( 1 )) = œÉÓÄê h( 1 )ÓÄæW( 2 ) : , kÓÄë . (20.28) ThebipartitestructuremakesGibbssamplinginadeepBoltzmannmachine eÔ¨Écient. ThenaiveapproachtoGibbssamplingistoupdateonlyonevariable atatime.RBMsallowallofthevisibleunitstobeupdatedinoneblockandall ofthehiddenunitstobeupdatedinasecondblock.Onemightnaivelyassume thataDBMwith llayersrequires l+1updates,witheachiterationupdatinga blockconsistingofonelayerofunits.Instead,itispossibletoupdateallofthe unitsinonlytwoiterations.Gibbssamplingcanbedividedintotwoblocksof updates,oneincludingallevenlayers(includingthevisiblelayer)andtheother includingalloddlayers.DuetothebipartiteDBMconnectionpattern,given theevenlayers,thedistributionovertheoddlayersisfactorialandthuscanbe sampledsimultaneouslyandindependentlyasablock.Likewise,giventheodd layers,theevenlayerscanbesampledsimultaneouslyandindependentlyasa block.EÔ¨Écientsamplingisespeciallyimportantfortrainingwiththestochastic maximumlikelihoodalgorithm. 20.4.1InterestingProperties DeepBoltzmannmachineshavemanyinterestingproperties. DBMsweredevelopedafterDBNs.ComparedtoDBNs,theposteriordistribu- tion P( h v|)issimplerforDBMs.Somewhatcounterintuitively,thesimplicityof thisposteriordistributionallowsricherapproximationsoftheposterior.Inthecase oftheDBN,weperformclassiÔ¨Åcationusingaheuristicallymotivatedapproximate inferenceprocedure,inwhichweguessthatareasonablevalueforthemeanÔ¨Åeld expectationofthehiddenunitscanbeprovidedbyanupwardpassthroughthe networkinanMLPthatusessigmoidactivationfunctionsandthesameweightsas theoriginalDBN.distribution A ny Q( h)maybeusedtoobtainavariationallower boundonthelog-likelihood.Thisheuristicprocedurethereforeallowsustoobtain suchabound.However,theboundisnotexplicitlyoptimizedinanyway,sothe boundmaybefarfromtight.Inparticular,theheuristicestimateof Qignores interactionsbetweenhiddenunitswithinthesamelayeraswellasthetop-down feedbackinÔ¨Çuenceofhiddenunitsindeeperlayersonhiddenunitsthatarecloser totheinput.BecausetheheuristicMLP-basedinferenceprocedureintheDBN isnotabletoaccountfortheseinteractions, theresulting Qispresumablyfar 6 6 5</div>
        </div>
    </div>

    <div class="question-card" id="q150">
        <div class="question-header">
            <span class="question-number">Question 150</span>
            <span class="question-type">multiple-choice</span>
        </div>

        <div class="question-text">In machine learning, regularization techniques are used to prevent overfitting and improve model generalization, often by penalizing the magnitude of model parameters. Two widely used methods are L1 and L2 regularization, each with distinct mathematical properties and implications for sparsity in solutions.

Which regularization technique is most likely to yield a model in which some parameter weights are set exactly to zero, thus enabling automatic feature selection?

1) L2 regularization (ridge regression)   
2) L1 regularization (LASSO)   
3) Early stopping regularization   
4) Dropout regularization   
5) Elastic Net regularization with dominant L2 component   
6) Bayesian regularization with Gaussian prior   
7) Data augmentation regularization</div>

        <div class="answer-section">
            <div class="answer-label">‚úì Correct Answer:</div>
            <div class="answer-text">The correct answer is 2) L1 regularization (LASSO).</div>
        </div>

        <div class="reference-section">
            <div class="reference-label">üìö Reference Text:</div>
            <button class="toggle-button" onclick="toggleReference(150)">
                Show/Hide Reference
            </button>
            <div id="ref150" class="reference-text hidden">o u l d re g u l a riz e t h e p a ra m e t e rs t o w a rd s a v a l u e t h a t i s n o t z e ro , b u t i n s t e a d t o wa rd s s o m e p a ra m e t e r v a l u e w( ) o. In t h a t c a s e t h e L1re g u l a riz a t i o n wo u l d i n t ro d u c e t h e t e rm‚Ñ¶() = Œ∏ ||‚àí w w( ) o|| 1=ÓÅê i| w i‚àí w( ) o i| . 2 3 4 CHAPTER7.REGULARIZATIONFORDEEPLEARNING ofregularization. Aswith L2weightdecay, L1weightdecaycontrolsthestrength oftheregularizationbyscalingthepenaltyusingapositivehyperparameter ‚Ñ¶ Œ±. Thus,theregularizedobjectivefunction Àú J , (;wXy)isgivenby Àú J , Œ± (;wXy) = ||||w 1+(; ) JwXy , , (7.19) withthecorrespondinggradient(actually,sub-gradient): ‚àá wÀú J , Œ± (;wXy) = sign( )+w ‚àá w J ,(Xyw;) (7.20) where issimplythesignofappliedelement-wise. sign( )w w Byinspectingequation,wecanseeimmediately thattheeÔ¨Äectof 7.20 L1 regularizationisquitediÔ¨Äerentfromthatof L2regularization. SpeciÔ¨Åcally,wecan seethattheregularizationcontributiontothegradientnolongerscaleslinearly witheach w i;insteaditisaconstantfactorwithasignequaltosign( w i).One consequenceofthisformofthegradientisthatwewillnotnecessarilyseeclean algebraicsolutionstoquadraticapproximationsof J(Xy ,;w)aswedidfor L2 regularization. Oursimplelinearmodelhasaquadraticcostfunctionthatwecanrepresent viaitsTaylorseries.Alternately,wecouldimaginethatthisisatruncatedTaylor seriesapproximatingthecostfunctionofamoresophisticatedmodel.Thegradient inthissettingisgivenby ‚àá wÀÜ J() = (wHww‚àí‚àó) , (7.21) where,again,istheHessianmatrixofwithrespecttoevaluatedat H J ww‚àó. Becausethe L1penaltydoesnotadmitcleanalgebraicexpressionsinthecase ofafullygeneralHessian,wewillalsomakethefurthersimplifyingassumption thattheHessianisdiagonal,H=diag([ H 1 1 , , . . . , H n , n]),whereeach H i , i >0. Thisassumptionholdsifthedataforthelinearregressionproblemhasbeen preprocessedtoremoveallcorrelationbetweentheinputfeatures,whichmaybe accomplishedusingPCA. Ourquadraticapproximationofthe L1regularizedobjectivefunctiondecom- posesintoasumovertheparameters: ÀÜ J , J (;wXy) = (w‚àó; )+Xy ,ÓÅò iÓÄî1 2H i , i(w i‚àíw‚àó i)2+ Œ± w| i|ÓÄï .(7.22) Theproblemofminimizingthisapproximatecostfunctionhasananalyticalsolution (foreachdimension),withthefollowingform: i w i= sign( w‚àó i)maxÓÄö | w‚àó i|‚àíŒ± H i , i,0ÓÄõ . (7.23) 2 3 5 CHAPTER7.REGULARIZATIONFORDEEPLEARNING Considerthesituationwhere w‚àó i > i 0forall.Therearetwopossibleoutcomes: 1.Thecasewhere w‚àó i‚â§Œ± H i , i.Heretheoptimalvalueof w iundertheregularized objectiveissimply w i= 0.Thisoccursbecausethecontributionof J(w;Xy ,) totheregularizedobjectiveÀú J(w;Xy ,)isoverwhelmed‚Äîindirection i‚Äîby the L1regularizationwhichpushesthevalueof w itozero. 2.Thecasewhere w‚àó i >Œ± H i , i.Inthiscase,theregularizationdoesnotmovethe optimalvalueof w itozerobutinsteaditjustshiftsitinthatdirectionbya distanceequaltoŒ± H i , i. Asimilarprocesshappenswhen w‚àó i <0,butwiththe L1penaltymaking w iless negativebyŒ± H i , i,or0. Incomparisonto L2regularization, L1regularizationresultsinasolutionthat ismoresparse.Sparsityinthiscontextreferstothefactthatsomeparameters haveanoptimalvalueofzero.Thesparsityof L1regularizationisaqualitatively diÔ¨Äerentbehaviorthanariseswith L2regularization. Equationgavethe7.13 solution Àú wfor L2regularization. Ifwerevisitthatequationusingtheassumption ofadiagonalandpositivedeÔ¨ÅniteHessianHthatweintroducedforouranalysisof L1regularization,weÔ¨ÅndthatÀú w i=H i , i H i , i + Œ±w‚àó i.If w‚àó iwasnonzero,then Àú w iremains nonzero.Thisdemonstratesthat L2regularizationdoesnotcausetheparameters tobecomesparse,while L1regularizationmaydosoforlargeenough. Œ± Thesparsitypropertyinducedby L1regularizationhasbeenusedextensively asafeatureselectionmechanism.FeatureselectionsimpliÔ¨Åesamachinelearning problembychoosingwhichsubsetoftheavailablefeaturesshouldbeused.In particular,thewellknownLASSO(,)(leastabsoluteshrinkageand Tibshirani1995 selectionoperator)modelintegratesan L1penaltywithalinearmodelandaleast squarescostfunction.The L1penaltycausesasubsetoftheweightstobecome zero,suggestingthatthecorrespondingfeaturesmaysafelybediscarded. Insection,wesawthatmanyregularizationstrategiescanbeinterpreted 5.6.1 asMAPBayesianinference,andthatinparticular, L2regularizationisequivalent toMAPBayesianinferencewithaGaussianpriorontheweights. For L1regu- larization,thepenalty Œ±‚Ñ¶(w)= Œ±ÓÅê i| w i|usedtoregularizeacostfunctionis equivalenttothelog-priortermthatismaximizedbyMAPBayesianinference whenthepriorisanisotropicLaplacedistribution(equation)over3.26w‚àà Rn: log() = pwÓÅò ilogLaplace( w i;0 ,1 Œ±) = ‚àí|||| Œ±w 1+log log2 n Œ± n‚àí .(7.24) 2 3 6 CHAPTER7.REGULARIZATIONFORDEEPLEARNING Fromthepointofviewoflearningviamaximization withrespecttow,wecan ignorethe termsbecausetheydonotdependon. log log2 Œ±‚àí w 7.2NormPenaltiesasConstrainedOptimization Considerthecostfunctionregularizedbyaparameternormpenalty: Àú J , J , Œ± . (;Œ∏Xy) = (;Œ∏Xy)+‚Ñ¶()Œ∏ (7.25) Recallfromsectionthatwecanminimizeafunctionsubjecttoconstraints 4.4 byconstructingageneralizedLagrangefunction,consistingoftheoriginalobjective functionplusasetofpenalties.EachpenaltyisaproductbetweenacoeÔ¨Écient, calledaKarush‚ÄìKuhn‚ÄìTucker(KKT)multiplier,andafunctionrepresenting whethertheconstraintissatisÔ¨Åed.Ifwewantedtoconstrain‚Ñ¶(Œ∏)tobelessthan someconstant,wecouldconstructageneralizedLagrangefunction k L ‚àí (; ) = (; )+(‚Ñ¶() Œ∏ , Œ±Xy , JŒ∏Xy , Œ±Œ∏ k .) (7.26) Thesolutiontotheconstrainedproblemisgivenby Œ∏‚àó= argmin Œ∏max Œ± , Œ±‚â• 0L()Œ∏ , Œ± . (7.27) Asdescribedinsection,solvingthisproblemrequiresmodifyingboth 4.4 Œ∏ and Œ±.Sectionprovidesaworkedexampleoflinearregressionwithan 4.5 L2 constraint.ManydiÔ¨Äerentproceduresarepossible‚Äîsomemayusegradientdescent,</div>
        </div>
    </div>

    <div class="question-card" id="q151">
        <div class="question-header">
            <span class="question-number">Question 151</span>
            <span class="question-type">multiple-choice</span>
        </div>

        <div class="question-text">Neural networks rely on activation functions and architectural design choices to determine their expressive power and training dynamics. The selection of activations and the depth or width of a network can significantly affect performance and efficiency in representing complex functions.

Which of the following statements correctly identifies a key advantage of deep feedforward networks with piecewise linear activations over shallow networks with a single hidden layer?

1) Deep networks are guaranteed to avoid the vanishing gradient problem.   
2) Shallow networks can always represent any function with fewer parameters than deep networks.   
3) Deep networks require all layers to be linear for optimal expressiveness.   
4) Shallow networks with sigmoid activations generalize better than deep networks.   
5) Deep networks can only model simple, non-hierarchical relationships.   
6) Deep networks with rectified linear units can represent complex functions exponentially more efficiently by partitioning input space into exponentially many linear regions.   
7) Shallow networks inherently outperform deep networks on all function approximation tasks.</div>

        <div class="answer-section">
            <div class="answer-label">‚úì Correct Answer:</div>
            <div class="answer-text">The correct answer is 6) Deep networks with rectified linear units can represent complex functions exponentially more efficiently by partitioning input space into exponentially many linear regions..</div>
        </div>

        <div class="reference-section">
            <div class="reference-label">üìö Reference Text:</div>
            <button class="toggle-button" onclick="toggleReference(151)">
                Show/Hide Reference
            </button>
            <div id="ref151" class="reference-text hidden">logisticsigmoidactivationfunction g z œÉ z () = () (6.38) orthehyperbolictangentactivationfunction g z z . () = tanh( ) (6.39) Theseactivationfunctionsarecloselyrelatedbecause . tanh( ) = 2(2)1 z œÉ z‚àí We havealready seen sigmoid unitsasoutput units, usedto predictthe probabilitythatabinaryvariableis.Unlikepiecewiselinearunits,sigmoidal 1 unitssaturateacrossmostoftheirdomain‚Äîthey saturatetoahighvaluewhen zisverypositive,saturatetoalowvaluewhen zisverynegative,andareonly stronglysensitivetotheirinputwhen zisnear0.Thewidespreadsaturationof sigmoidalunitscanmakegradient-basedlearningverydiÔ¨Écult.Forthisreason, theiruseashiddenunitsinfeedforwardnetworksisnowdiscouraged.Theiruse asoutputunitsiscompatiblewiththeuseofgradient-basedlearningwhenan appropriatecostfunctioncanundothesaturationofthesigmoidintheoutput layer. Whenasigmoidalactivationfunctionmustbeused,thehyperbolictangent activationfunctiontypicallyperformsbetterthanthelogisticsigmoid.Itresembles theidentityfunctionmoreclosely,inthesensethattanh(0) = 0while œÉ(0) =1 2. Becausetanhissimilartotheidentityfunctionnear,trainingadeepneural 0 networkÀÜ y=wÓÄætanh(UÓÄætanh(VÓÄæx))resemblestrainingalinearmodelÀÜ y= wÓÄæUÓÄæVÓÄæxsolongastheactivationsofthenetworkcanbekeptsmall.This makestrainingthenetworkeasier. tanh Sigmoidalactivationfunctionsaremorecommoninsettingsotherthanfeed- forwardnetworks.Recurrentnetworks,manyprobabilisticmodels,andsome autoencodershaveadditionalrequirementsthatruleouttheuseofpiecewise linearactivationfunctionsandmakesigmoidalunitsmoreappealingdespitethe drawbacksofsaturation. 1 9 5 CHAPTER6.DEEPFEEDFORWARDNETWORKS 6.3.3OtherHiddenUnits Manyothertypesofhiddenunitsarepossible,butareusedlessfrequently. Ingeneral,awidevarietyofdiÔ¨Äerentiable functionsperformperfectlywell. Manyunpublishedactivationfunctionsperformjustaswellasthepopularones. Toprovideaconcreteexample,theauthorstestedafeedforwardnetworkusing h=cos(Wx+b)ontheMNISTdatasetandobtainedanerrorrateoflessthan 1%,whichiscompetitivewithresultsobtainedusingmoreconventionalactivation functions.Duringresearchanddevelopmentofnewtechniques,itiscommon totestmanydiÔ¨ÄerentactivationfunctionsandÔ¨Åndthatseveralvariationson standardpracticeperformcomparably.Thismeansthatusuallynewhiddenunit typesarepublishedonlyiftheyareclearlydemonstratedtoprovideasigniÔ¨Åcant improvement.Newhiddenunittypesthatperformroughlycomparablytoknown typesaresocommonastobeuninteresting. Itwouldbeimpracticaltolistallofthehiddenunittypesthathaveappeared intheliterature.Wehighlightafewespeciallyusefulanddistinctiveones. Onepossibilityistonothaveanactivation g( z)atall.Onecanalsothinkof thisasusingtheidentityfunctionastheactivationfunction.Wehavealready seenthatalinearunitcanbeusefulastheoutputofaneuralnetwork.Itmay alsobeusedasahiddenunit.Ifeverylayeroftheneuralnetworkconsistsofonly lineartransformations,thenthenetworkasawholewillbelinear.However,it isacceptableforsomelayersoftheneuralnetworktobepurelylinear.Consider aneuralnetworklayerwith ninputsand poutputs,h= g(WÓÄæx+b).Wemay replacethiswithtwolayers,withonelayerusingweightmatrixUandtheother usingweightmatrixV.IftheÔ¨Årstlayerhasnoactivationfunction,thenwehave essentiallyfactoredtheweightmatrixoftheoriginallayerbasedonW.The factoredapproachistocomputeh= g(VÓÄæUÓÄæx+b).IfUproduces qoutputs, thenUandVtogethercontainonly ( n+ p) qparameters,whileWcontains n p parameters.Forsmall q,thiscanbeaconsiderablesavinginparameters.It comesatthecostofconstrainingthelineartransformationtobelow-rank,but theselow-rankrelationshipsareoftensuÔ¨Écient.LinearhiddenunitsthusoÔ¨Äeran eÔ¨Äectivewayofreducingthenumberofparametersinanetwork. Softmaxunitsareanotherkindofunitthatisusuallyusedasanoutput(as describedinsection)butmaysometimesbeusedasahiddenunit.Softmax 6.2.2.3 unitsnaturallyrepresentaprobabilitydistributionoveradiscretevariablewith k possiblevalues,sotheymaybeusedasakindofswitch.Thesekindsofhidden unitsareusuallyonlyusedinmoreadvancedarchitectures thatexplicitlylearnto manipulatememory,describedinsection.10.12 1 9 6 CHAPTER6.DEEPFEEDFORWARDNETWORKS Afewotherreasonablycommonhiddenunittypesinclude: ‚Ä¢RadialbasisfunctionorRBFunit: h i=expÓÄê ‚àí1 œÉ2 i||W : , i‚àí||x2ÓÄë .This functionbecomesmoreactiveasxapproachesatemplateW : , i.Becauseit saturatestoformost,itcanbediÔ¨Éculttooptimize. 0x ‚Ä¢Softplus: g( a) = Œ∂( a) =log(1+ ea).ThisisasmoothversionoftherectiÔ¨Åer, introducedby ()forfunctionapproximationandby Dugas e t a l .2001 Nair andHinton2010()fortheconditionaldistributionsofundirectedprobabilistic models. ()comparedthesoftplusandrectiÔ¨Åerandfound Glorot e t a l .2011a betterresultswiththelatter.Theuseofthesoftplusisgenerallydiscouraged. Thesoftplusdemonstratesthattheperformanceofhiddenunittypescan beverycounterintuitive‚Äîonemightexpectittohaveanadvantageover therectiÔ¨ÅerduetobeingdiÔ¨Äerentiableeverywhereorduetosaturatingless completely,butempiricallyitdoesnot. ‚Ä¢Hardtanh:thisisshapedsimilarlytothetanhandtherectiÔ¨Åerbutunlike thelatter,itisbounded, g( a)=max(‚àí1 ,min(1 , a)).Itwasintroduced by(). Collobert2004 Hiddenunitdesignremainsanactiveareaofresearchandmanyusefulhidden unittypesremaintobediscovered. 6. 4 A rc h i t ec t u re D es i gn Anotherkeydesignconsiderationforneuralnetworksisdeterminingthearchitecture. Thewordarchitecturereferstotheoverallstructureofthenetwork:howmany unitsitshouldhaveandhowtheseunitsshouldbeconnectedtoeachother. Mostneuralnetworksareorganizedintogroupsofunitscalledlayers. Most neuralnetworkarchitectures arrangetheselayersinachainstructure,witheach layerbeingafunctionofthelayerthatprecededit.Inthisstructure,theÔ¨Årstlayer isgivenby h( 1 )= g( 1 )ÓÄê W( 1 )ÓÄæxb+( 1 )ÓÄë , (6.40) thesecondlayerisgivenby h( 2 )= g( 2 )ÓÄê W( 2 )ÓÄæh( 1 )+b( 2 )ÓÄë , (6.41) andsoon. 1 9 7 CHAPTER6.DEEPFEEDFORWARDNETWORKS Inthesechain-basedarchitectures,themainarchitecturalconsiderationsare tochoosethedepthofthenetworkandthewidthofeachlayer.Aswewillsee, anetworkwithevenonehiddenlayerissuÔ¨ÉcienttoÔ¨Åtthetrainingset.Deeper networksoftenareabletousefarfewerunitsperlayerandfarfewerparameters andoftengeneralizetothetestset,butarealsooftenhardertooptimize. The idealnetworkarchitectureforataskmustbefoundviaexperimentationguidedby monitoringthevalidationseterror. 6.4.1UniversalApproximationPropertiesandDepth Alinearmodel,mappingfromfeaturestooutputsviamatrixmultiplication, can bydeÔ¨Ånitionrepresentonlylinearfunctions.Ithastheadvantageofbeingeasyto trainbecausemanylossfunctionsresultinconvexoptimization problemswhen appliedtolinearmodels.Unfortunately,weoftenwanttolearnnonlinearfunctions. AtÔ¨Årstglance,wemightpresumethatlearninganonlinearfunctionrequires designingaspecializedmodelfamilyforthekindofnonlinearitywewanttolearn. Fortunately,feedforwardnetworkswithhiddenlayersprovideauniversalapproxi- mationframework.SpeciÔ¨Åcally,theuniversalapproximationtheorem(Hornik e t a l .,;,)statesthatafeedforwardnetworkwithalinearoutput 1989Cybenko1989 layerandatleastonehiddenlayerwithany‚Äúsquashing‚Äùactivationfunction(such asthelogisticsigmoidactivationfunction)canapproximateanyBorelmeasurable functionfromoneÔ¨Ånite-dimensional spacetoanotherwithanydesirednon-zero amountoferror,providedthatthenetworkisgivenenoughhiddenunits.The derivativesofthefeedforwardnetworkcanalsoapproximate thederivativesofthe functionarbitrarilywell( ,).TheconceptofBorelmeasurability Hornik e t a l .1990 isbeyondthescopeofthisbook; forourpurposesitsuÔ¨Écestosaythatany continuousfunctiononaclosedandboundedsubsetof RnisBorelmeasurable andthereforemaybeapproximatedbyaneuralnetwork.Aneuralnetworkmay alsoapproximateanyfunctionmappingfromanyÔ¨Ånitedimensionaldiscretespace toanother.WhiletheoriginaltheoremswereÔ¨Årststatedintermsofunitswith activationfunctionsthatsaturatebothforverynegativeandforverypositive arguments,universalapproximation theoremshavealsobeenprovedforawider classofactivationfunctions,whichincludesthenowcommonlyusedrectiÔ¨Åedlinear unit( ,). Leshno e t a l .1993 Theuniversalapproximationtheoremmeansthatregardlessofwhatfunction wearetryingtolearn,weknowthatalargeMLPwillbeableto r e p r e s e ntthis function.However,wearenotguaranteedthatthetrainingalgorithmwillbeable to l e a r nthatfunction.EveniftheMLPisabletorepresentthefunction,learning canfailfortwodiÔ¨Äerentreasons.First,theoptimizationalgorithmusedfortraining 1 9 8 CHAPTER6.DEEPFEEDFORWARDNETWORKS maynotbeabletoÔ¨Åndthevalueoftheparametersthatcorrespondstothedesired function.Second,thetrainingalgorithmmightchoosethewrongfunctiondueto overÔ¨Åtting.Recallfromsectionthatthe‚Äúnofreelunch‚Äùtheoremshowsthat 5.2.1 thereisnouniversallysuperiormachinelearningalgorithm.Feedforwardnetworks provideauniversalsystemforrepresentingfunctions,inthesensethat,givena function,thereexistsafeedforwardnetworkthatapproximatesthefunction.There isnouniversalprocedureforexaminingatrainingsetofspeciÔ¨Åcexamplesand choosingafunctionthatwillgeneralizetopointsnotinthetrainingset. Theuniversalapproximationtheoremsaysthatthereexistsanetworklarge enoughtoachieveanydegreeofaccuracywedesire,butthetheoremdoesnot sayhowlargethisnetworkwillbe.()providessomeboundsonthe Barron1993 sizeofasingle-layernetworkneededtoapproximate abroadclassoffunctions. Unfortunately,intheworsecase,anexponentialnumberofhiddenunits(possibly withonehiddenunitcorrespondingtoeachinputconÔ¨Ågurationthatneedstobe distinguished)mayberequired.Thisiseasiesttoseeinthebinarycase:the numberofpossiblebinaryfunctionsonvectorsv‚àà{0 ,1}nis22nandselecting onesuchfunctionrequires 2nbits,whichwillingeneralrequire O(2n)degreesof freedom. Insummary,afeedforwardnetworkwithasinglelayerissuÔ¨Écienttorepresent anyfunction,butthelayermaybeinfeasiblylargeandmayfailtolearnand generalizecorrectly.Inmanycircumstances,usingdeepermodelscanreducethe numberofunitsrequiredtorepresentthedesiredfunctionandcanreducethe amountofgeneralization error. Thereexistfamiliesoffunctionswhichcanbeapproximated eÔ¨Écientlybyan architecturewithdepthgreaterthansomevalue d,butwhichrequireamuchlarger modelifdepthisrestrictedtobelessthanorequalto d.Inmanycases,thenumber ofhiddenunitsrequiredbytheshallowmodelisexponentialin n. Suchresults wereÔ¨Årstprovedformodelsthatdonotresemblethecontinuous,diÔ¨Äerentiable neuralnetworksusedformachinelearning,buthavesincebeenextendedtothese models.TheÔ¨Årstresultswereforcircuitsoflogicgates(,).Later H√•stad1986 workextendedtheseresultstolinearthresholdunitswithnon-negativeweights ( ,; ,),andthentonetworkswith H√•stadandGoldmann1991Hajnal e t a l .1993 continuous-valuedactivations(,; ,). Manymodern Maass1992Maass e t a l .1994 neuralnetworksuserectiÔ¨Åedlinearunits. ()demonstrated Leshno e t a l .1993 thatshallownetworkswithabroadfamilyofnon-polynomialactivationfunctions, includingrectiÔ¨Åedlinearunits,haveuniversalapproximation properties,butthese resultsdonotaddressthequestionsofdepthoreÔ¨Éciency‚Äîtheyspecifyonlythat asuÔ¨ÉcientlywiderectiÔ¨Åernetworkcouldrepresentanyfunction.Montufar e t a l . 1 9 9 CHAPTER6.DEEPFEEDFORWARDNETWORKS ()showedthatfunctionsrepresentablewithadeeprectiÔ¨Åernetcanrequire 2014 anexponentialnumberofhiddenunitswithashallow(onehiddenlayer)network. Moreprecisely,theyshowedthatpiecewiselinearnetworks(whichcanbeobtained fromrectiÔ¨Åernonlinearities ormaxoutunits)canrepresentfunctionswithanumber ofregionsthatisexponentialinthedepthofthenetwork.Figureillustrateshow 6.5 anetworkwithabsolutevaluerectiÔ¨Åcationcreatesmirrorimagesofthefunction computedontopofsomehiddenunit,withrespecttotheinputofthathidden unit.EachhiddenunitspeciÔ¨Åeswheretofoldtheinputspaceinordertocreate mirrorresponses(onbothsidesoftheabsolutevaluenonlinearity). Bycomposing thesefoldingoperations,weobtainanexponentiallylargenumberofpiecewise linearregionswhichcancaptureallkindsofregular(e.g.,repeating)patterns. Figure6.5:Anintuitive,geometricexplanationoftheexponentialadvantageofdeeper rectiÔ¨Åernetworksformallyby (). Montufar e t a l .2014 ( L e f t )AnabsolutevaluerectiÔ¨Åcation unithasthesameoutputforeverypairofmirrorpointsinitsinput.Themirroraxis ofsymmetryisgivenbythehyperplanedeÔ¨Ånedbytheweightsandbiasoftheunit.A functioncomputedontopofthatunit(thegreendecisionsurface)willbeamirrorimage ofasimplerpatternacrossthataxisofsymmetry.Thefunctioncanbeobtained ( C e n t e r ) byfoldingthespacearoundtheaxisofsymmetry.Anotherrepeatingpatterncan ( R i g h t ) befoldedontopoftheÔ¨Årst(byanotherdownstreamunit)toobtainanothersymmetry (whichisnowrepeatedfourtimes,withtwohiddenlayers).Figurereproducedwith permissionfrom (). Montufar e t a l .2014 Moreprecisely,themaintheoremin ()statesthatthe Montufar e t a l .2014 numberoflinearregionscarvedoutbyadeeprectiÔ¨Åernetworkwith dinputs, depth,andunitsperhiddenlayer,is l n OÓÄ†ÓÄín dÓÄìd l (‚àí 1 ) ndÓÄ° , (6.42) i.e.,exponentialinthedepth.InthecaseofmaxoutnetworkswithÔ¨Åltersper l k</div>
        </div>
    </div>

    <div class="question-card" id="q152">
        <div class="question-header">
            <span class="question-number">Question 152</span>
            <span class="question-type">multiple-choice</span>
        </div>

        <div class="question-text">Deep learning and artificial neural networks have evolved through several historical periods, drawing inspiration from neuroscience but often prioritizing computational efficiency and generality. Understanding the foundational limitations of early models is crucial to appreciating the advances that led to modern deep learning.

Which specific limitation of early linear neural network models, such as the perceptron, contributed significantly to the temporary decline of neural network research prior to the adoption of multi-layer architectures?

1) Inability to solve non-linearly separable problems like the XOR function   
2) Excessive computational requirements for training even small networks   
3) Lack of support for distributed representations across multiple layers   
4) Dependence on non-differentiable activation functions   
5) Poor performance on tasks involving sequence data and temporal relationships   
6) Difficulty integrating probabilistic reasoning and uncertainty modeling   
7) Inability to generalize across different sensory modalities</div>

        <div class="answer-section">
            <div class="answer-label">‚úì Correct Answer:</div>
            <div class="answer-text">The correct answer is 1) Inability to solve non-linearly separable problems like the XOR function.</div>
        </div>

        <div class="reference-section">
            <div class="reference-label">üìö Reference Text:</div>
            <button class="toggle-button" onclick="toggleReference(152)">
                Show/Hide Reference
            </button>
            <div id="ref152" class="reference-text hidden">o r k s(ANNs).Thecorresponding perspectiveondeeplearningmodelsisthattheyareengineeredsystemsinspired bythebiologicalbrain(whetherthehumanbrainorthebrainofanotheranimal). Whilethekindsofneuralnetworksusedformachinelearninghavesometimes beenusedtounderstandbrainfunction( ,),theyare HintonandShallice1991 generallynotdesignedtoberealisticmodelsofbiologicalfunction.Theneural perspectiveondeeplearningismotivatedbytwomainideas.Oneideaisthat thebrainprovidesaproofbyexamplethatintelligentbehaviorispossible,anda conceptuallystraightforwardpathtobuildingintelligenceistoreverseengineerthe computational principlesbehindthebrainandduplicateitsfunctionality.Another perspectiveisthatitwouldbedeeplyinterestingtounderstandthebrainandthe principlesthatunderliehumanintelligence,somachinelearningmodelsthatshed lightonthesebasicscientiÔ¨Åcquestionsareusefulapartfromtheirabilitytosolve engineeringapplications. Themodernterm‚Äúdeeplearning‚ÄùgoesbeyondtheneuroscientiÔ¨Åcperspective onthecurrentbreedofmachinelearningmodels.Itappealstoamoregeneral principleoflearning m u l t i p l e l e v e l s o f c o m p o s i t i o n,whichcanbeappliedinmachine learningframeworksthatarenotnecessarilyneurallyinspired. 1 3 CHAPTER1.INTRODUCTION 1940 1950 1960 1970 1980 1990 2000 Year0.0000000.0000500.0001000.0001500.0002000.000250FrequencyofWordorPhrase c y b e r n e t i c s ( c o n n e c t i o n i s m + n e u r a l n e t w o r k s ) Figure1.7:TheÔ¨ÅgureshowstwoofthethreehistoricalwavesofartiÔ¨Åcialneuralnets research,asmeasuredbythefrequencyofthephrases‚Äúcybernetics‚Äùand‚Äúconnectionism‚Äùor ‚Äúneuralnetworks‚ÄùaccordingtoGoogleBooks(thethirdwaveistoorecenttoappear).The Ô¨Årstwavestartedwithcyberneticsinthe1940s‚Äì1960s, withthedevelopmentoftheories ofbiologicallearning( ,;,)andimplementationsof McCullochandPitts1943Hebb1949 theÔ¨Årstmodelssuchastheperceptron(Rosenblatt1958,)allowingthetrainingofasingle neuron.Thesecondwavestartedwiththeconnectionistapproachofthe1980‚Äì1995period, withback-propagation( ,)totrainaneuralnetworkwithoneortwo Rumelhart e t a l .1986a hiddenlayers.Thecurrentandthirdwave,deeplearning,startedaround2006(Hinton e t a l . e t a l . e t a l . ,;2006Bengio,;2007Ranzato,),andisjustnowappearinginbook 2007a formasof2016.Theothertwowavessimilarlyappearedinbookformmuchlaterthan thecorrespondingscientiÔ¨Åcactivityoccurred. 1 4 CHAPTER1.INTRODUCTION Theearliestpredecessorsofmoderndeeplearningweresimplelinearmodels motivatedfromaneuroscientiÔ¨Åcperspective.Thesemodelsweredesignedto takeasetofninputvalues x 1,...,x nandassociatethemwithanoutput y. Thesemodelswouldlearnasetofweightsw 1,...,w nandcomputetheiroutput f ( x w, ) =x 1w 1 + ¬∑ ¬∑ ¬∑ +x nw n.ThisÔ¨Årstwaveofneuralnetworksresearchwas knownascybernetics,asillustratedinÔ¨Ågure.1.7 TheMcCulloch-PittsNeuron( ,)wasanearlymodel McCullochandPitts1943 ofbrainfunction.ThislinearmodelcouldrecognizetwodiÔ¨Äerentcategoriesof inputsbytestingwhether f ( x w, )ispositiveornegative.Ofcourse,forthemodel tocorrespondtothedesireddeÔ¨Ånitionofthecategories,theweightsneededtobe setcorrectly.Theseweightscouldbesetbythehumanoperator. Inthe1950s, theperceptron(Rosenblatt19581962,,)becametheÔ¨Årstmodelthatcouldlearn theweightsdeÔ¨Åningthecategoriesgivenexamplesofinputsfromeachcategory. The adapt i v e l i near e l e m e n t(ADALINE),whichdatesfromaboutthesame time,simplyreturnedthevalueoff ( x )itselftopredictarealnumber(Widrow andHoÔ¨Ä1960,),andcouldalsolearntopredictthesenumbersfromdata. ThesesimplelearningalgorithmsgreatlyaÔ¨Äectedthemodernlandscapeofma- chinelearning.ThetrainingalgorithmusedtoadapttheweightsoftheADALINE wasaspecialcaseofanalgorithmcalled st o c hast i c g r adi e n t desc e n t.Slightly modiÔ¨Åedversionsofthestochasticgradientdescentalgorithmremainthedominant trainingalgorithmsfordeeplearningmodelstoday. Modelsbasedonthef ( x w, )usedbytheperceptronandADALINEarecalled l i near m o del s.Thesemodelsremainsomeofthemostwidelyusedmachine learningmodels,thoughinmanycasestheyare t r a i ne dindiÔ¨Äerentwaysthanthe originalmodelsweretrained. Linearmodelshavemanylimitations.Mostfamously,theycannotlearnthe XORfunction,where f ( [ 0, 1], w ) = 1and f ( [ 1, 0], w ) = 1butf ( [ 1, 1], w ) = 0 andf ( [ 0, 0], w ) = 0.CriticswhoobservedtheseÔ¨Çawsinlinearmodelscaused abacklashagainstbiologicallyinspiredlearningingeneral(MinskyandPapert, 1969).ThiswastheÔ¨Årstmajordipinthepopularityofneuralnetworks. Today,neuroscienceisregardedasanimportantsourceofinspirationfordeep learningresearchers,butitisnolongerthepredominant guidefortheÔ¨Åeld. Themainreasonforthediminishedrole ofneuroscienceindeeplearning researchtodayisthatwesimplydonothaveenoughinformationaboutthebrain touseitasaguide.Toobtainadeepunderstandingoftheactualalgorithmsused bythebrain,wewouldneedtobeabletomonitortheactivityof(atthevery least)thousandsofinterconnectedneuronssimultaneously.Becausewearenot abletodothis,wearefarfromunderstandingevensomeofthemostsimpleand 1 5 CHAPTER1.INTRODUCTION well-studiedpartsofthebrain( ,). OlshausenandField2005 Neurosciencehasgivenusareasontohopethatasingledeeplearningalgorithm cansolvemanydiÔ¨Äerenttasks.Neuroscientistshavefoundthatferretscanlearnto ‚Äúsee‚Äùwiththeauditoryprocessingregionoftheirbrainiftheirbrainsarerewired tosendvisualsignalstothatarea(VonMelchner 2000 e t a l .,).Thissuggeststhat muchofthemammalianbrainmightuseasinglealgorithmtosolvemostofthe diÔ¨Äerenttasksthatthebrainsolves.Beforethishypothesis,machinelearning researchwasmorefragmented,withdiÔ¨Äerentcommunitiesofresearchersstudying naturallanguageprocessing,vision,motionplanningandspeechrecognition.Today, theseapplicationcommunitiesarestillseparate,butitiscommonfordeeplearning researchgroupstostudymanyorevenalloftheseapplicationareassimultaneously. Weareabletodrawsomeroughguidelinesfromneuroscience.Thebasicideaof havingmanycomputational unitsthatbecomeintelligentonlyviatheirinteractions witheachotherisinspiredbythebrain.TheNeocognitron(Fukushima1980,) introducedapowerfulmodelarchitectureforprocessingimagesthatwasinspired bythestructureofthemammalianvisualsystemandlaterbecamethebasis forthemodernconvolutionalnetwork( ,),aswewillseein LeCun e t a l .1998b section.Mostneuralnetworkstodayarebasedonamodelneuroncalled 9.10 the r e c t i Ô¨Åed l i near uni t.TheoriginalCognitron(Fukushima1975,)introduced amorecomplicatedversionthatwashighlyinspiredbyourknowledgeofbrain function.ThesimpliÔ¨Åedmodernversionwasdevelopedincorporatingideasfrom manyviewpoints,with ()and ()citing NairandHinton2010Glorot e t a l .2011a neuroscienceasaninÔ¨Çuence,and ()citingmoreengineering- Jarrett e t a l .2009 orientedinÔ¨Çuences.Whileneuroscienceisanimportantsourceofinspiration,it neednotbetakenasarigidguide.Weknowthatactualneuronscomputevery diÔ¨ÄerentfunctionsthanmodernrectiÔ¨Åedlinearunits,butgreaterneuralrealism hasnotyetledtoanimprovementinmachinelearningperformance.Also,while neurosciencehassuccessfullyinspiredseveralneuralnetwork a r c h i t e c t u r e s,we donotyetknowenoughaboutbiologicallearningforneurosciencetooÔ¨Äermuch guidanceforthe l e a r ning a l g o r i t h m sweusetotrainthesearchitectures. Mediaaccountsoftenemphasizethesimilarityofdeeplearningtothebrain. Whileitistruethatdeeplearningresearchersaremorelikelytocitethebrainasan inÔ¨ÇuencethanresearchersworkinginothermachinelearningÔ¨Åeldssuchaskernel machinesorBayesianstatistics,oneshouldnotviewdeeplearningasanattempt tosimulatethebrain.ModerndeeplearningdrawsinspirationfrommanyÔ¨Åelds, especiallyappliedmathfundamentalslikelinearalgebra,probability,information theory,andnumericaloptimization. Whilesomedeeplearningresearcherscite neuroscienceasanimportantsourceofinspiration,othersarenotconcernedwith 1 6 CHAPTER1.INTRODUCTION neuroscienceatall. Itis worth notingthat theeÔ¨Äorttounderstandhowthe brainworkson an algorithmic lev el is alive andwell.This endeavor is primarily knownas ‚Äúcomputational neuroscience‚ÄùandisaseparateÔ¨Åeldofstudyfromdeeplearning. ItiscommonforresearcherstomovebackandforthbetweenbothÔ¨Åelds.The Ô¨Åeldofdeeplearningisprimarilyconcernedwithhowtobuildcomputersystems thatareabletosuccessfullysolvetasksrequiringintelligence,whiletheÔ¨Åeldof computational neuroscienceisprimarilyconcernedwithbuildingmoreaccurate modelsofhowthebrainactuallyworks. Inthe1980s,thesecondwaveofneuralnetworkresearchemergedingreat partviaamovementcalled c o nnec t i o n i s mor par al l e l di st r i but e d pr o c e ss- i ng( ,; ,). Connectionism arosein Rumelhart e t a l .1986cMcClelland e t a l .1995 thecontextofcognitivescience.Cognitivescienceisaninterdisciplinaryapproach tounderstandingthemind,combiningmultiplediÔ¨Äerentlevelsofanalysis.During theearly1980s,mostcognitivescientistsstudiedmodelsofsymbolicreasoning. Despitetheirpopularity,symbolicmodelswerediÔ¨Éculttoexplainintermsof howthebraincouldactuallyimplementthemusingneurons.Theconnectionists begantostudymodelsofcognitionthatcouldactuallybegroundedinneural implementations(TouretzkyandMinton1985,),revivingmanyideasdatingback totheworkofpsychologistDonaldHebbinthe1940s(,).Hebb1949 Thecentralideainconnectionism isthatalargenumberofsimplecomputational unitscanachieveintelligentbehaviorwhennetworkedtogether.Thisinsight appliesequallytoneuronsinbiologicalnervoussystemsandtohiddenunitsin computational models. Severalkeyconceptsaroseduringtheconnectionism movementofthe1980s thatremaincentraltotoday‚Äôsdeeplearning. Oneoftheseconceptsisthatof</div>
        </div>
    </div>

    <div class="question-card" id="q153">
        <div class="question-header">
            <span class="question-number">Question 153</span>
            <span class="question-type">multiple-choice</span>
        </div>

        <div class="question-text">Deep learning models frequently use specialized output layers and loss functions to handle tasks like classification, regression, and probabilistic prediction. Proper parametrization is essential for stable training and for the model to represent uncertainty and multimodal distributions effectively.

Which of the following strategies is most effective for ensuring positive-definite covariance matrices when parametrizing full Gaussian outputs in neural networks?

1) Directly predicting covariance matrix entries without constraints   
2) Applying the sigmoid activation to all covariance matrix entries   
3) Using ReLU activation for off-diagonal covariance elements   
4) Summing random matrices to construct the covariance matrix   
5) Predicting variance terms and clipping negative values   
6) Initializing covariance matrices as identity and allowing unconstrained updates   
7) Employing matrix decompositions such as Cholesky factorization during parametrization </div>

        <div class="answer-section">
            <div class="answer-label">‚úì Correct Answer:</div>
            <div class="answer-text">The correct answer is 7) Employing matrix decompositions such as Cholesky factorization during parametrization.</div>
        </div>

        <div class="reference-section">
            <div class="reference-label">üìö Reference Text:</div>
            <button class="toggle-button" onclick="toggleReference(153)">
                Show/Hide Reference
            </button>
            <div id="ref153" class="reference-text hidden">CHAPTER6.DEEPFEEDFORWARDNETWORKS thefractionofcountsofeachoutcomeobservedinthetrainingset: softmax((;))zxŒ∏ i‚âàÓÅêm j = 1 1y() j= i , x() j= xÓÅêm j = 1 1x() j = x. (6.31) Becausemaximumlikelihoodisaconsistentestimator,thisisguaranteedtohappen solongasthemodelfamilyiscapableofrepresentingthetrainingdistribution.In practice,limitedmodelcapacityandimperfectoptimization willmeanthatthe modelisonlyabletoapproximatethesefractions. Manyobjectivefunctionsotherthanthelog-likelihooddonotworkaswell withthesoftmaxfunction.SpeciÔ¨Åcally,objectivefunctionsthatdonotusealogto undotheexpofthesoftmaxfailtolearnwhentheargumenttotheexpbecomes verynegative,causingthegradienttovanish.Inparticular,squarederrorisa poorlossfunctionforsoftmaxunits,andcanfailtotrainthemodeltochangeits output,evenwhenthemodelmakeshighlyconÔ¨Ådentincorrectpredictions(,Bridle 1990).Tounderstandwhytheseotherlossfunctionscanfail,weneedtoexamine thesoftmaxfunctionitself. Likethesigmoid,thesoftmaxactivationcansaturate.Thesigmoidfunctionhas asingleoutputthatsaturateswhenitsinputisextremelynegativeorextremely positive.Inthecaseofthesoftmax,therearemultipleoutputvalues.These outputvaluescansaturatewhenthediÔ¨Äerencesbetweeninputvaluesbecome extreme.Whenthesoftmaxsaturates,manycostfunctionsbasedonthesoftmax alsosaturate,unlesstheyareabletoinvertthesaturatingactivatingfunction. ToseethatthesoftmaxfunctionrespondstothediÔ¨Äerencebetweenitsinputs, observethatthesoftmaxoutputisinvarianttoaddingthesamescalartoallofits inputs: softmax() = softmax(+) zz c . (6.32) Usingthisproperty,wecanderiveanumericallystablevariantofthesoftmax: softmax() = softmax( max zz‚àí iz i) . (6.33) Thereformulatedversionallowsustoevaluatesoftmaxwithonlysmallnumerical errorsevenwhen zcontainsextremelylargeorextremelynegativenumbers.Ex- aminingthenumericallystablevariant,weseethatthesoftmaxfunctionisdriven bytheamountthatitsargumentsdeviatefrommax i z i. Anoutput softmax(z) isaturatestowhenthecorrespondinginputismaximal 1 ( z i=max i z i)and z iismuchgreaterthanalloftheotherinputs.Theoutput softmax(z) icanalsosaturatetowhen0 z iisnotmaximalandthemaximumis muchgreater.Thisisageneralization ofthewaythatsigmoidunitssaturate,and 1 8 6 CHAPTER6.DEEPFEEDFORWARDNETWORKS cancausesimilardiÔ¨Écultiesforlearningifthelossfunctionisnotdesignedto compensateforit. TheargumentztothesoftmaxfunctioncanbeproducedintwodiÔ¨Äerentways. Themostcommonissimplytohaveanearlierlayeroftheneuralnetworkoutput everyelementofz,asdescribedaboveusingthelinearlayerz=WÓÄæh+b.While straightforward,thisapproachactuallyoverparametrizes thedistribution.The constraintthatthe noutputsmustsumtomeansthatonly 1 n‚àí1parametersare necessary;theprobabilityofthe n-thvaluemaybeobtainedbysubtractingthe Ô¨Årst n‚àí1 1 probabilitiesfrom.Wecanthusimposearequirementthatoneelement ofzbeÔ¨Åxed.Forexample,wecanrequirethat z n=0.Indeed,thisisexactly whatthesigmoidunitdoes.DeÔ¨Åning P( y= 1|x) = œÉ( z)isequivalenttodeÔ¨Åning P( y= 1|x) =softmax(z) 1withatwo-dimensionalzand z 1= 0.Boththe n‚àí1 argumentandthe nargumentapproachestothesoftmaxcandescribethesame setofprobabilitydistributions,buthavediÔ¨Äerentlearningdynamics.Inpractice, thereisrarelymuchdiÔ¨Äerencebetweenusingtheoverparametrized versionorthe restrictedversion,anditissimplertoimplementtheoverparametrized version. FromaneuroscientiÔ¨Åcpointofview,itisinterestingtothinkofthesoftmaxas awaytocreateaformofcompetitionbetweentheunitsthatparticipateinit:the softmaxoutputsalwayssumto1soanincreaseinthevalueofoneunitnecessarily correspondstoadecreaseinthevalueofothers.Thisisanalogoustothelateral inhibitionthatisbelievedtoexistbetweennearbyneuronsinthecortex.Atthe extreme(whenthediÔ¨Äerencebetweenthemaximal a iandtheothersislargein magnitude)itbecomesaformofwinner-take-all(oneoftheoutputsisnearly1 andtheothersarenearly0). Thename‚Äúsoftmax‚Äùcanbesomewhatconfusing.Thefunctionismoreclosely relatedtotheargmaxfunctionthanthemaxfunction. Theterm‚Äúsoft‚Äùderives fromthefactthatthesoftmaxfunctioniscontinuousanddiÔ¨Äerentiable. The argmaxfunction,withitsresultrepresentedasaone-hotvector,isnotcontinuous ordiÔ¨Äerentiable. Thesoftmaxfunctionthusprovidesa‚Äúsoftened‚Äùversionofthe argmax.Thecorrespondingsoftversionofthemaximumfunctionissoftmax(z)ÓÄæz. Itwouldperhapsbebettertocallthesoftmaxfunction‚Äúsoftargmax,‚Äù butthe currentnameisanentrenchedconvention. 6.2.2.4OtherOutputTypes Thelinear, sigmoid, andsoftmaxoutputunitsdescribedabovearethemost common.Neuralnetworkscangeneralizetoalmostanykindofoutputlayerthat wewish.Theprincipleofmaximumlikelihoodprovidesaguideforhowtodesign 1 8 7 CHAPTER6.DEEPFEEDFORWARDNETWORKS agoodcostfunctionfornearlyanykindofoutputlayer. Ingeneral,ifwedeÔ¨Åneaconditionaldistribution p(yx|;Œ∏),theprincipleof maximumlikelihoodsuggestsweuse asourcostfunction. ‚àí | log( pyxŒ∏;) Ingeneral,wecanthinkoftheneuralnetworkasrepresentingafunction f(x;Œ∏). Theoutputsofthisfunctionarenotdirectpredictionsofthevaluey.Instead, f(x;Œ∏) =œâprovidestheparametersforadistributionover y.Ourlossfunction canthenbeinterpretedas . ‚àílog(;()) p yœâx Forexample,wemaywishtolearnthevarianceofaconditionalGaussianfor y, given x.Inthesimplecase,wherethevariance œÉ2isaconstant,thereisaclosed formexpressionbecausethemaximumlikelihoodestimatorofvarianceissimplythe empiricalmeanofthesquareddiÔ¨Äerencebetweenobservations yandtheirexpected value.Acomputationally moreexpensiveapproachthatdoesnotrequirewriting special-casecodeistosimplyincludethevarianceasoneofthepropertiesofthe distribution p( y|x)thatiscontrolledbyœâ= f(x;Œ∏).Thenegativelog-likelihood ‚àílog p(y;œâ(x))willthenprovideacostfunctionwiththeappropriateterms necessarytomakeouroptimization procedureincrementally learnthevariance.In thesimplecasewherethestandarddeviationdoesnotdependontheinput,we canmakeanewparameterinthenetworkthatiscopieddirectlyintoœâ.Thisnew parametermightbe œÉitselforcouldbeaparameter vrepresenting œÉ2oritcould beaparameter Œ≤representing1 œÉ2,dependingonhowwechoosetoparametrize thedistribution.WemaywishourmodeltopredictadiÔ¨Äerentamountofvariance in yfordiÔ¨Äerentvaluesof x.Thisiscalledaheteroscedasticmodel.Inthe heteroscedasticcase,wesimplymakethespeciÔ¨Åcationofthevariancebeoneof thevaluesoutputby f( x;Œ∏).AtypicalwaytodothisistoformulatetheGaussian distributionusingprecision,ratherthanvariance,asdescribedinequation.3.22 Inthemultivariatecaseitismostcommontouseadiagonalprecisionmatrix diag (6.34) ()Œ≤ . Thisformulationworkswellwithgradientdescentbecausetheformulaforthe log-likelihoodoftheGaussiandistributionparametrized byŒ≤involvesonlymul- tiplicationby Œ≤ iandadditionoflogŒ≤ i.Thegradientofmultiplication, addition, andlogarithmoperationsiswell-behaved.Bycomparison,ifweparametrized the outputintermsofvariance,wewouldneedtousedivision.Thedivisionfunction becomesarbitrarilysteepnearzero.Whilelargegradientscanhelplearning, arbitrarilylargegradientsusuallyresultininstability.Ifweparametrized the outputintermsofstandarddeviation,thelog-likelihoodwouldstillinvolvedivision, andwouldalsoinvolvesquaring.Thegradientthroughthesquaringoperation canvanishnearzero,makingitdiÔ¨Éculttolearnparametersthataresquared. 1 8 8 CHAPTER6.DEEPFEEDFORWARDNETWORKS Regardlessofwhetherweusestandarddeviation,variance,orprecision,wemust ensurethatthecovariancematrixoftheGaussianispositivedeÔ¨Ånite. Because theeigenvaluesoftheprecisionmatrixarethereciprocalsoftheeigenvaluesof thecovariancematrix,thisisequivalenttoensuringthattheprecisionmatrixis positivedeÔ¨Ånite.Ifweuseadiagonalmatrix,orascalartimesthediagonalmatrix, thentheonlyconditionweneedtoenforceontheoutputofthemodelispositivity. Ifwesupposethataistherawactivationofthemodelusedtodeterminethe diagonalprecision,wecanusethesoftplusfunctiontoobtainapositiveprecision vector:Œ≤= Œ∂(a) .Thissamestrategyappliesequallyifusingvarianceorstandard deviationratherthanprecisionorifusingascalartimesidentityratherthan diagonalmatrix. Itisraretolearnacovarianceorprecisionmatrixwithricherstructurethan diagonal. Ifthecovarianceisfullandconditional,thenaparametrization must bechosenthatguaranteespositive-deÔ¨Ånitenessofthepredictedcovariancematrix. Thiscanbeachievedbywriting Œ£() = ()xBxBÓÄæ()x,whereBisanunconstrained squarematrix.Onepracticalissueifthematrixisfullrankisthatcomputingthe likelihoodisexpensive,witha d d√ómatrixrequiring O( d3)computationforthe determinantandinverseof Œ£(x)(orequivalently,andmorecommonlydone,its eigendecompositionorthatof).Bx() Weoftenwanttoperformmultimodalregression,thatis,topredictrealvalues thatcomefromaconditionaldistribution p(yx|)thatcanhaveseveraldiÔ¨Äerent peaksinyspaceforthesamevalueofx.Inthiscase,aGaussianmixtureis anaturalrepresentationfortheoutput( ,;,). Jacobs e t a l .1991Bishop1994 NeuralnetworkswithGaussianmixturesastheiroutputareoftencalledmixture densitynetworks.AGaussianmixtureoutputwith ncomponentsisdeÔ¨Ånedby theconditionalprobabilitydistribution p( ) =yx|nÓÅò i = 1p i (= c |Nx)(;y¬µ( ) i()x , Œ£( ) i())x .(6.35) Theneuralnetworkmusthavethreeoutputs:avectordeÔ¨Åning p(c= i|x),a matrixproviding¬µ( ) i(x)forall i,andatensorproviding Œ£( ) i(x)forall i.These outputsmustsatisfydiÔ¨Äerentconstraints: 1.Mixturecomponents p(c= i|x):theseformamultinoullidistribution overthe ndiÔ¨Äerentcomponentsassociatedwithlatentvariable1c,andcan 1W e c o n s i d e r c t o b e l a t e n t b e c a u s e we d o n o t o b s e rv e i t i n t h e d a t a : g i v e n i n p u t x a n d t a rg e t y , i t i s n o t p o s s i b l e t o k n o w with c e rta i n t y wh i c h Ga u s s i a n c o m p o n e n t wa s re s p o n s i b l e f o r y , b u t w e c a n i m a g i n e t h a t y w a s g e n e ra t</div>
        </div>
    </div>

    <div class="question-card" id="q154">
        <div class="question-header">
            <span class="question-number">Question 154</span>
            <span class="question-type">multiple-choice</span>
        </div>

        <div class="question-text">In deep learning, regularization techniques are essential for preventing overfitting and improving the generalization capability of neural networks. Multiple approaches, including stochastic methods and noise injection, have been developed to enhance model robustness.

Which regularization technique specifically applies multiplicative noise to hidden units, approximates bagging through the training of many sub-models with shared parameters, and is particularly effective at forcing neural networks to learn redundant and robust features?

1) Weight decay   
2) Batch normalization   
3) Stochastic pooling   
4) Dropout   
5) Dataset augmentation   
6) DropConnect   
7) Tangent prop</div>

        <div class="answer-section">
            <div class="answer-label">‚úì Correct Answer:</div>
            <div class="answer-text">The correct answer is 4) Dropout.</div>
        </div>

        <div class="reference-section">
            <div class="reference-label">üìö Reference Text:</div>
            <button class="toggle-button" onclick="toggleReference(154)">
                Show/Hide Reference
            </button>
            <div id="ref154" class="reference-text hidden">CHAPTER7.REGULARIZATIONFORDEEPLEARNING eachinputfeature.Themagnitudeofeachfeature‚ÄôsweightdecaycoeÔ¨Écientis determinedbyitsvariance.Similarresultsholdforotherlinearmodels.Fordeep models,dropoutisnotequivalenttoweightdecay. Thestochasticityusedwhiletrainingwithdropoutisnotnecessaryforthe approach‚Äôssuccess.Itisjustameansofapproximating thesumoverallsub- models.WangandManning2013()derivedanalyticalapproximationstothis marginalization. Theirapproximation,knownasfastdropoutresultedinfaster convergencetimeduetothereducedstochasticityinthecomputationofthe gradient.Thismethodcanalsobeappliedattesttime,asamoreprincipled (butalsomorecomputationally expensive)approximation totheaverageoverall sub-networksthantheweightscalingapproximation.Fastdropouthasbeenused tonearlymatchtheperformanceofstandarddropoutonsmallneuralnetwork problems,buthasnotyetyieldedasigniÔ¨Åcantimprovementorbeenappliedtoa largeproblem. Justasstochasticityisnotnecessarytoachievetheregularizing eÔ¨Äect of dropout,itisalsonotsuÔ¨Écient.Todemonstratethis,Warde-Farley2014etal.() designedcontrolexperimentsusingamethodcalleddropoutboostingthatthey designedtouseexactlythesamemasknoiseastraditionaldropoutbutlack itsregularizingeÔ¨Äect.Dropoutboostingtrainstheentireensembletojointly maximizethelog-likelihoodonthetrainingset.Inthesamesensethattraditional dropoutisanalogoustobagging, this approachisanalogoustoboosting.As intended,experimentswithdropoutboostingshowalmostnoregularizationeÔ¨Äect comparedtotrainingtheentirenetworkasasinglemodel.Thisdemonstratesthat theinterpretationofdropoutasbagginghasvaluebeyondtheinterpretationof dropoutasrobustnesstonoise.TheregularizationeÔ¨Äectofthebaggedensembleis onlyachievedwhenthestochasticallysampledensemblemembersaretrainedto performwellindependently ofeachother. Dropouthasinspiredotherstochasticapproachestotrainingexponentially largeensemblesofmodelsthatshareweights. DropConnectisaspecialcaseof dropoutwhereeachproductbetweenasinglescalarweightandasinglehidden unitstateisconsideredaunitthatcanbedropped(Wan2013etal.,).Stochastic poolingisaformofrandomizedpooling(seesection)forbuildingensembles 9.3 ofconvolutionalnetworkswitheachconvolutionalnetworkattendingtodiÔ¨Äerent spatiallocationsofeachfeaturemap. Sofar,dropoutremainsthemostwidely usedimplicitensemblemethod. Oneofthekeyinsightsofdropoutisthattraininganetworkwithstochastic behaviorandmakingpredictionsbyaveragingovermultiplestochasticdecisions implementsaformofbaggingwithparametersharing.Earlier, wedescribed 2 6 6 CHAPTER7.REGULARIZATIONFORDEEPLEARNING dropoutas bagginganensembleofmodelsformedbyincludingor excluding units.However,thereisnoneedforthismodelaveragingstrategytobebasedon inclusionandexclusion.Inprinciple,anykindofrandommodiÔ¨Åcationisadmissible. Inpractice,wemustchoosemodiÔ¨Åcationfamiliesthatneuralnetworksareable tolearntoresist.Ideally,weshouldalsousemodelfamiliesthatallowafast approximateinferencerule.WecanthinkofanyformofmodiÔ¨Åcationparametrized byavector¬µastraininganensembleconsistingof p( y ,|x¬µ)forallpossible valuesof¬µ.Thereisnorequirementthat¬µhaveaÔ¨Ånitenumberofvalues.For example,¬µcanbereal-valued.Srivastava2014etal.()showedthatmultiplyingthe weightsby¬µ‚àºN( 1 , I)canoutperformdropoutbasedonbinarymasks.Because E[¬µ] = 1thestandardnetworkautomatically implementsapproximate inference intheensemble,withoutneedinganyweightscaling. SofarwehavedescribeddropoutpurelyasameansofperformingeÔ¨Écient, approximatebagging.However,thereisanotherviewofdropoutthatgoesfurther thanthis.Dropouttrainsnotjustabaggedensembleofmodels,butanensemble ofmodelsthatsharehiddenunits.Thismeanseachhiddenunitmustbeableto performwellregardlessofwhichotherhiddenunitsareinthemodel.Hiddenunits mustbepreparedtobeswappedandinterchangedbetweenmodels.Hintonetal. ()wereinspiredbyanideafrombiology:sexualreproduction,whichinvolves 2012c swappinggenesbetweentwodiÔ¨Äerentorganisms,createsevolutionarypressurefor genestobecomenotjustgood,buttobecomereadilyswappedbetweendiÔ¨Äerent organisms.Suchgenesandsuchfeaturesareveryrobusttochangesintheir environmentbecausetheyarenotabletoincorrectlyadapttounusualfeatures ofanyoneorganismormodel.Dropoutthusregularizeseachhiddenunittobe notmerelyagoodfeaturebutafeaturethatisgoodinmanycontexts. Warde- Farley2014etal.()compareddropouttrainingtotrainingoflargeensemblesand concludedthatdropoutoÔ¨Äersadditionalimprovementstogeneralization error beyondthoseobtainedbyensemblesofindependentmodels. Itisimportanttounderstandthatalargeportionofthepowerofdropout arisesfromthefactthatthemaskingnoiseisappliedtothehiddenunits.This canbeseenasaformofhighlyintelligent,adaptivedestructionoftheinformation contentoftheinputratherthandestructionoftherawvaluesoftheinput.For example,ifthemodellearnsahiddenunit h ithatdetectsafacebyÔ¨Åndingthenose, thendropping h icorrespondstoerasingtheinformationthatthereisanosein theimage.Themodelmustlearnanother h i,eitherthatredundantlyencodesthe presenceofanose,orthatdetectsthefacebyanotherfeature,suchasthemouth. Traditionalnoiseinjectiontechniquesthataddunstructurednoiseattheinputare notabletorandomlyerasetheinformationaboutanosefromanimageofaface unlessthemagnitudeofthenoiseissogreatthatnearlyalloftheinformationin 2 6 7 CHAPTER7.REGULARIZATIONFORDEEPLEARNING theimageisremoved.Destroyingextractedfeaturesratherthanoriginalvalues allowsthedestructionprocesstomakeuseofalloftheknowledgeabouttheinput distributionthatthemodelhasacquiredsofar. Anotherimportantaspectofdropoutisthatthenoiseismultiplicative. Ifthe noisewereadditivewithÔ¨Åxedscale,thenarectiÔ¨Åedlinearhiddenunit h iwith addednoise ÓÄècouldsimplylearntohave h ibecomeverylargeinordertomake theaddednoise ÓÄèinsigniÔ¨Åcantbycomparison.Multiplicativenoisedoesnotallow suchapathologicalsolutiontothenoiserobustnessproblem. Anotherdeeplearningalgorithm,batchnormalization, reparametrizes themodel inawaythatintroducesbothadditiveandmultiplicativenoiseonthehidden unitsattrainingtime.Theprimarypurposeofbatchnormalization istoimprove optimization, butthenoisecanhavearegularizingeÔ¨Äect,andsometimesmakes dropoutunnecessary.Batchnormalization isdescribedfurtherinsection.8.7.1 7.13AdversarialTraining Inmanycases,neuralnetworkshavebeguntoreachhumanperformancewhen evaluatedonani.i.d.testset.Itisnaturalthereforetowonderwhetherthese modelshaveobtainedatruehuman-levelunderstandingofthesetasks.Inorder toprobethelevelofunderstandinganetworkhasoftheunderlyingtask,wecan searchforexamplesthatthemodelmisclassiÔ¨Åes. ()foundthat Szegedy etal.2014b evenneuralnetworksthatperformathumanlevelaccuracyhaveanearly100% errorrateonexamplesthatareintentionallyconstructedbyusinganoptimization proceduretosearchforaninputxÓÄ∞nearadatapointxsuchthatthemodel outputisverydiÔ¨ÄerentatxÓÄ∞.Inmanycases,xÓÄ∞canbesosimilartoxthata humanobservercannottellthediÔ¨Äerencebetweentheoriginalexampleandthe adversarialexample,butthenetworkcanmakehighlydiÔ¨Äerentpredictions.See Ô¨Ågureforanexample.7.8 Adversarialexampleshavemanyimplications,forexample,incomputersecurity, thatarebeyondthescopeofthischapter. However,theyareinterestinginthe contextofregularizationbecauseonecanreducetheerrorrateontheoriginali.i.d. testsetviaadversarialtraining‚Äîtrainingonadversariallyperturbedexamples fromthetrainingset( ,; Szegedy etal.2014bGoodfellow2014betal.,). Goodfellow2014betal.()showedthatoneoftheprimarycausesofthese adversarial examplesis excessive linearity.Neural networks arebuilt out of primarilylinearbuildingblocks. Insomeexperimentstheoverallfunctionthey implementprovestobehighlylinearasaresult.Theselinearfunctionsareeasy 2 6 8 CHAPTER7.REGULARIZATIONFORDEEPLEARNING + .007√ó = x sign(‚àá x J(Œ∏x , , y))x+ ÓÄèsign(‚àá x J(Œ∏x , , y)) y=‚Äúpanda‚Äù ‚Äúnematode‚Äù‚Äúgibbon‚Äù w/57.7% conÔ¨Ådencew/8.2% conÔ¨Ådencew/99.3% conÔ¨Ådence Figure7.8: AdemonstrationofadversarialexamplegenerationappliedtoGoogLeNet ( ,)onImageNet.Byaddinganimperceptiblysmallvectorwhose Szegedy e t a l .2014a elementsareequaltothesignoftheelementsofthegradientofthecostfunctionwith respecttotheinput,wecanchangeGoogLeNet‚ÄôsclassiÔ¨Åcationoftheimage.Reproduced withpermissionfrom (). Goodfellow e t a l .2014b tooptimize.Unfortunately,thevalueofalinearfunctioncanchangeveryrapidly ifithasnumerousinputs.Ifwechangeeachinputby ÓÄè,thenalinearfunction withweightswcanchangebyasmuchas ÓÄè||||w 1,whichcanbeaverylarge amountifwishigh-dimensional.Adversarialtrainingdiscouragesthishighly sensitivelocallylinearbehaviorbyencouragingthenetworktobelocallyconstant intheneighborhoodofthetrainingdata.Thiscanbeseenasawayofexplicitly introducingalocalconstancypriorintosupervisedneuralnets. Adversarialtraininghelpstoillustratethepowerofusingalargefunction familyincombinationwithaggressiveregularization. Purelylinearmodels,like logisticregression,arenotabletoresistadversarialexamplesbecausetheyare forcedtobelinear.Neuralnetworksareabletorepresentfunctionsthatcanrange fromnearlylineartonearlylocallyconstantandthushavetheÔ¨Çexibilitytocapture lineartrendsinthetrainingdatawhilestilllearningtoresistlocalperturbation. Adversarialexamplesalsoprovideameansofaccomplishingsemi-supervised learning.Atapointxthatisnotassociatedwithalabelinthedataset,the modelitselfassignssomelabel ÀÜ y.Themodel‚Äôslabel ÀÜ ymaynotbethetruelabel, butifthemodelishighquality,thenÀÜ yhasahighprobabilityofprovidingthe truelabel.WecanseekanadversarialexamplexÓÄ∞thatcausestheclassiÔ¨Åerto outputalabel yÓÄ∞with yÓÄ∞ÓÄ∂=ÀÜ y.Adversarialexamplesgeneratedusingnotthetrue labelbutalabelprovidedbyatrainedmodelarecalledvirtualadversarial examples(Miyato2015etal.,).TheclassiÔ¨Åermaythenbetrainedtoassignthe samelabeltoxandxÓÄ∞.ThisencouragestheclassiÔ¨Åertolearnafunctionthatis 2 6 9 CHAPTER7.REGULARIZATIONFORDEEPLEARNING robusttosmallchangesanywherealongthemanifoldwheretheunlabeleddata lies.TheassumptionmotivatingthisapproachisthatdiÔ¨Äerentclassesusuallylie ondisconnectedmanifolds,andasmallperturbationshouldnotbeabletojump fromoneclassmanifoldtoanotherclassmanifold. 7.14Tangent Distance, TangentProp,and Manifold TangentClassiÔ¨Åer Manymachinelearningalgorithmsaimtoovercomethecurseofdimensionality byassumingthatthedataliesnearalow-dimensional manifold,asdescribedin section.5.11.3 Oneoftheearlyattemptstotakeadvantageofthemanifoldhypothesisisthe tangentdistancealgorithm( ,,).Itisanon-parametric Simard etal.19931998 nearest-neighboralgorithminwhichthemetricusedisnotthegenericEuclidean distancebutonethatisderivedfromknowledgeofthemanifoldsnearwhich probabilityconcentrates.Itisassumedthatwearetryingtoclassifyexamplesand thatexamplesonthesamemanifoldsharethesamecategory.SincetheclassiÔ¨Åer shouldbeinvarianttothelocalfactorsofvariationthatcorrespondtomovement onthemanifold,itwouldmakesensetouseasnearest-neighbordistancebetween pointsx 1andx 2thedistancebetweenthemanifolds M 1and M 2towhichthey respectivelybelong.Althoughthatmaybecomputationally diÔ¨Écult(itwould requiresolvinganoptimization problem,toÔ¨Åndthenearestpairofpointson M 1 and M 2),acheapalternativethatmakessenselocallyistoapproximate M ibyits tangentplaneatx iandmeasurethedistancebetweenthetwotangents,orbetween atangentplaneandapoint.Thatcanbeachievedbysolvingalow-dimensional linearsystem(inthedimensionofthemanifolds).Ofcourse,thisalgorithmrequires onetospecifythetangentvectors. Inarelatedspirit,thetangentpropalgorithm( ,)(Ô¨Ågure) Simardetal.19927.9 trainsaneuralnetclassiÔ¨Åerwithanextrapenaltytomakeeachoutput f(x)of theneuralnetlocallyinvarianttoknownfactorsofvariation.Thesefactorsof variationcorrespondtomovementalongthemanifoldnearwhichexamplesofthe sameclassconcentrate.Localinvarianceisachievedbyrequiring ‚àá x f(x)tobe orthogonaltotheknownmanifoldtangentvectorsv( ) iatx,orequivalentlythat thedirectionalderivativeof fatxinthedirectionsv( ) ibesmallbyaddinga regularizationpenalty:‚Ñ¶ ‚Ñ¶() = fÓÅò iÓÄê (‚àá x f())xÓÄæv( ) iÓÄë2 . (7.67) 2 7 0 CHAPTER7.REGULARIZATIONFORDEEPLEARNING Thisregularizercanofcoursebescaledbyanappropriatehyperparameter, and,for mostneuralnetworks,wewouldneedtosumovermanyoutputsratherthanthelone output f(x)describedhereforsimplicity.Aswiththetangentdistancealgorithm, thetangentvectorsarederivedapriori,usuallyfromtheformalknowledgeof theeÔ¨Äectoftransformationssuchastranslation,rotation,andscalinginimages. Tangentprophasbeenusednotjustforsupervisedlearning( ,) Simardetal.1992 butalsointhecontextofreinforcementlearning(,). Thrun1995 Tangentpropagation is closelyrelated todataset augmentation.In both cases,theuserofthealgorithmencodeshisorherpriorknowledgeofthetask byspecifyingasetoftransformationsthatshouldnotaltertheoutputofthe network.ThediÔ¨Äerenceisthatinthecaseofdatasetaugmentation, thenetworkis explicitlytrainedtocorrectlyclassifydistinctinputsthatwerecreatedbyapplying morethananinÔ¨Ånitesimalamountofthesetransformations.Tangentpropagation doesnotrequireexplicitlyvisitinganewinputpoint.Instead,itanalytically regularizesthemodeltoresistperturbationinthedirectionscorrespondingto the speciÔ¨Åed transformation.While thisanalytical approac h isintellectually elegant,ithastwomajordrawbacks.First,itonlyregularizesthemodeltoresist inÔ¨Ånitesimalperturbation.Explicitdatasetaugmentationconfersresistanceto largerperturbations.Second,theinÔ¨ÅnitesimalapproachposesdiÔ¨Écultiesformodels basedonrectiÔ¨Åedlinearunits.Thesemodelscanonlyshrinktheirderivatives byturningunitsoÔ¨Äorshrinkingtheirweights.Theyarenotabletoshrinktheir derivativesbysaturatingatahighvaluewithlargeweights,assigmoidortanh unitscan.Datasetaugmentation workswellwithrectiÔ¨Åedlinearunitsbecause diÔ¨ÄerentsubsetsofrectiÔ¨ÅedunitscanactivatefordiÔ¨Äerenttransformedversionsof eachoriginalinput. Tangentpropagationisalsorelatedtodoublebackprop(DruckerandLeCun, 1992)andadversarialtraining( ,; ,). Szegedy etal.2014bGoodfellowetal.2014b DoublebackpropregularizestheJacobiantobesmall,whileadversarialtraining Ô¨Åndsinputsneartheoriginalinputsandtrainsthemodeltoproducethesame outputontheseasontheoriginalinputs.Tangentpropagation anddataset augmentationusingmanuallyspeciÔ¨Åedtransformationsbothrequirethatthe modelshouldbeinvarianttocertainspeciÔ¨Åeddirectionsofchangeintheinput. Doublebackpropandadversarialtrainingbothrequirethatthemodelshouldbe invarianttodirectionsofchangeintheinputsolongasthechangeissmall.Just all asdatasetaugmentationisthenon-inÔ¨Ånitesimalversionoftangentpropagation, adversarialtrainingisthenon-inÔ¨Ånitesimalversionofdoublebackprop. ThemanifoldtangentclassiÔ¨Åer(,),eliminatestheneedto Rifaietal.2011c knowthetangentvectorsapriori.Aswewillseeinchapter,autoencoderscan 14 2 7 1 CHAPTER7.REGULARIZATIONFORDEEPLEARNING x 1x 2N o r m a lT a ng e nt Figure7.9: Illustrationofthemainideaofthetangentpropalgorithm( , Simard e t a l . 1992 Rifai2011c )andmanifoldtangentclassiÔ¨Åer( e t a l .,),whichbothregularizethe classiÔ¨Åeroutputfunction f(x).EachcurverepresentsthemanifoldforadiÔ¨Äerentclass, illustratedhereasaone-dimensionalmanifoldembeddedinatwo-dimensionalspace. Ononecurve,wehavechosenasinglepointanddrawnavectorthatistangenttothe classmanifold(paralleltoandtouchingthemanifold)andavectorthatisnormaltothe classmanifold(orthogonaltothemanifold).Inmultipledimensionstheremaybemany tangentdirectionsandmanynormaldirections.WeexpecttheclassiÔ¨Åcationfunctionto changerapidlyasitmovesinthedirectionnormaltothemanifold,andnottochangeas itmovesalongtheclassmanifold.Bothtangentpropagationandthemanifoldtangent classiÔ¨Åerregularize f(x) tonotchangeverymuchasxmovesalongthemanifold.Tangent propagationrequirestheusertomanuallyspecifyfunctionsthatcomputethetangent directions(suchasspecifyingthatsmalltranslationsofimagesremaininthesameclass manifold)whilethemanifoldtangentclassiÔ¨Åerestimatesthemanifoldtangentdirections bytraininganautoencodertoÔ¨Åtthetrainingdata.Theuseofautoencoderstoestimate manifoldswillbedescribedinchapter.14 estimatethemanifoldtangentvectors.ThemanifoldtangentclassiÔ¨Åermakesuse ofthistechniquetoavoidneedinguser-speciÔ¨Åedtangentvectors. Asillustrated inÔ¨Ågure,theseestimatedtangentvectorsgobeyondtheclassicalinvariants 14.10 thatariseoutofthegeometryofimages(suchastranslation,rotationandscaling) andincludefactorsthatmustbelearnedbecausetheyareobject-speciÔ¨Åc(suchas movingbodyparts).ThealgorithmproposedwiththemanifoldtangentclassiÔ¨Åer isthereforesimple:(1)useanautoencodertolearnthemanifoldstructureby unsupervisedlearning,and(2)usethesetangentstoregularizeaneuralnetclassiÔ¨Åer asintangentprop(equation).7.67 Thischapterhasdescribedmostofthegeneralstrategiesusedtoregularize neuralnetworks.Regularizationisacentralthemeofmachinelearningandassuch 2 7 2 CHAPTER7.REGULARIZATIONFORDEEPLEARNING willberevisitedperiodicallybymostoftheremainingchapters.Anothercentral themeofmachinelearningisoptimization, describednext. 2 7 3 C h a p t e r 8 OptimizationforTrainingDeep Models Deeplearningalgorithmsinvolveoptimization inmanycontexts.Forexample, performinginferenceinmodelssuchasPCAinvolvessolvinganoptimization problem.Weoftenuseanalyticaloptimization towriteproofsordesignalgorithms. Ofallofthemanyoptimization</div>
        </div>
    </div>

    <div class="question-card" id="q155">
        <div class="question-header">
            <span class="question-number">Question 155</span>
            <span class="question-type">multiple-choice</span>
        </div>

        <div class="question-text">In probabilistic modeling and variational inference, calculus of variations is used to optimize functionals over probability distributions, often under constraints like normalization, fixed mean, and fixed variance. The principle of maximum entropy guides the selection of distributions when limited information is available.

Which probability distribution arises as the solution to maximizing differential entropy for continuous variables subject to normalization, fixed mean, and fixed variance constraints?

1) Uniform distribution   
2) Exponential distribution   
3) Bernoulli distribution   
4) Laplace distribution   
5) Normal (Gaussian) distribution   
6) Beta distribution   
7) Dirac delta distribution</div>

        <div class="answer-section">
            <div class="answer-label">‚úì Correct Answer:</div>
            <div class="answer-text">The correct answer is 5) Normal (Gaussian) distribution.</div>
        </div>

        <div class="reference-section">
            <div class="reference-label">üìö Reference Text:</div>
            <button class="toggle-button" onclick="toggleReference(155)">
                Show/Hide Reference
            </button>
            <div id="ref155" class="reference-text hidden">CHAPTER19.APPROXIMATEINFERENCE Togainsomeintuitionforthisidentity,onecanthinkof f( x)asbeingavector withuncountablymanyelements,indexedbyarealvector x.Inthis(somewhat incompleteview),theidentityprovidingthefunctionalderivativesisthesameas wewouldobtainforavector Œ∏‚àà Rnindexedbypositiveintegers: ‚àÇ ‚àÇ Œ∏ iÓÅò jg Œ∏( j , j) =‚àÇ ‚àÇ Œ∏ ig Œ∏( i , i .) (19.47) Manyresultsinothermachinelearningpublicationsarepresentedusingthemore generalEuler-Lagrangeequationwhichallows gtodependonthederivatives of faswellasthevalueof f,butwedonotneedthisfullygeneralformforthe resultspresentedinthisbook. Tooptimizeafunctionwithrespecttoavector,wetakethegradientofthe functionwithrespecttothevectorandsolveforthepointwhereeveryelementof thegradientisequaltozero.Likewise,wecanoptimizeafunctionalbysolvingfor thefunctionwherethefunctionalderivativeateverypointisequaltozero. Asanexampleofhowthisprocessworks,considertheproblemofÔ¨Åndingthe probabilitydistributionfunctionover x‚àà RthathasmaximaldiÔ¨Äerentialentropy. RecallthattheentropyofaprobabilitydistributionisdeÔ¨Ånedas p x() H p[] = ‚àí E xlog() p x . (19.48) Forcontinuousvalues,theexpectationisanintegral: H p[] = ‚àíÓÅö p x p x d x . ()log() (19.49) Wecannotsimplymaximize H[ p] withrespecttothefunction p( x),becausethe resultmightnotbeaprobabilitydistribution.Instead,weneedtouseLagrange multipliers toadd aconstraint that p( x)integratesto 1.Also,theentropy increaseswithoutboundasthevarianceincreases.Thismakesthequestionof whichdistributionhasthegreatestentropyuninteresting.Instead,weaskwhich distributionhasmaximalentropyforÔ¨Åxedvariance œÉ2.Finally,theproblem isunderdetermined becausethedistributioncanbeshiftedarbitrarilywithout changingtheentropy.Toimposeauniquesolution,weaddaconstraintthatthe meanofthedistributionbe ¬µ. TheLagrangianfunctionalforthisoptimization problemis L[] = p Œª 1ÓÄíÓÅö p x d x()‚àí1ÓÄì + Œª 2([] )+ E x‚àí ¬µ Œª 3ÓÄÄ E[( ) x ¬µ‚àí2]‚àí œÉ2ÓÄÅ +[] H p(19.50) 6 4 6 CHAPTER19.APPROXIMATEINFERENCE =ÓÅöÓÄÄ Œª 1 p x Œª ()+ 2 p x x Œª ()+ 3 p x x ¬µ ()(‚àí)2‚àí p x p x ()log()ÓÄÅd x Œª‚àí 1‚àí ¬µ Œª 2‚àí œÉ2Œª 3 . (19.51) TominimizetheLagrangianwithrespectto p,wesetthefunctionalderivatives equalto0: ‚àÄ x ,Œ¥ Œ¥ p x()L= Œª 1+ Œª 2 x Œª+ 3( ) x ¬µ‚àí2‚àí‚àí1log() = 0 p x .(19.52) Thisconditionnowtellsusthefunctionalformof p( x).Byalgebraically re-arrangingtheequation,weobtain p x() = expÓÄÄ Œª 1+ Œª 2 x Œª+ 3( ) x ¬µ‚àí2‚àí1ÓÄÅ . (19.53) Weneverassumeddirectlythat p( x)wouldtakethisfunctionalform;we obtainedtheexpressionitselfbyanalyticallyminimizingafunctional.ToÔ¨Ånish theminimization problem,wemustchoosethe Œªvaluestoensurethatallofour constraintsaresatisÔ¨Åed.Wearefreetochooseany Œªvalues,becausethegradient oftheLagrangianwithrespecttothe Œªvariablesiszerosolongastheconstraints aresatisÔ¨Åed.Tosatisfyalloftheconstraints,wemayset Œª 1=1‚àílog œÉ‚àö 2 œÄ, Œª 2= 0,and Œª 3= ‚àí1 2 œÉ2toobtain p x x ¬µ , œÉ () = (N;2) . (19.54) Thisisonereasonforusingthenormaldistributionwhenwedonotknowthe truedistribution.Becausethenormaldistributionhasthemaximumentropy,we imposetheleastpossibleamountofstructurebymakingthisassumption. WhileexaminingthecriticalpointsoftheLagrangianfunctionalfortheentropy, wefoundonlyonecriticalpoint,correspondingtomaximizingtheentropyfor Ô¨Åxedvariance.Whatabouttheprobabilitydistributionfunctionthat m i nim i z e s theentropy?WhydidwenotÔ¨Åndasecondcriticalpointcorrespondingtothe minimum?ThereasonisthatthereisnospeciÔ¨Åcfunctionthatachievesminimal entropy.Asfunctionsplacemoreprobabilitydensityonthetwopoints x= ¬µ+ œÉ and x= ¬µ œÉ‚àí,andplacelessprobabilitydensityonallothervaluesof x,theylose entropywhilemaintainingthedesiredvariance.However,anyfunctionplacing exactlyzeromassonallbuttwopointsdoesnotintegratetoone,andisnota validprobabilitydistribution.Therethusisnosingleminimalentropyprobability distributionfunction,muchasthereisnosingleminimalpositiverealnumber. Instead,wecansaythatthereisasequenceofprobabilitydistributionsconverging towardputtingmassonlyonthesetwopoints.Thisdegeneratescenariomaybe 6 4 7 CHAPTER19.APPROXIMATEINFERENCE describedasamixtureofDiracdistributions.BecauseDiracdistributionsare notdescribedbyasingleprobabilitydistributionfunction,noDiracormixtureof DiracdistributioncorrespondstoasinglespeciÔ¨Åcpointinfunctionspace.These distributionsarethusinvisibletoourmethodofsolvingforaspeciÔ¨Åcpointwhere thefunctionalderivativesarezero.Thisisalimitationofthemethod.Distributions suchastheDiracmustbefoundbyothermethods,suchasguessingthesolution andthenprovingthatitiscorrect. 19.4.3ContinuousLatentVariables Whenourgraphicalmodelcontainscontinuouslatentvariables, wemaystill performvariationalinferenceandlearningbymaximizing L.However,wemust nowusecalculusofvariationswhenmaximizing withrespectto. L q( ) h v| Inmostcases,practitioners neednotsolveanycalculusofvariationsproblems themselves.Instead,thereisageneralequationforthemeanÔ¨ÅeldÔ¨Åxedpoint updates.IfwemakethemeanÔ¨Åeldapproximation q( ) = h v|ÓÅô iq h( i| v) , (19.55) andÔ¨Åx q( h j| v)forall jÓÄ∂= i,thentheoptimal q( h i| v)maybeobtainedby normalizingtheunnormalized distribution Àú q h( i| v) = expÓÄÄ E h ‚àí i‚àº q ( h ‚àí i| v )log Àú p ,( v h)ÓÄÅ (19.56) solongas pdoesnotassignprobabilitytoanyjointconÔ¨Ågurationofvariables. 0 Carryingouttheexpectationinsidetheequationwillyieldthecorrectfunctional formof q( h i| v).Itisonlynecessarytoderivefunctionalformsof qdirectlyusing calculusofvariationsifonewishestodevelopanewformofvariationallearning; equationyieldsthemeanÔ¨Åeldapproximation foranyprobabilisticmodel. 19.56 EquationisaÔ¨Åxedpointequation,designedtobeiterativelyappliedfor 19.56 eachvalueof irepeatedlyuntilconvergence.However,italsotellsusmorethan that.Ittellsusthefunctionalformthattheoptimalsolutionwilltake,whether wearrivetherebyÔ¨Åxedpointequationsornot.Thismeanswecantakethe functionalformfromthatequationbutregardsomeofthevaluesthatappearinit asparameters,thatwecanoptimizewithanyoptimization algorithmwelike. Asanexample,consideraverysimpleprobabilisticmodel,withlatentvariables h‚àà R2andjustonevisiblevariable, v.Supposethat p( h)=N( h;0 , I)and p( v| h)=N( v; wÓÄæh;1).Wecouldactuallysimplifythismodelbyintegrating out h;theresultisjustaGaussiandistributionover v. Themodelitselfisnot 6 4 8 CHAPTER19.APPROXIMATEINFERENCE interesting;wehaveconstructeditonlytoprovideasimpledemonstrationofhow calculusofvariationsmaybeappliedtoprobabilisticmodeling. Thetrueposteriorisgiven,uptoanormalizingconstant,by p( ) h v| (19.57) ‚àù p ,( h v) (19.58) =( p h 1)( p h 2)( ) p v h| (19.59) ‚àùexpÓÄí ‚àí1 2ÓÄÇ h2 1+ h2 2+( v h‚àí 1 w 1‚àí h 2 w 2)2ÓÄÉÓÄì (19.60) =expÓÄí ‚àí1 2ÓÄÇ h2 1+ h2 2+ v2+ h2 1 w2 1+ h2 2 w2 2‚àí2 v h 1 w 1‚àí2 v h 2 w 2+2 h 1 w 1 h 2 w 2ÓÄÉÓÄì . (19.61) Duetothepresenceofthetermsmultiplying h 1and h 2together,wecanseethat thetrueposteriordoesnotfactorizeover h 1and h 2. Applyingequation,weÔ¨Åndthat 19.56 Àú q h( 1| v) (19.62) =expÓÄÄ E h 2‚àº q ( h 2| v )log Àú p ,(</div>
        </div>
    </div>

    <div class="question-card" id="q156">
        <div class="question-header">
            <span class="question-number">Question 156</span>
            <span class="question-type">multiple-choice</span>
        </div>

        <div class="question-text">In probabilistic modeling, evaluating and comparing models often requires estimating partition functions, which are critical for determining likelihoods but are typically intractable for complex distributions. Techniques such as importance sampling and its advanced variants like annealed importance sampling and bridge sampling are widely used to address these computational challenges.

Which statement accurately describes why annealed importance sampling (AIS) can provide better partition function ratio estimates than standard importance sampling for high-dimensional, multimodal distributions?

1) AIS uses only samples from the target distribution, reducing variance.   
2) AIS introduces a sequence of intermediate distributions to gradually bridge the gap between proposal and target, improving weight stability.   
3) AIS eliminates the need for proposal distributions entirely by direct integration.   
4) AIS relies on deterministic algorithms, guaranteeing exact estimates in all cases.   
5) AIS combines proposal and target distributions into a single mixture for sampling.   
6) AIS requires that proposal and target distributions be identical for accuracy.   
7) AIS estimates partition functions by averaging over uniform samples from all possible configurations.</div>

        <div class="answer-section">
            <div class="answer-label">‚úì Correct Answer:</div>
            <div class="answer-text">The correct answer is 2) AIS introduces a sequence of intermediate distributions to gradually bridge the gap between proposal and target, improving weight stability..</div>
        </div>

        <div class="reference-section">
            <div class="reference-label">üìö Reference Text:</div>
            <button class="toggle-button" onclick="toggleReference(156)">
                Show/Hide Reference
            </button>
            <div id="ref156" class="reference-text hidden">AÀú p A(x; Œ∏ A)andmodelM BdeÔ¨Åningaprobability distribution p B(x; Œ∏ B)=1 Z BÀú p B(x; Œ∏ B).Acommonwaytocomparethemodels istoevaluateandcomparethelikelihoodthatbothmodelsassigntoani.i.d. testdataset.Supposethetestsetconsistsof mexamples { x( 1 ), . . . , x( ) m}.IfÓÅë i p A(x( ) i; Œ∏ A) >ÓÅë i p B(x( ) i; Œ∏ B)orequivalentlyif ÓÅò ilog p A(x( ) i; Œ∏ A)‚àíÓÅò ilog p B(x( ) i; Œ∏ B) 0 > ,(18.38) thenwesaythatM AisabettermodelthanM B(or,atleast,itisabettermodel ofthetestset),inthesensethatithasabettertestlog-likelihood.Unfortunately, testingwhetherthisconditionholdsrequiresknowledgeofthepartitionfunction. Unfortunately,equationseemstorequireevaluatingthelogprobabilitythat 18.38 themodelassignstoeachpoint,whichinturnrequiresevaluatingthepartition function.Wecansimplifythesituationslightlybyre-arrangingequation18.38 intoaformwhereweneedtoknowonlythe r at i oofthetwomodel‚Äôspartition functions: ÓÅò ilog p A(x( ) i; Œ∏ A)‚àíÓÅò ilog p B(x( ) i; Œ∏ B) =ÓÅò iÓÄ† logÀú p A(x( ) i; Œ∏ A) Àú p B(x( ) i; Œ∏ B)ÓÄ° ‚àí mlogZ( Œ∏ A) Z( Œ∏ B). (18.39) 623 CHAPTER18.CONFRONTINGTHEPARTITIONFUNCTION Wecanthusdeterminewhether M AisabettermodelthanM Bwithoutknowing thepartitionfunctionofeithermodelbutonlytheirratio.Aswewillseeshortly, wecanestimatethisratiousingimportancesampling,providedthatthetwomodels aresimilar. If,however,wewantedtocomputetheactualprobabilityofthetestdataunder either M AorM B,wewouldneedtocomputetheactualvalueofthepartition functions.Thatsaid,ifweknewtheratiooftwopartitionfunctions, r=Z ( Œ∏ B ) Z ( Œ∏ A ), andweknewtheactualvalueofjustoneofthetwo,say Z( Œ∏ A),wecouldcompute thevalueoftheother: Z( Œ∏ B) = ( r Z Œ∏ A) =Z( Œ∏ B) Z( Œ∏ A)Z( Œ∏ A) . (18.40) Asimplewaytoestimatethe partition functionistouse aMonteCarlo methodsuchassimpleimportancesampling.Wepresenttheapproachinterms ofcontinuousvariablesusingintegrals,butitcanbereadilyappliedtodiscrete variablesbyreplacingtheintegralswithsummation.Weuseaproposaldistribution p 0(x)=1 Z0Àú p 0(x)whichsupportstractablesamplingandtractableevaluationof boththepartitionfunction Z 0andtheunnormalized distributionÀú p 0()x. Z 1=ÓÅö Àú p 1()x dx (18.41) =ÓÅöp 0()x p 0()xÀú p 1()x dx (18.42) = Z 0ÓÅö p 0()xÀú p 1()x Àú p 0()xdx (18.43) ÀÜ Z 1=Z 0 KKÓÅò k = 1Àú p 1(x( ) k) Àú p 0(x( ) k)st: . .x( ) k‚àº p 0 (18.44) Inthelastline,wemakeaMonteCarloestimator,ÀÜ Z 1,oftheintegralusingsamples drawnfrom p 0(x)andthenweighteachsamplewiththeratiooftheunnormalized Àú p 1andtheproposal p 0. Weseealsothatthisapproachallowsustoestimatetheratiobetweenthe partitionfunctionsas 1 KK ÓÅò k = 1Àú p 1(x( ) k) Àú p 0(x( ) k)st: . .x( ) k‚àº p 0 . (18.45) Thisvaluecanthenbeuseddirectlytocomparetwomodelsasdescribedin equation.18.39 624 CHAPTER18.CONFRONTINGTHEPARTITIONFUNCTION Ifthedistribution p 0iscloseto p 1,equationcanbeaneÔ¨Äectivewayof 18.44 estimatingthepartitionfunction(Minka2005,).Unfortunately,mostofthetime p 1isbothcomplicated(usuallymultimodal)anddeÔ¨Ånedoverahighdimensional space.ItisdiÔ¨ÉculttoÔ¨Åndatractable p 0thatissimpleenoughtoevaluatewhile stillbeingcloseenoughto p 1toresultinahighqualityapproximation.If p 0and p 1arenotclose,mostsamplesfrom p 0willhavelowprobabilityunder p 1and thereforemake(relatively)negligiblecontributiontothesuminequation.18.44 Havingfewsamples withsigniÔ¨Åcantweightsinthis sumwillresultinan estimatorthatisofpoorqualityduetohighvariance. This canbeunderstood quantitativelythroughanestimateofthevarianceofourestimate ÀÜ Z 1: ÀÜVarÓÄê ÀÜ Z 1ÓÄë =Z 0 K2K ÓÅò k = 1ÓÄ† Àú p 1(x( ) k) Àú p 0(x( ) k)‚àíÀÜ Z 1ÓÄ°2 . (18.46) ThisquantityislargestwhenthereissigniÔ¨Åcantdeviationinthevaluesofthe importanceweightsÀú p1 ( x() k) Àú p0 ( x() k ). Wenowturntotworelatedstrategiesdevelopedtocopewiththechalleng- ingtaskofestimatingpartitionfunctionsforcomplexdistributionsoverhigh- dimensionalspaces: annealedimportancesamplingandbridgesampling.Both startwiththesimpleimportancesamplingstrategyintroducedaboveandboth attempttoovercometheproblemoftheproposal p 0beingtoofarfrom p 1by introducingintermediatedistributionsthatattemptto between b r i d g e t h e g a p p 0 and p 1. 1 8 . 7 . 1 A n n ea l ed Im p o rt a n ce S a m p l i n g Insituationswhere D K L( p 0ÓÅ´ p 1)islarge(i.e.,wherethereislittleoverlapbetween p 0and p 1),astrategycalled annealed i m p o r t anc e sampling(AIS)attempts tobridgethegapbyintroducingintermediate distributions(,;, Jarzynski1997Neal 2001).Considerasequenceofdistributions p Œ∑0 , . . . , p Œ∑ n,with 0 = Œ∑ 0 < Œ∑ 1 < <¬∑¬∑¬∑ Œ∑ n ‚àí 1 < Œ∑ n= 1sothattheÔ¨Årstandlastdistributionsinthesequenceare p 0and p 1 respectively. Thisapproachallowsustoestimatethepartitionfunctionofamultimodal distributiondeÔ¨Ånedoverahigh-dimensionalspace(suchasthedistributiondeÔ¨Åned byatrainedRBM).Webeginwithasimplermodelwithaknownpartitionfunction (suchasanRBMwithzeroesforweights)andestimatetheratiobetweenthetwo model‚Äôspartitionfunctions. Theestimateofthisratioisbasedontheestimate oftheratiosofasequenceofmanysimilardistributions,suchasthesequenceof RBMswithweightsinterpolatingbetweenzeroandthelearnedweights. 625</div>
        </div>
    </div>

    <div class="question-card" id="q157">
        <div class="question-header">
            <span class="question-number">Question 157</span>
            <span class="question-type">multiple-choice</span>
        </div>

        <div class="question-text">Modern deep learning frameworks utilize computational graphs and automatic differentiation to train complex neural network architectures efficiently. Understanding the distinction between symbolic and numeric differentiation approaches is crucial for leveraging advanced features in neural network optimization.

Which differentiation approach enables the construction of explicit computational graphs for derivatives, allowing for flexible manipulation and computation of higher-order gradients in deep learning frameworks?

1) Symbol-to-symbol differentiation   
2) Symbol-to-number differentiation   
3) Chain rule traversal   
4) Jacobian-vector product   
5) Forward-mode automatic differentiation   
6) Static graph differentiation   
7) Gradient descent optimization</div>

        <div class="answer-section">
            <div class="answer-label">‚úì Correct Answer:</div>
            <div class="answer-text">The correct answer is 1) Symbol-to-symbol differentiation.</div>
        </div>

        <div class="reference-section">
            <div class="reference-label">üìö Reference Text:</div>
            <button class="toggle-button" onclick="toggleReference(157)">
                Show/Hide Reference
            </button>
            <div id="ref157" class="reference-text hidden">J L= (ÀÜyy ,)+‚Ñ¶() Œª Œ∏ 6.5.5Symbol-to-SymbolDerivatives Algebraicexpressionsandcomputational graphsbothoperateonsymbols,or variables thatdo not havespeciÔ¨Åc values.Thesealgebraic and graph-based representationsarecalledsymbolicrepresentations.Whenweactuallyuseor trainaneuralnetwork,wemustassignspeciÔ¨Åcvaluestothesesymbols.We replaceasymbolicinputtothenetworkxwithaspeciÔ¨Åcnumericvalue,suchas [123765 18] . , . ,‚àí .ÓÄæ. 2 1 2 CHAPTER6.DEEPFEEDFORWARDNETWORKS Algorithm6.4Backwardcomputationforthedeepneuralnetworkofalgo- rithm,whichusesinadditiontotheinput 6.3 xatargety.Thiscomputation yieldsthegradientsontheactivationsa( ) kforeachlayer k,startingfromthe outputlayerandgoingbackwardstotheÔ¨Årsthiddenlayer.Fromthesegradients, whichcanbeinterpretedasanindicationofhoweachlayer‚Äôsoutputshouldchange toreduceerror,onecanobtainthegradientontheparametersofeachlayer.The gradientsonweightsandbiasescanbeimmediately usedaspartofastochas- ticgradientupdate(performingtheupdaterightafterthegradientshavebeen computed)orusedwithothergradient-basedoptimization methods. Aftertheforwardcomputation,computethegradientontheoutputlayer: g‚Üê‚àá ÀÜ y J= ‚àá ÀÜ y L(ÀÜyy ,) for do k l , l , . . . , = ‚àí1 1 Convert thegradienton thelayer‚Äôs output into a gradient into thepre- nonlinearityactivation(element-wisemultiplicationifiselement-wise): f g‚Üê‚àáa() k J f = gÓÄåÓÄ∞(a( ) k) Computegradientsonweightsandbiases(includingtheregularizationterm, whereneeded): ‚àáb() k J Œª = +g ‚àáb() k‚Ñ¶() Œ∏ ‚àáW() k J= gh( 1 ) k‚àíÓÄæ+ Œª‚àáW() k‚Ñ¶() Œ∏ Propagatethegradientsw.r.t.thenextlower-levelhiddenlayer‚Äôsactivations: g‚Üê‚àáh(1) k ‚àí J= W( ) kÓÄæg endfor 2 1 3 CHAPTER6.DEEPFEEDFORWARDNETWORKS z z xxyy w wfffz z xxyy w wfff d z d yd z d yfÓÄ° d y d xd y d xfÓÄ° d z d xd z d x√ó d x d wd x d wfÓÄ° d z d wd z d w√ó Figure6.10:Anexampleofthesymbol-to-symbolapproachtocomputingderivatives.In thisapproach,theback-propagationalgorithmdoesnotneedtoeveraccessanyactual speciÔ¨Åcnumericvalues.Instead,itaddsnodestoacomputationalgraphdescribinghow tocomputethesederivatives.Agenericgraphevaluationenginecanlatercomputethe derivativesforanyspeciÔ¨Åcnumericvalues. ( L e f t )Inthisexample,webeginwithagraph representing z= f( f( f( w))).Weruntheback-propagationalgorithm,instructing ( R i g h t ) ittoconstructthegraphfortheexpressioncorrespondingtod z d w.Inthisexample,wedo notexplainhowtheback-propagationalgorithmworks.Thepurposeisonlytoillustrate whatthedesiredresultis:acomputationalgraphwithasymbolicdescriptionofthe derivative. Someapproachestoback-propagationtakeacomputational graphandaset ofnumericalvaluesfortheinputstothegraph,thenreturnasetofnumerical valuesdescribingthegradientatthoseinputvalues.Wecallthisapproach‚Äúsymbol- to-number‚ÄùdiÔ¨Äerentiation. ThisistheapproachusedbylibrariessuchasTorch ( ,)andCaÔ¨Äe(,). Collobert e t a l .2011b Jia2013 Anotherapproachistotakeacomputational graphandaddadditionalnodes tothegraphthatprovideasymbolicdescriptionofthedesiredderivatives.This istheapproachtakenbyTheano( ,; ,) Bergstra e t a l .2010Bastien e t a l .2012 andTensorFlow( ,).Anexampleofhowthisapproachworks Abadi e t a l .2015 isillustratedinÔ¨Ågure.Theprimaryadvantageofthisapproachisthat 6.10 thederivativesaredescribedinthesamelanguageastheoriginalexpression. Becausethederivativesarejustanothercomputational graph,itispossibletorun back-propagationagain,diÔ¨Äerentiating thederivativesinordertoobtainhigher derivatives.Computation ofhigher-orderderivativesisdescribedinsection.6.5.10 Wewillusethelatterapproachanddescribetheback-propagationalgorithmin 2 1 4 CHAPTER6.DEEPFEEDFORWARDNETWORKS termsofconstructingacomputational graphforthederivatives.Anysubsetofthe graphmaythenbeevaluatedusingspeciÔ¨Åcnumericalvaluesatalatertime.This allowsustoavoidspecifyingexactlywheneachoperationshouldbecomputed. Instead,agenericgraphevaluationenginecanevaluateeverynodeassoonasits parents‚Äôvaluesareavailable. Thedescriptionofthesymbol-to-symbolbasedapproachsubsumesthesymbol- to-numberapproach.Thesymbol-to-numberapproachcanbeunderstoodas performingexactlythesamecomputations asaredoneinthegraphbuiltbythe symbol-to-symbolapproach.ThekeydiÔ¨Äerenceisthatthesymbol-to-number approachdoesnotexposethegraph. 6.5.6GeneralBack-Propagation Theback-propagationalgorithmisverysimple.Tocomputethegradientofsome scalar zwithrespecttooneofitsancestorsxinthegraph,webeginbyobserving thatthegradientwithrespectto zisgivenbyd z d z=1.Wecanthencompute thegradientwithrespecttoeachparentof zinthegraphbymultiplyingthe currentgradientbytheJacobianoftheoperationthatproduced z.Wecontinue multiplyingbyJacobianstravelingbackwardsthroughthegraphinthiswayuntil wereachx.Foranynodethatmaybereachedbygoingbackwardsfrom zthrough twoormorepaths,wesimplysumthegradientsarrivingfromdiÔ¨Äerentpathsat thatnode. Moreformally,eachnodeinthegraph Gcorrespondstoavariable.Toachieve maximumgenerality,wedescribethisvariableasbeingatensor V. Tensorcan ingeneralhaveanynumberofdimensions. Theysubsumescalars,vectors,and matrices. Weassumethateachvariableisassociatedwiththefollowingsubroutines: V ‚Ä¢ g e t o p e r a t i o n_ ( V):Thisreturnstheoperationthatcomputes V,repre- sentedbytheedgescominginto Vinthecomputational graph.Forexample, theremaybeaPythonorC++classrepresentingthematrixmultiplication operation,andtheget_operationfunction.Supposewehaveavariablethat iscreatedbymatrixmultiplication,C=AB.Then g e t o p e r a t i o n_ ( V) returnsapointertoaninstanceofthecorrespondingC++class. ‚Ä¢ g e t c o n s u m e r s_ ( V ,G):Thisreturnsthelistofvariablesthatarechildrenof Vinthecomputational graph.G ‚Ä¢ G g e t i n p u t s_ ( V ,):Thisreturnsthelistofvariablesthatareparentsof V inthecomputational graph.G 2 1 5 CHAPTER6.DEEPFEEDFORWARDNETWORKS Eachoperationopisalsoassociatedwithabpropoperation.Thisbprop operationcancomputeaJacobian-vectorproductasdescribedbyequation.6.47 Thisishowtheback-propagationalgorithmisabletoachievegreatgenerality. Eachoperationisresponsibleforknowinghowtoback-propagate throughthe edgesinthegraphthatitparticipatesin.Forexample,wemightuseamatrix multiplicationoperationtocreateavariableC=AB.Supposethatthegradient ofascalar zwithrespecttoCisgivenbyG.Thematrixmultiplication operation isresponsiblefordeÔ¨Åningtwoback-propagation rules,oneforeachofitsinput arguments.Ifwecallthebpropmethodtorequestthegradientwithrespectto AgiventhatthegradientontheoutputisG,thenthe b p r o pmethodofthe matrixmultiplicationoperationmuststatethatthegradientwithrespecttoA isgivenbyGBÓÄæ.Likewise,ifwecallthe b p r o pmethodtorequestthegradient withrespecttoB,thenthematrixoperationisresponsibleforimplementing the b p r o pmethodandspecifyingthatthedesiredgradientisgivenbyAÓÄæG.The back-propagationalgorithmitselfdoesnotneedtoknowanydiÔ¨Äerentiation rules.It onlyneedstocalleachoperation‚Äôsbpropruleswiththerightarguments.Formally, o p b p r o p i n p u t s . ( , , X G)mustreturn ÓÅò i(‚àá X o p f i n p u t s .( ) i) G i , (6.54) whichisjustanimplementation ofthechainruleasexpressedinequation.6.47 Here, i n p u t sisalistofinputsthataresuppliedtotheoperation, op.fisthe mathematical functionthattheoperationimplements, Xistheinputwhosegradient wewishtocompute,andisthegradientontheoutputoftheoperation. G Theop.bpropmethodshouldalwayspretendthatallofitsinputsaredistinct fromeachother,eveniftheyarenot.Forexample,ifthemuloperatorispassed twocopiesof xtocompute x2,theop.bpropmethodshouldstillreturn xasthe derivativewithrespecttobothinputs.Theback-propagation algorithmwilllater addbothoftheseargumentstogethertoobtain 2 x,whichisthecorrecttotal derivativeon. x Softwareimplementationsofback-propagation usuallyprovideboththeopera- tionsandtheirbpropmethods,sothatusersofdeeplearningsoftwarelibrariesare abletoback-propagatethroughgraphsbuiltusingcommonoperationslikematrix multiplication, exponents,logarithms,andsoon.Softwareengineerswhobuilda newimplementationofback-propagationoradvanceduserswhoneedtoaddtheir ownoperationtoanexistinglibrarymustusuallyderivetheop.bpropmethodfor anynewoperationsmanually. Theback-propagationalgorithmisformallydescribedinalgorithm .6.5 2 1 6 CHAPTER6.DEEPFEEDFORWARDNETWORKS Algorithm6.5Theoutermostskeletonoftheback-propagation algorithm.This portiondoessimplesetupandcleanupwork.Mostoftheimportantworkhappens inthe subroutineofalgorithm build_grad 6.6. Require: T,thetargetsetofvariableswhosegradientsmustbecomputed. Require:G,thecomputational graph Require: z,thevariabletobediÔ¨Äerentiated LetGÓÄ∞beGprunedtocontainonlynodesthatareancestorsof zanddescendents</div>
        </div>
    </div>

    <div class="question-card" id="q158">
        <div class="question-header">
            <span class="question-number">Question 158</span>
            <span class="question-type">multiple-choice</span>
        </div>

        <div class="question-text">Unsupervised learning in artificial intelligence often involves analyzing high-dimensional datasets, such as images or genetic information, where each data point can have thousands or millions of variables. This creates both statistical and computational problems for effective model training and inference.

Which aspect of probabilistic modeling in high-dimensional unsupervised learning most directly leads to intractable computations due to the need to sum over all possible variable configurations?

1) The dependency on supervised labels for training   
2) The requirement for deterministic feature extraction   
3) The use of linear classifiers for dimensionality reduction   
4) The assumption of independent and identically distributed variables   
5) The necessity of normalization and marginalization over hidden variables   
6) The application of simple clustering algorithms   
7) The utilization of greedy optimization strategies</div>

        <div class="answer-section">
            <div class="answer-label">‚úì Correct Answer:</div>
            <div class="answer-text">The correct answer is 5) The necessity of normalization and marginalization over hidden variables.</div>
        </div>

        <div class="reference-section">
            <div class="reference-label">üìö Reference Text:</div>
            <button class="toggle-button" onclick="toggleReference(158)">
                Show/Hide Reference
            </button>
            <div id="ref158" class="reference-text hidden">out how w e c an make progres s in t his Ô¨Å e ld. A c e ntral c aus e of t he diÔ¨Éculties with uns upervis e d learning is t he high di- mens iona lit y of t he r andom v ariables being mo deled. This brings t wo dis t inct c hallenges : a s t atis t ical c hallenge and a c omputational c hallenge. The s t a t i s t i c a l c h a l l e ng e r e gards generalization: t he num b e r of c onÔ¨Ågurations we may wan t t o dis t inguis h c an grow e x p onentially with t he num b e r of dimens ion s of in t e r e s t , and t his q uickly b e c omes muc h larger t han t he num b e r of e x amples one c an p os s ibly ha v e ( or us e with b ounded c omputational r e s ources ) . The c o m p u t a t i o na l c h a l l e ng e as s o c iated with high-dimens ional dis t r ibuti ons aris e s b e c aus e man y algorithms f or learning or us ing a t r ained mo del ( e s p e c ially t hos e bas e d on e s t imatin g an e x plicit probabilit y f unction) in v olv e in t r actable c omputations t hat gro w e x ponent ially with t he n um b e r of dimens ion s . With probabilis t ic mo dels , t his c omputational c hallenge aris e s f r om t he need t o p e r f orm intractable inference or s imply f r om t he need t o normalize t he dis t r ibuti on. ‚Ä¢ I nt r a c t a b l e i nfe r e nc e : inference is dis c us s e d mos t ly in c hapter . It r e gards 19 t he q ues t ion of gues s ing t he probable v alues of s ome v ariables a , given other v ariables b , with r e s p e c t t o a mo del t hat c aptures t he j oin t dis t r ibuti on ov e r 4 8 7 a , b and c . In order t o e v e n c ompute s uch c onditional probabilities one needs t o s um ov e r t he v alues of t he v ariables c , as</div>
        </div>
    </div>

    <div class="question-card" id="q159">
        <div class="question-header">
            <span class="question-number">Question 159</span>
            <span class="question-type">multiple-choice</span>
        </div>

        <div class="question-text">In machine learning, the design of algorithms often involves selecting models, cost functions, and optimizers, while accounting for challenges such as high-dimensional data and the potential structure of real-world datasets. Understanding concepts like the curse of dimensionality and manifold learning is crucial for developing effective methods in this domain.

Which statement most accurately describes the manifold hypothesis and its practical significance in machine learning?

1) The manifold hypothesis asserts that all high-dimensional data points are uniformly distributed throughout the input space, simplifying generalization.   
2) The manifold hypothesis claims that overfitting is impossible when data lies on a low-dimensional manifold.   
3) It proposes that data points with complex structures cannot be represented in fewer dimensions without losing important information.   
4) The hypothesis suggests that local smoothness priors are sufficient for generalization in high-dimensional spaces.   
5) It states that the optimization algorithm determines whether the data forms a manifold.   
6) The manifold hypothesis posits that meaningful real-world data occupies a low-dimensional manifold embedded in a high-dimensional space, enabling more efficient learning and generalization.   
7) It indicates that linear models can always exploit the manifold structure of data without additional priors.</div>

        <div class="answer-section">
            <div class="answer-label">‚úì Correct Answer:</div>
            <div class="answer-text">The correct answer is 6) The manifold hypothesis posits that meaningful real-world data occupies a low-dimensional manifold embedded in a high-dimensional space, enabling more efficient learning and generalization..</div>
        </div>

        <div class="reference-section">
            <div class="reference-label">üìö Reference Text:</div>
            <button class="toggle-button" onclick="toggleReference(159)">
                Show/Hide Reference
            </button>
            <div id="ref159" class="reference-text hidden">,y ‚àºÀÜ pdatalogpmodel( )y|x, (5.100) themodelspeciÔ¨Åcationpmodel(y|x) =N(y;xÓÄæw+b,1),and,inmostcases,the optimization algorithmdeÔ¨Ånedbysolvingforwherethegradientofthecostiszero usingthenormalequations. Byrealizingthatwecanreplaceanyofthesecomponentsmostlyindependently fromtheothers,wecanobtainaverywidevarietyofalgorithms. Thecostfunctiontypicallyincludesatleastonetermthatcausesthelearning processtoperformstatisticalestimation.Themostcommoncostfunctionisthe negativelog-likelihood,sothatminimizingthecostfunctioncausesmaximum likelihoodestimation. Thecostfunctionmayalsoincludeadditionalterms,suchasregularization terms.Forexample,wecanaddweightdecaytothelinearregressioncostfunction toobtain J,bŒª (w) = ||||w2 2‚àí E x ,y ‚àºÀÜ pdatalogpmodel( )y|x.(5.101) Thisstillallowsclosed-formoptimization. Ifwechangethemodeltobenonlinear,thenmostcostfunctionscannolonger beoptimizedinclosedform.Thisrequiresustochooseaniterativenumerical optimization procedure,suchasgradientdescent. Therecipeforconstructingalearningalgorithmbycombiningmodels,costs,and optimization algorithmssupportsbothsupervisedandunsupervisedlearning.The linearregressionexampleshowshowtosupportsupervisedlearning.Unsupervised learningcanbesupportedbydeÔ¨ÅningadatasetthatcontainsonlyXandproviding anappropriateunsupervisedcostandmodel.Forexample,wecanobtaintheÔ¨Årst PCAvectorbyspecifyingthatourlossfunctionis J() = w E x ‚àºÀÜ pdata||‚àí ||xr(;)xw2 2 (5.102) whileourmodelisdeÔ¨Ånedtohavewwithnormoneandreconstructionfunction r() = xwÓÄæxw. Insomecases,thecostfunctionmaybeafunctionthatwecannotactually evaluate,forcomputational reasons.Inthesecases,wecanstillapproximately minimizeitusingiterativenumericaloptimization solongaswehavesomewayof approximatingitsgradients. Mostmachinelearningalgorithmsmakeuseofthisrecipe,thoughitmaynot immediatelybeobvious.Ifamachinelearningalgorithmseemsespeciallyuniqueor 1 5 4 CHAPTER5.MACHINELEARNINGBASICS hand-designed,itcanusuallybeunderstoodasusingaspecial-caseoptimizer.Some modelssuchasdecisiontreesork-meansrequirespecial-caseoptimizersbecause theircostfunctionshaveÔ¨Çatregionsthatmaketheminappropriate forminimization bygradient-basedoptimizers.Recognizingthatmostmachinelearningalgorithms canbedescribedusingthisrecipehelpstoseethediÔ¨Äerentalgorithmsaspartofa taxonomyofmethodsfordoingrelatedtasksthatworkforsimilarreasons,rather thanasalonglistofalgorithmsthateachhaveseparatejustiÔ¨Åcations. 5.11ChallengesMotivatingDeepLearning Thesimplemachinelearningalgorithmsdescribedinthischapterworkverywellon awidevarietyofimportantproblems.However,theyhavenotsucceededinsolving thecentralproblemsinAI,suchasrecognizingspeechorrecognizingobjects. Thedevelopmentofdeeplearningwasmotivatedinpartbythefailureof traditionalalgorithmstogeneralizewellonsuchAItasks. Thissectionisabouthowthechallengeofgeneralizingtonewexamplesbecomes exponentiallymorediÔ¨Écultwhenworkingwithhigh-dimensionaldata,andhow themechanismsusedtoachievegeneralization intraditionalmachinelearning areinsuÔ¨Écienttolearncomplicatedfunctionsinhigh-dimensionalspaces.Such spacesalsooftenimposehighcomputational costs.Deeplearningwasdesignedto overcometheseandotherobstacles. 5.11.1TheCurseofDimensionality ManymachinelearningproblemsbecomeexceedinglydiÔ¨Écultwhenthenumber ofdimensionsinthedataishigh.Thisphenomenon isknownasthecurseof dimensionality.Ofparticularconcernisthatthenumberofpossibledistinct conÔ¨Ågurations ofasetofvariablesincreasesexponentiallyasthenumberofvariables increases. 1 5 5 CHAPTER5.MACHINELEARNINGBASICS Figure5.9:Asthenumberofrelevantdimensionsofthedataincreases(fromleftto right),thenumberofconÔ¨Ågurationsofinterestmaygrowexponentially. ( L e f t )Inthis one-dimensionalexample,wehaveonevariableforwhichweonlycaretodistinguish10 regionsofinterest.Withenoughexamplesfallingwithineachoftheseregions(eachregion correspondstoacellintheillustration),learningalgorithmscaneasilygeneralizecorrectly. Astraightforwardwaytogeneralizeistoestimatethevalueofthetargetfunctionwithin eachregion(andpossiblyinterpolatebetweenneighboringregions).With2 ( C e n t e r ) dimensionsitismorediÔ¨Éculttodistinguish10diÔ¨Äerentvaluesofeachvariable. Weneed tokeeptrackofupto10√ó10=100regions,andweneedatleastthatmanyexamplesto coverallthoseregions.With3dimensionsthisgrowsto ( R i g h t ) 103= 1000regionsandat leastthatmanyexamples.Forddimensionsandvvaluestobedistinguishedalongeach axis,weseemtoneedO(vd)regionsandexamples. Thisisaninstanceofthecurseof dimensionality.FiguregraciouslyprovidedbyNicolasChapados. Thecurseofdimensionalityarisesinmanyplacesincomputerscience,and especiallysoinmachinelearning. Onechallengeposedbythecurseofdimensionalityisastatisticalchallenge. AsillustratedinÔ¨Ågure,astatisticalchallengearisesbecausethenumberof 5.9 possibleconÔ¨Ågurations ofxismuchlargerthanthenumberoftrainingexamples. Tounderstandtheissue,letusconsiderthattheinputspaceisorganizedintoa grid,likeintheÔ¨Ågure.Wecandescribelow-dimensional spacewithalownumber ofgridcellsthataremostlyoccupiedbythedata.Whengeneralizingtoanewdata point,wecanusuallytellwhattodosimplybyinspectingthetrainingexamples thatlieinthesamecellasthenewinput.Forexample,ifestimatingtheprobability densityatsomepointx,wecanjustreturnthenumberoftrainingexamplesin thesameunitvolumecellasx,dividedbythetotalnumberoftrainingexamples. Ifwewishtoclassifyanexample,wecanreturnthemostcommonclassoftraining examplesinthesamecell. Ifwearedoingregressionwecanaveragethetarget valuesobservedovertheexamplesinthatcell.Butwhataboutthecellsforwhich wehaveseennoexample?Becauseinhigh-dimensionalspacesthenumberof conÔ¨Ågurations ishuge,muchlargerthanournumberofexamples,atypicalgridcell hasnotrainingexampleassociatedwithit.Howcouldwepossiblysaysomething 1 5 6 CHAPTER5.MACHINELEARNINGBASICS meaningfulaboutthesenewconÔ¨Ågurations? Manytraditionalmachinelearning algorithmssimplyassumethattheoutputatanewpointshouldbeapproximately thesameastheoutputatthenearesttrainingpoint. 5.11.2LocalConstancyandSmoothnessRegularization Inordertogeneralizewell,machinelearningalgorithmsneedtobeguidedbyprior beliefsaboutwhatkindoffunctiontheyshouldlearn.Previously,wehaveseen thesepriorsincorporatedasexplicitbeliefsintheformofprobabilitydistributions overparametersofthemodel.Moreinformally,wemayalsodiscusspriorbeliefsas directlyinÔ¨Çuencingtheitselfandonlyindirectlyactingontheparameters function viatheireÔ¨Äectonthefunction.Additionally,weinformallydiscusspriorbeliefsas beingexpressedimplicitly,bychoosingalgorithmsthatarebiasedtowardchoosing someclassoffunctionsoveranother,eventhoughthesebiasesmaynotbeexpressed (orevenpossibletoexpress)intermsofaprobabilitydistributionrepresentingour degreeofbeliefinvariousfunctions. Amongthemostwidelyusedoftheseimplicit‚Äúpriors‚Äù isthesmoothness priororlocalconstancyprior.Thispriorstatesthatthefunctionwelearn shouldnotchangeverymuchwithinasmallregion. Manysimpleralgorithmsrelyexclusivelyonthispriortogeneralizewell,and asaresulttheyfailtoscaletothestatisticalchallengesinvolvedinsolvingAI- leveltasks.Throughoutthisbook,wewilldescribehowdeeplearningintroduces additional(explicit andimplicit)priorsinorder toreducethegeneralization erroronsophisticatedtasks.Here,weexplainwhythesmoothnessprioraloneis insuÔ¨Écientforthesetasks. TherearemanydiÔ¨Äerentwaystoimplicitlyorexplicitlyexpressapriorbelief thatthelearnedfunctionshouldbesmoothorlocallyconstant.AllofthesediÔ¨Äerent methodsaredesignedtoencouragethelearningprocesstolearnafunctionf‚àóthat satisÔ¨Åesthecondition f‚àó() x‚âàf‚àó(+)xÓÄè (5.103) formostconÔ¨ÅgurationsxandsmallchangeÓÄè.Inotherwords,ifweknowagood answerforaninputx(forexample,ifxisalabeledtrainingexample)thenthat answerisprobablygoodintheneighborhoodofx.Ifwehaveseveralgoodanswers insomeneighborhoodwewouldcombinethem(bysomeformofaveragingor interpolation)toproduceananswerthatagreeswithasmanyofthemasmuchas possible. Anextremeexampleofthelocalconstancyapproachisthek-nearestneighbors familyoflearningalgorithms.Thesepredictorsareliterallyconstantovereach 1 5 7 CHAPTER5.MACHINELEARNINGBASICS regioncontainingallthepointsxthathavethesamesetofknearestneighborsin thetrainingset.Fork= 1,thenumberofdistinguishableregionscannotbemore thanthenumberoftrainingexamples. Whilethek-nearestneighborsalgorithmcopiestheoutputfromnearbytraining examples,mostkernelmachinesinterpolatebetweentrainingsetoutputsassociated withnearbytrainingexamples.Animportantclassofkernelsisthefamilyoflocal kernelswherek(uv,)islargewhenu=vanddecreasesasuandvgrowfarther apartfromeachother.Alocalkernelcanbethoughtofasasimilarityfunction thatperformstemplatematching,bymeasuringhowcloselyatestexamplex resembleseachtrainingexamplex() i. Muchofthemodernmotivationfordeep learningisderivedfromstudyingthelimitationsoflocaltemplatematchingand howdeepmodelsareabletosucceedincaseswherelocaltemplatematchingfails ( ,). Bengioetal.2006b DecisiontreesalsosuÔ¨Äerfromthelimitationsofexclusivelysmoothness-based learningbecausetheybreaktheinputspaceintoasmanyregionsasthereare leavesanduseaseparateparameter(orsometimesmanyparametersforextensions ofdecisiontrees)ineachregion.Ifthetargetfunctionrequiresatreewithat leastnleavestoberepresentedaccurately,thenatleastntrainingexamplesare requiredtoÔ¨Åtthetree.Amultipleofnisneededtoachievesomelevelofstatistical conÔ¨Ådenceinthepredictedoutput. Ingeneral,todistinguishO(k)regionsininputspace,allofthesemethods requireO(k) examples.TypicallythereareO(k) parameters,withO(1) parameters associatedwitheachoftheO(k)regions.Thecaseofanearestneighborscenario, whereeachtrainingexamplecanbeusedtodeÔ¨Åneatmostoneregion,isillustrated inÔ¨Ågure.5.10 Isthereawaytorepresentacomplexfunctionthathasmanymoreregions tobedistinguishedthanthenumberoftrainingexamples?Clearly,assuming onlysmoothnessoftheunderlyingfunctionwillnotallowalearnertodothat. For example, imagine that thetargetfunctionis akind ofcheckerboard.A checkerboardcontainsmanyvariationsbutthereisasimplestructuretothem. Imaginewhathappenswhenthenumberoftrainingexamplesissubstantially smallerthanthenumberofblackandwhitesquaresonthecheckerboard.Based ononlylocalgeneralization andthesmoothnessorlocalconstancyprior,wewould beguaranteedtocorrectlyguessthecolorofanewpointifitlieswithinthesame checkerboardsquareasatrainingexample.Thereisnoguaranteethatthelearner couldcorrectlyextendthecheckerboardpatterntopointslyinginsquaresthatdo notcontaintrainingexamples.Withthisprioralone,theonlyinformationthatan exampletellsusisthecolorofitssquare,andtheonlywaytogetthecolorsofthe 1 5 8 CHAPTER5.MACHINELEARNINGBASICS Figure5.10: Illustrationofhowthenearestneighboralgorithmbreaksuptheinputspace intoregions. Anexample(representedherebyacircle)withineachregiondeÔ¨Ånesthe regionboundary(representedherebythelines).Theyvalueassociatedwitheachexample deÔ¨Åneswhattheoutputshouldbeforallpointswithinthecorrespondingregion. The regionsdeÔ¨ÅnedbynearestneighbormatchingformageometricpatterncalledaVoronoi diagram.Thenumberofthesecontiguousregionscannotgrowfasterthanthenumber oftrainingexamples.WhilethisÔ¨Ågureillustratesthebehaviorofthenearestneighbor algorithmspeciÔ¨Åcally,othermachinelearningalgorithmsthatrelyexclusivelyonthe localsmoothnesspriorforgeneralizationexhibitsimilarbehaviors:eachtrainingexample onlyinformsthelearnerabouthowtogeneralizeinsomeneighborhoodimmediately surroundingthatexample. 1 5 9 CHAPTER5.MACHINELEARNINGBASICS entirecheckerboardrightistocovereachofitscellswithatleastoneexample. Thesmoothnessassumptionandtheassociatednon-parametric learningalgo- rithmsworkextremelywellsolongasthereareenoughexamplesforthelearning algorithmtoobservehighpointsonmostpeaksandlowpointsonmostvalleys ofthetrueunderlyingfunctiontobelearned.Thisisgenerallytruewhenthe functiontobelearnedissmoothenoughandvariesinfewenoughdimensions. Inhighdimensions,evenaverysmoothfunctioncanchangesmoothlybutina diÔ¨Äerentwayalongeachdimension.IfthefunctionadditionallybehavesdiÔ¨Äerently indiÔ¨Äerentregions,itcanbecomeextremelycomplicatedtodescribewithasetof trainingexamples.Ifthefunctioniscomplicated(wewanttodistinguishahuge numberofregionscomparedtothenumberofexamples),isthereanyhopeto generalizewell? Theanswertobothofthesequestions‚Äîwhetheritispossibletorepresent acomplicatedfunctioneÔ¨Éciently,andwhetheritispossiblefortheestimated functiontogeneralizewelltonewinputs‚Äîisyes.Thekeyinsightisthatavery largenumberofregions,e.g.,O(2k),canbedeÔ¨ÅnedwithO(k)examples,solong asweintroducesomedependenciesbetweentheregionsviaadditionalassumptions abouttheunderlyingdatageneratingdistribution.Inthisway,wecanactually generalizenon-locally( ,; ,).Many BengioandMonperrus2005Bengioetal.2006c diÔ¨Äerentdeeplearningalgorithmsprovideimplicitorexplicitassumptionsthatare reasonableforabroadrangeofAItasksinordertocapturetheseadvantages. Otherapproachestomachinelearningoftenmakestronger,task-speciÔ¨Åcas- sumptions.Forexample,wecouldeasilysolvethecheckerboardtaskbyproviding theassumptionthatthetargetfunctionisperiodic.Usuallywedonotincludesuch strong,task-speciÔ¨Åcassumptionsintoneuralnetworkssothattheycangeneralize toamuchwidervarietyofstructures.AItaskshavestructurethatismuchtoo complextobelimitedtosimple,manuallyspeciÔ¨Åedpropertiessuchasperiodicity, sowewantlearningalgorithmsthatembodymoregeneral-purpos eassumptions. Thecoreideaindeeplearningisthatweassumethatthedatawasgeneratedby thecompositionoffactorsorfeatures,potentiallyatmultiplelevelsinahierarchy. Manyothersimilarlygenericassumptionscanfurtherimprovedeeplearningal- gorithms. Theseapparentlymildassumptionsallowanexponentialgaininthe relationshipbetweenthenumberofexamplesandthenumberofregionsthatcan bedistinguished.Theseexponentialgainsaredescribedmorepreciselyinsections 6.4.115.415.5,and.Theexponentialadvantagesconferredbytheuseofdeep, distributedrepresentationscountertheexponentialchallengesposedbythecurse ofdimensionality. 1 6 0 CHAPTER5.MACHINELEARNINGBASICS 5.11.3ManifoldLearning Animportantconceptunderlyingmanyideasinmachinelearningisthatofa manifold. Amanifoldisaconnected region. Mathematically , it isasetofpoints, associatedwithaneighborhoodaroundeachpoint.Fromanygivenpoint,the manifoldlocallyappearstobeaEuclideanspace.Ineverydaylife,weexperience thesurfaceoftheworldasa2-Dplane,butitisinfactasphericalmanifoldin 3-Dspace. ThedeÔ¨Ånitionofaneighborhoodsurroundingeachpointimpliestheexistence oftransformationsthatcanbeappliedtomoveonthemanifoldfromoneposition toaneighboringone.Intheexampleoftheworld‚Äôssurfaceasamanifold,onecan walknorth,south,east,orwest. Althoughthereisaformalmathematical meaningtotheterm‚Äúmanifold,‚Äùin machinelearningittendstobeusedmorelooselytodesignateaconnectedset ofpointsthatcanbeapproximatedwellbyconsideringonlyasmallnumberof degreesoffreedom,ordimensions,embeddedinahigher-dimens ionalspace.Each dimensioncorrespondstoalocaldirectionofvariation.SeeÔ¨Ågureforan5.11 exampleoftrainingdatalyingnearaone-dimensional manifoldembeddedintwo- dimensionalspace.Inthecontextofmachinelearning,weallowthedimensionality ofthemanifoldtovaryfromonepointtoanother. This oftenhappenswhena manifoldintersectsitself.Forexample,aÔ¨Ågureeightisamanifoldthathasasingle dimensioninmostplacesbuttwodimensionsattheintersectionatthecenter. 0 5 1 0 1 5 2 0 2 5 3 0 3 5 4 0 . . . . . . . .‚àí 1 0 .‚àí 0 5 .0 0 .0 5 .1 0 .1 5 .2 0 .2 5 . Figure5.11:Datasampledfromadistributioninatwo-dimensionalspacethatisactually concentratednearaone-dimensionalmanifold,likeatwistedstring.Thesolidlineindicates theunderlyingmanifoldthatthelearnershouldinfer. 1 6 1 CHAPTER5.MACHINELEARNINGBASICS Manymachinelearningproblemsseemhopelessifweexpectthemachine learningalgorithmtolearnfunctionswithinterestingvariationsacrossallof Rn. Manifoldlearningalgorithmssurmountthisobstaclebyassumingthatmost of Rnconsistsofinvalidinputs, andthatinterestinginputsoccuronlyalong acollectionofmanifoldscontainingasmallsubsetofpoints,withinteresting variationsintheoutputofthelearnedfunctionoccurringonlyalongdirections thatlieonthemanifold,orwithinterestingvariationshappeningonlywhenwe movefromonemanifoldtoanother.Manifoldlearningwasintroducedinthecase ofcontinuous-valueddataandtheunsupervisedlearningsetting,althoughthis probabilityconcentrationideacanbegeneralizedtobothdiscretedataandthe supervisedlearningsetting:thekeyassumptionremainsthatprobabilitymassis highlyconcentrated. Theassumptionthatthedataliesalongalow-dimensional manifoldmaynot alwaysbecorrectoruseful.WearguethatinthecontextofAItasks,suchas thosethatinvolveprocessingimages,sounds,ortext,themanifoldassumptionis atleastapproximatelycorrect.Theevidenceinfavorofthisassumptionconsists oftwocategoriesofobservations. TheÔ¨Årstobservationinfavorofthemanifoldhypothesisisthattheproba- bilitydistributionoverimages,textstrings,andsoundsthatoccurinreallifeis highlyconcentrated.Uniformnoiseessentiallyneverresemblesstructuredinputs fromthesedomains. Figureshowshow,instead,uniformlysampledpoints 5.12 looklikethepatternsofstaticthatappearonanalogtelevisionsetswhennosignal isavailable.Similarly,ifyougenerateadocumentbypickinglettersuniformlyat random,whatistheprobabilitythatyouwillgetameaningfulEnglish-language text?Almostzero,again,becausemostofthelongsequencesoflettersdonot correspondtoanaturallanguagesequence:thedistributionofnaturallanguage sequencesoccupiesaverysmallvolumeinthetotalspaceofsequencesofletters. 1 6 2 CHAPTER5.MACHINELEARNINGBASICS Figure5.12:Samplingimagesuniformlyatrandom(byrandomlypickingeachpixel accordingtoauniformdistribution)givesrisetonoisyimages.Althoughthereisanon- zeroprobabilitytogenerateanimageofafaceoranyotherobjectfrequentlyencountered inAIapplications,weneveractuallyobservethishappeninginpractice.Thissuggests thattheimagesencounteredinAIapplicationsoccupyanegligibleproportionofthe volumeofimagespace. Ofcourse,concentratedprobabilitydistributionsarenotsuÔ¨Écienttoshow thatthedataliesonareasonablysmallnumberofmanifolds.Wemustalso establishthattheexamplesweencounterareconnectedtoeachotherbyother 1 6 3 CHAPTER5.MACHINELEARNINGBASICS examples,witheachexamplesurroundedbyotherhighlysimilarexamplesthat maybereachedbyapplyingtransformationstotraversethemanifold.Thesecond argumentinfavorofthemanifoldhypothesisisthatwecanalsoimaginesuch neighborhoodsandtransformations,atleastinformally.Inthecaseofimages,we cancertainlythinkofmanypossibletransformationsthatallowustotraceouta manifoldinimagespace:wecangraduallydimorbrightenthelights,gradually moveorrotateobjectsintheimage,graduallyalterthecolorsonthesurfacesof objects,etc.Itremainslikelythattherearemultiplemanifoldsinvolvedinmost applications.Forexample,themanifoldofimagesofhumanfacesmaynotbe connectedtothemanifoldofimagesofcatfaces. Thesethoughtexperimentssupportingthemanifoldhypothesesconveysomein- tuitivereasonssupportingit.Morerigorousexperiments (Cayton2005Narayanan,; andMitter2010Sch√∂lkopf1998RoweisandSaul2000Tenenbaum ,; etal.,; ,; etal., 2000Brand2003BelkinandNiyogi2003DonohoandGrimes2003Weinberger ;,; ,; ,; andSaul2004,)clearlysupportthehypothesisforalargeclassofdatasetsof interestinAI. Whenthedataliesonalow-dimensional manifold,itcanbemostnatural formachinelearningalgorithmstorepresentthedataintermsofcoordinateson themanifold,ratherthanintermsofcoordinatesin Rn.Ineverydaylife,wecan thinkofroadsas1-Dmanifoldsembeddedin3-Dspace.Wegivedirectionsto speciÔ¨Åcaddressesintermsofaddressnumbersalongthese1-Droads,notinterms ofcoordinatesin3-Dspace.Extractingthesemanifoldcoordinatesischallenging, butholdsthepromisetoimprovemanymachinelearningalgorithms.Thisgeneral</div>
        </div>
    </div>

    <div class="question-card" id="q160">
        <div class="question-header">
            <span class="question-number">Question 160</span>
            <span class="question-type">multiple-choice</span>
        </div>

        <div class="question-text">Machine learning models often require large amounts of labeled data to achieve strong performance on tasks like classification and prediction. Recent research has focused on developing algorithms that work well when data is limited or when models must handle many different types of tasks.

Which approach is specifically designed to enable models to rapidly adapt to new tasks with minimal data by learning generalizable patterns across tasks?

1) Regularization   
2) Meta-learning   
3) Data augmentation   
4) Hyperparameter tuning   
5) Batch normalization   
6) Ensemble learning   
7) Gradient clipping</div>

        <div class="answer-section">
            <div class="answer-label">‚úì Correct Answer:</div>
            <div class="answer-text">The correct answer is 2) Meta-learning.</div>
        </div>

        <div class="reference-section">
            <div class="reference-label">üìö Reference Text:</div>
            <button class="toggle-button" onclick="toggleReference(160)">
                Show/Hide Reference
            </button>
            <div id="ref160" class="reference-text hidden">modelthatapproximates somedesiredfunction.Withenoughtrainingdata,this approachisextremelypowerful.Wenowturntopart,inwhichwestepintothe III territoryofresearch‚Äîmethodsthataredesignedtoworkwithlesstrainingdata ortoperformagreatervarietyoftasks,wherethechallengesaremorediÔ¨Écult andnotasclosetobeingsolvedasthesituationswehavedescribedsofar. 4 8 5</div>
        </div>
    </div>

    <div class="question-card" id="q161">
        <div class="question-header">
            <span class="question-number">Question 161</span>
            <span class="question-type">multiple-choice</span>
        </div>

        <div class="question-text">Deep learning models utilize architectural strategies and regularization techniques to improve efficiency, generalization, and robustness. Ensemble methods and sparsity are central themes in advancing predictive performance in neural networks.

Which technique approximates the effect of bagging in large neural networks by randomly omitting units during training, thereby creating an implicit ensemble of subnetworks within a single model?

1) Data augmentation   
2) Batch normalization   
3) Weight decay   
4) Early stopping   
5) Gradient clipping   
6) L1 regularization   
7) Dropout </div>

        <div class="answer-section">
            <div class="answer-label">‚úì Correct Answer:</div>
            <div class="answer-text">The correct answer is 7) Dropout.</div>
        </div>

        <div class="reference-section">
            <div class="reference-label">üìö Reference Text:</div>
            <button class="toggle-button" onclick="toggleReference(161)">
                Show/Hide Reference
            </button>
            <div id="ref161" class="reference-text hidden">ofparametersharingoccursinconvolutionalneuralnetworks(CNNs)applied tocomputervision. Naturalimageshavemanystatisticalpropertiesthatareinvarianttotranslation. Forexample,aphotoofacatremainsaphotoofacatifitistranslatedonepixel totheright.CNNstakethispropertyintoaccountbysharingparametersacross multipleimagelocations.Thesamefeature(ahiddenunitwiththesameweights) iscomputedoverdiÔ¨Äerentlocationsintheinput.ThismeansthatwecanÔ¨Ånda catwiththesamecatdetectorwhetherthecatappearsatcolumn iorcolumn i+1intheimage. ParametersharinghasallowedCNNstodramaticallylowerthenumberofunique modelparametersandtosigniÔ¨Åcantlyincreasenetworksizeswithoutrequiringa correspondingincreaseintrainingdata. Itremainsoneofthebestexamplesof howtoeÔ¨Äectivelyincorporatedomainknowledgeintothenetworkarchitecture. CNNswillbediscussedinmoredetailinchapter.9 7.10SparseRepresentations Weightdecayactsbyplacingapenaltydirectlyonthemodelparameters.Another strategyistoplaceapenaltyontheactivationsoftheunitsinaneuralnetwork, encouragingtheiractivationstobesparse.Thisindirectlyimposesacomplicated penaltyonthemodelparameters. Wehave alreadydiscussed (insection)how7.1.2 L1penalizationinduces asparseparametrization‚Äîmeaning thatmanyoftheparametersbecomezero (orcloseto zero).Representationalsparsity, on theother hand, des cribesa representationwheremanyoftheelementsoftherepresentationarezero(orclose tozero).AsimpliÔ¨Åedviewofthisdistinctioncanbeillustratedinthecontextof linearregression: Ô£Æ Ô£ØÔ£ØÔ£ØÔ£ØÔ£∞18 5 15 ‚àí9 ‚àí3Ô£π Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª=Ô£Æ Ô£ØÔ£ØÔ£ØÔ£ØÔ£∞400 20 0 ‚àí 00 10 3 0 ‚àí 050 0 0 0 100 10 4 ‚àí ‚àí 100 0 50 ‚àíÔ£π Ô£∫Ô£∫Ô£∫Ô£∫Ô£ªÔ£Æ Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞2 3 ‚àí2 ‚àí5 1 4Ô£π Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª y‚àà RmA‚àà Rm n√óx‚àà Rn(7.46) 2 5 4 CHAPTER7.REGULARIZATIONFORDEEPLEARNING Ô£Æ Ô£ØÔ£ØÔ£ØÔ£ØÔ£∞‚àí14 1 19 2 23Ô£π Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª=Ô£Æ Ô£ØÔ£ØÔ£ØÔ£ØÔ£∞3 12 54 1 ‚àí ‚àí 4 2 3 11 3 ‚àí ‚àí ‚àí ‚àí ‚àí 15 4 2 3 2 3 1 2 30 3 ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí 54 22 5 1Ô£π Ô£∫Ô£∫Ô£∫Ô£∫Ô£ªÔ£Æ Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞0 2 0 0 ‚àí3 0Ô£π Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª y‚àà RmB‚àà Rm n√óh‚àà Rn(7.47) IntheÔ¨Årstexpression,wehaveanexampleofasparselyparametrized linear regressionmodel.Inthesecond,wehavelinearregressionwithasparserepresenta- tionhofthedatax.Thatis,hisafunctionofxthat,insomesense,represents theinformationpresentin,butdoessowithasparsevector. x Representationalregularizationisaccomplishedbythesamesortsofmechanisms thatwehaveusedinparameterregularization. Normpenaltyregularizationofrepresentationsisperformedbyaddingtothe lossfunction Janormpenaltyontherepresentation.Thispenaltyisdenoted ‚Ñ¶()h.Asbefore,wedenotetheregularizedlossfunctionbyÀú J: Àú J , J , Œ± (;Œ∏Xy) = (;Œ∏Xy)+‚Ñ¶()h (7.48) where Œ±‚àà[0 ,‚àû)weightstherelativecontributionofthenormpenaltyterm,with largervaluesofcorrespondingtomoreregularization. Œ± Justasan L1penaltyontheparametersinducesparametersparsity,an L1 penaltyontheelementsoftherepresentationinducesrepresentationalsparsity: ‚Ñ¶(h) =||||h 1=ÓÅê i| h i|. Ofcourse,the L1penaltyisonlyonechoiceofpenalty thatcanresultinasparserepresentation.Othersincludethepenaltyderivedfrom aStudent- tpriorontherepresentation( ,;,) OlshausenandField1996Bergstra2011 andKLdivergencepenalties( ,)thatareespecially LarochelleandBengio2008 usefulforrepresentationswithelementsconstrainedtolieontheunitinterval. Lee2008Goodfellow 2009 etal.()and etal.()bothprovideexamplesofstrategies basedonregularizingtheaverageactivationacrossseveralexamples,1 mÓÅê ih( ) i,to benearsometargetvalue,suchasavectorwith.01foreachentry. Otherapproachesobtainrepresentationalsparsitywithahardconstrainton theactivationvalues.Forexample,orthogonalmatchingpursuit(Patietal., 1993)encodesaninputxwiththerepresentationhthatsolvestheconstrained optimization problem argmin h h ,ÓÅ´ÓÅ´ 0 < kÓÅ´‚àí ÓÅ´xWh2, (7.49) where ÓÅ´ÓÅ´h 0isthenumberofnon-zeroentriesofh. Thisproblemcanbesolved eÔ¨ÉcientlywhenWisconstrainedtobeorthogonal.Thismethodisoftencalled 2 5 5 CHAPTER7.REGULARIZATIONFORDEEPLEARNING OMP- kwiththevalueof kspeciÔ¨Åedtoindicatethenumberofnon-zerofeatures allowed. ()demonstratedthatOMP-canbeaveryeÔ¨Äective CoatesandNg2011 1 featureextractorfordeeparchitectures. Essentiallyanymodelthathashiddenunitscanbemadesparse.Throughout thisbook,wewillseemanyexamplesofsparsityregularizationusedinavarietyof contexts. 7.11BaggingandOtherEnsembleMethods Bagging(shortforbootstrapaggregating)isatechniqueforreducinggen- eralizationerrorbycombiningseveralmodels(,).Theideaisto Breiman1994 trainseveraldiÔ¨Äerentmodelsseparately,thenhaveallofthemodelsvoteonthe outputfortestexamples.Thisisanexampleofageneralstrategyinmachine learningcalledmodelaveraging.Techniquesemployingthisstrategyareknown asensemblemethods. ThereasonthatmodelaveragingworksisthatdiÔ¨Äerentmodelswillusually notmakeallthesameerrorsonthetestset. Considerforexampleasetof kregressionmodels.Supposethateachmodel makesanerror ÓÄè ioneachexample, withtheerrorsdrawnfromazero-mean multivariatenormaldistributionwithvariances E[ ÓÄè2 i] = vandcovariances E[ ÓÄè i ÓÄè j] = c. Thentheerrormadebytheaveragepredictionofalltheensemblemodelsis 1 kÓÅê i ÓÄè i.Theexpectedsquarederroroftheensemblepredictoris EÔ£Æ Ô£∞ÓÄ† 1 kÓÅò iÓÄè iÓÄ°2Ô£π Ô£ª=1 k2EÔ£Æ Ô£∞ÓÅò iÔ£´ Ô£≠ ÓÄè2 i+ÓÅò j iÓÄ∂=ÓÄè i ÓÄè jÔ£∂ Ô£∏Ô£π Ô£ª(7.50) =1 kv+k‚àí1 kc . (7.51) Inthecasewheretheerrorsareperfectlycorrelatedand c= v,themeansquared errorreducesto v,sothemodelaveragingdoesnothelpatall.Inthecasewhere theerrorsareperfectlyuncorrelated and c= 0,theexpectedsquarederrorofthe ensembleisonly1 kv.Thismeansthattheexpectedsquarederroroftheensemble decreaseslinearlywiththeensemblesize.Inotherwords,onaverage,theensemble willperformatleastaswellasanyofitsmembers,andifthemembersmake independenterrors,theensemblewillperformsigniÔ¨Åcantlybetterthanitsmembers. DiÔ¨ÄerentensemblemethodsconstructtheensembleofmodelsindiÔ¨Äerentways. Forexample,eachmemberoftheensemblecouldbeformedbytrainingacompletely 2 5 6 CHAPTER7.REGULARIZATIONFORDEEPLEARNING 8 8F i r s t e nse m b l e m e m b e r Se c ond e nse m b l e m e m b e rO r i gi nal data s e t F i r s t r e s am pl e d d a t a s e t Se c ond re s am p l e d d a t a s e t Figure7.5:Acartoondepictionofhowbaggingworks.Supposewetrainan8detectoron thedatasetdepictedabove,containingan8,a6anda9.SupposewemaketwodiÔ¨Äerent resampleddatasets.Thebaggingtrainingprocedureistoconstructeachofthesedatasets bysamplingwithreplacement.TheÔ¨Årstdatasetomitsthe9andrepeatsthe8.Onthis dataset,thedetectorlearnsthataloopontopofthedigitcorrespondstoan8.On theseconddataset,werepeatthe9andomitthe6.Inthiscase,thedetectorlearns thatalooponthebottomofthedigitcorrespondstoan8.Eachoftheseindividual classiÔ¨Åcationrulesisbrittle,butifweaveragetheiroutputthenthedetectorisrobust, achievingmaximalconÔ¨Ådenceonlywhenbothloopsofthe8arepresent. diÔ¨ÄerentkindofmodelusingadiÔ¨Äerentalgorithmorobjectivefunction.Bagging isamethodthatallowsthesamekindofmodel,trainingalgorithmandobjective functiontobereusedseveraltimes. SpeciÔ¨Åcally,bagginginvolvesconstructing kdiÔ¨Äerentdatasets.Eachdataset hasthesamenumberofexamplesastheoriginaldataset,buteachdatasetis constructedbysamplingwithreplacementfromtheoriginaldataset.Thismeans that,withhighprobability,eachdatasetismissingsomeoftheexamplesfromthe originaldatasetandalsocontainsseveralduplicateexamples(onaveragearound 2/3oftheexamplesfromtheoriginaldatasetarefoundintheresultingtraining set,ifithasthesamesizeastheoriginal).Model iisthentrainedondataset i.ThediÔ¨Äerencesbetweenwhichexamplesareincludedineachdatasetresultin diÔ¨Äerencesbetweenthetrainedmodels.SeeÔ¨Ågureforanexample.7.5 Neuralnetworksreachawideenoughvarietyofsolutionpointsthattheycan oftenbeneÔ¨Åtfrommodelaveragingevenifallofthemodelsaretrainedonthesame dataset.DiÔ¨Äerencesinrandominitialization, randomselectionofminibatches, diÔ¨Äerencesinhyperparameters,ordiÔ¨Äerentoutcomesofnon-determinis ticimple- mentationsofneuralnetworksareoftenenoughtocausediÔ¨Äerentmembersofthe 2 5 7 CHAPTER7.REGULARIZATIONFORDEEPLEARNING ensembletomakepartiallyindependenterrors. Modelaveragingisanextremelypowerfulandreliablemethodforreducing generalization error.Itsuseisusuallydiscouragedwhenbenchmarkingalgorithms forscientiÔ¨Åcpapers,becauseanymachinelearningalgorithmcanbeneÔ¨Åtsubstan- tiallyfrommodelaveragingatthepriceofincreasedcomputationandmemory. Forthisreason,benchmarkcomparisonsareusuallymadeusingasinglemodel. Machinelearningcontestsareusuallywonbymethodsusingmodelaverag- ingoverdozensofmodels.ArecentprominentexampleistheNetÔ¨ÇixGrand Prize(Koren2009,). Notalltechniquesforconstructingensemblesaredesignedtomaketheensemble moreregularizedthantheindividualmodels.Forexample,atechniquecalled boosting(FreundandSchapire1996ba,,)constructsanensemblewithhigher capacitythantheindividualmodels.Boostinghasbeenappliedtobuildensembles ofneuralnetworks(SchwenkandBengio1998,)byincrementallyaddingneural networkstotheensemble.Boostinghasalsobeenappliedinterpretinganindividual neuralnetworkasanensemble( ,),incrementallyaddinghidden Bengioetal.2006a unitstotheneuralnetwork. 7.12Dropout Dropout(Srivastava2014etal.,)providesacomputationally inexpensivebut powerfulmethodofregularizingabroadfamilyofmodels.ToaÔ¨Årstapproximation, dropoutcanbethoughtofasamethodofmakingbaggingpracticalforensembles ofverymanylargeneuralnetworks.Bagginginvolvestrainingmultiplemodels, andevaluatingmultiplemodelsoneachtestexample.Thisseemsimpractical wheneachmodelisalargeneuralnetwork,sincetrainingandevaluatingsuch networksiscostlyintermsofruntimeandmemory.Itiscommontouseensembles ofÔ¨Åvetotenneuralnetworks‚Äî ()usedsixtowintheILSVRC‚Äî Szegedy etal.2014a butmorethanthisrapidlybecomesunwieldy.Dropoutprovidesaninexpensive approximationtotrainingandevaluatingabaggedensembleofexponentiallymany neuralnetworks. SpeciÔ¨Åcally,dropouttrainstheensembleconsistingofallsub-networksthat canbeformedbyremovingnon-outputunitsfromanunderlyingbasenetwork, asillustratedinÔ¨Ågure.Inmostmodernneuralnetworks,basedonaseriesof 7.6 aÔ¨Énetransformationsandnonlinearities, wecaneÔ¨Äectivelyremoveaunitfroma networkbymultiplyingitsoutputvaluebyzero. Thisprocedurerequiressome slightmodiÔ¨Åcationformodelssuchasradialbasisfunctionnetworks,whichtake 2 5 8 CHAPTER7.REGULARIZATIONFORDEEPLEARNING thediÔ¨Äerencebetweentheunit‚Äôsstateandsomereferencevalue.Here,wepresent thedropoutalgorithmintermsofmultiplication byzeroforsimplicity,butitcan betriviallymodiÔ¨Åedtoworkwithotheroperationsthatremoveaunitfromthe network. Recallthattolearnwithbagging,wedeÔ¨Åne kdiÔ¨Äerentmodels,construct k diÔ¨Äerentdatasetsbysamplingfromthetrainingsetwithreplacement,andthen trainmodel iondataset i.Dropoutaimstoapproximatethisprocess,butwithan exponentiallylargenumberofneuralnetworks.SpeciÔ¨Åcally,totrainwithdropout, weuseaminibatch-bas edlearningalgorithmthatmakessmallsteps,suchas stochasticgradientdescent.Eachtimeweloadanexampleintoaminibatch,we randomlysampleadiÔ¨Äerentbinarymasktoapplytoalloftheinputandhidden unitsinthenetwork.Themaskforeachunitissampledindependentlyfromallof theothers.Theprobabilityofsamplingamaskvalueofone(causingaunittobe included)isahyperparameter Ô¨Åxedbeforetrainingbegins. Itisnotafunction ofthecurrentvalueofthemodelparametersortheinputexample.Typically, aninputunitisincludedwithprobability0.8andahiddenunitisincludedwith probability0.5.Wethenrunforwardpropagation, back-propagation,andthe learningupdateasusual.Figureillustrateshowtorunforwardpropagation 7.7</div>
        </div>
    </div>

    <div class="question-card" id="q162">
        <div class="question-header">
            <span class="question-number">Question 162</span>
            <span class="question-type">multiple-choice</span>
        </div>

        <div class="question-text">Deep generative modeling involves complex neural architectures and training techniques to learn how to generate realistic data samples. Evaluating and training these models requires specialized procedures to avoid issues like spurious modes or invalid comparisons.

Which training procedure is specifically designed to help denoising autoencoders and Generative Stochastic Networks learn better stationary distributions and avoid spurious modes by performing multiple encode-decode cycles during training?

1) Contrastive Divergence   
2) Importance Sampling   
3) Annealed Importance Sampling (AIS)   
4) Clamping Procedure   
5) Walk-back Training   
6) Approximate Bayesian Computation (ABC)   
7) Inception Score</div>

        <div class="answer-section">
            <div class="answer-label">‚úì Correct Answer:</div>
            <div class="answer-text">The correct answer is 5) Walk-back Training.</div>
        </div>

        <div class="reference-section">
            <div class="reference-label">üìö Reference Text:</div>
            <button class="toggle-button" onclick="toggleReference(162)">
                Show/Hide Reference
            </button>
            <div id="ref162" class="reference-text hidden">f,yielding h= f(Àú x), (c)decodingtheresultwithfunction g,yieldingparameters œâforthereconstruction distribution,and(d)given œâ,samplinganewstatefromthereconstructiondistribution p(x | œâ= g( f(Àú x))).Inthetypicalsquaredreconstructionerrorcase, g( h)=ÀÜ x,which estimates E[ x|Àú x],corruptionconsistsinaddingGaussiannoiseandsamplingfrom p(x| œâ)consistsinaddingGaussiannoise,asecondtime,tothereconstructionÀÜ x.The latternoiselevelshouldcorrespondtothemeansquarederrorofreconstructions,whereas theinjectednoiseisahyperparameterthatcontrolsthemixingspeedaswellasthe extenttowhichtheestimatorsmoothstheempiricaldistribution(,).Inthe Vincent2011 exampleillustratedhere,onlythe Cand pconditionalsarestochasticsteps( fand gare deterministiccomputations),althoughnoisecanalsobeinjectedinsidetheautoencoder, asingenerativestochasticnetworks( ,). Bengio e t a l .2014 7 1 2 CHAPTER20.DEEPGENERATIVEMODELS Bengio2014 e t a l .()showedthatiftheautoencoder p(x |Àúx)formsaconsistent estimatorofthecorrespondingtrueconditionaldistribution,thenthestationary distributionoftheaboveMarkovchainformsaconsistentestimator(albeitan implicitone)ofthedatageneratingdistributionof.x 20.11.2ClampingandConditionalSampling SimilarlytoBoltzmannmachines,denoisingautoencodersandtheirgeneralizations (suchasGSNs,describedbelow)canbeusedtosamplefromaconditionaldistri- bution p(x f|x o),simplybyclampingthe o b s e r v e dunits x fandonlyresampling the f r e eunits x ogivenx fandthesampledlatentvariables(ifany).Forexample, MP-DBMscanbeinterpretedasaformofdenoisingautoencoder,andareable tosamplemissinginputs.GSNslatergeneralizedsomeoftheideaspresentin MP-DBMstoperformthesameoperation( ,). () Bengio e t a l .2014Alain e t a l .2015 identiÔ¨ÅedamissingconditionfromProposition1of (),whichis Bengio e t a l .2014 thatthetransitionoperator(deÔ¨Ånedbythestochasticmappinggoingfromone stateofthechaintothenext)shouldsatisfyapropertycalleddetailedbalance, whichspeciÔ¨ÅesthataMarkovChainatequilibriumwillremaininequilibrium whetherthetransitionoperatorisruninforwardorreverse. Anexperimentinclampinghalfofthepixels(therightpartoftheimage)and runningtheMarkovchainontheotherhalfisshowninÔ¨Ågure.20.12 7 1 3 CHAPTER20.DEEPGENERATIVEMODELS Figure20.12:IllustrationofclampingtherighthalfoftheimageandrunningtheMarkov Chainbyresamplingonlythelefthalfateachstep. ThesesamplescomefromaGSN trainedtoreconstructMNISTdigitsateachtimestepusingthewalkbackprocedure. 20.11.3Walk-BackTrainingProcedure Thewalk-backtrainingprocedurewasproposedby ()asaway Bengio e t a l .2013c toacceleratetheconvergenceofgenerativetrainingofdenoisingautoencoders. Insteadofperformingaone-stepencode-decodereconstruction,thisprocedure consistsinalternativemultiplestochasticencode-decodesteps(asinthegenerative Markovchain)initializedatatrainingexample(justlikewiththecontrastive divergencealgorithm,describedinsection)andpenalizingthelastprobabilistic 18.2 reconstructions(orallofthereconstructionsalongtheway). Trainingwith kstepsisequivalent(inthesenseofachievingthesamestationary distribution)astrainingwithonestep,butpracticallyhastheadvantagethat spuriousmodesfurtherfromthedatacanberemovedmoreeÔ¨Éciently. 20.12GenerativeStochasticNetworks GenerativestochasticnetworksorGSNs( ,)aregeneraliza- Bengio e t a l .2014 tionsofdenoisingautoencodersthatincludelatentvariables hinthegenerative 7 1 4 CHAPTER20.DEEPGENERATIVEMODELS Markovchain,inadditiontothevisiblevariables(usuallydenoted).x AGSNisparametrized bytwoconditionalprobabilitydistributionswhich specifyonestepoftheMarkovchain: 1. p(x( ) k|h( ) k)tellshowtogeneratethenextvisiblevariablegiventhecurrent latentstate.Sucha‚Äúreconstructiondistribution‚Äùisalsofoundindenoising autoencoders,RBMs,DBNsandDBMs. 2. p(h( ) k|h( 1 ) k‚àí,x( 1 ) k‚àí)tellshowtoupdatethelatentstatevariable,given thepreviouslatentstateandvisiblevariable. DenoisingautoencodersandGSNsdiÔ¨Äerfromclassicalprobabilisticmodels (directedorundirected)inthattheyparametrizethegenerativeprocessitselfrather thanthemathematical speciÔ¨Åcationofthejointdistributionofvisibleandlatent variables.Instead,thelatterisdeÔ¨Åned ,,asthestationary i m p l i c i t l y i f i t e x i s t s distributionofthegenerativeMarkovchain.Theconditionsforexistenceofthe stationarydistributionaremildandarethesameconditionsrequiredbystandard MCMCmethods(seesection).Theseconditionsarenecessarytoguarantee 17.3 thatthechainmixes,buttheycanbeviolatedbysomechoicesofthetransition distributions(forexample,iftheyweredeterministic). OnecouldimaginediÔ¨ÄerenttrainingcriteriaforGSNs.Theoneproposedand evaluatedby ()issimplyreconstructionlog-probabilit yonthe Bengio e t a l .2014 visibleunits,justlikefordenoisingautoencoders.Thisisachievedbyclamping x( 0 )= xtotheobservedexampleandmaximizingtheprobabilityofgenerating x atsomesubsequenttimesteps,i.e.,maximizing log p(x( ) k= x|h( ) k),where h( ) k issampledfromthechain,givenx( 0 )= x. Inordertoestimatethegradientof log p(x( ) k= x|h( ) k)withrespecttotheotherpiecesofthemodel,Bengio e t a l . ()usethereparametrization trick,introducedinsection. 2014 20.9 Thewalk-backtrainingprotocol(describedinsection)wasused( 20.11.3 Ben- gio2014 e t a l .,)toimprovetrainingconvergenceofGSNs. 20.12.1DiscriminantGSNs TheoriginalformulationofGSNs( ,)wasmeantforunsupervised Bengio e t a l .2014 learningandimplicitlymodeling p(x)forobserveddatax,butitispossibleto modifytheframeworktooptimize . p( )y| x Forexample,ZhouandTroyanskaya2014()generalizeGSNsinthisway,by onlyback-propagatingthereconstructionlog-probabilit yovertheoutputvariables, keepingtheinputvariablesÔ¨Åxed.Theyappliedthissuccessfullytomodelsequences 7 1 5 CHAPTER20.DEEPGENERATIVEMODELS (proteinsecondarystructure)andintroduceda(one-dimensional) convolutional structureinthetransitionoperatoroftheMarkovchain.Itisimportantto rememberthat,foreachstepoftheMarkovchain,onegeneratesanewsequence foreachlayer,andthatsequenceistheinputforcomputingotherlayervalues(say theonebelowandtheoneabove)atthenexttimestep. HencetheMarkovchainisreallyovertheoutputvariable(andassociatedhigher- levelhiddenlayers),andtheinputsequenceonlyservestoconditionthatchain, withback-propagationallowingtolearnhowtheinputsequencecanconditionthe outputdistributionimplicitlyrepresentedbytheMarkovchain.Itisthereforea caseofusingtheGSNinthecontextofstructuredoutputs. Z√∂hrerandPernkopf2014()introducedahybridmodelthatcombinesasuper- visedobjective(asintheabovework)andanunsupervisedobjective(asinthe originalGSNwork),bysimplyadding(withadiÔ¨Äerentweight)thesupervisedand unsupervisedcostsi.e.,thereconstructionlog-probabilities ofyandxrespectively. SuchahybridcriterionhadpreviouslybeenintroducedforRBMsbyLarochelle andBengio2008().TheyshowimprovedclassiÔ¨Åcationperformanceusingthis scheme. 20.13OtherGenerationSchemes ThemethodswehavedescribedsofaruseeitherMCMCsampling,ancestral sampling,orsomemixtureofthetwotogeneratesamples. Whilethesearethe mostpopularapproachestogenerativemodeling,theyarebynomeanstheonly approaches. Sohl-Dickstein2015 e t a l .()developedadiÔ¨Äusioninversiontrainingscheme forlearningagenerativemodel,basedonnon-equilibrium thermodynamics.The approachisbasedontheideathattheprobabilitydistributionswewishtosample fromhavestructure.ThisstructurecangraduallybedestroyedbyadiÔ¨Äusion processthatincrementally changestheprobabilitydistributiontohave more entropy.Toformagenerativemodel,wecanruntheprocessinreverse,bytraining amodelthatgraduallyrestoresthestructuretoanunstructureddistribution. Byiterativelyapplyingaprocessthatbringsadistributionclosertothetarget one,wecangraduallyapproachthattargetdistribution.Thisapproachresembles MCMCmethodsinthesensethatitinvolvesmanyiterationstoproduceasample. However,themodelisdeÔ¨Ånedtobetheprobabilitydistributionproducedby theÔ¨Ånalstepofthechain. Inthissense,thereisnoapproximation inducedby theiterativeprocedure.Theapproachintroducedby () Sohl-Dickstein e t a l .2015 isalsoveryclosetothegenerativeinterpretation ofthedenoisingautoencoder 7 1 6 CHAPTER20.DEEPGENERATIVEMODELS (section).Aswiththedenoisingautoencoder,diÔ¨Äusioninversiontrainsa 20.11.1 transitionoperatorthatattemptstoprobabilisticallyundotheeÔ¨Äectofadding somenoise.ThediÔ¨ÄerenceisthatdiÔ¨Äusioninversionrequresundoingonlyonestep ofthediÔ¨Äusionprocess,ratherthantravelingallthewaybacktoacleandatapoint. Thisaddressesthefollowingdilemmapresentwiththeordinaryreconstruction log-likelihoodobjectiveofdenoisingautoencoders:withsmalllevelsofnoisethe learneronlyseesconÔ¨Ågurations nearthedatapoints,whilewithlargelevelsof noiseitisaskedtodoanalmostimpossiblejob(becausethedenoisingdistribution ishighlycomplexandmulti-modal). WiththediÔ¨Äusioninversionobjective,the learnercanlearntheshapeofthedensityaroundthedatapointsmoreprecisely aswellasremovespuriousmodesthatcouldshowupfarfromthedatapoints. AnotherapproachtosamplegenerationistheapproximateBayesiancom- putation(ABC)framework(,).Inthisapproach,samplesare Rubin e t a l .1984 rejectedormodiÔ¨Åedinordertomakethemomentsofselectedfunctionsofthe samplesmatchthoseofthedesireddistribution.Whilethisideausesthemoments ofthesampleslikeinmomentmatching,itisdiÔ¨Äerentfrommomentmatching becauseitmodiÔ¨Åesthesamplesthemselves,ratherthantrainingthemodelto automatically emitsampleswiththecorrectmoments. () BachmanandPrecup2015 showedhowtouseideasfromABCinthecontextofdeeplearning,byusingABC toshapetheMCMCtrajectoriesofGSNs. Weexpectthatmanyotherpossibleapproachestogenerativemodelingawait discovery. 20.14EvaluatingGenerativeModels Researchersstudyinggenerativemodelsoftenneedtocompareonegenerative modeltoanother,usuallyinordertodemonstratethatanewlyinventedgenerative modelisbetteratcapturingsomedistributionthanthepre-existingmodels. ThiscanbeadiÔ¨Écultandsubtletask.Inmanycases,wecannotactually evaluatethelogprobabilityofthedataunderthemodel,butonlyanapproximation. Inthesecases,itisimportanttothinkandcommunicateclearlyaboutexactlywhat isbeingmeasured.Forexample,supposewecanevaluateastochasticestimateof thelog-likelihoodformodelA,andadeterministiclowerboundonthelog-likelihood formodelB.IfmodelAgetsahigherscorethanmodelB,whichisbetter?Ifwe careaboutdeterminingwhichmodelhasabetterinternalrepresentationofthe distribution,weactuallycannottell,unlesswehavesomewayofdetermininghow loosetheboundformodelBis.However,ifwecareabouthowwellwecanuse themodelinpractice,forexampletoperformanomalydetection,thenitisfairto 7 1 7 CHAPTER20.DEEPGENERATIVEMODELS saythatamodelispreferablebasedonacriterionspeciÔ¨Åctothepracticaltaskof interest,e.g.,basedonrankingtestexamplesandrankingcriteriasuchasprecision andrecall. Anothersubtletyofevaluatinggenerativemodelsisthattheevaluationmetrics areoftenhardresearchproblemsinandofthemselves.ItcanbeverydiÔ¨Écult toestablishthatmodelsarebeingcomparedfairly.Forexample,supposeweuse AIStoestimate log Zinordertocompute log Àú p( x)‚àílog Zforanewmodelwe havejustinvented.Acomputationally economicalimplementation ofAISmayfail toÔ¨Åndseveralmodesofthemodeldistributionandunderestimate Z,whichwill resultinusoverestimatinglog p( x).ItcanthusbediÔ¨Éculttotellwhetherahigh likelihoodestimateisduetoagoodmodelorabadAISimplementation. OtherÔ¨Åeldsofmachinelearningusuallyallowforsomevariationinthepre- processingofthedata.Forexample,whencomparingtheaccuracyofobject recognitionalgorithms,itisusuallyacceptabletopreprocesstheinputimages slightlydiÔ¨Äerentlyforeachalgorithmbasedonwhatkindofinputrequirements ithas.GenerativemodelingisdiÔ¨Äerentbecausechangesinpreprocessing,even verysmallandsubtleones,arecompletelyunacceptable. Anychangetotheinput datachangesthedistributiontobecapturedandfundamentallyaltersthetask. Forexample,multiplyingtheinputby0.1willartiÔ¨Åciallyincreaselikelihoodbya factorof10. Issueswithpreprocessingcommonlyarisewhenbenchmarkinggenerativemodels ontheMNISTdataset,oneofthemorepopulargenerativemodelingbenchmarks. MNISTconsistsofgrayscaleimages.SomemodelstreatMNISTimagesaspoints inarealvectorspace,whileotherstreatthemasbinary.Yetotherstreatthe grayscalevaluesasprobabilities forabinarysamples.Itisessentialtocompare real-valuedmodelsonlytootherreal-valuedmodelsandbinary-valuedmodelsonly tootherbinary-valuedmodels. Otherwisethelikelihoodsmeasuredarenotonthe samespace.Forbinary-valuedmodels,thelog-likelihoodcanbeatmostzero,while forreal-valuedmodelsitcanbearbitrarilyhigh,sinceitisthemeasurementofa density.Amongbinarymodels,itisimportanttocomparemodelsusingexactly thesamekindofbinarization. Forexample,wemightbinarizeagraypixelto0or1 bythresholdingat0.5,orbydrawingarandomsamplewhoseprobabilityofbeing 1isgivenbythegraypixelintensity.Ifweusetherandombinarization, wemight binarizethewholedatasetonce,orwemightdrawadiÔ¨Äerentrandomexamplefor eachstepoftrainingandthendrawmultiplesamplesforevaluation.Eachofthese threeschemesyieldswildlydiÔ¨Äerentlikelihoodnumbers,andwhencomparing diÔ¨Äerentmodelsitisimportantthatbothmodelsusethesamebinarizationscheme fortrainingandforevaluation.Infact,researcherswhoapplyasinglerandom 7 1 8 CHAPTER20.DEEPGENERATIVEMODELS binarizationstepshareaÔ¨Ålecontainingtheresultsoftherandombinarization, so thatthereisnodiÔ¨ÄerenceinresultsbasedondiÔ¨Äerentoutcomesofthebinarization step. Becausebeingabletogeneraterealisticsamplesfromthedatadistribution isoneofthegoalsofagenerativemodel,practitioners oftenevaluategenerative modelsbyvisuallyinspectingthesamples.Inthebestcase,thisisdonenotbythe researchersthemselves,butbyexperimentalsubjectswhodonotknowthesource ofthesamples(Denton2015 e t a l .,).Unfortunately,itispossibleforaverypoor probabilisticmodeltoproduceverygoodsamples.Acommonpracticetoverifyif themodelonlycopiessomeofthetrainingexamplesisillustratedinÔ¨Ågure.16.1 Theideaistoshowforsomeofthegeneratedsamplestheirnearestneighborin thetrainingset,accordingtoEuclideandistanceinthespaceof x. Thistestis intendedtodetectthecasewherethemodeloverÔ¨Åtsthetrainingsetandjust reproducestraininginstances.ItisevenpossibletosimultaneouslyunderÔ¨Åtand overÔ¨Åtyetstillproducesamplesthatindividuallylookgood.Imagineagenerative modeltrainedonimagesofdogsandcatsthatsimplylearnstoreproducethe trainingimagesofdogs.SuchamodelhasclearlyoverÔ¨Åt,becauseitdoesnot producesimagesthatwerenotinthetrainingset,butithasalsounderÔ¨Åt,because itassignsnoprobabilitytothetrainingimagesofcats.Yetahumanobserver wouldjudgeeachindividualimageofadogtobehighquality.Inthissimple example,itwouldbeeasyforahumanobserverwhocaninspectmanysamplesto determinethatthecatsareabsent.Inmorerealisticsettings,agenerativemodel trainedondatawithtensofthousandsofmodesmayignoreasmallnumberof modes,andahumanobserverwouldnoteasilybeabletoinspectorremember enoughimagestodetectthemissingvariation. Since thevisual</div>
        </div>
    </div>

    <div class="question-card" id="q163">
        <div class="question-header">
            <span class="question-number">Question 163</span>
            <span class="question-type">multiple-choice</span>
        </div>

        <div class="question-text">Deep learning is a specialized approach within artificial intelligence that models data using layered neural networks to extract increasingly abstract representations. Its success has transformed fields like computer vision, speech, and natural language processing by leveraging large datasets and computational resources.

Which hierarchical sequence best describes the progression of feature abstraction in a typical deep neural network processing visual data?

1) Shapes ‚Üí Object parts ‚Üí Edges ‚Üí Whole objects ‚Üí Pixels   
2) Pixels ‚Üí Whole objects ‚Üí Edges ‚Üí Object parts ‚Üí Shapes   
3) Object parts ‚Üí Edges ‚Üí Pixels ‚Üí Whole objects ‚Üí Shapes   
4) Whole objects ‚Üí Edges ‚Üí Pixels ‚Üí Shapes ‚Üí Object parts   
5) Pixels ‚Üí Edges ‚Üí Shapes ‚Üí Object parts ‚Üí Whole objects   
6) Edges ‚Üí Pixels ‚Üí Shapes ‚Üí Whole objects ‚Üí Object parts   
7) Shapes ‚Üí Pixels ‚Üí Edges ‚Üí Object parts ‚Üí Whole objects</div>

        <div class="answer-section">
            <div class="answer-label">‚úì Correct Answer:</div>
            <div class="answer-text">The correct answer is 5) Pixels ‚Üí Edges ‚Üí Shapes ‚Üí Object parts ‚Üí Whole objects.</div>
        </div>

        <div class="reference-section">
            <div class="reference-label">üìö Reference Text:</div>
            <button class="toggle-button" onclick="toggleReference(163)">
                Show/Hide Reference
            </button>
            <div id="ref163" class="reference-text hidden">CHAPTER1.INTRODUCTION Visible layer (input pixels)1st hidden layer (edges)2nd hidden layer (corners and contours)3rd hidden layer (object parts)CARPERSONANIMALOutput (object identity) Figure1.2:Illustrationofadeeplearningmodel.ItisdiÔ¨Écultforacomputertounderstand themeaningofrawsensoryinputdata,suchasthisimagerepresentedasacollection ofpixelvalues.Thefunctionmappingfromasetofpixelstoanobjectidentityisvery complicated.Learningorevaluatingthismappingseemsinsurmountableiftackleddirectly. DeeplearningresolvesthisdiÔ¨Écultybybreakingthedesiredcomplicatedmappingintoa seriesofnestedsimplemappings,eachdescribedbyadiÔ¨Äerentlayerofthemodel.The inputispresentedatthevisiblelayer,sonamedbecauseitcontainsthevariablesthat weareabletoobserve.Thenaseriesofhiddenlayersextractsincreasinglyabstract featuresfromtheimage.Theselayersarecalled‚Äúhidden‚Äùbecausetheirvaluesarenotgiven inthedata;insteadthemodelmustdeterminewhichconceptsareusefulforexplaining therelationshipsintheobserveddata.Theimagesherearevisualizationsofthekind offeaturerepresentedbyeachhiddenunit.Giventhepixels,theÔ¨Årstlayercaneasily identifyedges,bycomparingthebrightnessofneighboringpixels.GiventheÔ¨Årsthidden layer‚Äôsdescriptionoftheedges,thesecondhiddenlayercaneasilysearchforcornersand extendedcontours,whicharerecognizableascollectionsofedges.Giventhesecondhidden layer‚Äôsdescriptionoftheimageintermsofcornersandcontours,thethirdhiddenlayer candetectentirepartsofspeciÔ¨Åcobjects,byÔ¨ÅndingspeciÔ¨Åccollectionsofcontoursand corners.Finally,thisdescriptionoftheimageintermsoftheobjectpartsitcontainscan beusedtorecognizetheobjectspresentintheimage.Imagesreproducedwithpermission fromZeilerandFergus2014(). 6 CHAPTER1.INTRODUCTION x 1 x 1œÉ w 1 w 1√ó x 2 x 2 w 2 w 2√ó+El e me n t S e t + √ó œÉ xx wwEl e me n t S e t L ogi s t i c R e gr e s s i onL ogi s t i c R e gr e s s i on Figure1.3:Illustrationofcomputationalgraphsmappinganinputtoanoutputwhere eachnodeperformsanoperation.Depthisthelengthofthelongestpathfrominputto outputbutdependsonthedeÔ¨Ånitionofwhatconstitutesapossiblecomputationalstep. Thecomputationdepictedinthesegraphsistheoutputofalogisticregressionmodel, œÉ ( wTx ),whereœÉisthelogisticsigmoidfunction.Ifweuseaddition,multiplicationand logisticsigmoidsastheelementsofourcomputerlanguage,thenthismodelhasdepth three.Ifweviewlogisticregressionasanelementitself,thenthismodelhasdepthone. instructionscanreferbacktotheresultsofearlierinstructions.Accordingtothis viewofdeeplearning,notalloftheinformationinalayer‚Äôsactivationsnecessarily encodesfactorsofvariationthatexplaintheinput.Therepresentationalsostores stateinformationthathelpstoexecuteaprogramthatcanmakesenseoftheinput. Thisstateinformationcouldbeanalogoustoacounterorpointerinatraditional computerprogram.IthasnothingtodowiththecontentoftheinputspeciÔ¨Åcally, butithelpsthemodeltoorganizeitsprocessing. Therearetwomainwaysofmeasuringthedepthofamodel.TheÔ¨Årstviewis basedonthenumberofsequentialinstructionsthatmustbeexecutedtoevaluate thearchitecture.Wecanthinkofthisasthelengthofthelongestpaththrough aÔ¨Çowchartthatdescribeshowtocomputeeachofthemodel‚Äôsoutputsgiven itsinputs.JustastwoequivalentcomputerprogramswillhavediÔ¨Äerentlengths dependingonwhichlanguagetheprogramiswrittenin,thesamefunctionmay bedrawnasaÔ¨ÇowchartwithdiÔ¨Äerentdepthsdependingonwhichfunctionswe allowtobeusedasindividualstepsintheÔ¨Çowchart.Figureillustrateshowthis 1.3 choiceoflanguagecangivetwodiÔ¨Äerentmeasurementsforthesamearchitecture. Anotherapproach,usedbydeepprobabilisticmodels,regardsthedepthofa modelasbeingnotthedepthofthecomputational graphbutthedepthofthe graphdescribinghowconceptsarerelatedtoeachother.Inthiscase,thedepth 7 CHAPTER1.INTRODUCTION oftheÔ¨Çowchartofthecomputations neededtocomputetherepresentationof eachconceptmaybemuchdeeperthanthegraphoftheconceptsthemselves. Thisisbecausethesystem‚ÄôsunderstandingofthesimplerconceptscanbereÔ¨Åned giveninformationaboutthemorecomplexconcepts.Forexample,anAIsystem observinganimageofafacewithoneeyeinshadowmayinitiallyonlyseeoneeye. Afterdetectingthatafaceispresent,itcantheninferthatasecondeyeisprobably presentaswell. Inthiscase,thegraphofconceptsonlyincludestwolayers‚Äîa layerforeyesandalayerforfaces‚Äîbutthegraphofcomputations includes 2n layersifwereÔ¨Åneourestimateofeachconceptgiventheothertimes. n Becauseitisnotalwaysclearwhichofthesetwoviews‚Äîthedepthofthe computational graph,orthedepthoftheprobabilisticmodelinggraph‚Äîismost relevant,andbecausediÔ¨ÄerentpeoplechoosediÔ¨Äerentsetsofsmallestelements fromwhichtoconstructtheirgraphs,thereisnosinglecorrectvalueforthe depthofanarchitecture,justasthereisnosinglecorrectvalueforthelengthof acomputerprogram. Nor isthereaconsensusabouthowmuchdepthamodel requirestoqualifyas‚Äúdeep.‚ÄùHowever,deeplearningcansafelyberegardedasthe studyofmodelsthateitherinvolveagreateramountofcompositionoflearned functionsorlearnedconceptsthantraditionalmachinelearningdoes. Tosummarize,deeplearning,thesubjectofthisbook,isanapproachtoAI. SpeciÔ¨Åcally,itisatypeofmachinelearning,atechniquethatallowscomputer systemstoimprovewithexperienceanddata. Accordingtotheauthorsofthis book,machinelearningistheonlyviableapproachtobuildingAIsystemsthat canoperateincomplicated,real-worldenvironments.Deeplearningisaparticular kindofmachinelearningthatachievesgreatpowerandÔ¨Çexibilitybylearningto representtheworldasanestedhierarchyofconcepts,witheachconceptdeÔ¨Ånedin relationtosimplerconcepts,andmoreabstractrepresentationscomputedinterms oflessabstractones.FigureillustratestherelationshipbetweenthesediÔ¨Äerent 1.4 AIdisciplines.Figuregivesahigh-levelschematicofhoweachworks. 1.5 1. 1 Wh o S h ou l d R ead T h i s Bo ok ? Thisbookcanbeusefulforavarietyofreaders,butwewroteitwithtwomain targetaudiencesinmind.Oneofthesetargetaudiencesisuniversitystudents (undergraduate orgraduate)learningaboutmachinelearning,includingthosewho arebeginningacareerindeeplearningandartiÔ¨Åcialintelligenceresearch.The othertargetaudienceissoftwareengineerswhodonothaveamachinelearning orstatisticsbackground, butwanttorapidlyacquireoneandbeginusingdeep learningintheirproductorplatform.Deeplearninghasalreadyprovenusefulin 8 CHAPTER1.INTRODUCTION AIMachine learningRepresentation learningDeep learning Example: Knowledge basesExample: Logistic regressionExample: Shallow autoencoders Example: MLPs Figure1.4:AVenndiagramshowinghowdeeplearningisakindofrepresentationlearning, whichisinturnakindofmachinelearning,whichisusedformanybutnotallapproaches toAI.EachsectionoftheVenndiagramincludesanexampleofanAItechnology. 9 CHAPTER1.INTRODUCTION InputHand- designed programOutput InputHand- designed featuresMapping from featuresOutput InputFeaturesMapping from featuresOutput InputSimple featuresMapping from featuresOutput Additional layers of more abstract features Rule-based systemsClassic machine learning Representation learningDeep learning Figure1.5: FlowchartsshowinghowthediÔ¨ÄerentpartsofanAIsystemrelatetoeach otherwithindiÔ¨ÄerentAIdisciplines.Shadedboxesindicatecomponentsthatareableto learnfromdata. 1 0 CHAPTER1.INTRODUCTION manysoftwaredisciplinesincludingcomputervision,speechandaudioprocessing, naturallanguageprocessing,robotics,bioinformatics andchemistry,videogames, searchengines,onlineadvertisingandÔ¨Ånance. Thisbookhasbeenorganizedintothreepartsinordertobestaccommodatea varietyofreaders.Partintroducesbasicmathematical toolsandmachinelearning I concepts.Partdescribesthemostestablisheddeeplearningalgorithmsthatare II essentiallysolvedtechnologies.Partdescribesmorespeculativeideasthatare III widelybelievedtobeimportantforfutureresearchindeeplearning. Readersshouldfeelfreetoskippartsthatarenotrelevantgiventheirinterests orbackground. Readersfamiliarwithlinearalgebra,probability,andfundamental machinelearningconceptscanskippart,forexample,whilereaderswhojustwant I toimplementaworkingsystemneednotreadbeyondpart.Tohelpchoosewhich II chapterstoread,Ô¨ÅgureprovidesaÔ¨Çowchartshowingthehigh-levelorganization 1.6 ofthebook. Wedoassumethatallreaderscomefromacomputersciencebackground. We assumefamiliaritywithprogramming, abasicunderstandingofcomputational performanceissues,complexitytheory,introductory levelcalculusandsomeofthe terminologyofgraphtheory. 1. 2 Hi s t or i c a l T ren d s i n D eep L earni n g Itiseasiesttounderstanddeeplearningwithsomehistoricalcontext.Ratherthan providingadetailedhistoryofdeeplearning,weidentifyafewkeytrends: ‚Ä¢Deeplearninghashadalongandrichhistory,buthasgonebymanynames reÔ¨ÇectingdiÔ¨Äerentphilosophicalviewpoints,andhaswaxedandwanedin popularity. ‚Ä¢Deeplearninghasbecomemoreusefulastheamountofavailabletraining datahasincreased. ‚Ä¢Deeplearningmodelshavegrowninsizeovertimeascomputerinfrastructure (bothhardwareandsoftware)fordeeplearninghasimproved. ‚Ä¢Deeplearninghassolvedincreasinglycomplicatedapplicationswithincreasing accuracyovertime. 1 1 CHAPTER1.INTRODUCTION 1. Introduction Part I: Applied Math and Machine Learning Basics 2. Linear Algebra3. Probability and Information Theory 4. Numerical Computation5. Machine Learning Basics Part II: Deep Networks: Modern Practices 6. Deep Feedforward Networks 7. Regularization8. Optimization 9. CNNs10. RNNs 11. Practical Methodology12. Applications Part III: Deep Learning Research 13. Linear Factor Models14. Autoencoders15. Representation Learning 16. Structured Probabilistic Models17. Monte Carlo Methods 18. Partition Function19. Inference 20. Deep Generative Models Figure1.6:Thehigh-levelorganizationofthebook.Anarrowfromonechaptertoanother indicatesthattheformerchapterisprerequisitematerialforunderstandingthelatter. 1 2 CHAPTER1.INTRODUCTION 1 . 2 . 1 T h e Ma n y Na m es a n d Ch a n g i n g F o rt u n es o f Neu ra l Net - w o rks Weexpectthatmanyreadersofthisbookhaveheardofdeeplearningasan excitingnewtechnology,andaresurprisedtoseeamentionof‚Äúhistory‚Äùinabook aboutanemergingÔ¨Åeld.Infact,deeplearningdatesbacktothe1940s.Deep learningonly a p p e a r stobenew,becauseitwasrelativelyunpopularforseveral yearsprecedingitscurrentpopularity,andbecauseithasgonethroughmany diÔ¨Äerentnames,andhasonlyrecentlybecomecalled‚Äúdeeplearning.‚ÄùTheÔ¨Åeld hasbeenrebrandedmanytimes,reÔ¨ÇectingtheinÔ¨ÇuenceofdiÔ¨Äerentresearchers anddiÔ¨Äerentperspectives. Acomprehensivehistoryofdeeplearningisbeyondthescopeofthistextbook. However,somebasiccontextisusefulforunderstandingdeeplearning.Broadly speaking,therehavebeenthreewavesofdevelopmentofdeeplearning:deep learning known as c y b e r net i c sin the 1940s‚Äì1960s, deep learning knownas c o nnec t i o n i s minthe1980s‚Äì1990s,andthecurrentresurgenceunderthename deeplearningbeginningin2006.ThisisquantitativelyillustratedinÔ¨Ågure.1.7 Someoftheearliestlearningalgorithmswerecognizetodaywereintended tobecomputational modelsofbiologicallearning,i.e.modelsofhowlearning happensorcouldhappeninthebrain. Asaresult,oneofthenamesthatdeep learninghasgonebyis ar t i Ô¨Åc i al neur al net w</div>
        </div>
    </div>

    <div class="question-card" id="q164">
        <div class="question-header">
            <span class="question-number">Question 164</span>
            <span class="question-type">multiple-choice</span>
        </div>

        <div class="question-text">In machine learning, probabilistic models are used to represent uncertainty and learn hidden structure in data. Deep learning models with latent variables differ significantly from traditional graphical models in terms of scalability, interpretability, and computational approaches.

Which statement most accurately characterizes a key computational trade-off between deep graphical models and traditional graphical models involving latent variables?

1) Deep graphical models use sparse connectivity to enable exact inference algorithms.   
2) Traditional graphical models rely on Monte Carlo sampling for scalable training on large datasets.   
3) Deep models require interpretable latent variables to facilitate human understanding.   
4) Deep graphical models sacrifice exact inference and interpretability for scalability and flexibility, often relying on approximate sampling methods instead of tractable exact algorithms.   
5) Traditional models use dense matrix operations on GPUs to accelerate training of distributed representations.   
6) Deep models always compute true marginal probabilities before performing any optimization.   
7) Traditional models typically contain more latent variables than deep models, necessitating distributed representations.</div>

        <div class="answer-section">
            <div class="answer-label">‚úì Correct Answer:</div>
            <div class="answer-text">The correct answer is 4) Deep graphical models sacrifice exact inference and interpretability for scalability and flexibility, often relying on approximate sampling methods instead of tractable exact algorithms..</div>
        </div>

        <div class="reference-section">
            <div class="reference-label">üìö Reference Text:</div>
            <button class="toggle-button" onclick="toggleReference(164)">
                Show/Hide Reference
            </button>
            <div id="ref164" class="reference-text hidden">CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING usuallynotveryeasyforahumantointerpretafterthefact,thoughvisualization techniquesmayallowsomeroughcharacterization ofwhattheyrepresent.When latentvariablesareusedinthecontextoftraditionalgraphicalmodels,theyare oftendesignedwithsomespeciÔ¨Åcsemanticsinmind‚Äîthetopicofadocument, theintelligenceofastudent,thediseasecausingapatient‚Äôssymptoms,etc.These modelsareoftenmuchmoreinterpretable byhumanpractitioners andoftenhave moretheoreticalguarantees,yetarelessabletoscaletocomplexproblemsandare notreusableinasmanydiÔ¨Äerentcontextsasdeepmodels. AnotherobviousdiÔ¨Äerenceisthekindofconnectivitytypicallyusedinthe deeplearningapproach.Deepgraphicalmodelstypicallyhavelargegroupsofunits thatareallconnectedtoothergroupsofunits,sothattheinteractionsbetween twogroupsmaybedescribedbyasinglematrix.Traditionalgraphicalmodels haveveryfewconnectionsandthechoiceofconnectionsforeachvariablemaybe individuallydesigned.Thedesignofthemodelstructureistightlylinkedwith thechoiceofinferencealgorithm.Traditionalapproachestographicalmodels typicallyaimtomaintainthetractabilityofexactinference.Whenthisconstraint istoolimiting,apopularapproximate inferencealgorithmisanalgorithmcalled loopybeliefpropagation.Bothoftheseapproachesoftenworkwellwithvery sparselyconnectedgraphs.Bycomparison,modelsusedindeeplearningtendto connecteachvisibleunitv itoverymanyhiddenunitsh j,sothathcanprovidea distributedrepresentationofv i(andprobablyseveralotherobservedvariablestoo). Distributedrepresentationshavemanyadvantages,butfromthepointofview ofgraphicalmodelsandcomputational complexity,distributedrepresentations havethedisadvantageofusuallyyieldinggraphsthatarenotsparseenoughfor thetraditionaltechniquesofexactinferenceandloopybeliefpropagationtobe relevant.Asaconsequence,oneofthemoststrikingdiÔ¨Äerencesbetweenthelarger graphicalmodelscommunityandthedeepgraphicalmodelscommunityisthat loopybeliefpropagationisalmostneverusedfordeeplearning.Mostdeepmodels areinsteaddesignedtomakeGibbssamplingorvariationalinferencealgorithms eÔ¨Écient.Anotherconsiderationisthatdeeplearningmodelscontainaverylarge numberoflatentvariables,makingeÔ¨Écientnumericalcodeessential.Thisprovides anadditionalmotivation,besidesthechoiceofhigh-levelinferencealgorithm,for groupingtheunitsintolayerswithamatrixdescribingtheinteractionbetween twolayers.Thisallowstheindividualstepsofthealgorithmtobeimplemented witheÔ¨Écientmatrixproductoperations,orsparselyconnectedgeneralizations ,like blockdiagonalmatrixproductsorconvolutions. Finally,thedeeplearningapproachtographicalmodelingischaracterizedby amarkedtoleranceoftheunknown.Ratherthansimplifyingthemodeluntil allquantitieswemightwantcanbecomputedexactly,weincreasethepowerof 5 8 6 CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING themodeluntilitisjustbarelypossibletotrainoruse.Weoftenusemodels whosemarginaldistributionscannotbecomputed,andaresatisÔ¨Åedsimplytodraw approximatesamplesfromthesemodels.Weoftentrainmodelswithanintractable objectivefunctionthatwecannotevenapproximate inareasonableamountof time,butwearestillabletoapproximately trainthemodelifwecaneÔ¨Éciently obtainanestimateofthegradientofsuchafunction.Thedeeplearningapproach isoftentoÔ¨Ågureoutwhattheminimumamountofinformationweabsolutely needis,andthentoÔ¨Ågureouthowtogetareasonableapproximation ofthat informationasquicklyaspossible. 1 6 . 7 . 1 E xa m p l e: T h e Rest ri ct ed B o l t zm a n n Ma c h i n e TherestrictedBoltzmannmachine(RBM)(,)or Smolensky1986harmonium isthequintessentialexampleofhowgraphicalmodelsareusedfordeeplearning. TheRBMisnotitselfadeepmodel.Instead,ithasasinglelayeroflatentvariables thatmaybeusedtolearnarepresentationfortheinput.Inchapter,wewill20 seehowRBMscanbeusedtobuildmanydeepermodels.Here,weshowhowthe RBMexempliÔ¨Åesmanyofthepracticesusedinawidevarietyofdeepgraphical models: itsunitsareorganizedintolargegroupscalledlayers,theconnectivity betweenlayersisdescribedbyamatrix,theconnectivityisrelativelydense,the modelisdesignedtoalloweÔ¨ÉcientGibbssampling,andtheemphasisofthemodel designisonfreeingthetrainingalgorithmtolearnlatentvariableswhosesemantics werenotspeciÔ¨Åedbythedesigner.Later,insection,wewillrevisittheRBM 20.2 inmoredetail. ThecanonicalRBMisanenergy-basedmodelwithbinaryvisibleandhidden units.Itsenergyfunctionis E ,( v h b ) = ‚àíÓÄæv c‚àíÓÄæh v‚àíÓÄæW h , (16.10) where b, c,and Wareunconstrained,real-valued,learnableparameters.Wecan seethatthemodelisdividedintotwogroupsofunits: vand h,andtheinteraction betweenthemisdescribedbyamatrix W.Themodelisdepictedgraphically inÔ¨Ågure.AsthisÔ¨Åguremakesclear,animportantaspectofthismodelis 16.14 thattherearenodirectinteractionsbetweenanytwovisibleunitsorbetweenany twohiddenunits(hencethe‚Äúrestricted,‚ÄùageneralBoltzmannmachinemayhave arbitraryconnections). TherestrictionsontheRBMstructureyieldtheniceproperties p( ) = Œ† hv| i p(h i|v) (16.11) 5 8 7 CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING h 1 h 1 h 2 h 2 h 3 h 3 v 1 v 1 v 2 v 2 v 3 v 3h 4 h 4 Figure16.14:AnRBMdrawnasaMarkovnetwork. and p( ) = Œ† vh| i p(v i|h) . (16.12) Theindividualconditionalsaresimpletocomputeaswell.ForthebinaryRBM weobtain: P(h i= 1 ) = |v œÉÓÄê vÓÄæW : , i+ b iÓÄë , (16.13) P(h i= 0 ) = 1 |v ‚àí œÉÓÄê vÓÄæW : , i+ b iÓÄë . (16.14) TogetherthesepropertiesallowforeÔ¨ÉcientblockGibbssampling,whichalter- natesbetweensamplingallofhsimultaneouslyandsamplingallofvsimultane- ously.SamplesgeneratedbyGibbssamplingfromanRBMmodelareshownin Ô¨Ågure.16.15 Sincetheenergyfunctionitselfisjustalinearfunctionoftheparameters,itis easytotakeitsderivatives.Forexample, ‚àÇ ‚àÇ W i , jE ,(vh) = ‚àív ih j . (16.15) Thesetwoproperties‚ÄîeÔ¨ÉcientGibbssamplingandeÔ¨Écientderivatives‚Äîmake trainingconvenient.Inchapter,wewillseethatundirectedmodelsmaybe 18 trainedbycomputingsuchderivativesappliedtosamplesfromthemodel. Trainingthemodelinducesarepresentation hofthedata v.Wecanoftenuse E h h‚àº p (| v )[] hasasetoffeaturestodescribe. v Overall,theRBMdemonstratesthetypicaldeeplearningapproachtograph- icalmodels: representationlearningaccomplishedvialayersoflatentvariables, combinedwitheÔ¨Écientinteractionsbetweenlayersparametrized bymatrices. Thelanguageofgraphicalmodelsprovidesanelegant,Ô¨Çexibleandclearlanguage fordescribingprobabilisticmodels.Inthechaptersahead,weusethislanguage, amongotherperspectives,todescribeawidevarietyofdeepprobabilisticmodels. 5 8 8 CHAPTER16.STRUCTUREDPROBABILISTICMODELSFORDEEPLEARNING Figure16.15:SamplesfromatrainedRBM,anditsweights.Imagereproducedwith permissionfrom(). LISA2008 ( L e f t )SamplesfromamodeltrainedonMNIST,drawn usingGibbssampling.EachcolumnisaseparateGibbssamplingprocess.Eachrow representstheoutputofanother1,000stepsofGibbssampling.Successivesamplesare highlycorrelatedwithoneanother.Thecorrespondingweightvectors.Compare ( R i g h t ) thistothesamplesandweightsofalinearfactormodel,showninÔ¨Ågure.Thesamples 13.2 herearemuchbetterbecausetheRBMprior p( h)isnotconstrainedtobefactorial.The RBMcanlearnwhichfeaturesshouldappeartogetherwhensampling.Ontheotherhand, theRBMposterior isfactorial,whilethesparsecodingposterior isnot, p( ) h v| p( ) h v| sothesparsecodingmodelmaybebetterforfeatureextraction.Othermodelsareable tohavebothanon-factorialandanon-factorial. p() h p( ) h v| 5 8 9 C h a p t e r 1 7 Mon t e C arl o Me t h o d s Randomizedalgorithmsfallintotworoughcategories:LasVegasalgorithmsand MonteCarloalgorithms.LasVegasalgorithmsalwaysreturnpreciselythecorrect answer(orreportthattheyfailed).Thesealgorithmsconsumearandomamount ofresources,usuallymemoryortime.Incontrast,MonteCarloalgorithmsreturn answerswitharandomamountoferror.Theamountoferrorcantypicallybe reducedbyexpendingmoreresources(usuallyrunningtimeandmemory).Forany Ô¨Åxedcomputational budget,aMonteCarloalgorithmcanprovideanapproximate answer. ManyproblemsinmachinelearningaresodiÔ¨Écultthatwecanneverexpectto obtainpreciseanswerstothem.Thisexcludesprecisedeterministicalgorithmsand LasVegasalgorithms.Instead,wemustusedeterministicapproximatealgorithms orMonteCarloapproximations.Bothapproachesareubiquitousinmachine learning.Inthischapter,wefocusonMonteCarlomethods. 17.1SamplingandMonteCarloMethods Manyimportanttechnologiesusedtoaccomplishmachinelearninggoalsarebased ondrawingsamplesfromsomeprobabilitydistributionandusingthesesamplesto formaMonteCarloestimateofsomedesiredquantity. 1 7 . 1 . 1 Wh y S a m p l i n g ? Therearemanyreasonsthatwemaywishtodrawsamplesfromaprobability distribution.SamplingprovidesaÔ¨Çexiblewaytoapproximatemanysumsand 590 CHAPTER17.MONTECARLOMETHODS integralsatreducedcost.SometimesweusethistoprovideasigniÔ¨Åcantspeedupto acostlybuttractablesum,asinthecasewhenwesubsamplethefulltrainingcost withminibatches.Inothercases,ourlearningalgorithmrequiresustoapproximate anintractablesumorintegral,suchasthegradientofthelogpartitionfunctionof anundirectedmodel.Inmanyothercases,samplingisactuallyourgoal,inthe sensethatwewanttotrainamodelthatcansamplefromthetrainingdistribution. 1 7 . 1 . 2 B a s i cs o f Mo n t e Ca rl o S a m p l i n g Whenasumoranintegralcannotbecomputedexactly(forexamplethesum hasanexponentialnumberoftermsandnoexactsimpliÔ¨Åcationisknown)itis oftenpossibletoapproximate itusingMonteCarlosampling.Theideaistoview thesumorintegralasifitwasanexpectationundersomedistributionandto a p p r o x i m a t e t h e e x p e c t a t i o n b y a c o r r e s p o nding a v e r a g e.Let s=ÓÅò xp f E () x() = x p[()] f x (17.1) or s=ÓÅö p f d E () x() x x= p[()] f x (17.2) bethesumorintegraltoestimate,rewrittenasanexpectation,withtheconstraint that pisaprobabilitydistribution(forthesum)oraprobabilitydensity(forthe integral)overrandomvariable. x Wecanapproximate sbydrawing nsamples</div>
        </div>
    </div>

    <div class="question-card" id="q165">
        <div class="question-header">
            <span class="question-number">Question 165</span>
            <span class="question-type">multiple-choice</span>
        </div>

        <div class="question-text">Energy-based models such as Restricted Boltzmann Machines (RBMs) are trained using algorithms that rely on Markov Chain Monte Carlo (MCMC) sampling to approximate gradients of the log-likelihood. The design and initialization of these Markov chains strongly influence both training efficiency and the ability to suppress spurious modes in the learned distribution.

Which training method for energy-based models is specifically designed to improve mixing and suppress spurious modes by maintaining Markov chains across updates, rather than reinitializing them from data points at each step?

1) Gibbs Sampling with random restarts   
2) Layer-wise Contrastive Divergence   
3) Fast Persistent Contrastive Divergence with momentum   
4) Stochastic Maximum Likelihood (Persistent Contrastive Divergence)   
5) Variational Autoencoding   
6) Contrastive Divergence with multiple negative steps   
7) Deterministic Annealing</div>

        <div class="answer-section">
            <div class="answer-label">‚úì Correct Answer:</div>
            <div class="answer-text">The correct answer is 4) Stochastic Maximum Likelihood (Persistent Contrastive Divergence).</div>
        </div>

        <div class="reference-section">
            <div class="reference-label">üìö Reference Text:</div>
            <button class="toggle-button" onclick="toggleReference(165)">
                Show/Hide Reference
            </button>
            <div id="ref165" class="reference-text hidden">availableinthedataset.Initially,thedatadistributionisnotclosetothemodel distribution,sothenegativephaseisnotveryaccurate.Fortunately,thepositive phasecanstillaccuratelyincreasethemodel‚Äôsprobabilityofthedata.Afterthe positivephasehashadsometimetoact,themodeldistributionisclosertothe datadistribution,andthenegativephasestartstobecomeaccurate. Al g o r i t hm 1 8 . 2Thecontrastivedivergencealgorithm,usinggradientascentas theoptimization procedure. Set,thestepsize,toasmallpositivenumber. ÓÄè Set k,thenumberofGibbssteps,highenoughtoallowaMarkovchainsampling from p(x; Œ∏)tomixwheninitializedfrom pdata.Perhaps1-20totrainanRBM onasmallimagepatch. whi l enotconverged do Sampleaminibatchofexamples m {x( 1 ), . . . ,x( ) m}fromthetrainingset. g‚Üê1 mÓÅêm i = 1‚àá Œ∏log Àú p(x( ) i;) Œ∏ . f o r do i m = 1to Àúx( ) i‚Üêx( ) i. e nd f o r f o r do i k = 1to f o r do j m = 1to Àúx( ) j‚Üêgibbs_update(Àúx( ) j) . e nd f o r e nd f o r gg‚Üê‚àí1 mÓÅêm i = 1‚àá Œ∏log Àú p(Àúx( ) i;) Œ∏ . Œ∏ Œ∏‚Üê + ÓÄè .g e nd whi l e Ofcourse,CDisstillanapproximation tothecorrectnegativephase.The mainwaythatCDqualitativelyfailstoimplementthecorrectnegativephase isthatitfailstosuppressregionsofhighprobabilitythatarefarfromactual trainingexamples.Theseregionsthathavehighprobabilityunderthemodelbut lowprobabilityunderthedatageneratingdistributionarecalled spur i o us m o des. Figureillustrateswhythishappens.Essentially,itisbecausemodesinthe 18.2 modeldistributionthatarefarfromthedatadistributionwillnotbevisitedby 610 CHAPTER18.CONFRONTINGTHEPARTITIONFUNCTION xp(x )p mo d e l ( ) x p d a t a ( ) x Figure18.2: Anillustrationofhowthenegativephaseofcontrastivedivergence(algo- rithm)canfailtosuppressspuriousmodes.Aspuriousmodeisamodethatis 18.2 presentinthemodeldistributionbutabsentinthedatadistribution.Becausecontrastive divergenceinitializesitsMarkovchainsfromdatapointsandrunstheMarkovchainfor onlyafewsteps,itisunlikelytovisitmodesinthemodelthatarefarfromthedata points.Thismeansthatwhensamplingfromthemodel,wewillsometimesgetsamples thatdonotresemblethedata.Italsomeansthatduetowastingsomeofitsprobability massonthesemodes,themodelwillstruggletoplacehighprobabilitymassonthecorrect modes.Forthepurposeofvisualization,thisÔ¨ÅgureusesasomewhatsimpliÔ¨Åedconcept ofdistance‚Äîthespuriousmodeisfarfromthecorrectmodealongthenumberlinein R.ThiscorrespondstoaMarkovchainbasedonmakinglocalmoveswithasingle x variablein R.Formostdeepprobabilisticmodels,theMarkovchainsarebasedonGibbs samplingandcanmakenon-localmovesofindividualvariablesbutcannotmoveallof thevariablessimultaneously.Fortheseproblems,itisusuallybettertoconsidertheedit distancebetweenmodes,ratherthantheEuclideandistance.However,editdistanceina highdimensionalspaceisdiÔ¨Éculttodepictina2-Dplot. Markovchainsinitializedattrainingpoints,unlessisverylarge. k Carreira-Perpi√±anandHinton2005()showed experimentallythatthe CD estimatorisbiasedforRBMsandfullyvisibleBoltzmannmachines,inthatit convergestodiÔ¨Äerentpointsthanthemaximumlikelihoodestimator.Theyargue thatbecausethebiasissmall,CDcouldbeusedasaninexpensivewaytoinitialize amodelthatcouldlaterbeÔ¨Åne-tunedviamoreexpensiveMCMCmethods.Bengio andDelalleau2009()showedthatCDcanbeinterpretedasdiscardingthesmallest termsofthecorrectMCMCupdategradient,whichexplainsthebias. CDisusefulfortrainingshallowmodelslikeRBMs.Thesecaninturnbe stackedtoinitializedeepermodelslikeDBNsorDBMs. However,CDdoesnot providemuchhelpfortrainingdeepermodelsdirectly.ThisisbecauseitisdiÔ¨Écult 611 CHAPTER18.CONFRONTINGTHEPARTITIONFUNCTION toobtainsamplesofthehiddenunitsgivensamplesofthevisibleunits.Sincethe hiddenunitsarenotincludedinthedata,initializingfromtrainingpointscannot solvetheproblem.Evenifweinitializethevisibleunitsfromthedata,wewillstill needtoburninaMarkovchainsamplingfromthedistributionoverthehidden unitsconditionedonthosevisiblesamples. TheCDalgorithmcanbethoughtofaspenalizingthemodelforhavinga Markovchainthatchangestheinputrapidlywhentheinputcomesfromthedata. ThismeanstrainingwithCDsomewhatresemblesautoencodertraining.Even thoughCDismorebiasedthansomeoftheothertrainingmethods,itcanbe usefulforpretrainingshallowmodelsthatwilllaterbestacked.Thisisbecause theearliestmodelsinthestackareencouragedtocopymoreinformationupto theirlatentvariables,therebymakingitavailabletothelatermodels.Thisshould bethoughtofmoreofasanoften-exploitable sideeÔ¨ÄectofCDtrainingratherthan aprincipleddesignadvantage. SutskeverandTieleman2010()showedthattheCDupdatedirectionisnotthe gradientofanyfunction.ThisallowsforsituationswhereCDcouldcycleforever, butinpracticethisisnotaseriousproblem. AdiÔ¨ÄerentstrategythatresolvesmanyoftheproblemswithCDistoinitial- izetheMarkovchainsateachgradientstepwiththeirstatesfromtheprevious gradientstep.ThisapproachwasÔ¨Årstdiscoveredunderthename st o c hast i c m ax - i m um l i k e l i ho o d(SML)intheappliedmathematics andstatisticscommunity (Younes1998,)andlaterindependently rediscoveredunderthename p e r si st e n t c o n t r ast i v e di v e r g e n c e(PCD,orPCD- ktoindicatetheuseof kGibbssteps perupdate)inthedeeplearningcommunity(,).Seealgorithm . Tieleman2008 18.3 Thebasicideaofthisapproachisthat,solongasthestepstakenbythestochastic gradientalgorithmaresmall,thenthemodelfromthepreviousstepwillbesimilar tothemodelfromthecurrentstep.Itfollowsthatthesamplesfromtheprevious model‚Äôsdistributionwillbeveryclosetobeingfairsamplesfromthecurrent model‚Äôsdistribution,soaMarkovchaininitializedwiththesesampleswillnot requiremuchtimetomix. BecauseeachMarkovchainiscontinuallyupdatedthroughoutthelearning process,ratherthanrestartedateachgradientstep,thechainsarefreetowander farenoughtoÔ¨Åndallofthemodel‚Äôsmodes.SMListhusconsiderablymore resistanttoformingmodelswithspuriousmodesthanCDis.Moreover,because itispossibletostorethestateofallofthesampledvariables,whethervisibleor latent,SMLprovidesaninitialization pointforboththehiddenandvisibleunits. CDisonlyabletoprovideaninitialization forthevisibleunits,andtherefore requiresburn-infordeepmodels.SMLisabletotraindeepmodelseÔ¨Éciently. 612 CHAPTER18.CONFRONTINGTHEPARTITIONFUNCTION Marlin 2010 e t a l .()comparedSMLtomanyoftheothercriteriapresentedin thischapter.TheyfoundthatSMLresultsinthebesttestsetlog-likelihoodfor anRBM,andthatiftheRBM‚ÄôshiddenunitsareusedasfeaturesforanSVM classiÔ¨Åer,SMLresultsinthebestclassiÔ¨Åcationaccuracy. SMLisvulnerabletobecominginaccurateifthestochasticgradientalgorithm canmovethemodelfasterthantheMarkovchaincanmixbetweensteps.This canhappenif kistoosmallor ÓÄèistoolarge.Thepermissiblerangeofvaluesis unfortunately highlyproblem-dependent.Thereisnoknownwaytotestformally whetherthechainissuccessfullymixingbetweensteps.Subjectively,ifthelearning rateistoohighforthenumberofGibbssteps,thehumanoperatorwillbeable toobservethatthereismuchmorevarianceinthenegativephasesamplesacross gradientstepsratherthanacrossdiÔ¨ÄerentMarkovchains.Forexample,amodel trainedonMNISTmightsampleexclusively7sononestep.Thelearningprocess willthenpushdownstronglyonthemodecorrespondingto7s,andthemodel mightsampleexclusively9sonthenextstep. Al g o r i t hm 1 8 . 3Thestochasticmaximumlikelihood/persistentcontrastive divergencealgorithmusinggradientascentastheoptimization procedure. Set,thestepsize,toasmallpositivenumber. ÓÄè Set k,thenumberofGibbssteps,highenoughtoallowaMarkovchainsampling from p(x; Œ∏+ ÓÄèg)toburnin,startingfromsamplesfrom p(x; Œ∏).Perhaps1for RBMonasmallimagepatch,or5-50foramorecomplicatedmodellikeaDBM. Initializeasetof msamples {Àúx( 1 ), . . . ,Àúx( ) m}torandomvalues(e.g.,froma uniformornormaldistribution,orpossiblyadistributionwithmarginalsmatched tothemodel‚Äôsmarginals). whi l enotconverged do Sampleaminibatchofexamples m {x( 1 ), . . . ,x( ) m}fromthetrainingset. g‚Üê1 mÓÅêm i = 1‚àá Œ∏log Àú p(x( ) i;) Œ∏ . f o r do i k = 1to f o r do j m = 1to Àúx( ) j‚Üêgibbs_update(Àúx( ) j) . e nd f o r e nd f o r gg‚Üê‚àí1 mÓÅêm i = 1‚àá Œ∏log Àú p(Àúx( ) i;) Œ∏ . Œ∏ Œ∏‚Üê + ÓÄè .g e nd whi l e Caremustbetakenwhenevaluatingthesamplesfromamodeltrainedwith SML.ItisnecessarytodrawthesamplesstartingfromafreshMarkovchain 613 CHAPTER18.CONFRONTINGTHEPARTITIONFUNCTION initializedfromarandomstartingpointafterthemodelisdonetraining.The samplespresentinthepersistentnegativechainsusedfortraininghavebeen inÔ¨Çuencedbyseveralrecentversionsofthemodel,andthuscanmakethemodel appeartohavegreatercapacitythanitactuallydoes. BerglundandRaiko2013()performedexperimentstoexaminethebiasand varianceintheestimateofthegradientprovidedbyCDandSML.CDprovesto havelowervariancethantheestimatorbasedonexactsampling.SMLhashigher variance.ThecauseofCD‚Äôslowvarianceisitsuseofthesametrainingpoints inboththepositiveandnegativephase.Ifthenegativephaseisinitializedfrom diÔ¨Äerenttrainingpoints,thevariancerisesabovethatoftheestimatorbasedon exactsampling. AllofthesemethodsbasedonusingMCMCtodrawsamplesfromthemodel caninprinciplebeusedwithalmostanyvariantofMCMC.Thismeansthat techniquessuchasSMLcanbeimprovedbyusinganyoftheenhancedMCMC techniquesdescribedinchapter,suchasparalleltempering( , 17 Desjardins e t a l . 2010Cho2010; e t a l .,). Oneapproachtoacceleratingmixingduringlearningreliesnotonchanging theMonteCarlosamplingtechnologybutratheronchangingtheparametrization ofthemodelandthecostfunction. F ast P CDorFPCD( , TielemanandHinton 2009)involvesreplacingtheparameters Œ∏ofatraditionalmodelwithanexpression Œ∏ Œ∏= ( )slow+ Œ∏( )fast. (18.16) Therearenowtwiceasmanyparametersasbefore,andtheyareaddedtogether element-wisetoprovidetheparametersusedbytheoriginalmodeldeÔ¨Ånition.The fastcopyoftheparametersistrainedwithamuchlargerlearningrate,allowing ittoadaptrapidlyinresponsetothenegativephaseoflearningandpushthe Markovchaintonewterritory.ThisforcestheMarkovchaintomixrapidly,though thiseÔ¨Äectonlyoccursduringlearningwhilethefastweightsarefreetochange. TypicallyonealsoappliessigniÔ¨Åcantweightdecaytothefastweights,encouraging themtoconvergetosmallvalues,afteronlytransientlytakingonlargevalueslong enoughtoencouragetheMarkovchaintochangemodes. OnekeybeneÔ¨ÅttotheMCMC-basedmethodsdescribedinthissectionisthat theyprovideanestimateofthegradientoflog Z,andthuswecanessentially decomposetheproblemintothelog Àú</div>
        </div>
    </div>

    <script>
        function toggleReference(num) {
            const ref = document.getElementById('ref' + num);
            ref.classList.toggle('hidden');
        }

        function scrollToTop() {
            window.scrollTo({ top: 0, behavior: 'smooth' });
        }

        // Add scroll to top button when scrolling
        window.addEventListener('scroll', function() {
            const btn = document.getElementById('topBtn');
            if (btn && window.pageYOffset > 300) {
                btn.style.display = 'block';
            } else if (btn) {
                btn.style.display = 'none';
            }
        });
    </script>

    <button id="topBtn" class="nav-button" onclick="scrollToTop()"
            style="position: fixed; bottom: 20px; right: 20px; display: none; z-index: 999;">
        ‚Üë Top
    </button>
</body>
</html>
